,BugId,Project,FixHashId,BUG,BIC,Comments,title,description
312,1242662,neutron,7d44629c2e8003f5bbb87e9b697eec54b4cce793,1,1,Exception does not exist,exceptions.Error does not exist,"def add_extension(self, ext):
        # Do nothing if the extension doesn't check out
        if not self._check_extension(ext):
            return
        alias = ext.get_alias()
        LOG.info(_('Loaded extension: %s'), alias)
        if alias in self.extensions:
            raise exceptions.Error(_(""Found duplicate extension: %s"") %
                                   alias)
        self.extensions[alias] = ext"
313,1242819,nova,8c985874c7885f31871204d3f83ce547fefc5fb6,1,1,Optimize the code,Remove unnecessary steps for cold snapshots,"When creating cold snapshots we first stop the instance, create a snapshot of that instance, extract the snapshot to a file, delete the snapshot and bring the instance back up.
If the instance is stopped, then there's no need to create a snapshot because there's no concurrent writer, so the snapshot can be extracted directly and save us from the two unnecessary steps (creation and deletion of a snapshot)."
314,1242933,neutron,cecd7591533e2c046aedba3b8e5d14a5b2fa7fe9,0,0,Add a feature. This allows instances to do SLAAC configuration,Security Groups does not allow RA packets in by de...,"Provisioning a VM using Neutron, with RA's being broadcast by upstream switches, the VM was not getting the packets.
Changing the rules manually to:
ip6tables -I neutron-openvswi-ie90990dd-0 1 -p ipv6-icmp -j ACCEPT
Allowed RA packets through."
315,1242942,cinder,5be4620ae5bb50c8436de0e11269c85a095ed40b,1,1, does not handle missing volume group gracefully,cinder does not handle missing volume group gracef...,"Tested with Havana rc2 from the UCA on Precise.
If the cinder-volumes (in a default configuration) LVM volume group does not exist, cinder will try to create a volume, which is bound to fail. The volume then gets stuck in the ""creating"" state and can't be deleted. The log will contain:
2013-10-17 09:29:58.188 16676 ERROR cinder.brick.local_dev.lvm [req-5c03777b-4acc-4784-b463-278dee0d2e08 None None] Unable to locate Volume Group cinder-volumes
2013-10-17 09:29:58.189 16676 ERROR cinder.volume.manager [req-5c03777b-4acc-4784-b463-278dee0d2e08 None None] Error encountered during initialization of driver: LVMISCSIDriver
2013-10-17 09:29:58.189 16676 ERROR cinder.volume.manager [req-5c03777b-4acc-4784-b463-278dee0d2e08 None None] Bad or unexpected response from the storage volume backend API: Volume Group cinder-volumes does not exist
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager Traceback (most recent call last):
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager   File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 190, in init_host
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager     self.driver.check_for_setup_error()
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager   File ""/usr/lib/python2.7/dist-packages/cinder/volume/drivers/lvm.py"", line 94, in check_for_setup_error
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager     raise exception.VolumeBackendAPIException(data=message)
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Volume Group cinder-volumes does not exist
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager
2013-10-17 09:30:47.198 16676 WARNING cinder.volume.manager [req-d5f8a463-c097-4518-829b-504bf02763b2 None None] Unable to update stats, driver is uninitialized
Resetting the state with ""cinder reset-state"" will get the volume to the ""available"" state, which it isn't. Deleting or force-deleting will also fail, getting stuck in ""deleting"" state forever. The only solution I found was to directly kill the relevant rows in the volumes table and cinder-manage db sync.
I know in grizzly, cinder-volume would refuse to start if it couldn't find the volume-group. I thought that behavior was better. Failing when attempting to create the volume, instead of getting stuck in the creating state, would also be acceptable."
316,1242980,cinder,b104d1c0c70898b59b6668970e05cebadecd064a,1,1,,volume transfer url is different from its href in ...,"After creating a volume transfer, I got a detail information of volume transfer like this:
{
    ""transfer"": {
        ""auth_key"": ""cfcb4552f7c1f2df"",
        ""links"": [
            {
                ""href"": ""http://192.168.83.241:8776/v2/c7d8da28afff47189ff1d651992b593b/transfers/dfedf224-1aba-420a-a387-3b971447a28f"",
                ""rel"": ""self""
            },
            {
                ""href"": ""http://192.168.83.241:8776/c7d8da28afff47189ff1d651992b593b/transfers/dfedf224-1aba-420a-a387-3b971447a28f"",
                ""rel"": ""bookmark""
            }
        ],
        ""created_at"": ""2013-10-22T01:18:35.627545"",
        ""volume_id"": ""f636037c-1aa0-4533-aef2-bcbd7ab3896c"",
        ""id"": ""dfedf224-1aba-420a-a387-3b971447a28f"",
        ""name"": null
    }
}
But I cannot describe its detail information with this href of this retrun.
        ""links"": [
            {
                ""href"": ""http://192.168.83.241:8776/v2/c7d8da28afff47189ff1d651992b593b/transfers/dfedf224-1aba-420a-a387-3b971447a28f"",
                ""rel"": ""self""
            },
The correct url is:
""http://192.168.83.241:8776/v2/c7d8da28afff47189ff1d651992b593b/os-volume-transfer/dfedf224-1aba-420a-a387-3b971447a28f""
Then I found that the define of collection_name are defferent between /cinder/api/views/transfers.py and /cinder/api/contrib/volume_transfer.py.
So I think this is a bug."
317,1243037,nova,01e1c027754b47239b9f9a6c40196512ec262986,1,1,Forgot to check float numbers that are not allowed,parameter checking about quota update api is incor...,"In this API, parameters are checked whether they are integer or not.
But when pass float values to this API, error doesn't occur.
Float values should be forbidden."
318,1243062,nova,65df7336994b03b12aecf9bcdafcfb0d839815f9,1,1,Wrong description,wrong description when updating quotas,"when updating quota for a resource, with 'force_update=false', an exception will be raised when the new value is less than the used quota values(used and reserved), but the exception information is not correct, which should be fixed."
319,1243073,neutron,9a64271bd9f741b13f6cf4a9074524d3854fdd14,1,1,,Mistake in usage drop_constraint parameters,"In miration 63afba73813_ovs_tunnelendpoints_id_unique mistake in usage drop_constraint parameters tablename and type instead of table_name and type_.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/63afba73813_ovs_tunnelendpoints_id_unique.py"", line 63, in downgrade
    type='unique'
TypeError: drop_constraint() takes at least 2 arguments (1 given)"
320,1243083,nova,0fcbb7b286888d642d329db1299eb9f3708d1f50,1,1,,lxc container rootfs is attached to more than one ...,"Operation details are as following:
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------+------------+-------------+------------------+
| ID                                   | Name | Status  | Task State | Power State | Networks         |
+--------------------------------------+------+---------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | ACTIVE  | None       | Running     | private=10.0.0.2 |
+--------------------------------------+------+---------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
ubuntu@lxc-gq:~$ nova reboot lxc1 --hard
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+-------------+----------------+-------------+------------------+
| ID                                   | Name | Status      | Task State     | Power State | Networks         |
+--------------------------------------+------+-------------+----------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | HARD_REBOOT | rebooting_hard | Running     | private=10.0.0.2 |
+--------------------------------------+------+-------------+----------------+-------------+------------------+
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------+------------+-------------+------------------+
| ID                                   | Name | Status  | Task State | Power State | Networks         |
+--------------------------------------+------+---------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | ACTIVE  | None       | Running     | private=10.0.0.2 |
+--------------------------------------+------+---------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop0: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
ubuntu@lxc-gq:~$ nova resize lxc1 m1.lxc
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------------+------------+-------------+------------------+
| ID                                   | Name | Status        | Task State | Power State | Networks         |
+--------------------------------------+------+---------------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | VERIFY_RESIZE | None       | Running     | private=10.0.0.2 |
+--------------------------------------+------+---------------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop0: [fd01]:136419 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9_resize/disk)
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
ubuntu@lxc-gq:~$ nova resize-confirm lxc1
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------+------------+-------------+------------------+
| ID                                   | Name | Status  | Task State | Power State | Networks         |
+--------------------------------------+------+---------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | ACTIVE  | None       | Running     | private=10.0.0.2 |
+--------------------------------------+------+---------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ vir list
Id    Name                           State
----------------------------------------------------
9312  instance-00000052              running
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop0: [fd01]:136419 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9_resize/disk (deleted))
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)"
321,1243101,nova,21045a58497818f76f880fe74b3eb5ab09088cf3,1,0,architecture,nova docker driver cannot find cgroup in /proc/mou...,"I'm using the nova docker driver on RHEL 3.10.11-1.el6.x86_64 (rebuilt kernel). Based on the format of /proc/mounts on RHEL, the cgroup devices path cannot be found.
On my box the line in /proc/mounts for cgroups looks like this:
none /sys/fs/cgroup cgroup rw,relatime,perf_event,blkio,net_cls,freezer,devices,memory,cpuacct,cpu,cpuset,clone_children 0 0
In the docker driver the method which searches /proc/mounts for the cgroup path looks like this:
    def _find_cgroup_devices_path(self):
        for ln in open('/proc/mounts'):
            if ln.startswith('cgroup ') and 'devices' in ln:
                return ln.split(' ')[1]
Therefore the method cannot find my cgroup path. I hacked around this with a 1 LOC change to the _find_cgroup_devices_path method which looks for 'cgroup' as the 3rd item in the line split:
if ln.split(' ')[2] == 'cgroup' and 'devices' in ln:
The update method in its entirety looks like:
169     def _find_cgroup_devices_path(self):
170         for ln in open('/proc/mounts'):
171             if ln.split(' ')[2] == 'cgroup' and 'devices' in ln:
172                 return ln.split(' ')[1]
Based on the format in /proc/mounts on my ubuntu box, this change *should* work on ubuntu as well as rhel.
I did read that docker is only supported with devstack + unbuntu, so I realize this defect may get deferred or even closed. However I wanted to surface it as I believe future efforts of openstack + docker will need to consider non-ubuntu + devstack envs."
322,1243108,nova,aab3c6178571456ea3f2523f401d0a18c04e573c,1,1,Bug in a fixing bug,Image device should be reset after mounting and te...,"When fixing bug#1208387, Image device info is returned in action setup_container.
But in that patch(https://review.openstack.org/#/c/41891/), I missed to reset the image device value in mount() and teardown().
I'm sorry to make such easy mistake, a new patch will be commited soon."
323,1243129,neutron,a1de76d1407952572cfe081c4872d7a6127995b3,0,0,this should be prohibited,[LBaaS] Deletion of associated-to-pool(s) health m...,"Version
=======
Havana on rhel
Description
===========
It's possible to delete health monitor while they are associated to one or more pools, I think that this should be prohibited, and dissociation from pool should be required first."
324,1243148,neutron,258aad077279b2020b47f4136747c1e5aa4bf5a8,1,1,,Mistake in usage drop_table in downgrade in migrat...,"In method downgrade_cisco in migration folsom_initial instead of local method drop_tables op.drop_tables is used
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 468, in downgrade
    downgrade_cisco()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 541, in downgrade_cisco
    op.drop_tables(
AttributeError: 'module' object has no attribute 'drop_tables'"
325,1243193,nova,9ceee09594cd35c264cab3e3b3d1d80aaa3fdbc4,1,1,,Bug #1243193 “VMware,"When a volume is attached to an instance and we backup the instance or try to create an image from it, the volume's disk is being backed up and not the instance's primary disk.
More info: https://communities.vmware.com/community/vmtn/openstack/blog/2013/08/28/introducing-vova-an-easy-way-to-try-out-openstack-on-vsphere#comment-29775"
326,1243213,neutron,355ff10f6e5e4111f14c79b1bdfc48854c81f8b3,1,0,postgre specific,Enums are not deleted after dropping tables in som...,"For downgrade with postgres it is not enough to just drop_table if table contains enums. They should be removed separately after
deleting the table.
This problem takes place in migrations like 52ff27f7567a_support_for_vpnaas, 39cf3f799352_fwaas_havana_2_model, 569e98a8132b_metering, f489cf14a79c_lbaas_havana.
Also if some migration use the same enums in postgres work with
them should be organized differently. In migration 3c6e57a23db4_add_multiprovider if does not use create_type=False downgrade fails  with this error http://paste.openstack.org/show/84486/"
327,1243301,nova,38018d32f9db5b9222436d0f860244ce38ba97fb,1,1,"The status should be deleted, but was error",Changes-since not returning deleted servers,"Based on the OpenStack documentation at http://docs.openstack.org/api/openstack-compute/2/content/ChangesSince.html...""To allow clients to keep track of changes, the changes-since filter displays items that have been recently deleted. Both images and servers contain a DELETED status that indicates that the resource has been removed.""  This allows OpenStack consumers to determine when images and servers are deleted when using the changes-since support.  OpenStack works as documented for images.  However, changes-since support in OpenStack Havana may not return deleted servers with a DELETED status.
The recreate scenario is as follows:
1) Boot a new server and wait for the boot to complete.
2) Save the current time
3) Use changes-since to get all servers changed since the time saved in step 2.  It should be none.
4) Delete the server created in step 1 and wait for the delete to complete.
5) Use changes-since to get all servers changed since the time saved in step 2.  It should include the server deleted in step 4 and the server's status should be DELETED.  The actual result is that the server is included but its status is ERROR.
For the above recreate scenario, the boot failed because no valid host was found.  As a result, the server is in ERROR status after step 1.  I have not tested with a successful boot."
328,1243485,cinder,4adf35778b3aae3db99a90a1bbb94e668ec7963d,1,1,Exceptions wasnt handled,Volume or Snapshot not found ERROR in c-api log af...,"Happens a log, for example in http://logs.openstack.org/81/52181/8/check/check-tempest-devstack-vm-postgres-full/aa99a8c/logs/screen-c-api.txt.gz
2013-10-23 01:20:17.784 22210 ERROR cinder.api.middleware.fault [req-fec0ce89-321d-4cac-9a13-11edb6270993 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] Caught error: Snapshot 7e7b9afe-d872-4ae0-b8d7-42b326dbb978 could not be found.
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/middleware/fault.py"", line 77, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return req.get_response(self.application)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     application, catch_exc_info=False)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     app_iter = application(self.environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 571, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return self.app(env, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     response = self.app(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     resp = self.call_func(req, *args, **self.kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return self.func(req, *args, **kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 827, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     content_type, body, accept)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 875, in _process_stack
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     action_result = self.dispatch(meth, request, action_args)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 951, in dispatch
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return method(req=request, **action_args)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/v1/volumes.py"", line 383, in create
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/volume/api.py"", line 330, in get_snapshot
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     rv = self.db.snapshot_get(context, snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/api.py"", line 267, in snapshot_get
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return IMPL.snapshot_get(context, snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1480, in snapshot_get
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return _snapshot_get(context, snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1473, in _snapshot_get
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     raise exception.SnapshotNotFound(snapshot_id=snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault SnapshotNotFound: Snapshot 7e7b9afe-d872-4ae0-b8d7-42b326dbb978 could not be found.
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault
2013-10-23 01:20:17.786 22210 INFO cinder.api.middleware.fault [req-fec0ce89-321d-4cac-9a13-11edb6270993 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] http://127.0.0.1:8776/v1/6240d8b99d584ebd87a89b63e39f3a0b/volumes returned with HTTP 404
2013-10-23 01:20:17.797 22210 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:558
2013-10-23 01:20:17.797 22210 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:617
2013-10-23 01:20:17.798 22210 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token ba5f0f940d4cc2f687d5825436378d55 _cache_get /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:1016
2013-10-23 01:20:17.798 22210 DEBUG keystoneclient.middleware.auth_token [-] Received request from user: bcf3db5a242f42d9b2e05d2adf9e4b13 with project_id : 6240d8b99d584ebd87a89b63e39f3a0b and roles: _member_  _build_user_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:922
2013-10-23 01:20:17.799 22210 DEBUG routes.middleware [-] Matched POST /6240d8b99d584ebd87a89b63e39f3a0b/volumes __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-10-23 01:20:17.799 22210 DEBUG routes.middleware [-] Route path: '/{project_id}/volumes', defaults: {'action': u'create', 'controller': <cinder.api.openstack.wsgi.Resource object at 0x4809910>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-10-23 01:20:17.799 22210 DEBUG routes.middleware [-] Match dict: {'action': u'create', 'controller': <cinder.api.openstack.wsgi.Resource object at 0x4809910>, 'project_id': u'6240d8b99d584ebd87a89b63e39f3a0b'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-10-23 01:20:17.799 22210 INFO cinder.api.openstack.wsgi [req-2de5058a-06d4-468f-be2a-e50928757e25 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] POST http://127.0.0.1:8776/v1/6240d8b99d584ebd87a89b63e39f3a0b/volumes
2013-10-23 01:20:17.800 22210 DEBUG cinder.api.v1.volumes [req-2de5058a-06d4-468f-be2a-e50928757e25 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] Create volume request body: {'volume': {'scheduler_hints': {}, 'metadata': {u'Type': u'work'}, 'display_name': u'Volume--tempest-572743380', 'source_volid': u'e4974f44-969d-4cef-944d-2fd9b2b78458', 'size': u'1'}} create /opt/stack/new/cinder/cinder/api/v1/volumes.py:358
2013-10-23 01:20:17.813 22210 WARNING cinder.quota [req-1f98a6d8-005d-4976-a9ee-68c9a0ff4c63 fcd199daea8940918ded6d1a9dd8862a 487a496cac7f4b639ebc6bcb85ce87bf] Deprecated: Default quota for resource: gigabytes is set by the default quota flag: quota_gigabytes, it is now deprecated. Please use the the default quota class for default quota.
2013-10-23 01:20:17.813 22210 WARNING cinder.quota [req-1f98a6d8-005d-4976-a9ee-68c9a0ff4c63 fcd199daea8940918ded6d1a9dd8862a 487a496cac7f4b639ebc6bcb85ce87bf] Deprecated: Default quota for resource: volumes is set by the default quota flag: quota_volumes, it is now deprecated. Please use the the default quota class for default quota.
2013-10-23 01:20:17.817 22210 ERROR cinder.api.middleware.fault [req-2de5058a-06d4-468f-be2a-e50928757e25 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] Caught error: Volume e4974f44-969d-4cef-944d-2fd9b2b78458 could not be found.
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/middleware/fault.py"", line 77, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return req.get_response(self.application)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     application, catch_exc_info=False)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     app_iter = application(self.environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 571, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return self.app(env, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     response = self.app(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     resp = self.call_func(req, *args, **self.kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return self.func(req, *args, **kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 827, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     content_type, body, accept)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 875, in _process_stack
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     action_result = self.dispatch(meth, request, action_args)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 951, in dispatch
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return method(req=request, **action_args)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/v1/volumes.py"", line 390, in create
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     source_volid)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/volume/api.py"", line 335, in get_volume
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     rv = self.db.volume_get(context, volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/api.py"", line 213, in volume_get
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return IMPL.volume_get(context, volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1149, in volume_get
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return _volume_get(context, volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1142, in _volume_get
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     raise exception.VolumeNotFound(volume_id=volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault VolumeNotFound: Volume e4974f44-969d-4cef-944d-2fd9b2b78458 could not be found.
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault
2013-10-23 01:23:50.139 22210 ERROR cinder.api.middleware.fault [req-3c68c3cd-7647-4717-a5be-5e1a0bffcab9 c562188b400e44a3b9d3900495ecf748 5dada9a2750641eab97cc64931d55961] Caught error: Snapshot 6095dd59-1e8a-46d5-bfee-4d2dccbf7f91 could not be found.
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/middleware/fault.py"", line 77, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return req.get_response(self.application)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     application, catch_exc_info=False)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     app_iter = application(self.environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 571, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return self.app(env, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     response = self.app(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     resp = self.call_func(req, *args, **self.kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return self.func(req, *args, **kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 827, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     content_type, body, accept)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 875, in _process_stack
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     action_result = self.dispatch(meth, request, action_args)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 951, in dispatch
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return method(req=request, **action_args)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/v1/volumes.py"", line 383, in create
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/volume/api.py"", line 330, in get_snapshot
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     rv = self.db.snapshot_get(context, snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/api.py"", line 267, in snapshot_get
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return IMPL.snapshot_get(context, snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1480, in snapshot_get
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return _snapshot_get(context, snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1473, in _snapshot_get
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     raise exception.SnapshotNotFound(snapshot_id=snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault SnapshotNotFound: Snapshot 6095dd59-1e8a-46d5-bfee-4d2dccbf7f91 could not be found.
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault
2013-10-23 01:23:50.140 22210 INFO cinder.api.middleware.fault [req-3c68c3cd-7647-4717-a5be-5e1a0bffcab9 c562188b400e44a3b9d3900495ecf748 5dada9a2750641eab97cc64931d55961] http://127.0.0.1:8776/v1/5dada9a2750641eab97cc64931d55961/volumes returned with HTTP 404"
329,1243488,nova,0a84a7fb24a4605f0da863407512612651890003,1,1,,unplugging vif ERROR in n-cpu log after successful...,"Happens a lot. For example:  http://logs.openstack.org/50/51750/8/check/check-tempest-devstack-vm-neutron/7b73c73/logs/screen-n-cpu.txt.gz
2013-10-22 20:29:59.161 3294 ERROR nova.virt.libvirt.driver [-] [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] During wait destroy, instance disappeared.
2013-10-22 20:29:59.163 DEBUG nova.virt.libvirt.vif [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] vif_type=ovs instance={'vm_state': u'building', 'availability_zone': None, 'terminated_at': None, 'ephemeral_gb': 0, 'instance_type_id': 6, 'user_data': None, 'cleaned': False, 'vm_mode': None, 'deleted_at': None, 'reservation_id': u'r-5n6tip35', 'id': 18, 'disable_terminate': False, 'display_name': u'Server 88a265da-046a-4fc4-b812-59164e7e35c0', 'uuid': '88a265da-046a-4fc4-b812-59164e7e35c0', 'default_swap_device': None, 'hostname': u'server-88a265da-046a-4fc4-b812-59164e7e35c0', 'launched_on': u'devstack-precise-hpcloud-az3-598823', 'display_description': u'', 'key_data': None, 'kernel_id': u'1b2d8a22-2d48-4240-9987-e27de5999746', 'power_state': 0, 'default_ephemeral_device': None, 'progress': 0, 'project_id': u'd66f874bdc1e49a2adedb607234b2f99', 'launched_at': None, 'config_drive': u'', 'node': u'devstack-precise-hpcloud-az3-598823', 'ramdisk_id': u'8243eabc-098b-44cc-8629-73102741d145', 'access_ip_v6': None, 'access_ip_v4': None, 'deleted': False, 'key_name': None, 'updated_at': datetime.datetime(2013, 10, 22, 20, 29, 57, tzinfo=<iso8601.iso8601.Utc object at 0x2b0cf50>), 'host': u'devstack-precise-hpcloud-az3-598823', 'architecture': u'x86_64', 'user_id': u'ebc6b389f1044bed9ebfdc9ef111d268', 'system_metadata': {u'image_architecture': u'x86_64', u'instance_type_memory_mb': u'64', u'instance_type_swap': u'0', u'instance_type_vcpu_weight': None, u'instance_type_root_gb': u'0', u'instance_type_id': u'6', u'image_image_state': u'available', u'instance_type_name': u'm1.nano', u'image_image_location': u's3bucket--tempest-1023546658/cirros-0.3.1-x86_64-blank.img.manifest.xml', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'image_disk_format': u'ami', u'instance_type_flavorid': u'42', u'image_container_format': u'ami', u'instance_type_vcpus': u'1', u'image_min_ram': u'0', u'image_min_disk': u'0', u'image_base_image_ref': u'7979e34a-8f16-4d0b-8d6f-4b521d832aee'}, 'task_state': u'deleting', 'shutdown_terminate': False, 'cell_name': None, 'root_gb': 0, 'locked': False, 'name': 'instance-00000012', 'created_at': datetime.datetime(2013, 10, 22, 20, 29, 55, tzinfo=<iso8601.iso8601.Utc object at 0x2b0cf50>), 'locked_by': None, 'launch_index': 0, 'metadata': {}, 'memory_mb': 64, 'vcpus': 1, 'image_ref': u'7979e34a-8f16-4d0b-8d6f-4b521d832aee', 'root_device_name': u'/dev/vda', 'auto_disk_config': False, 'os_type': None, 'scheduled_at': datetime.datetime(2013, 10, 22, 20, 29, 55, tzinfo=<iso8601.iso8601.Utc object at 0x2b0cf50>)} vif=VIF({'ovs_interfaceid': u'725ac9ef-5f13-4eb3-b00b-0bbf1aad3809', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'10.1.0.5'})], 'version': 4, 'meta': {'dhcp_server': u'10.1.0.3'}, 'dns': [], 'routes': [], 'cidr': u'10.1.0.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'10.1.0.1'})})], 'meta': {'injected': False, 'tenant_id': u'd66f874bdc1e49a2adedb607234b2f99'}, 'id': u'2b5cb11b-edf9-4e26-a068-bd820c95e106', 'label': u'private'}), 'devname': u'tap725ac9ef-5f', 'qbh_params': None, 'meta': {}, 'address': u'fa:16:3e:31:dd:da', 'type': u'ovs', 'id': u'725ac9ef-5f13-4eb3-b00b-0bbf1aad3809', 'qbg_params': None}) unplug /opt/stack/new/nova/nova/virt/libvirt/vif.py:755
2013-10-22 20:29:59.164 DEBUG nova.openstack.common.processutils [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl delif qbr725ac9ef-5f qvb725ac9ef-5f execute /opt/stack/new/nova/nova/openstack/common/processutils.py:147
2013-10-22 20:29:59.178 DEBUG nova.openstack.common.processutils [req-7e492c69-7a37-438e-9918-f3932e88c617 demo demo] Result was 1 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:172
2013-10-22 20:29:59.179 DEBUG nova.openstack.common.processutils [req-7e492c69-7a37-438e-9918-f3932e88c617 demo demo] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link show dev qvod0a9091e-91 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:147
2013-10-22 20:29:59.301 DEBUG nova.openstack.common.processutils [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] Result was 1 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:172
2013-10-22 20:29:59.302 ERROR nova.virt.libvirt.vif [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Failed while unplugging vif
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Traceback (most recent call last):
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]   File ""/opt/stack/new/nova/nova/virt/libvirt/vif.py"", line 636, in unplug_ovs_hybrid
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]     utils.execute('brctl', 'delif', br_name, v1_name, run_as_root=True)
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]   File ""/opt/stack/new/nova/nova/utils.py"", line 174, in execute
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]     return processutils.execute(*cmd, **kwargs)
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]   File ""/opt/stack/new/nova/nova/openstack/common/processutils.py"", line 178, in execute
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]     cmd=' '.join(cmd))
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] ProcessExecutionError: Unexpected error while running command.
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Command: sudo nova-rootwrap /etc/nova/rootwrap.conf brctl delif qbr725ac9ef-5f qvb725ac9ef-5f
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Exit code: 1
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Stdout: ''
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Stderr: 'interface qvb725ac9ef-5f does not exist!\n'
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]
2"
330,1243613,cinder,ce889d99f61aecd0b4002d44866bbfff8ff6a2c3,1,1,Typo in code,Volume-transfer Creation doesn't support 'name' sp...,"Volume-transfer creation provides to specify the 'name' param in cinder.
I tested it on json format, everything's ok.
But when I create it on xml format, the 'name' param is not transmitted to Cinder.
--------------
Here is input-body of my test:
    <transfer name=""transfer6"" volume_id=""8d37404a-4d60-466c-a8b8-78501db208da""/>
Here's the result, you can found the 'name' is 'None' in response.
<?xml version='1.0' encoding='UTF-8'?>
<transfer
    xmlns:os-volume-transfer=""http://docs.openstack.org/volume/ext/volume-transfer/api/v1.1"" auth_key=""9266c59563c84774"" created_at=""2013-10-23 07:59:07.231575"" id=""7e7d872b-791a-40a6-887f-9b718b4e04b0"" name=""None"" volume_id=""8d37404a-4d60-466c-a8b8-78501db208da""/>
--------------
The reason is due to the inconsistent processing between xml and json.
The CreateDeserializer of xml-format uses 'display_name' param, not 'name':
        --> attributes = ['volume_id', 'display_name']
But the processing of create() in volume-transfer, uses 'name' not 'display_name':
        --> name = transfer.get('name', None)
Therefore, no matter the 'name' is specified, its value in response will also be 'None'."
331,1243652,neutron,b9ddfa566c6caad2384c791d1ae1873ce70c261e,1,0,Bug in postgre,Enums do not have names in some migration,"Enums must contain name parameter for PostgreSQL
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/c88b6b5fea3_cisco_n1kv_tables.py"", line 90, in upgrade
    sa.PrimaryKeyConstraint('tenant_id', 'profile_id')
  File ""<string>"", line 7, in create_table
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 631, in create_table
    self._table(name, *columns, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/ddl/impl.py"", line 148, in create_table
    _ddl_runner=self)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/event.py"", line 389, in __call__
    fn(*args, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 293, in __call__
    return getattr(self.target, self.name)(*arg, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/types.py"", line 1835, in _on_table_create
    t._on_table_create(target, bind, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 576, in _on_table_create
    self.create(bind=bind, checkfirst=checkfirst)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 527, in create
    bind.execute(CreateEnumType(self))
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1536, in _execute_ddl
    compiled = ddl.compile(dialect=dialect)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/sql/expression.py"", line 1778, in compile
    return self._compiler(dialect, bind=bind, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 2927, in _compiler
    return dialect.ddl_compiler(dialect, self, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 705, in __init__
    self.string = self.process(self.statement)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 724, in process
    return obj._compiler_dispatch(self, **kwargs)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py"", line 72, in _compiler_dispatch
    return getter(visitor)(self, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 757, in visit_create_enum_type
    self.preparer.format_type(type_),
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 898, in format_type
    raise exc.CompileError(""Postgresql ENUM type requires a name."")
CompileError: Postgresql ENUM type requires a name."
332,1243704,glance,c0e1d5bd2944a011c0cbaba61d284aedec086b60,0,0,it becomes easier to log the error message – feature,Log image_id with all BadStoreURI error messages,"In the following migration scripts, not all BadStoreUri error messages are logged with image_id. Hence it might become difficult to track the record which has got BadStoreUri.  Including the image_id with the error log messages would help in tracking the faulty records easily when BadStoreUri exception is raised.
015_quote_swift_credentials.py
017_quote_encrypted_swift_credentials.py"
333,1243840,cinder,587fd9021982e3b1fb0c5765df6bad2723701031,1,0,Bug: Error message when it is just could be a warning. Also done because: ‘This is to comply with the desire of the QA team’,flaky log ERRORs in c-vol after successful tempest...,"These showed up 9 times out of 156 runs and are of several varieties. Here is a sample of each:
https://review.openstack.org/#/c/51966/7
http://logs.openstack.org/66/51966/7/check/check-tempest-devstack-vm-postgres-full/a884365
2013-10-22 21:06:08.354 | Log File: c-vol
2013-10-22 21:06:08.355 | 2013-10-22 20:57:49.993 24174 ERROR cinder.brick.iscsi.iscsi [req-077902c7-a5e6-414a-b241-2f429766a569 c6dbf14b4d794430bb3b20395dad67d7 44b8cb996d984ea0aba77a30fa9531d0] Failed to create iscsi target for volume id:volume-52749038-fe54-4f93-9be1-50c480df4b6a: Unexpected error while running command.
https://review.openstack.org/#/c/47712/4
http://logs.openstack.org/12/47712/4/check/check-tempest-devstack-vm-neutron/843f8e8
2013-10-22 23:35:40.881 | Log File: c-vol
2013-10-22 23:35:40.882 | 2013-10-22 23:29:57.004 6308 ERROR cinder.brick.local_dev.lvm [req-4e6c087e-c345-4072-9c18-0f39861680d9 75f6a750c42745ca9440629f79dbaa18 21a7f4942ba7490d8198acf0510fe4fe] Error reported running lvremove: CMD: sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvremove -f stack-volumes/volume-472b303e-7d40-4956-859e-e91506947641, RESPONSE:   /dev/dm-1: stat failed: No such file or directory
https://review.openstack.org/#/c/53255/1
http://logs.openstack.org/55/53255/1/check/check-tempest-devstack-vm-neutron-pg/2319aee
2013-10-23 00:38:37.188 | Log File: c-vol
2013-10-23 00:38:37.188 | 2013-10-23 00:32:36.192 4619 ERROR cinder.brick.local_dev.lvm [req-025aa642-42ac-4a14-8b5c-0165c6c2c268 6105fe9f840049fba93305e06d8cab2c c085cabe4d8b46a19b35708f4ec1c21a] Error reported running lvremove: CMD: sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvremove -f stack-volumes/volume-f0845e0f-2fbb-4d68-879d-ea21a10d152a, RESPONSE:   Can't remove open logical volume ""volume-f0845e0f-2fbb-4d68-879d-ea21a10d152a"""
334,1243849,nova,661fa0c83309f002bbbdd67d81f3e6fbb6bde829,1,0,New decision,flaky “Getting disk size,"This happened in 14 out of 156 runs. Here is a sample:
https://review.openstack.org/#/c/51751/9
http://logs.openstack.org/51/51751/9/check/check-tempest-devstack-vm-postgres-full/ab64cc3
2013-10-22 20:48:20.036 | 2013-10-22 20:41:24.129 21399 ERROR nova.virt.libvirt.driver [-] Getting disk size of instance-00000069: [Errno 2] No such file or directory: '/opt/stack/data/nova/instances/cd1428aa-fa8a-43d9-8180-888e832c35c2/disk'"
335,1243878,neutron,50da460869661ba63971a17c9c1b514bf22874a2,1,1,,Bug #1243878 “metadata service returns Content-Type,"When making an http request against the OpenStack metadata service, it returns ""Content-Type: text/html"", despite the returned content being in json format.
For example (using httpie):
$ http get http://169.254.169.254/openstack/latest/meta_data.json
HTTP/1.1 200 OK
Content-Length: 1283
Content-Type: text/html; charset=UTF-8
Date: Wed, 23 Oct 2013 18:55:17 GMT
{""random_seed"": ... }
The returned content type should really be something like:
Content-Type: application/json; charset=UTF-8
Version: grizzly"
336,1244018,nova,b9b55935cb61cc1726525b1c4963e352a3a81ea9,1,1,,update security group raise HttpError500 exception...,"1.Set the item ""security_group_api=nova"" in nova.conf
2.Restart nova
3.Create a security group
4.Update the security group
   PUT http://192.168.83.241:8774/v2/99a7b3d4bd6540aaaceae89ac74bfab6/os-security-groups/7
   {
    ""security_group"": {
        ""name"": ""huangtianhua"",
        ""description"":""for test""
        }
   }
5.The server raises exception as bellow:
   {
    ""computeFault"": {
        ""message"": ""The server has either erred or is incapable of performing the requested operation."",
        ""code"": 500
       }
   }
6.I think it's a bug.When traversal the rules of the group before returning throws error:
   ""DetachedInstanceError: Parent instance &lt;SecurityGroup at 0x789eed0&gt; is not bound to a Session; lazy load operation of        attribute 'rules' cannot proceed."""
337,1244092,glance,4ba5cbc48f471c39826aa38244f86a2a6252b2e9,1,1,,db connection retrying doesn't work against db2,"When I start Openstack following below steps, Openstack services can't be started without db2 connection:
1, start openstack services;
2, start db2 service.
I checked codes in session.py under nova/openstack/common/db/sqlalchemy, the root cause is db2 connection error code ""-30081"" isn't in conn_err_codes in _is_db_connection_error function, connection retrying codes are skipped against db2, in order to enable connection retrying function against db2, we need add db2 support in _is_db_connection_error function"
338,1244203,nova,73ee7346675ffb38276583ac7b941970f7037181,0,0,The code shows that add a feature,Evacuate operation should honor the enable_instanc...,"Currently the Evacuate operation (enabled by the os-evacuate) extension always returns a generated (or user supplied) password, although not all hypervisors support password injection.
For server create and rebuild and rescue operations the configuration option ""enable_instance_password=False"" can be used to suppress returning a meaningless and confusing password, and the evacuate operation should also honor this setting"
339,1244220,nova,a528302a248f4128c3440665f38b964a3593c3f5,1,0,Evolution,Consoleauth check_token is broken with rpcapi v2,"After adding the 2.0 rpc API in consoleauth manager (See [1]), the method check_token in the new proxy class for v2 lacks the return statement and causes all calls to verify a token from other components to fail.
[1] https://review.openstack.org/#/c/51731/"
340,1244238,cinder,bfb66019edd197141ea1462ba78dd27a2ed0e40d,1,1,,Bug #1244238 “GlusterFS,"I am working with gluster as cinder's backend on 2 computes for Havana.
when I try to boot an instance from a cloned volume (cinder create 10 --source-volid f7416ba6-af45-47d3-a333-478447a1ab54 --display-name from_vol1) we get a rootwrap error in volumes log and the instance moves to status ERROR.
more info that might be helpful, I tried booting an instance from a newly created volume and a volume created from image and instance is started correctly.
This error is for cloned from volume only.
[root@cougar06 ~(keystone_admin)]# nova list
+--------------------------------------+------+--------+------------+-------------+----------+
| ID                                   | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+--------+------------+-------------+----------+
| 4dc50e69-9d84-4b19-b7ec-4bf0628d751b | na   | ERROR  | None       | NOSTATE     |          |
+--------------------------------------+------+--------+------------+-------------+----------+
[root@cougar06 ~(keystone_admin)]#
2013-10-24 16:44:00.413 2483 ERROR cinder.openstack.common.rpc.common [req-26521926-e9cb-4308-aacd-425ba2a1932a a660044c9b074450aaa45fba0d641fcc e27aae2598b94dca88cd0408406e0848] ['Traceback (most recent call last):\n', '  File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data\n    **args)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch\n    return getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 808, in wrapper\n    return func(self, *args, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 605, in initialize_connection\n    conn_info = self.driver.initialize_connection(volume, connector)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 851, in initialize_connection\n    info = self._qemu_img_info(path)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 132, in _qemu_img_info\n    info = image_utils.qemu_img_info(path)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/image/image_utils.py"", line 191, in qemu_img_info\n    out, err = utils.execute(*cmd, run_as_root=True)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 142, in execute\n    return processutils.execute(*cmd, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/processutils.py"", line 173, in execute\n    cmd=\' \'.join(cmd))\n', 'ProcessExecutionError: Unexpected error while running command.\nCommand: sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452\nExit code: 1\nStdout: \'\'\nStderr: ""Could not open \'/var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452\': No such file or directory\\n""\n']
2013-10-24 16:44:04.763 2483 ERROR cinder.openstack.common.rpc.amqp [req-53eb21ec-fdee-44f4-85fe-37ea71ebf1a1 a660044c9b074450aaa45fba0d641fcc e27aae2598b94dca88cd0408406e0848] Exception during message handling
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 808, in wrapper
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 605, in initialize_connection
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     conn_info = self.driver.initialize_connection(volume, connector)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 851, in initialize_connection
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     info = self._qemu_img_info(path)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 132, in _qemu_img_info
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     info = image_utils.qemu_img_info(path)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/image/image_utils.py"", line 191, in qemu_img_info
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     out, err = utils.execute(*cmd, run_as_root=True)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 142, in execute
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     return processutils.execute(*cmd, **kwargs)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/processutils.py"", line 173, in execute
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     cmd=' '.join(cmd))
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp ProcessExecutionError: Unexpected error while running command.
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Exit code: 1
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Stdout: ''
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Stderr: ""Could not open '/var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452': No such file or directory\n""
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp
2013-10-24 16:44:04.765 2483 ERROR cinder.openstack.common.rpc.common [req-53eb21ec-fdee-44f4-85fe-37ea71ebf1a1 a660044c9b074450aaa45fba0d641fcc e27aae2598b94dca88cd0408406e0848] Returning exception Unexpected error while running command.
Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452
Exit code: 1
Stdout: ''
Stderr: ""Could not open '/var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452': No such file or directory\n"" to caller
the volume does appear as if it exists and available in cinder list
[root@cougar06 ~(keystone_admin)]# cinder list
/usr/lib/python2.6/site-packages/babel/__init__.py:33: UserWarning: Module backports was already imported from /usr/lib64/python2.6/site-packages/backports/__init__.pyc, but /usr/lib/python2.6/site-packages is being added to sys.path
  from pkg_resources import get_distribution, ResolutionError
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| 0466799b-0810-4c69-a894-0f395fe89452 | available |   from_vol   |  10  |     None    |   true   |             |
| 1e36f3ac-27ef-46ea-b5fa-686c4da9f449 | available |     test     |  10  |     None    |  false   |             |
| 5d658297-5037-4203-9482-b072a2bc7526 | available |  from_vol1   |  10  |     None    |   true   |             |
| f7416ba6-af45-47d3-a333-478447a1ab54 | available |   from_img   |  10  |     None    |   true   |             |
| f9d6b98f-8394-4a01-9424-f23897382d87 | available |    dafna     |  10  |     None    |  false   |             |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
[root@cougar06 ~(keystone_admin)]#
but if I look under mnt its not there:
root@cougar06 ~(keystone_admin)]# ls -l /var/lib/cinder/
conversion/  mnt/         .novaclient/ tmp/
[root@cougar06 ~(keystone_admin)]# ls -l /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-
volume-1e36f3ac-27ef-46ea-b5fa-686c4da9f449                                       volume-f9d6b98f-8394-4a01-9424-f23897382d87
volume-f7416ba6-af45-47d3-a333-478447a1ab54                                       volume-f9d6b98f-8394-4a01-9424-f23897382d87.24804bb3-3846-45f8-8f24-5320e6d57184
volume-f7416ba6-af45-47d3-a333-478447a1ab54-clone                                 volume-f9d6b98f-8394-4a01-9424-f23897382d87.73262c7c-e762-44cf-85d4-14693ab0dd31
volume-f7416ba6-af45-47d3-a333-478447a1ab54.info                                  volume-f9d6b98f-8394-4a01-9424-f23897382d87.info
[root@cougar06 ~(keystone_admin)]# ls -l /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-^C"
341,1244257,cinder,8185d1b5db421441ebba128f23f840a42f9bf050,1,1,,KeyError from storwize driver when 'host' is not p...,"When user tries to delete an instance, if the nova compute driver for the instance's host is down, the nova api code goes into _local_delete().  The following code creates a fake ""connector"" without a 'host' key/vale.
        for bdm in bdms:
            if bdm['volume_id']:
                # NOTE(vish): We don't have access to correct volume
                #             connector info, so just pass a fake
                #             connector. This can be improved when we
                #             expose get_volume_connector to rpc.
                connector = {'ip': '127.0.0.1', 'initiator': 'iqn.fake'}
                self.volume_api.terminate_connection(context,
                                                     bdm['volume_id'],
                                                     connector)
When code flow reaches the cinder driver, which expects a 'host', a ""KeyError: 'host'"" exception is raised, and deletion of the instance fails.
The failure in deletion is as expected since the nova driver is not up. This issue is requesting a modification in nova api code, so that if the nova compute server is not available, a more meaningful error message (""nova compute server is not available"") could be returned, before getting to the cinder code."
342,1244415,cinder,2737c76cb2fb436f117a4f635aebca7a01691d88,1,1,,check_ssh_injection not handling quoted args corre...,"check_ssh_injection in cinder/utils.py is disallowing args with spaces even when the arg is quoted. This leads to an SSHInjectionThreat being raised when a volume driver needs to send a quoted arg containing spaces, e.g. when a storage pool name contains a space."
343,1244609,cinder,e011130104950c854bc9e139e24422b753653063,1,1,Typo in message,error message on wrong volume size is different be...,"I am working with gluster configured as cinder backend.
if I try to clone a volume from image when the new volume size is smaller than the image we get the following error:
[root@cougar06 ~(keystone_admin)]# cinder create 10 --image-id 0924d3a4-d163-4ae5-8f57-fb9f1912ee26 --display-name dafna_new
ERROR: Invalid input received: Image minDisk size 20 is larger than the volume size 10.
if I try to clone a volume from a volume when the new volume is smaller than the volume we clone from we get the following error:
[root@cougar06 ~(keystone_admin)]# cinder create 10 --source-volid c3b6cb41-d78d-420d-b64c-d5f3782f7772 --display-name new_clone
ERROR: Invalid input received: Clones currently disallowed when 10 < 20. They must be >= original volume size."
344,1244829,nova,c0e546ace478277cc2911b1c5bef1e082b76b546,1,1,"Broken method, I dont see a evolution context",destroy() method broken on Docker virt driver,"Just saw that virt drivers now take an extra arg ""context"" on the destroy method. For some reason, it has not been added to the docker driver... The destroy method with the driver enabled currently fails with the following error:
2013-10-25 23:03:20.764 ERROR nova.openstack.common.rpc.amqp [req-75ff872a-fd4d-4b63-a587-93b9fd3ede4b demo demo] Exception during message handling
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 353, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 243, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 229, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 294, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 271, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 258, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1792, in terminate_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     do_terminate_instance(instance, bdms)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1784, in do_terminate_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     reservations=reservations)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/hooks.py"", line 105, in inner
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     rv = f(*args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1757, in _delete_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     user_id=user_id)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1729, in _delete_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     self._shutdown_instance(context, db_inst, bdms)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1662, in _shutdown_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     requested_networks)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1652, in _shutdown_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     context=context)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp TypeError: destroy() got an unexpected keyword argument 'context'
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp"
345,1244860,neutron,8cf394b896e3644ff51edf6a0d462501fb6e6843,1,1,,Two DHCP ports on same network due to cleanup fail...,"On a network, ""neutron port-list --network_id <net-id> --device_owner 'network:dhcp'"" shows there are two ports.  This is checked from the mysql database:
mysql> select * from ports where tenant_id='abcd' and device_owner='network:dhcp' and network_id='7d2e3d47-396d-4867-a2b0-0311465a8454';
+----------------+--------------------------------------+------+--------------------------------------+-------------------+----------------+--------+-------------------------------------------------------------------------------+--------------+
| tenant_id      | id                                   | name | network_id                           | mac_address       | admin_state_up | status | device_id                                                                     | device_owner |
+----------------+--------------------------------------+------+--------------------------------------+-------------------+----------------+--------+-------------------------------------------------------------------------------+--------------+
| abcd | 3d6a7627-6af9-4fb6-9cf6-591c1373d349 |      | 7d2e3d47-396d-4867-a2b0-0311465a8454 | fa:16:3e:60:83:3f |              1 | ACTIVE | dhcp4fff1f08-9922-5c44-b6f8-fd9780f48512-7d2e3d47-396d-4867-a2b0-0311465a8454 | network:dhcp |
| abcd | a4c0eb19-407e-4970-90a8-0128259fb048 |      | 7d2e3d47-396d-4867-a2b0-0311465a8454 | fa:16:3e:e1:1b:8f |              1 | ACTIVE | dhcpce80c236-6a89-571d-970b-a1d4bb787827-7d2e3d47-396d-4867-a2b0-0311465a8454 | network:dhcp |
+----------------+--------------------------------------+------+--------------------------------------+-------------------+----------------+--------+-------------------------------------------------------------------------------+--------------+
2 rows in set (0.00 sec)
However, the ""neutron dhcp-agent-list-hosting-net 7d2e3d47-396d-4867-a2b0-0311465a8454 shows only one DHCP-server running.
This problem is observed in an environment with 4 nodes running dhcp-agents.  The neutron API server and the DHCP agents are NOT running on the same node.
What happened is that error occurred when the DHCP server is being ""moved"" from DHCP-agentA running on nodeA to DHCP-agentB running on nodeB.  The sequence is
  neutron dhcp-agent-network-remove <agentA> <net-id> (1)
  neutron dhcp-agent-network-add <agentB> <net-id>  (2)
Right before or during the time step 1 is done, nodeA was rebooted.  So the DHCP-port ws never removed.  When nodeA came back and the DHCP-agent restarted, it didn't do the unplug of the dhcp port device.  THe DHCP agent also failed to make the release_dhcp_port RPC call to the API-server to have the port deleted from mysql."
346,1244918,nova,34e14fbbb5293c52ef5dbfeb1172390f15d79e1f,1,0,I think is a VMware ESX specific error,Bug #1244918 “VMware ESX,"When trying to perform boot instance from volume using the VMwareESXDriver, the operation errors out.
Command:
$ nova boot --flavor 1  --block-device-mapping vda=222e8ece-8723-4930-803c-8ae5cf233a87:::0 vm1
Log messages
d3-59fce43903e8] Root volume attach. Driver type: vmdk attach_root_volume /opt/stack/nova/nova/virt/vmwareapi/volumeops.py:458
2013-10-26 14:49:13.393 30706 WARNING nova.virt.vmwareapi.driver [-] Task [RelocateVM_Task] (returnval){
   value = ""haTask-162-vim.VirtualMachine.relocate-327302855""
   _type = ""Task""
 } status: error The operation is not supported on the object.
2013-10-26 14:49:13.394 30706 ERROR nova.compute.manager [req-e95b7262-a70c-436b-a9d5-0b8045cbf3f5 4471d6567a6b4dd29affbc849f3814d9 256df8ea370d4de2b40edfe9b0ea4063] [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] Instance failed to spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] Traceback (most recent call last):
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1410, in _spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     block_device_info)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 178, in spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     admin_password, network_info, block_device_info)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 538, in spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     data_store_ref)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/virt/vmwareapi/volumeops.py"", line 467, in attach_root_volume
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     self._relocate_vmdk_volume(volume_ref, res_pool, datastore)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/virt/vmwareapi/volumeops.py"", line 295, in _relocate_vmdk_volume
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     self._session._wait_for_task(task.value, task)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 901, in _wait_for_task
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     ret_val = done.wait()
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     return hubs.get_hub().switch()
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     return self.greenlet.switch()
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] NovaException: The operation is not supported on the object.
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]"
347,1245208,neutron,d52f84d96a2fadcf90fdac7a3e265c45d2ec234a,0,0,Tests… ‘Should not’,Bug #1245208 “LBaaS,"Radware plugin driver uses task queue to perform interaction with the backend device.
Several operations such as lbaas objects deletion are performed in async manner.
In the unit test code actual object deletion happens in separate thread; it leads to a need for tricks like putting test thread to sleep.
Such unit tests are not reliable and could lead to failures that are hard to catch or debug.
Unit test code should be refactored in such way that it uses single-threaded strategy to perform driver operations."
348,1245310,neutron,9b083d7636508cbc5addfe3457c1aac706f0e267,0,0,Might potentially  cause consumers to not work properly,Bug #1245310 “'binding,"When I try to update port for additional fixed IP or any other attributes other than portbinding, _process_portbindings_create_and_update is set 'None' in port update messages sent to the agents in spite of it has host_id.
It may not be a problem if the agents do not use portbinding information but if the agent uses 'binding:host_id' information for port update it will cause a problem, which is in my case.
When look at the codes, Neutron server sets 'binding:host_id' to 'None' as long as 'binding:host_id' is not in the requested update items. It should query DB to set correct port binding instead of 'None' in that case."
365,1246258,nova,b12da559b6a40fba4e4431d74372b5e350047525,1,1,,Bug #1246258 “UnboundLocalError,"The exception occurs when trying to create/delete an instance that is using a network that is not owned by the admin tenant. This prevents the deletion of the instance.
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 294, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     function(self, context, *args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1616, in run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     do_run_instance()
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1615, in do_run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     legacy_bdm_in_spec)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 965, in _run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     notify(""error"", msg=unicode(e))  # notify that build failed
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 949, in _run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     instance, image_meta, legacy_bdm_in_spec)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1078, in _build_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     filter_properties, bdms, legacy_bdm_in_spec)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1122, in _reschedule_or_error
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     self._log_original_error(exc_info, instance_uuid)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1117, in _reschedule_or_error
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     bdms, requested_networks)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1642, in _shutdown_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     network_info = self._get_instance_nw_info(context, instance)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 879, in _get_instance_nw_info
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     instance)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 455, in get_instance_nw_info
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     result = self._get_instance_nw_info(context, instance, networks)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 463, in _get_instance_nw_info
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     nw_info = self._build_network_info_model(context, instance, networks)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 1009, in _build_network_info_model
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     subnets)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 962, in _nw_info_build_network
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     label=network_name,
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp UnboundLocalError: local variable 'network_name' referenced before assignment"
366,1246276,nova,04e53669dd3e6acd8086fd06ea073a02207ffb24,1,1,,Bug #1246276 “xenapi,"In cleaning up VDIs we call destroy_vdi.
However, should the destroy fail, it masks the real error, for example:
Unable to destroy VDI OpaqueRef:f53f35a9-31e1-af85-87a8-aa0dc7eecd43
  nova/compute/manager.py"", _build_instance
...
  nova/virt/xenapi/vmops.py"", line, in _attach_disks
    DEVICE_SWAP, name_label, swap_mb)
  nova/virt/xenapi/vm_utils.py"", line, in generate_swap
    'swap', swap_mb, fs_type)
 nova/virt/xenapi/vm_utils.py"", line, in _generate_disk
    destroy_vdi(session, vdi_ref)
  nova/virt/xenapi/vm_utils.py"", line, in destroy_vdi
    _('Unable to destroy VDI %s') % vdi_ref)
We should instead use the safe_destroy_vdi call."
367,1246291,cinder,d281149ea0f860c7c862f7897710d2e1fcb681e7,0,0,Remove unnecesary calls (¿To improve the performance?) I think is just refactoring the code,Remove unnecessary db calls made to fetch original...,"Found in commit 75bcf70ed4698f0ffba4c6a7236a1e30b1214b57
Original metadata is retrieved unnecessarily from the database when delete flag is set to True in the following methods in cinder volume api.
update_volume_metadata
update_volume_admin_metadata
update_snapshot_metadata"
368,1246327,nova,482dfeb113f1e0ff814aa1fc980d67e0c8b06d76,1,0,Evolution of Havana,the snapshot of a volume-backed instance cannot be...,"After the changes in the block device mappings introduced for Havana, if we try to create an snapshot of a volume-backed instance the resulting image cannot be used to boot a new instance due to conflicts with the bootindex between the block_device_mapping stored in the image properties and the current image.
The steps to reproduce are:
$ glance image-create --name f20 --disk-format qcow2 --container-format bare --min-disk 2 --is-public True --min-ram 512 --copy-from http://download.fedoraproject.org/pub/fedora/linux/releases/test/20-Alpha/Images/x86_64/Fedora-x86_64-20-Alpha-20130918-sda.qcow2
$ cinder create --image-id <uuid of the new image> --display-name f20 2
$ nova boot --boot-volume <uuid of the new volume> --flavor m1.tiny test-instance
$ nova image-create test-instance test-snap
This will create an snapshot of the volume and an image in glance with a block_device_mapping containing the snapshot_id and all the other values from the original block_device_mapping (id, connection_info, instance_uuid, ...):
| Property 'block_device_mapping' | [{""instance_uuid"": ""989f03dc-2736-4884-ab66-97360102d804"", ""virtual_name"": null, ""no_device"": null, ""connection_info"": ""{\""driver_volume_type\"": \""iscsi\"", \""serial\"": \""cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7\"", \""data\"": {\""access_mode\"": \""rw\"", \""target_discovered\"": false, \""encrypted\"": false, \""qos_spec\"": null, \""device_path\"": \""/dev/disk/by-path/ip-192.168.122.2:3260-iscsi-iqn.2010-10.org.openstack:volume-cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7-lun-1\"", \""target_iqn\"": \""iqn.2010-10.org.openstack:volume-cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7\"", \""target_portal\"": \""192.168.122.2:3260\"", \""volume_id\"": \""cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7\"", \""target_lun\"": 1, \""auth_password\"": \""wh5bWkAjKv7Dy6Ptt4nY\"", \""auth_username\"": \""oPbN9FzbEPQ3iFpPhv5d\"", \""auth_method\"": \""CHAP\""}}"", ""created_at"": ""2013-10-30T13:18:57.000000"", ""snapshot_id"": ""f6a25cc2-b3af-400b-9ef9-519d28239920"", ""updated_at"": ""2013-10-30T13:19:08.000000"", ""device_name"": ""/dev/vda"", ""deleted"": 0, ""volume_size"": null, ""volume_id"": null, ""id"": 3, ""deleted_at"": null, ""delete_on_termination"": false}] |
When we try latter to use this image to boot a new instance, the API won't let us because both, the device in the image bdm and the image (which is empty) are considered to be the boot device:
$ nova boot --image test-snap --flavor m1.nano test-instance2
ERROR: Block Device Mapping is Invalid: Boot sequence for the instance and image/block device mapping combination is not valid. (HTTP 400) (Request-ID: req-3e502a29-9cd3-4c0c-8ddc-a28d315d21ea)
If we check the internal flow we can see that nova considers the image to be the boot device even thought the image itself doesn't define any local disk but only a block_device_mapping pointing to the snapshot.
To be able to generate proper images from volume-backed instances we should:
 1. copy only the relevant keys from the original block_device_mapping to prevent duplicities in DB
 2. prevent nova from adding a new block device for the image if this one doesn't define any local disk"
369,1246592,nova,e74ceb68e1642d773f2ad0c655e6354f7a7fb362,1,1,typo error,Nova live migration failed due to OLE error,"When migrate vm on hyperV, command fails with the following error:
2013-10-25 03:35:40.299 12396 ERROR nova.openstack.common.rpc.amqp [req-b542e0fd-74f5-4e53-889c-48a3b44e2887 3a75a18c8b60480d9369b25ab06519b3 0d44e4afd3d448c6acf0089df2dc7658] Exception during message handling
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\openstack\common\rpc\amqp.py"", line 461, in _process_data
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\openstack\common\rpc\dispatcher.py"", line 172, in dispatch
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\exception.py"", line 90, in wrapped
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\exception.py"", line 73, in wrapped
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\compute\manager.py"", line 4103, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     block_migration, migrate_data)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\driver.py"", line 118, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     block_migration, migrate_data)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationops.py"", line 44, in wrapper
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     return function(self, *args, **kwds)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationops.py"", line 76, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     recover_method(context, instance_ref, dest, block_migration)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationops.py"", line 69, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     dest)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationutils.py"", line 231, in live_migrate_vm
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     disk_paths = self._get_physical_disk_paths(vm_name)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationutils.py"", line 114, in _get_physical_disk_paths
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     ide_paths = self._vmutils.get_controller_volume_paths(ide_ctrl_path)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\vmutils.py"", line 553, in get_controller_volume_paths
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     ""parent"": controller_path})
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\wmi.py"", line 1009, in query
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     return [ _wmi_object (obj, instance_of, fields) for obj in self._raw_query(wql) ]
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\win32com\client\util.py"", line 84, in next
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     return _get_good_object_(self._iter_.next(), resultCLSID = self.resultCLSID)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp com_error: (-2147217385, 'OLE error 0x80041017', None, None)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp"
370,1246737,neutron,0d131ff0e9964cb6a65f64809270f9d597c2d5d1,1,1,,ML2 plugin deletes port even if associated with mu...,"On subnet deletion ml2 plugin deletes all the ports associated with this subnet and does not check if a port is associated with other subnets.
Steps to reproduce:
1) create a network with two subnets
2) create dhcp port for the network, port is associated with both subnets
3) delete one of the subnets
4) dhcp port is getting deleted
Though new dhcp port is created shortly I think it's not ok to delete existing dhcp port."
371,1246788,neutron,3c3e6eef90c87440c87803dba3325e7cc8c79a17,1,0,postgres specific,NotSupportedError on postgres db backend,"Change I1e8a87d7dc1a1cb9309aeefd41619e20f49f95a6 has introduced another of those SELECT FOR UPDATE NOT SUPPORTED errors in postgres. Even though this error occurs, that does not seem to upset the gate. This is one of the logs where it occurs:
http://logs.openstack.org/52/54752/1/check/check-tempest-devstack-vm-neutron-pg-isolated/8c288a4/logs/screen-q-svc.txt.gz
And one of the runs where it passes:
https://review.openstack.org/#/c/54752/1"
372,1246848,nova,1c1570ff7ac8470d7dba28c893300835b22e0966,1,1,,Bug #1246848 “VMWare,"When an exceptin occurs in _wait_for_task and a failure occurs, for example a file is requested and it does not exists then another exception is also thrown:
013-10-31 10:49:52.617 WARNING nova.virt.vmwareapi.driver [-] In vmwareapi:_poll_task, Got this error Trying to re-send() an already-triggered event.
2013-10-31 10:49:52.618 ERROR nova.openstack.common.loopingcall [-] in fixed duration looping call
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall Traceback (most recent call last):
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall   File ""/opt/stack/nova/nova/openstack/common/loopingcall.py"", line 78, in _inner
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall     self.f(*self.args, **self.kw)
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 941, in _poll_task
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall     done.send_exception(excep)
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 208, in send_exception
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall     return self.send(None, args)
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 150, in send
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall     assert self._result is NOT_USED, 'Trying to re-send() an already-triggered event.'
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall AssertionError: Trying to re-send() an already-triggered event.
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall"
373,1247106,nova,6e8a094c48dc77fdfe68c72c7a09b7819710d4b6,1,1,Inconsistent states,SHUTOFF instance resize status does not match ACTI...,"OpenStack allows an instance to be resized in either a SHUTOFF or ACTIVE state.  However, the resize behavior between the states is not consistent.  In particular, resize of an ACTIVE instance will result in the instance's status to be mapped to RESIZE while a resize task is in progress.  The same is not true of SHUTOFF
Before resize request:
| f13dd4a2-eb8f-4a8b-91a1-dafed08049ad | rtheis-migrate-test-1    | SHUTOFF | None       | Shutdown    | VLAN164=10.164.0.14  |
| 813e5a44-41ba-4ee7-8b7b-442d3fc017a7 | rtheis-sce-test-1        | ACTIVE  | None       | Running     | VLAN164=10.164.0.16  |
During resize request:
| f13dd4a2-eb8f-4a8b-91a1-dafed08049ad | rtheis-migrate-test-1    | SHUTOFF | resize_finish | Shutdown    | VLAN164=10.164.0.14  |
| 813e5a44-41ba-4ee7-8b7b-442d3fc017a7 | rtheis-sce-test-1        | RESIZE  | resize_prep   | Running     | VLAN164=10.164.0.16  |
After resize and confirmation:
| f13dd4a2-eb8f-4a8b-91a1-dafed08049ad | rtheis-migrate-test-1    | SHUTOFF | None             | Shutdown    | VLAN164=10.164.0.14  |
| 813e5a44-41ba-4ee7-8b7b-442d3fc017a7 | rtheis-sce-test-1        | ACTIVE  | None       | Running     | VLAN164=10.164.0.16  |"
374,1247185,nova,8c985874c7885f31871204d3f83ce547fefc5fb6,1,0,if qemu-img >= 0.12.1,libvirt extract snapshot fails if qemu-img >= 0.12...,"Running stable/havana tempest against havana 2013.2 nova with this test:
tempest/tempest/api/compute/images/test_images_oneserver.py:test_create_second_image_when_first_image_is_being_saved
Using the libvirt driver on x86_64 RHEL 6.5 fails with this:
http://paste.openstack.org/show/50398/
Essentially: Stderr: ""convert: invalid option -- 's'\n""
This is the code:
https://github.com/openstack/nova/blob/2013.2/nova/virt/libvirt/utils.py#L549
Looks like it's not a new change, it's just incompatible with the latest qemu-img:
https://github.com/openstack/nova/commit/b216ed51914986087ea7dee57bc29904fda001a0
Looks like in the latest qemu-img the snapshot was moved into it's own sub-command:
[root@rhel62 ~]# qemu-img --help
qemu-img version 0.12.1, Copyright (c) 2004-2008 Fabrice Bellard
usage: qemu-img command [command options]
QEMU disk image utility
Command syntax:
  check [-f fmt] filename
  create [-f fmt] [-o options] filename [size]
  commit [-f fmt] [-t cache] filename
  convert [-c] [-p] [-f fmt] [-t cache] [-O output_fmt] [-o options] [-S sparse_size] filename [filename2 [...]] output_filename
  info [-f fmt] filename
  snapshot [-l | -a snapshot | -c snapshot | -d snapshot] filename
  rebase [-f fmt] [-t cache] [-p] [-u] -b backing_file [-F backing_fmt] filename
  resize filename [+ | -]size"
375,1247217,nova,187140c2fcafb56752f0160832607e374a4a94ff,1,1,,Sanitize passwords when logging payload in wsgi fo...,"The fix for bug 1231263 ( https://bugs.launchpad.net/nova/+bug/1231263 ) addressed not logging the clear-text password in the nova wsgi.py module for the adminPass attribute for the Server Change Password REST API, but this only addressed that specific attribute.  Since Nova has support for the ability to add REST API Extensions (in the contrib directory), there could any number of other password-related attributes in the request/response body for those additional extensions.
Although it would not be possible to know all of the various sensitive attributes that these API's would pass in the request/response (the only way to totally eliminate the exposure would be to not log the request/response which is useful for debugging), I would like to propose a change similar to the one that was made in keystone (under https://bugs.launchpad.net/keystone/+bug/1166697) to mask the password in the log statement for any attribute that contains the ""password"" sub-string in it.
The change would in essence be to update the _SANITIZE_KEYS / _SANITIZE_PATTERNS lists in the nova/api/openstack/wsgi.py module to include a pattern for the ""password"" sub-string.
Also, for a slight performance benefit, it may be useful to put a check in to see if debug logging level is enabled around the debug statement that does the sanitize call (since the request/response bodies could be fairly large and wouldn't want to take the hit to do the pattern matches if debug isn't on)."
376,1247427,nova,88861df2b95a9c66a1a9d1969ad9e560786bb848,1,1,,Bug #1247427 “VMware,"Traceback (most recent call last):
File ""/opt/stack/nova/nova/compute/manager.py"", line 2727, in rescue_instance
rescue_image_meta, admin_password)
File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 688, in rescue
_vmops.rescue(context, instance, network_info, image_meta)
File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1026, in rescue
None, None, network_info)
File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 523, in spawn
cookies)
File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 566, in _create_config_drive
extra_md=extra_md)
File ""/opt/stack/nova/nova/api/metadata/base.py"", line 145, in __init__
obj_base.obj_to_primitive(instance))
File ""/opt/stack/nova/nova/conductor/api.py"", line 302, in get_ec2_ids
return self._manager.get_ec2_ids(context, instance)
File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 456, in get_ec2_ids
instance=instance_p)
File ""/opt/stack/nova/nova/rpcclient.py"", line 85, in call
return self._invoke(self.proxy.call, ctxt, method, **kwargs)
File ""/opt/stack/nova/nova/rpcclient.py"", line 63, in _invoke
return cast_or_call(ctxt, msg, **self.kwargs)
File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 126, in call
result = rpc.call(context, real_topic, msg, timeout)
File ""/opt/stack/nova/nova/openstack/common/rpc/__init__.py"", line 139, in call
return _get_impl().call(CONF, context, topic, msg, timeout)
File ""/opt/stack/nova/nova/openstack/common/rpc/impl_kombu.py"", line 816, in call
rpc_amqp.get_connection_pool(conf, Connection))
File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 574, in call
rv = list(rv)
File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 539, in __iter__
raise result
ValueError: invalid literal for int() with base 10: 'bd915bb5-6cae-4d8d-8755-3f6583713eff-rescue'
Traceback (most recent call last):
File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
**args)
File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
result = getattr(proxyobj, method)(ctxt, **kwargs)
File ""/opt/stack/nova/nova/conductor/manager.py"", line 521, in get_ec2_ids
ec2_ids['instance-id'] = ec2utils.id_to_ec2_inst_id(instance['uuid'])
File ""/opt/stack/nova/nova/api/ec2/ec2utils.py"", line 193, in id_to_ec2_inst_id
return id_to_ec2_id(instance_id)
File ""/opt/stack/nova/nova/api/ec2/ec2utils.py"", line 181, in id_to_ec2_id
return template % int(instance_id)
ValueError: invalid literal for int() with base 10: 'bd915bb5-6cae-4d8d-8755-3f6583713eff-rescue'"
377,1247556,nova,8797aeaf14352a353ebfd1ed011f63a5b2fa3447,1,1,,Bug #1247556 “VMware,IDE disks are not hot addable. They will cannot be added when a VM is running.
378,1247610,nova,0ed493b829e8b8844b86a80da3cd70c69b1fe23a,1,1,Bad status,resource_tracker  didn't report resize statue when...,"We can resize a shutoff instance , but resouce_tracker doesn't include this case when judge if an instance is in resize state.
Will show a warning like
""2013-10-31 16:35:18.969 18483 WARNING nova.compute.resource_tracker [-] [instance: 813e5a44-41ba-4ee7-8b7b-442d3fc017a7] Instance not resizing, skipping migration."
379,1247743,cinder,10108f4d2e640a9fc12037ece39a833421d27149,1,0,NFS specific,cinder failed to create a backup with NFS driver,"Description of problem:
The action cinder backup-create fails when trying to backup a newly created volume.
- The Cinder is using the cinder.volume.drivers.nfs.NfsDriver driver.
- All the other OS components are installed on 1 host and the cinder on a different host.
Version-Release number of selected component (if applicable):
openstack-cinder-2013.2-1.el6ost.noarch
How reproducible:
evevrytime
Steps to Reproduce:
1. create a volume
2. backup the volume
3.
Actual results:
the backup failed
Expected results:
the backup is available
Additional info:
2013-11-04 09:14:04.278 15089 ERROR cinder.openstack.common.rpc.amqp [req-2ae3d45e-40c6-4422-be59-1bba466f086c c7fdf6f628554d56aad363ad501ce412 add3de2deaa445c1a1e71c1721bc8976] Exception during message handling
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 808, in wrapper
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/backup/manager.py"", line 270, in create_backup
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     'fail_reason': unicode(err)})
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/backup/manager.py"", line 263, in create_backup
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     backup_service)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/nfs.py"", line 352, in backup_volume
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     raise NotImplementedError()
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp NotImplementedError
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp"
380,1247844,nova,7070e4a66f576b2e6e4a4f1121445200e7854942,0,0,The client needs to provide a more granular exception. Feature,PortLimit thrown when specifying used fixed ip,"PortLimit thrown when specifying used fixed ip
The nova/network/neutronv2/api.py _create_port will catch errors thrown from the neutron client.  The code asserts that a 409 error is an over-quota error.  However, this will hide other errors that may be occurring within the system.
The code is currently taking a calculated risk by assuming that all 409 errors that come through this code path will be over quota's. However, another high traffic code path is specifying a fixed_ip.  If a user specifies a fixed ip address, this code will now incorrectly throw a PortLimitExceeded error.  This leads the users to believe that they have run out of their quota limit.
Example exception (then wrapped exception):
NV-6FC38FD Neutron error creating port on network 7d360984-e12c-4bb3-819d-4b93c4ca4269
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 182, in _create_port
    port_id = port_client.create_port(port_req_body)['port']['id']
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 108, in with_params
    ret = self.function(instance, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 308, in create_port
    return self.post(self.ports_path, body=body)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1188, in post
    headers=headers, params=params)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1111, in do_request
    self._handle_fault_response(status_code, replybody)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1081, in _handle_fault_response
    exception_handler_v20(status_code, des_error_body)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 93, in exception_handler_v20
    message=msg)
NeutronClientException: 409-{u'NeutronError': {u'message': u'Unable to complete operation for network 7d360984-e12c-4bb3-819d-4b93c4ca4269. The IP address 10.0.0.2 is in use.', u'type': u'IpAddressInUse', u'detail': u''}}
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1236, in _allocate_network_async
    dhcp_options=dhcp_options)
  File ""/usr/lib/python2.6/site-packages/nova/network/api.py"", line 49, in wrapper
    res = f(self, context, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 358, in allocate_for_instance
    LOG.exception(msg, port_id)
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 335, in allocate_for_instance
    security_group_ids, available_macs, dhcp_opts))
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 191, in _create_port
    raise exception.PortLimitExceeded()
PortLimitExceeded: Maximum number of ports exceeded"
381,1247902,neutron,bb2c4ef9a26b12c68ca1e6710d0c92ce00c464cb,0,0,I think we need to handle collections such as… Feature,Bulk create need to notify dhcp agent,"I bulk created subnets and ports by neutron api.
Using these ports boot instance. I found that the vms could not get ip address.
Then I saw the source code and found that _send_dhcp_notification function only handle ['network', 'subnet', 'port'] resource.
I think we need to handle collections such as  ['networks', 'subnets', 'ports'] here."
382,1247976,neutron,5e8043e8a2675b4485a52d688ea86d1165d81a29,0,0,Add VM (live) migration support to the ML2 cisco nexus mechanism,Bug #1247976 “ML2 Cisco Nexus MD,"Port this change in the cisco plugin, https://review.openstack.org/#/c/50389/ (Detect and process live-migration in Cisco plugin) to ML2. Changes under this bug were made to the cisco plugin update_port() method."
383,1247998,cinder,e066158b5235a3879fe90fa3bd813fc3363c01f5,1,1,There is a problem,Create volume from image does not convert with cep...,"When creating a volume from image using the RBD backend for cinder, the image will be copied directly and not converted even if the image is not raw (e.g. qcow2). This causes mounting and booting to fail as the volume is not raw.
The problem, I think, is in clone_image in the RBD volume driver, which doesn't check the type of the source image and doesn't convert it.  It is called from CreateVolumeFromSpecTask#_create_from_image. Implementations of clone_image for other drivers seem to do the conversion correctly."
384,1248002,neutron,504f496d6b4fd6234c3e65ba636ee12b004fae1f,0,0,Bug in tests,bandwidth metering - excluded option doesn't work,"When I using 'meter-label-rule-create' command with neutronclient, excluded option is doesn't work.
The option exclude this cidr from the label.
You have to fix below code:
neutron / neutron / services / metering / drivers / iptables / iptables_driver.py
(line number is 157)
def _process_metering_label_rules(self, rm, rules, label_chain,
                                      rules_chain):
        im = rm.iptables_manager
        ext_dev = self.get_external_device_name(rm.router['gw_port_id'])
        if not ext_dev:
            return
        for rule in rules:
            remote_ip = rule['remote_ip_prefix']
            dir = '-i ' + ext_dev
            if rule['direction'] == 'egress':
                dir = '-o ' + ext_dev
            if rule['excluded'] == 'true':   ------> fix it : True (boolean type)
                ipt_rule = dir + ' -d ' + remote_ip + ' -j RETURN'
                im.ipv4['filter'].add_rule(rules_chain, ipt_rule, wrap=False,
                                           top=True)
            else:
                ipt_rule = dir + ' -d ' + remote_ip + ' -j ' + label_chain
                im.ipv4['filter'].add_rule(rules_chain, ipt_rule,
                                           wrap=False, top=False)"
385,1248019,nova,17d131661740e452e9729d9ac8881e6fede23dd7,1,0,Issue with NFS,OSError occours when try to resize-confirm an inst...,"when using at least two compute nodes using KVM, and use NFS share_storage to test resize an instance.
The configuration of NFS used the introduction about live-migration using NFS in community doc.
when executed command ""nova resize ae6f9472-3080-4e86-8a52-f8e642081d15"", can work well, and the instance's state will change to ""VERIFY_RESIZE',  Then I resize-confirm it, nova met the issue as follow:
{u'message': u""[Errno 39] Directory not empty: '/KVM/stack/data/nova/instances/ae6f9472-3080-4e86-8a52-f8e642081d15_resize'"", u'code': 500, u'details': u'  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 263, in decorated_function |
|                                      |     return function(self, context, *args, **kwargs)                                                                                                                                                                                                               |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 2700, in confirm_resize                                                                                                                                                                   |
|                                      |     do_confirm_resize(context, instance, migration_id)                                                                                                                                                                                                            |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner                                                                                                                                                                  |
|                                      |     return f(*args, **kwargs)                                                                                                                                                                                                                                     |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 2697, in do_confirm_resize                                                                                                                                                                |
|                                      |     migration=migration)                                                                                                                                                                                                                                          |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 2724, in _confirm_resize                                                                                                                                                                  |
|                                      |     network_info)                                                                                                                                                                                                                                                 |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 4623, in confirm_migration                                                                                                                                                            |
|                                      |     self._cleanup_resize(instance, network_info)                                                                                                                                                                                                                  |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1018, in _cleanup_resize                                                                                                                                                              |
|                                      |     shutil.rmtree(target)                                                                                                                                                                                                                                         |
|                                      |   File ""/usr/lib64/python2.6/shutil.py"", line 221, in rmtree                                                                                                                                                                                                      |
|                                      |     onerror(os.rmdir, path, sys.exc_info())                                                                                                                                                                                                                       |
|                                      |   File ""/usr/lib64/python2.6/shutil.py"", line 219, in rmtree                                                                                                                                                                                                      |
|                                      |     os.rmdir(path)                                                                                                                                                                                                                                                |
|                                      | ', u'created': u'2013-10-22T15:10:50Z'}
cd /KVM/stack/data/nova/instances/be962096-a539-46c7-ae66-9ea383809e9b_resize
[root@cc be962096-a539-46c7-ae66-9ea383809e9b_resize]# ls -al
total 24340
drwxr-xr-x  2 nobody nobody     4096 Oct 18  2013 .
drwxrwxrwx 14 root   root       4096 Oct 18  2013 ..
-rw-r--r--  1 nobody nobody 25034752 Oct 18  2013 .nfs000000000714002e00000001"
386,1248214,glance,66f52c0b0d03362b72f65631af3e562cc999f84a,0,0,Remove unused method,Need remove unused method setup_logging  in  glanc...,"Method setup_logging in glance/common/config.py wasn't used anywhere.
Similar function is in oslo, and some modules in Glance are using function log.setup from oslo,
like glance/cmd/manage.py#L123.
So we need remove this method ."
387,1248216,cinder,2f62736e47367404ac56525689412d123d69c283,0,0,Remove deprecated command in tests,Need remove deprecated module commands,"The commands module was deprecated since version 2.6 and it has been
removed in Python 3. Use the subprocess module instead.
See http://docs.python.org/2/library/commands#module-commands"
388,1248219,neutron,2fb996642a597e7050ecc13ef6f3ad4aaae34997,1,1,,ExtraRoute_db_mixin._get_extra_routes_by_router_id...,"Tonight I created a new router and attached an new subnet to it, then I tried to detach this subnet but failed. I debug the code and found ExtraRoute_db_mixin._get_extra_routes_by_router_id always returns all routes, because of the third line:
1.    def _get_extra_routes_by_router_id(self, context, id):
2.        query = context.session.query(RouterRoute)
3.        query.filter(RouterRoute.router_id == id)
4.        return self._make_extra_route_list(query)
   It should be:
          query = query.filter(RouterRoute.router_id == id)"
389,1248222,neutron,0da18edec2ca213a9d87778a4679afb891d58a28,1,1,,The Loadbalancer agent's binary name as appears in...,"Version
=======
Havana on rhel
openstack-neutron-2013.2-3.el6ost
Description
===========
The Loadbalancer agent's binary in the ""agents"" database table differs from the real binary name and service name.
# mysql -u root ovs_neutron
mysql> select * from agents where agent_type like ""Loadbalancer agent"";
+--------------------------------------+--------------------+----------------------------+-----------------------------+-------------------------------+----------------+---------------------+---------------------+---------------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| id                                   | agent_type         | binary                     | topic                       | host                          | admin_state_up | created_at          | started_at          | heartbeat_timestamp | description | configurations                                                                                                                                                                            |
+--------------------------------------+--------------------+----------------------------+-----------------------------+-------------------------------+----------------+---------------------+---------------------+---------------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 7f0b2ac3-5478-4a80-9c09-5856956d0160 | Loadbalancer agent | neutron-loadbalancer-agent | lbaas_process_on_host_agent | shtutgmuralegamrey.redhat.com |              1 | 2013-11-05 13:02:45 | 2013-11-05 13:02:45 | 2013-11-05 14:54:09 | NULL        | {""device_driver"": ""neutron.services.loadbalancer.drivers.haproxy.namespace_driver.HaproxyNSDriver"", ""interface_driver"": ""neutron.agent.linux.interface.OVSInterfaceDriver"", ""devices"": 1} |
+--------------------------------------+--------------------+----------------------------+-----------------------------+-------------------------------+----------------+---------------------+---------------------+---------------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.00 sec)
The binary here is ""neutron-loadbalancer-agent"" while the real binary should probably be ""neutron-lbaas-agent"".
# rpm -ql openstack-neutron  | grep neutron-loadbalancer-agent
<empty output>
# rpm -ql openstack-neutron  | grep neutron-lbaas-agent
/etc/rc.d/init.d/neutron-lbaas-agent
/usr/bin/neutron-lbaas-agent
/usr/share/neutron/neutron-lbaas-agent.upstart
# service neutron-lbaas-agent status
neutron-lbaas-agent (pid  2321) is running...
# service neutron-loadbalancer-agent status
neutron-loadbalancer-agent: unrecognized service
# ps -elf | grep neutron-loadbalancer-agent | grep -v grep
<empty output>
# ps -elf | grep neutron-lbaas-agent | grep -v grep
0 S neutron   2321     1  0  80   0 - 70231 ep_pol 15:02 ?        00:01:10 /usr/bin/python /usr/bin/neutron-lbaas-agent --log-file /var/log/neutron/lbaas-agent.log --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/lbaas_agent.ini"
390,1248377,neutron,29e93f2248df880f411bbf83316fd67d93a33b7d,0,0,Tests,cisco n1kv unit test failing depending on threadin...,"The test_n1kv_plugin unit tests have segment IDs that overlap with segment IDs in test_n1kv_db.
https://github.com/openstack/neutron/blob/ba242d91c54b36628e03332a91adfef40e32ac32/neutron/tests/unit/cisco/n1kv/test_n1kv_plugin.py#L259
https://github.com/openstack/neutron/blob/64c0d5d272fd48c84b044a771ca6122700b69a29/neutron/tests/unit/cisco/n1kv/test_n1kv_db.py#L47
This causes a unit test for segment uniqueness to fail when they end up executing simultaneously. (e.g. http://logs.openstack.org/85/54485/10/check/gate-neutron-python27/e18c12f/console.html)"
391,1248415,cinder,a9267644ee09591e2d642d6c1204d94a9fdd8c82,0,0,Add functionality,Bug #1248415 “cinder extend Bug ,"When using 'cinder extend' command, tgt update process doesn't implemented.
So, If you want to see extended size of volume inside your vm, you must run 'tgt-admin --update iqn.2010-10.org.openstack:volume-xxxxx' in hostmachine that has the extended volume .
See my test case as follow.
1. Create cinder volume
# cinder create --volume-type LVM01-type --display-name test-extend-vol01 3
2. List cinder volume & confirm the size 3 GB
# cinder list
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
|                  ID                  |   Status  |    Display Name   | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
| 885f850f-8e03-4576-a60d-5d3d1362a9da | available | test-extend-vol01 |  3   |  LVM01-type |  false   |             |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
3. run lvs command to confirm the size 3 GB
# lvs | grep 885f850f-8e03-4576-a60d-5d3d1362a9da
  volume-885f850f-8e03-4576-a60d-5d3d1362a9da cinder-volumes -wi-ao 3.00g
4. run 'tgt-admin --show' command to confirm the size 3 GB
# tgt-admin --show
Target 1: iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET     00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: null
            Backing store path: None
            Backing store flags:
        LUN: 1
            Type: disk
            SCSI ID: IET     00010001
            SCSI SN: beaf11
            Size: 3221 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/cinder-volumes/volume-885f850f-8e03-4576-a60d-5d3d1362a9da
            Backing store flags:
    Account information:
    ACL information:
        ALL
5. Extend cinder volume
# cinder extend 885f850f-8e03-4576-a60d-5d3d1362a9da 5
6. List cinder volume and confirm the extended size to 5 GB
# cinder list
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
|                  ID                  |   Status  |    Display Name   | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
| 885f850f-8e03-4576-a60d-5d3d1362a9da | available | test-extend-vol01 |  5   |  LVM01-type |  false   |             |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
7. Run 'lvs' command to confirm the extended size to 5 GB
# lvs | grep 885f850f-8e03-4576-a60d-5d3d1362a9da
  volume-885f850f-8e03-4576-a60d-5d3d1362a9da cinder-volumes -wi-ao 5.00g
8. Run 'tgt-admin --show' command to confirm the extended size to 5 GB , but it still 3 GB because didn't implemented 'tgt update'
# tgt-admin --show
Target 1: iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET     00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: null
            Backing store path: None
            Backing store flags:
        LUN: 1
            Type: disk
            SCSI ID: IET     00010001
            SCSI SN: beaf11
            Size: 3221 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/cinder-volumes/volume-885f850f-8e03-4576-a60d-5d3d1362a9da
            Backing store flags:
    Account information:
    ACL information:
        ALL
9. Run 'tgt-admin --update' command to update
# tgt-admin --update iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
10. Run 'tgt-admin --show' command to confirm 5 GB updated
# tgt-admin --show
Target 1: iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET     00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: null
            Backing store path: None
            Backing store flags:
        LUN: 1
            Type: disk
            SCSI ID: IET     00010001
            SCSI SN: beaf11
            Size: 5369 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/cinder-volumes/volume-885f850f-8e03-4576-a60d-5d3d1362a9da
            Backing store flags:
    Account information:
    ACL information:
        ALL
Please check this bug and answer to me.
Thank you."
392,1248424,nova,94c1ad5c0eb0fd574f659a9477b222a7d6c84c4b,1,1,,RequestContext initialization failed in nova,"RequestContext initialization failed in nova because of the following error:
                   ""TypeError: 'in <string>' requires string as left operand, not NoneType""
My operations as follows:
1.Call keystone api to create a service without type.
2.Call keystone api to add a endpoint with the service.
3.Call nova api to list servers.
Then the error TypeError: 'in <string>' requires string as left operand, not NoneType"" has been thrown."
393,1248434,cinder,539eaaab89eea42f3f92a4ce069183c7730034af,1,1,,RequestContext initialization failed in cinder.,"RequestContext initialization failed in cinder because of the following error:
                   ""TypeError: 'in <string>' requires string as left operand, not NoneType""
My operations as follows:
1.Call keystone api to create a service without type.
2.Call keystone api to add a endpoint with the service.
3.Call cinder api to list servers.
Then the error TypeError: 'in <string>' requires string as left operand, not NoneType"" has been thrown."
394,1248443,nova,47c95694e93b361e3b0738afac8e724557a36c8a,1,1,,Wrong comparisons with “in,"In the code there are several comparisons using the ""in"" operator to check if a value occurs in a tuple with only one element, like:
    if a in (""foo""):
        do_something()
This comparison is wrong, since that is not a tuple with one element [1], therefore if the variable ""a"" had a value of ""f"" it will match.
[1] http://docs.python.org/2/tutorial/datastructures.html#tuples-and-sequences"
395,1248463,nova,6847a0ef927929e631e9c6db2e2812528ca93151,1,1,Issue with the API and add more,Cannot resize VM to a different compute node when ...,"The bug is when you resize a VM to another compute node using Hyper-V VMUtilsV2, there will be an exception in the compute node which the VM  located before resizing.
The exception is ""Cannot find boot VHD file for instance: instance-0000000e"".
After debuged, the issue maybe in funtion get_vm_storage_paths of vmutils.py.
If using Hyper-V v1, the get_vm_storage_paths function in vmutils.py can find the Virtual Hard Disk, all the ResourceSubType of Msvm_ResourceAllocationSettingData are
[u'Microsoft Virtual Keyboard',
u'Microsoft Virtual PS2 Mouse',
u'Microsoft S3 Display Controller',
u'Microsoft Synthetic Diskette Drive',
None,
u'Microsoft Serial Controller',
u'Microsoft Serial Port',
u'Microsoft Serial Port',
u'Microsoft Synthetic Disk Drive',
u'Microsoft Virtual Hard Disk',
u'Microsoft Synthetic DVD Drive',
u'Microsoft Virtual CD/DVD Disk',
u'Microsoft Emulated IDE Controller',
u'Microsoft Emulated IDE Controller',
u'Microsoft Synthetic Mouse',
u'Microsoft Synthetic Display Controller',
u'Microsoft Synthetic SCSI Controller']
If using Hyper-V v2, the get_vm_storage_paths function in vmutils.py can not find the Virtual Hard Disk, all the ResourceSubType of Msvm_ResourceAllocationSettingData are
Microsoft:Hyper-V:Virtual Keyboard
Microsoft:Hyper-V:Virtual PS2 Mouse
Microsoft:Hyper-V:S3 Display Controller
Microsoft:Hyper-V:Synthetic Diskette Drive
None
Microsoft:Hyper-V:Serial Controller
Microsoft:Hyper-V:Serial Port
Microsoft:Hyper-V:Serial Port
Microsoft:Hyper-V:Synthetic Disk Drive
Microsoft:Hyper-V:Synthetic DVD Drive
Microsoft:Hyper-V:Emulated IDE Controller
Microsoft:Hyper-V:Emulated IDE Controller
Microsoft:Hyper-V:Synthetic Mouse
Microsoft:Hyper-V:Synthetic Display Controller
Microsoft:Hyper-V:Synthetic SCSI Controller
I also find in Hyper-V v2 I can find Microsoft Virtual Hard Disk from class Msvm_StorageAllocationSettingData.
Maybe the Hyper-V v2 api changed, but the codes in nova didn't change."
396,1248699,nova,edc66467fc0dcf729990d8e7baed254f8ded9bfa,1,1,"It says Ubuntu, but I think is more general with the use of the command",Fix checking if an ip interface exist when machine...,"In Ubuntu when you don't specify the hostname of the machine in /etc/hosts , and you try to run sudo <whatever> you get this message shown in your terminal:
sudo: unable to resolve host ....
But this shouldn't be causing nova compute to fail, but it does because of this function (nova/network/linux_net.py):
def device_exists(device):
    (_out, err) = _execute('ip', 'link', 'show', 'dev', device,
                                             check_exit_code=False, run_as_root=True)
    return not err
As you can see this function instead of checking the return code of the command ""ip link show"", it try to check whether anything was displayed to stderr or no,  and guess what ? the message ""sudo: unable to resolve host ...."" is sent to stderr !!
I will go as far as to tell (And maybe i am wrong) that there is only one way and one preferable way to check wether a command succeed or failed is by checking the return code of this latter, and if there is some special case where a command is returning wrong return code, than it should be treated as it is, i.e. special case. Remember ""Special cases aren't special enough to break the rules."""
397,1248799,nova,c8ded6429616b798947072a62cb1b5ee4ea51209,1,1,,vm would be stuck in unshelving when unshelve fail...,"when unshelve a vm, if this vm has been offloaded, the process would involve re-scheduling.
in nova/conductor/manager.py def  unshelve_instance(self, context, instance):
elif instance.vm_state == vm_states.SHELVED_OFFLOADED:
            try:
                with compute_utils.EventReporter(context, self.db,
                        'get_image_info', instance.uuid):
                    image = self._get_image(context,
                            sys_meta['shelved_image_id'])
            except exception.ImageNotFound:
                with excutils.save_and_reraise_exception():
                    LOG.error(_('Unshelve attempted but vm_state not SHELVED '
                                'or SHELVED_OFFLOADED'), instance=instance)
                    instance.vm_state = vm_states.ERROR
                    instance.save()
            filter_properties = {}
            hosts = self._schedule_instances(context, image,
                                             filter_properties,instance)           <<<<<this re-scheduling would cause exception,when it occurs,the
         <<<<<<instance will be stuck in task_state: unshelving forever
            host = hosts.pop(0)['host']
            self.compute_rpcapi.unshelve_instance(context, instance, host,
                    image)"
398,1248802,nova,dd6765f751f9c6f36c027ba7c945c919ecf825e2,1,1,Bug in the sequence of the commands,shelve/unshelve notification may be out of order,"when shelve a vm and CONF.shelved_offload_time == 0,the vm would be offloaded right now.
referring to nova/compute/manager.py:def shelve_instance(self, context, instance, image_id):
        if CONF.shelved_offload_time == 0:
            instance.task_state = task_states.SHELVING_OFFLOADING
        instance.power_state = current_power_state
        instance.save(expected_task_state=[
                task_states.SHELVING,
                task_states.SHELVING_IMAGE_UPLOADING])
        if CONF.shelved_offload_time == 0:
            self.shelve_offload_instance(context, instance)
        self._notify_about_instance_usage(context, instance, 'shelve.end')
thus,notification may occurs like:
compute.instance.shelve.start
compute.instance.shelve_offload.start
compute.instance.shelve_offload.end
compute.instance.shelve.end
in fact,order should be like:
compute.instance.shelve.start
compute.instance.shelve.end
compute.instance.shelve_offload.start
compute.instance.shelve_offload.end
I suggest that ""self._notify_about_instance_usage(context, instance, 'shelve.end')"" should be moved before
 "" if CONF.shelved_offload_time == 0:
            self.shelve_offload_instance(context, instance)"""
399,1248815,cinder,0d91b1a86a211610f78ac5c0df5a0b02b5a2f3a1,1,1,Is using 200 OK instead of 400 Bad Request. Bad code,Update quotas shouldn't allow bad keys in the requ...,"When user passes bad keys to update quota api, it returns 200 OK response. It should return 400 Bad Request error for bad keys.
PUT
​http://127.0.0.1:8776/v2/ad5d29fef329473598031ca4080288a7/os-quota-sets/ad5d29fef329473598031ca4080288a7
Request Body
{
""quota_set"": {
""gigabytes"": 5,
""xyz"": 2
}
}
Actual Response: 200 OK
{""quota_set"": {""gigabytes"": 5, ""snapshots"": 1000, ""volumes"": 10}}
Expected Response: 400
{""badRequest"": {""message"": ""Bad key(s) xyz in quota_set"", ""code"": 400}}
Note: Nova doesn't allow bad keys in the os-quota-sets put method."
469,1254639,neutron,3f29c06d54de32f0a0859f53e942e1f18326b4d4,0,0,"'suggests’, ‘should also’",NVP advanced LBaaS should change operation on dele...,"Bug #1243129 suggests to check for associations before deleting health monitor, so NVP advanced LBaaS should also change its operation when deleting a healthmonitor."
470,1254664,nova,497bdfec93d696c7200a71b85a0680d99ce07bb9,1,1,Something is ignored,Network quotas are ignored by libvirt when Open vS...,"I created a flavor with quotas on the network bandwidth and created a VM with this flavor: upload limited 1 MB/sec, download limited to 500 kB/sec. The limits are ignored: copy from DevStack to the VM is faster than 13 MB/sec.
I'm using Neutron with Open vSwitch (OVS) for the network.
It looks like a regression in nova/virt/libvirt/vif.py. According to a colleague, it was maybe introduced when the OVS, LinuxBridge and HyperV classes were merged into one LibvirtGenericVIFDriver class. designer.set_vif_bandwidth_config() is only called for bridge types:
- get_config_ovs_hybrid()
- get_config_ivs_hybrid()
- network_model.VIF_TYPE_BRIDGE
Command to create the flavor and create a VM with this flavor:
---
nova flavor-create --ephemeral=1 victor_test_vif 50 256 1 1
nova flavor-key victor_test_vif set quota:vif_inbound_average=1000
nova flavor-key victor_test_vif set quota:vif_inbound_peak=1000
nova flavor-key victor_test_vif set quota:vif_outbound_peak=500
nova flavor-key victor_test_vif set quota:vif_outbound_average=500
nova boot --flavor=victor_test_vif --image=cirros-0.3.1-x86_64-uec victor_test
---
Command to start a small and fast TCP server on DevStack, uploading a file of 10 MB:
---
ip netns
# copy the qrouter-xxx name
sudo ip netns exec qrouter-128db593-a0db-40c3-84c7-e6383d40c75f bash
# following commands are executed in the qrouter namespace to reach the VM network
dd if=/dev/urandom of=random10MB bs=1024 count=10240
nc -l 0.0.0.0 12345 < random
---
Command to download the file on the VM:
---
time nc 10.0.0.1 12345 > /dev/null
---
Current result: timing smaller than 1 second (faster than 10 MB/sec)
Expected result: timing higher than 10 second (1 MB/sec or slower)
The problem is that the <bandwidth> tag is not generated in the libvirt.xml file of the VM.
I will provide a patch."
471,1254727,nova,842b2abfe76dede55b3b61ebaad5a90c356c5ace,0,0,We are able to increase the min required libvirt to 0.9.11,Increase min required libvirt to 0.9.11 to allow u...,"Based on the data in this wiki
https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
and discussions on the dev & operator mailing lists
  http://lists.openstack.org/pipermail/openstack-dev/2013-November/019767.html
  http://lists.openstack.org/pipermail/openstack-operators/2013-November/003748.html
We are able to increase the min required libvirt to 0.9.11
This will allow us to switch to using the (soon to be released) standalone libvirt python binding from PyPI, as well as removing some old compat code."
472,1254811,nova,22cd7cc27ca8285c1674e370e256ec84d3f2a20a,0,0, Deprecate and remove libvirt.api_thread_pool option ,Deprecate and remove libvirt.api_thread_pool optio...,"We should deprecate the libvirt.api_thread_pool config option in icehouse and remove it in the J release.  Looks like the last time it was touched was in January for some refactoring:
https://github.com/openstack/nova/commit/ce27bca5497600ff9ab7195e27ede94e7cffe5d0
The reasons why this should be removed are detailed here:
https://review.openstack.org/#/c/57000"
473,1254820,nova,afc517e093e171c2b143e91972dea19351f62592,1,0, fails on older versions of libvirt ,GlusterFS disk attach fails on older versions of l...,"Some versions of libvirt (such as RHEL6's 0.10.2) require a port specification with a GlusterFS network disk, or they reject the disk with an ""XML error: missing port for host"" error.
We can provide a blank value, rather than omitting the field, which allows qemu to default to a reasonable port."
474,1254902,nova,ff221c6cc61f7b98b62462ab4f0e0e1cf691a9a8,0,0,I think is evolution of the code. No bug,Conductor returns new instance objects to older cl...,"Conductor will return new Instance objects to older clients when they do actions on the InstanceList object. This is because we only consider the version of their InstanceList object, and happily fill it with objects newer than they can handle."
475,1254924,neutron,09c3051fb2819b647f4d8196e6e0fbc66f0ccc04,0,0,removes the other redundant notification sent. Just delete ,security group update notification sent twice,"For several plugins the security group update notification on port-update is sent twice.
1) https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L634
2) https://github.com/openstack/neutron/blob/master/neutron/db/securitygroups_rpc_base.py#L101
the notification at #1 is sent but not that at #2 if the security group on the port was not updated.
However if the notication at #2 is sent, then the notification at #1 will always be sent too.
Therefore the notification at #2 is probably redundant
This affects at least the following plugins:
- nec
- ryu
- linuxbridge
- openvswitch
- ryu
This causes a lot more traffic on the agent side thus slowing all the operations on the agent, including wiring VIF ports."
476,1254963,cinder,a0693a91f55b2b08a46f1396dd1a1102783df9c6,0,0,removes redundant conde,Remove the redundant check for 'os-migrate_volume_...,"Remove the redundant KeyError check for 'os-migrate_volume_completion'
The fact that this extension is registered as WSGI action implies that the body must contain a key with the same name as action."
477,1254975,cinder,4a8ad10c3d5d79e09805c28486bd0f8a5a16c511,0,0,removes redundant conde,The body validation is redundant for os-volume_upl...,"The body validation is redundant for os-volume_upload_image, should remove the redundant KeyError for 'os-volume_upload_image'. In fact that this extension is registered as WSGI action implies that the body must contain a key with the same name as action."
478,1254978,cinder,018ba6302b0f5b909e9f32f44a14c21313a2663e,1,1,typo in code,_update_volume_status() is not called by anyone,"_update_volume_status() method in cinder/volume/driver.py is not called by anyone because misspelling.
_update_volume_status() method in cinder/volume/drivers/eqlx.py is also the same.
I think it should be _update_volume_stats()."
479,1254988,nova,3001bc67edacb89a852b06a7d840da41f42e0e5b,0,0,removes redundant conde,The body validation is redundant for createBackup,"The body validation is redundant for createBackup, should remove the redundant KeyError for 'createBackup'.
 In fact that this extension is registered as WSGI action implies that the body must contain a key with the same name as action."
480,1255001,nova,545a4976fc7896e8afee8c70661d109c464c8f44,1,1,Bad code,"Bug #1255001 “Fix exception for os-migrateLive "" ","Several exception is not correct in os-migrateLive action:
1.If the server state conflict to live-migrate, it's raises 400 exception.I think we should raise HTTPConflict instead of HTTPBadRequest.
2.the error msg is not accurate while several exception such as :
                exception.NoValidHost,
                exception.InvalidLocalStorage,
                exception.InvalidSharedStorage,
                exception.MigrationPreCheckError"
481,1255023,nova,8a8b0ba9abcf7237dc4a7d8ddfc739939c6e9164,0,0,tests,Supplement 'os-migrateLive' in actions list in tes...,"Action ""os-migrateLive"" not defined in actions list in test_actions_with_non_existed_instance func.
It should not test the action ""os-migrateLive""."
482,1255035,nova,1447b8117bc4de27243913208ead210b8f36d635,1,1,Bad argument,Incorrect argument while retrieving quota with pro...,"The `user_id` passed in db.quota_get() should be a keyword argument.
The prototype of quota_get():
def quota_get(context, project_id, resource, user_id=None)
But In DbQuotaDriver::get_by_project_and_user():
return db.quota_get(context, project_id, user_id, resource)
which should be:
return db.quota_get(context, project_id, resource, user_id=user_id)"
483,1255058,neutron,3c00dd43f613c838f713a7cbf3cedb6767a8c52a,1,1,  Bugfix and refactoring,ovs_lib.OVSBridge mod_flow() method doesn't work,"During my OVS-related prototyping I spotted this. For bug verification I used 'ovs_lib' as a standalone python package (without Openstack runnig).
OVS info:
    ovs-ofctl (Open vSwitch) 1.4.0+build0
    Compiled Feb 18 2013 13:13:22
    OpenFlow versions 0x1:0x1
I can define flow with no problems:
    from neutron.agent.linux import ovs_lib
    int_br = ovs_lib.OVSBridge('br-int', root_helper='sudo')
    int_br.add_flow(priority=777,dl_type=2048, actions='normal')
Run 'ovs-ofctl dump-flows br-int'
    'cookie=0x0, duration=115.956s, table=0, n_packets=0, n_bytes=0, priority=777,ip actions=NORMAL'
However when I try to modify this flow:
    int_br.mod_flow(dl_type=2048, actions='drop')
I see
    'Command: ['sudo', 'ovs-ofctl', 'mod-flows', 'br-int', 'hard_timeout=0,idle_timeout=0,priority=0,dl_type=2048,actions=drop']
    Exit code: 1
    Stdout: ''
    Stderr: 'ovs-ofctl: unknown keyword hard_timeout\n'"
485,1255145,neutron,eb23b345876796f4002cab3632bad1c840289d04,1,1,No need to trigger a notification,address pairs in update request always trigger age...,"currently some plugins always notify the agent of a port_update even if the allowed address pairs attribute is specified: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L600
However, if there's no change in allowed address pairs there is no need for triggering a notification
Affected plugins:
ml2
openvswitch
nec"
486,1255150,neutron,7599d9d0526e22977caf6de13c751884a5d8f2f5,1,1,Add unit tests BUT find some errors,Missing plugin unit test coverage for allowed_addr...,"Some plugins, at least openvswitch and ml2, are not inheriting the test case for allowed address pairs.
This means that the bits related to address pairs use cases in these plugins are currently not covered by unit tests."
487,1255153,neutron,b22515e443449a83f423620cb1df5b92770bef32,1,1,There is a bug: it takes the whole interface down.,Linuxbridge plugin doesn't return IP to VLAN inter...,"Given a network setup like this on the network node (before starting any of the neutron services):
eth0: inet 192.168.100.230/24
eth0.1001@eth0: inet 172.24.4.224/28
Now when the linuxbridge-agent needs to create a VLAN-bridge for the 1001 VLAN it will correctly enslave eth0.1001 into the bridge and move the IP Address from eth0.1001 to the bridge.
But when linuxbridge removes the bridge again (e.g. because the last port on it was deleted) it will not reassign the IP back to the eth0.1001 device, but instead just take the whole interface down."
488,1255183,neutron,121189401a97e5f471a287c453d7f2013c92477a,1,1,Sending a lot of notifications,port update triggered even if sec group did not ch...,"the code at: https://github.com/openstack/neutron/blob/master/neutron/db/securitygroups_rpc_base.py#L73
will trigger a rebind of the security group of the port, as well as instruct the plugin to notify the agent if the port update request has the 'security_groups' attribute.
Actually these operations are not needed if there's no change from the current value for security groups.
This condition is causing a fair amount of port_update notification to be sent to the agent; the DHCP port is updated quite often, and since the plugin function is called from the RPC dispatcher, the security groups attribute is not missing, but is an empty list.
For this reason the notification is then sent every time a DHCP port is updated."
489,1255347,nova,18a1486293b15c5713ac1352b9b9b30fdf2b7413,1,1,,cinder cross_az_attach uses instance AZ value,"When checking if an instance is in the same AZ as a volume nova uses the instances availability_zone attribute. This isn't the correct way to get an instances AZ, it should use the value gotten through querying the aggregate the instance is on"
490,1255449,nova,0fe3001d639df81ae4e5f77bcfc32bc1bffb014e,0,0,Add a feature. ‘should use’,Libvirt Driver - Custom disk_bus setting is being ...,"Currently, custom disk_bus configuration will be defaulted to virtio when
a user will try to power off + power on or hard reboot his instance.
It is happening since hard_reboot() doesn't consider the image_meta  when constructing
the disk_info"
491,1255519,neutron,0cbcdcfc5094fcb52679550bc67db8136441adf6,1,1,Bug. Check is an integer to fix it,NVP connection fails because port is a string,"On a dev machine I've recently create I noticed failures at startup when Neutron is configured with the NVP plugin.
I root caused the failure to port being explicitly passed to HTTPSConnection constructor as a string rather than an integer.
This can be easily fixed ensuring port is always an integer.
I am not sure of the severity of this bug as it might strictly related to this specific dev env, but it might be worth applying and backporting it"
492,1255532,nova,0f07f8546fda9732a7e3597a2de78156f1fb5a34,0,0,"Is a test bug, but add some commits of other bugs :/",Import of unexisting openstack.common.test.py file...,"In keystone/openstack/common/db/sqlalchemy/test_migrations.py, line 30 imports `test` what is not exists. Oslo project contains  appropriate file. Solution ­— synchronize keystone.openstack.common with corresponding file in Oslo.
The bug reproduces under nosetests."
494,1255609,nova,2f49ed4b5dbb5c954fc7a9b42ee7b170c38c775c,1,1,Race condition,Bug #1255609 “VMware,"We assign VNC ports to VM instances with the following method:
def _get_vnc_port(vm_ref):
    """"""Return VNC port for an VM.""""""
    vm_id = int(vm_ref.value.replace('vm-', ''))
    port = CONF.vmware.vnc_port + vm_id % CONF.vmware.vnc_port_total
    return port
the vm_id is a simple counter in vSphere which increments fast and there is a chance to get the same port number if the vm_ids are equal modulo vnc_port_total (10000 by default).
A report was received that if the port number is reused you may get access to the VNC console of another tenant. We need to fix the implementation to always choose a port number which is not taken or report an error if there are no free ports available."
495,1255680,neutron,2b375c0f15fd43af23fbd28b85929d63a753548b,1,1,Sending a lot of notifications,Bug #1255680 “ml2,"process_port_binding is always called when a port is updated: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L617
However, its current implementation: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L202
Return True, hence triggering a notification in many cases.
The code has a check for returning false when the host is not set and the vif is already bound: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L213
However, this is perhaps not triggered in many cases.
As an example, in this job: http://logs.openstack.org/20/57420/9/experimental/check-tempest-devstack-vm-neutron-isolated-parallel/269a314
Of 538 update_port calls to the plugin, process_port_binding turns need_port_update_notify from False to Truue in
It is therefore worth ensuring no notification is sent if the host doesn't change (the other binding parameters should not change on update and even if they can they should not trigger a notification) in 72 cases out of 82 cases where the flag was false before invoking process_port_binding
Looking at the logs - the port should not have been notified because of bindings in any case."
496,1255802,cinder,b523699ce039d77e0bc02eb2c00e436c2e28a274,1,1,,"Bug #1255802 “A mistake verification for 'readonly' "" ","A mistake verification for 'readonly' .
Because the param of  'readonly'  is bool , can not be checked in that way:
 readonly_flag = body['os-update_readonly_flag'].get('readonly')
        if not readonly_flag:
            msg = _(""Must specify readonly in request."")
            raise webob.exc.HTTPBadRequest(explanation=msg)
when the readonly == false , it will be throw  HTTPBadRequest"
497,1255871,neutron,dd71021347f782608575d551f2329715c00546ce,0,0,It should… Add a feature I think,Bug #1255871 “l3_agent,"How to reproduce:
  See the test cases in the following patch."
498,1255908,cinder,d02ae9225d8ba4f7373ecced49c2d7c1cd17a732,0,0,Typo in comments,Fix typo in cinder,"sesssion -> session     cinder\openstack\common\db\sqlalchemy\models.py
explicity -> explicitly   cinder\openstack\common\db\sqlalchemy\models.py
tranfers -> transfers
recurse -> recursive
parens -> parents  cinder\openstack\common\periodic_task.py
satisified -> satisfied cinder\taskflow\exceptions.py"
499,1255914,nova,180d686d2ff73d24c8259cf9784adef582befaef,0,0,Typo in comments,Misc typos in Openstack Compute (Nova),"Typos in few files like
1. nova/availability_zones.py +100
Return available and unavailable zones on demands -> Return available and unavailable zones on demand.
2. nova/exception.py +578
Either Network uuid %(network_uuid)s is not present -> Either network uuid %(network_uuid)s is not present
3. nova/exception.py +944
Instance Type %(instance_type_id)s has no extra specs with -> Instance type %(instance_type_id)s has no extra specs with
4. novaclient/base.py +108, 122
typicaly -> typically
And few other"
500,1255917,cinder,b625f558862465184dd28da7215f34c77ec1ece6,1,1,Change to case insensitive,The response is incorrect while creating snapshot ...,"1.Create metadata for a snapshot with the request body:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
2.The server accept it, and return the response body, as bellow:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
3.Get the metadata of the snapshot, the server returned:
{""metadata"":{
             ""key1"": ""value1""
}}
4.I find that case ignore in cinder, so the server just add one metadata for the snapshot
5.So i think the response body should return the one which the server added when create action:
  {""metadata"":{
             ""key1"": ""value1""
}}"
501,1255925,cinder,be4102b0577937c0193703ace9ae7591197341de,0,0, Remove unused reservation methods from db.api ,unused reservation methods from db api,"There are several unused reservation methods are not called directly, so it could be removed from the db API.
Here are the methods:
* reservation_create https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L695
* reservation_get https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L702
* reservation_get_all_by_project https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L707
* reservation_destroy https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L712"
502,1256036,neutron,35f64f8026f7607c749727ac6b3ee93c956bdfd2,1,1,,Metering iptables driver doesn't read the right ro...,"The metering iptables driver doesn't read the root_helper param, thus it uses the default value."
503,1256041,neutron,272912b6d82af00a6f7bafc26d8ce7a79b09ef26,1,1, Fix a typo in log exception in the metering agent ,Typo in the metering agent when there is an except...,Typo in the metering agent when there is an exception when it invokes a driver method.
504,1256217,glance,1408ff71f4d849c319866be92414361d8103acdd,0,0,Typo in comments,Fix docstring on detail in glance/api/v1/images.py...,"""Returns detailed information for all public, available images"" is inappropriate on detail, because the private image can be returned also.
""Returns detailed information for all  available images"" is better."
505,1256243,neutron,52f0750d80e8742efc253b48c6bc121ae9853cc1,0,0,Evolution: Add Session Persistence support ,Add Session Persistence support for NVP advanced L...,"Need to add session persistence support for NVP advanced LBaaS, as the present session persistence implementation on Edge is set by default."
506,1256264,nova,751b1a04c9b14bdaf20d555e0aaa41bc276955c4,1,1,Doesnt respect cpu limit…,[Docker] Driver doesn't respect CPU limit,"The Nova Docker driver doesn't seem to respect the CPU limit defined by nova.
Please note I just reviewed the latest source code, haven't tested the behavior.
My assumption is that a docker container has access to all CPU resources by default.
On the docker command line this can be handled:
    docker run -c=<relative-weight> centos"
507,1256427,nova,2d2e30dba5c571c05dd4191c1b023fe4014c1751,0,0,Duplicate exception,Duplicate FlavorNotFound handled in server create ...,"The FlavorNotFound exception is handled in multiple places in the server create API, the first being:
https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/servers.py#L951
and the latter being:
https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/servers.py#L970
They both return a 400 response and only the former is used, so remove the latter.
This is both in the v2 and v3 API."
508,1256483,cinder,d54c49a22e3ce147003cc9a926b7b72e8dc9c1a8,0,0,'Use model_query() instead of session.query’ for no reason,Use model_query() instead of session.query in db.*...,"Use model_query() instead of session.query in  db.volume_destroy, db.volume_type_destroy, db.transfer_destroy and db.snapshot_destroy."
509,1256696,nova,477b3d4dec1a815edeae02531db987b75be4194c,1,1,Typo in code and comment,correct network_device_mtu help string,mtu setting for network interface not vlan.
510,1256734,glance,5fc030cce44ca0849cc3d2c1830ead6804c70cf9,0,0,tests,Use assertEqual instead of assertEquals in unittte...,"The method assertEquals has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using assertEquals
therefore we should use assertEqual instead."
511,1256737,cinder,aa453576ff13c97c63dcb3cd5941675ffdc93360,0,0,Tests and new version,Use assertNotEqual instead of assertNotEquals in u...,"The method assertNotEquals has been deprecated.
In Python 3, a deprecated warning is raised when using assertNotEquals
therefore we should use assertNotEqual instead."
512,1256738,cinder,aa453576ff13c97c63dcb3cd5941675ffdc93360,0,0,tests,Use assertEqual instead of assertEquals in unittte...,"The method assertEquals has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using assertEquals
therefore we should use assertEqual instead."
513,1256763,cinder,24d6b57d1cb0c4f91545f85ea9c766d1e0ec50a0,1,1,confusing message,Extending a volume without enough quota isn't caug...,"I think this should be caught at API level, it means instant feedback for the user and also the volume goes to ""error_extending"" which can confuse a user thinking the volume is no longer usable."
514,1256766,neutron,3f110c9a571249c6626ebd1ef7dbf67b71273fa5,1,1, confusing message,vpnstateinvalid has confusing message,"after  I create an vpnservice without a name, and then I want to update it with a name, the following msg come out:
2013-12-02 12:09:18.691 3623 ERROR neutron.api.v2.resource [req-aab03b48-08a4-415f-9cd1-7ee4f5a16b19 8fb8b278e48e4d0cbde162e5626032a1 c3b9072bc0f741aa98f72e794dba7ea6] update failed
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource   File ""/mnt/data/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource   File ""/mnt/data/opt/stack/neutron/neutron/api/v2/base.py"", line 492, in update
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource     obj = obj_updater(request.context, id, **kwargs)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource   File ""/mnt/data/opt/stack/neutron/neutron/db/vpn/vpn_db.py"", line 579, in update_vpnservice
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource     self.assert_update_allowed(vpns_db)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource   File ""/mnt/data/opt/stack/neutron/neutron/db/vpn/vpn_db.py"", line 198, in assert_update_allowed
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource     raise vpnaas.VPNStateInvalid(id=id, state=status)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource VPNStateInvalid: Invalid state PENDING_CREATE of vpnaas resource <built-in function id>
There are two problems in above msg:
1. built-in function id
2. the msg itself: I think we can use: the vpn service cannot be updated when it is in state PENDING_CREATE
but I think we should be able to modify the vpn service when it is not used by IPsecSiteConnection"
515,1256838,nova,2593469103aa7d9d2bcb759b78d5f8637911a1e0,1,1,Race condition,Race between imagebackend and imagecache,"After ImageCacheManager judges a base image is not used recently and marks it as to be removed, there is some time before the image is actually removed. So if an instance using the image is launched during the time, the image will be removed unfortunately."
516,1256873,nova,d122b2e05a9e13d5caea3f8f6578bb473fbb9c5e,0,0,Server actions should raise – Feature,Catch InstanceIsLocked exception instead of invali...,"Server actions should raise ""Instance is locked"" while instance is locked by admin, the actions include:
1. reboot  in v2 and v3
2. delete  in v2 and v3
3. resize/confirm resize/revert resize in v2 and v3
4. shelve/unshelve/shelve_offload in v2 and v3
5. attach_volume/swap_volume/detach_volume in v2 and v3
6. rebuild in v2 and v3"
517,1256981,nova,93bc58765f393536c640494ec129bf39dbdc30c4,1,1,Typo in code,network_device_mtu should be IntOpt,network_device_mtu should be IntOpt
518,1257014,neutron,a3c944bb1860faeb4eb0e90534ace07c1e869ea1,0,0, Atomically setup OVS ports  - Feature,Atomically setup OVS ports,"Normal operation of the openvswitch-agent results in various warning and error messages being logged to /var/log/openvswitch/ovs-vswitchd.log, which, although not representing actual failures, can cause concern.
For example, setting up the patch port connecting br-int and br-tun results in:
2013-12-02T16:35:45Z|00103|netdev_vport|ERR|patch-tun: patch type requires valid 'peer' argument
2013-12-02T16:35:45Z|00104|bridge|WARN|could not configure network device patch-tun (Invalid argument)
2013-12-02T16:35:45Z|00105|netdev_vport|ERR|patch-tun: patch type requires valid 'peer' argument
2013-12-02T16:35:45Z|00106|bridge|WARN|could not configure network device patch-tun (Invalid argument)
2013-12-02T16:35:45Z|00107|bridge|INFO|bridge br-int: added interface patch-tun on port 1
2013-12-02T16:35:46Z|00108|netdev_linux|WARN|ethtool command ETHTOOL_GFLAGS on network device patch-int failed: No such device
2013-12-02T16:35:46Z|00109|dpif|WARN|system@ovs-system: failed to add patch-int as port: No such device
2013-12-02T16:35:46Z|00110|netdev_vport|ERR|patch-int: patch type requires valid 'peer' argument
2013-12-02T16:35:46Z|00111|bridge|WARN|could not configure network device patch-int (Invalid argument)
2013-12-02T16:35:46Z|00112|bridge|INFO|bridge br-tun: added interface patch-int on port 1
And setting up a tunnel port on br-tun results in:
2013-12-02T16:35:48Z|00113|netdev_linux|WARN|ethtool command ETHTOOL_GFLAGS on network device gre-1 failed: No such device
2013-12-02T16:35:48Z|00114|dpif|WARN|system@ovs-system: failed to add gre-1 as port: No such device
2013-12-02T16:35:49Z|00115|netdev_vport|ERR|gre-1: gre type requires valid 'remote_ip' argument
2013-12-02T16:35:49Z|00116|bridge|WARN|could not configure network device gre-1 (Invalid argument)
2013-12-02T16:35:49Z|00117|bridge|INFO|bridge br-tun: added interface gre-1 on port 2
These messages seem to be due to the neutron.agent.linux.ovs_lib.add_patch_port() and neutron.agent.linux.ovs_lib.add_tunnel_port() functions setting DB attributes one-by-one, which results in invalid intermediate states. In addition to the noise in the log files, attempts to use the ports while in these intermediate states could result in transient incorrect behavior.
This can be resolved by changing these functions to atomically create and configure the OVS ports using a single ovs-vsctl command, with multiple sub-commands separated by ""--"" as arguments. This would also have the benefit of reducing the overhead of executing multiple commands via rootwrap when a single command would do."
519,1257038,nova,f024b5f9fa0c6b772c597298792b94d01c075384,0,0,A better solution should be,Bug #1257038 “VMware,"Currently the VMware Nova Driver relies on the VM name in vCenter/ESX to match the UUID in Nova. The name can be easily edited by vCenter administrators and break Nova administration of VMs. A better solution should be found allowing the Nova Compute Driver for vSphere to look up VMs by a less volatile and publicly visible mechanism.
EDIT:
A fix would make the link between vSphere and Nova more solid and involve using a vSphere metadata value that cannot be easily edited. Currently the UUID is stored as an extra config metadata property inside vSphere (associated with the instance's virtual-machine) and
this value is not easy to accidentally change. That would make the link much more robust."
520,1257068,cinder,5896966d40884a7bdba2878386ef8f2966d08d1a,0,0,tests,Use assertAlmostEqual instead of failUnlessAlmostE...,"The method failUnlessAlmostEqual has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using failUnlessAlmostEqual therefore we should use assertAlmostEqual instead."
521,1257119,neutron,b6e23caddf05457b2337ac03d33b066a36aad154,1,1,The id parameter is missing in… Typo?,PLUMgrid plugin is missing id in update_floatingip...,"In PLUMgrid plugin, the update_floatingip is missing id as shown above:
def update_floatingip(self, net_db, floating_ip):
        self.plumlib.update_floatingip(net_db, floating_ip, id)"
522,1257135,nova,71d900543217b8fe1c9cdc93f1a6503b3ea9c7c5,0,0,Not an error. Just in case.,Make sure “volumeId,"We should check whether the para ""volumeId""  is in request body."
523,1257146,nova,d5ff12a700495ba6b685cae08a0a8f017e3a976a,0,0, Need to handle…,Need to handle InstanceUserDataMalformed in create...,"This patch points out the need for this in the v2 API and the need for a test in the v3 API to cover InstanceUserDataMalformed for the server create call.
https://review.openstack.org/#/c/54202/6/nova/api/openstack/compute/servers.py
It was out of scope to add/cover that in that patch, so reporting it as a bug to keep track of it."
524,1257151,nova,f045a5dfa66aa46e11d3302ee707f067e86c9b59,0,0,Typo in docs,Fix docstring on SnapshotController,"""The Volumes API controller for the OpenStack API.""  is inappropriate for SnapshotController."
525,1257168,nova,fb9b60fb5d3084d4cab29936fd1f534e5599f231,1,1,Race condition,Bug #1257168 “Cells,"Getting the following error when trying to unlock an instance
2013-12-03 15:14:31.198 31688 DEBUG nova.compute.api [req-266c1f02-b77c-440a-8983-948f6ab2357c 671dcaba8087487c8a28afe42b6672fa e4eee8dbc16a49dcbc76edac96674e96] [instance: fb468a1d-2e64-4560-850d-31a5fd698305] Unlocking unlock /usr/lib/python2.7/dist-packages/nova/compute/api.py:2612
2013-12-03 15:14:31.200 31688 ERROR nova.cells.messaging [req-266c1f02-b77c-440a-8983-948f6ab2357c 671dcaba8087487c8a28afe42b6672fa e4eee8dbc16a49dcbc76edac96674e96] Error processing message locally: Object '<Instance at 0x5948a90>' is already attached to session '1247' (this is '1248')
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging Traceback (most recent call last):
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 205, in _process_locally
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 1476, in _process_message_locally
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 708, in run_compute_api_method
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return self._run_api_method(message, method_info, fn)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 702, in _run_api_method
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return fn(message.ctxt, *args, **method_info['method_kwargs'])
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 198, in wrapped
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 2615, in unlock
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging instance.save()
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/models.py"", line 52, in save
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging session.add(self)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1399, in add
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._save_or_update_state(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1411, in _save_or_update_state
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._save_or_update_impl(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1667, in _save_or_update_impl
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._update_impl(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1661, in _update_impl
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._attach(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1749, in _attach
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging state.session_id, self.hash_key))
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging InvalidRequestError: Object '<Instance at 0x5948a90>' is already attached to session '1247' (this is '1248')
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging"
526,1257198,cinder,3b2842bf2df46daf4e4d26695c06963499968181,0,0,it's necessary to unify all ,All Cinder Public API controllers should inherit f...,"Most of the Cinder Public API Controllers  inherit from wsgi.Controller, but still some Cinder Public APIs controllers directly inherit from object. So it's necessary to unify all the Cinder Public API Controllers inherit from wsgi.Controller."
527,1257222,nova,1cd59abef5e9947fa8a98932d71a6b3aeae88ae5,0,0,test,Fix errors in test_server_start_stop.py,"There are incorrect body dict for stop actions. Such as:
def test_stop_not_ready(self):
        self.stubs.Set(db, 'instance_get_by_uuid', fake_instance_get)
        self.stubs.Set(compute_api.API, 'stop', fake_start_stop_not_ready)
        req = fakes.HTTPRequest.blank('/v2/fake/servers/test_inst/action')
        body = dict(start="""")
        self.assertRaises(webob.exc.HTTPConflict,
            self.controller._stop_server, req, 'test_inst', body)"
528,1257225,neutron,db5ea5c705757ca20a5789839f96518877c91820,1,0,Bug in a duplicate name. Evolution of the project?,Bug #1257225 “duplicate name of LBaaS obj is not allowed on vShi... ,"Duplicate name of LBaaS obj(pool/vip/app_profile) is not allowed on vShield Edge, so here need a naming convention to ensure name uniqueness on the edge side."
529,1257234,glance,720d5abfc56e4cfdbc135400f4799c2d1d8cd8c3,0,0,Add logs. Add a feature,HTTP Store is hiding errors from the remote locati...,"If the remote location fails to return the image and raises an error, the http store doesn't show it in the logs, which makes debugging very difficult:
https://github.com/openstack/glance/blob/master/glance/store/http.py
        # Check for bad status codes
        if resp.status >= 400:
            reason = _(""HTTP URL returned a %s status code."") % resp.status
            raise exception.BadStoreUri(loc.path, reason)"
531,1257274,cinder,aa453576ff13c97c63dcb3cd5941675ffdc93360,1,0,Issue of version,Bump hacking to 0.8,"Due to Bump hacking dependency is not the 0.8, some compatibility checks with python 3.x are not being done on gate and it is bringing code issues."
532,1257282,glance,2eca65eb5febae59eaca9e949ecd6a6f9492350a,1,0,Issue of version,Bump hacking to 0.8,"Due to Bump hacking dependency is not the 0.8, some compatibility checks with python 3.x are not being done on gate and it is bringing code issues."
533,1257293,cinder,cbd7acfc294c4bf2ae0014bcf04646382bed469d,1,0,This bug affects only Topology  Version 2,[messaging] QPID broadcast RPC requests to all ser...,"According to the oslo.messaging documentation, when a RPC request is made to a given topic, and there are multiple servers for that topic, only _one_ server should service that RPC request.  See http://docs.openstack.org/developer/oslo.messaging/target.html
""topic (str) – A name which identifies the set of interfaces exposed by a server. Multiple servers may listen on a topic and messages will be dispatched to one of the servers in a round-robin fashion.""
In the case of a QPID-based deployment using topology version 2, this is not the case.  Instead, each listening server gets a copy of the RPC and will process it.
For more detail, see
https://bugs.launchpad.net/oslo/+bug/1178375/comments/26"
535,1257354,neutron,901d676b8bcbb3e731ba44a4e574e9ae998f486e,1,0,"Since the old L3 mixin has been moved as a service plugin, the metering service plugin doesn't respect anymore…",Metering doesn't anymore respect the l3 agent bind...,"Since the old L3 mixin has been moved as a service plugin, the metering service plugin doesn't respect anymore the l3 agent binding. So instead of using the cast rpc method it uses the fanout_cast method."
536,1257355,nova,c61c67c5afd100749b7b55b251bd4e4e3bb556a2,1,1, live migration fails when using non-image backed disk ,live migration fails when using non-image backed d...,"running live migration with --block-migrate fails if the disk was resized before (aka detached from the cow image). This is because nova.virt.libvirt.driver.py uses disk_size, not virt_disk_size for re-creating the qcow2 file on the destination host. in the case of qcow2 files, qemu-img however needs to get the virt_disk size passed down, otherwise the block migration step will not be able to convert all blocks."
537,1257467,neutron,794a1325dfc8a6937412a597f19acecf08d4b34a,0,0,feature,extra_dhcp_opts allows empty strings,"the extra_dhcp_opts allows empty string '      ', as an option_value, which dnsmasq has been detected ans segment faulting when encountering a  tag:ece4c8aa-15c9-4f6b-8c42-7d4e285734bf,option:server-ip-address, option in the opts file.
Checks are need in the create and update protions of the extra_dhcp_opts extension to prevent this."
538,1257496,glance,5ec9baa8857646e0608cd9a3135d172f3b92327c,1,1,Creating image with bad scheme in location causes 500,Bug #1257496 “Glance v1,"When creating an image in glance v1 and specifyig the location with a bad scheme you receive an HTTP 500. In this case 'http+swift"" is a bad scheme.
glance image-create --name bad-location --disk-format=vhd --container-format=ovf --location=""http+swift://bah""
Request returned failure status.
HTTPInternalServerError (HTTP 500)
2013-12-03 21:24:32.009 6312 INFO glance.wsgi.server [402e831a-935d-4e14-b4c8-64653c14263d 1c3848b015f94b70866e
a33fa52945f0 54bc4959075343ff80f460b77e783a49] Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/wsgi.py"", line 389, in handle_one_response
    result = self.application(self.environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 581, in __call__
    return self.app(env, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/lib/python2.7/dist-packages/paste/urlmap.py"", line 203, in __call__
    return app(environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
    response = self.app(environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 599, in __call__
    request, **action_args)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 618, in dispatch
    return method(*args, **kwargs)
  File ""/opt/stack/glance/glance/common/utils.py"", line 422, in wrapped
    return func(self, req, *args, **kwargs)
  File ""/opt/stack/glance/glance/api/v1/images.py"", line 754, in create
    image_meta = self._reserve(req, image_meta)
  File ""/opt/stack/glance/glance/api/v1/images.py"", line 488, in _reserve
    store = get_store_from_location(location)
  File ""/opt/stack/glance/glance/store/__init__.py"", line 263, in get_store_from_location
    loc = location.get_location_from_uri(uri)
  File ""/opt/stack/glance/glance/store/location.py"", line 73, in get_location_from_uri
    raise exception.UnknownScheme(scheme=pieces.scheme)
UnknownScheme: Unknown scheme 'http+swift' found in URI
2013-12-03 21:24:32.011 6312 INFO glance.wsgi.server [402e831a-935d-4e14-b4c8-64653c14263d 1c3848b015f94b70866ea33fa52945f0 54bc4959075343ff80f460b77e783a49] localhost - - [03/Dec/2013 21:24:32] ""POST /v1/images HTTP/1.1"" 500 139 0.216952"
539,1257607,neutron,51ef6a1dc30576f31a8a120066d81feedd0bf224,1,1,Typo in code,Mistake in usage drop_constraint parameters,"In miration e197124d4b9_add_unique_constrain mistake in usage drop_constraint parameter type_ and positional agruments name
and table_name.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/e197124d4b9_add_unique_constrain.py"", line 64, in downgrade
    type='unique'
TypeError: drop_constraint() takes at least 2 arguments (1 given)
The same mistake was already fixed in miration 63afba73813_ovs_tunnelendpoints_id_unique."
540,1257661,nova,2e758452ddcb8871f7dbc6c859e3414923cd18bc,0,0, should handle boolean string parameters,should handle boolean string parameters through mi...,"If specifying false string (""False"") as ""block_migration"" parameter of migrate_live API like the following, nova considers it as True.
$ curl -i 'http://10.21.42.81:8774/v2/[..]/servers/[..]/action' -X POST [..] -d '{""os-migrateLive"": {""disk_over_commit"": ""False"", ""block_migration"": ""False"", ""host"": ""localhost""}}'
On the other hand, nova can consider false string as false in the case of ""on_shared_storage"" parameter of evacuate API.
That behavior seems API inconsistency."
541,1257726,nova,b48bd3c0cf1f4ccfada24a6aebc7ced308d44927,0,0,Refactoring code,Bug #1257726 “VMware,"Recently I have been doing some queries for extraConfig VM options and found that the most efficient way to retrieve a given property is to do:
session._call_method(vim_util, 'get_dynamic_property', vm_ref, 'VirtualMachine', 'config.extraConfig[""some_prop_here""]')
Right now we ask for all extraConfig options and then we iterate over the result set to find a particular one."
542,1257815,neutron,89f25623a4345b5044a58cff36e7fd103c6b88e8,1,1,,Internal server error while deleting subnet(can no...,"when trying to delete a subnet, sometimes the following error comes out.
Icehouse, Database  in use is DB2, but I guess it might happen for other databases too.
================================
2013-12-04 03:49:48.275 26604 TRACE neutron.plugins.ml2.plugin
2013-12-04 03:49:48.277 26604 ERROR neutron.api.v2.resource [req-e8e78c50-25b0-4e19-b5f0-796041d7b464 f53f4f5b40154ad6b1ec1ac08f88ecf2 b93ff0
8b44da407185a26033768101f5] NT-C3C9C57 delete failed
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 443
, in delete_network
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     self.delete_subnet(context, subnet.id)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 530, in delete_subnet
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     break
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 449,in __exit__
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     self.commit()
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 361, in commit
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     self._prepare_impl()
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 340,in _prepare_impl
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     self.session.flush()
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 545, in _wrap
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     raise exception.DBError(e)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource DBError: (Error) ibm_db_dbi::Error:
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource Error 1: [IBM][CLI Driver][DB2/LINUXX8664] SQL0100W  No row was found for FETCH,UPDATE or DELETE; or the result of a query is an empty table.  SQLSTATE=02000 SQLCODE=100
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource  'DELETE FROM ipavailabilityranges WHERE ipavailabilityranges.allocation_pool_id= ? AND ipavailabilityranges.first_ip = ? AND ipavailabilityranges.last_ip = ?' (('e376f33d-a224-4468-91eb-82191e158726', '10.100.0.2', '10.100.0.2'), ('e376f33d-a224-4468-91eb-82191e158726', '10.100.0.4', '10.100.0.14'))"
544,1257940,nova,b18b2333a98889f6a4a2fd0e859b466116ce4aac,0,0,Add debugs,Exceptions in _heal_instance_info_cache should be ...,If an exception occurs in _heal_instance_info_cache the actual stack trace is not shown and is hard to track down something is failing as it's log level is debug.
545,1258000,cinder,62617e656fe5895de30ce55971b07680f7b0a6b2,0,0,Typo in docstring,Fix docstring for snapshot metadata Controller,"""The volume metadata API controller for the OpenStack API."" is inappropriate description for snapshot metadata controller."
546,1258004,cinder,b91e6e31ece12d5b1204680aa0a912e3b9569f3b,1,1,Incorrect response,The response is incorrect while creating volume me...,"1.Create metadata for a volume with the request body:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
2.The server accept it, and return the response body, as bellow:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
3.Get the metadata of the volume, the server returned:
{""metadata"":{
             ""key1"": ""value1""
}}
4.I find that case ignore in cinder, so the server just add one metadata for the volume
5.So i think the response body should return the one which the server added when create action:
  {""metadata"":{
             ""key1"": ""value1""
}}"
547,1258033,cinder,dd9536ac6e6df0e1ae6754e580b99cfbfd05eb77,0,0,Evolution: should use optional key value pairs for adding metadata,Use optional key-value pair metadata to track Open...,"Currently, when a volume is created on the 3PAR, metadata is added onto “comment” section onto the 3PAR which can be modified by users directly from management console. Hence for any new driver related tasks, to avoid manipulation from user, the 3PAR driver should use optional key value pairs for adding metadata. Comments section would be mainly used for informational purposes. All metadata keys should start with a custom prefix for ease of use."
548,1258048,nova,d9237dff7ace30dab331151c736d7cca97618e04,0,0,Typo in docstring,Wrong monkey_patch docstring  in nova/utils.py,"""Flags.monkey_patch"" should be ""CONF.monkey_patch"" in the docstring of monkey_patch function in nova/utils.py"
549,1258065,neutron,c011b7668f83b25f1b8e026a909243af96f8f85c,1,1,,The implementation of utils.str2dict fails to conv...,"from neutron.common import utils
print utils.str2dict('inside_addr=10.0.1.2,inside_port=22,outside_addr=172.16.0.1,outside_port=2222,protocol=tcp')
returns
{'inside_addr': '10.0.1.2', 'inside_port': '22,outside_addr=172.16.0.1,outside_port=2222,protocol=tcp'}
expected value should be
{'outside_port': '2222', 'inside_addr': '10.0.1.2', 'protocol': 'tcp', 'inside_port': '22', 'outside_addr': '172.16.0.1'}
The reason is that in the third line of the implementation below,  string.split(',', 1) only splits out two key-value pairs.
quote from neutron/common/utils.py:181:
def str2dict(string):
    res_dict = {}
    for keyvalue in string.split(',', 1):
        (key, value) = keyvalue.split('=', 1)
        res_dict[key] = value
    return res_dict
a quick fix might be remove "",1"" from string.split. But it turns out that str2dict/dict2str may also fail when input values containing characters like '=' or ','. A better fix might be using json encode/decode to deal with it."
550,1258100,cinder,7aabee6f76fb390faac68f8195dab4018917bae4,1,1,Typo in help,Time unit is missing in the description of option ...,"Time unit is missing in the description of option ""vmware_task_poll_interval"" in cinder.conf.sample. This description is used in the auto-generated config reference at http://docs.openstack.org/trunk/config-reference/content/vmware-vmdk-driver.html"
551,1258103,nova,e8c86fc82df85a87ee7ddec0db633567924a4369,0,0,Unused code,Unused imports in libvirt fake utilities,Removed unused imports and assignments
552,1258128,cinder,69ce114232f9102220b45c8242eda3e78872a3e6,1,1,Wrong arg position,LVM brick instantiation fails on volume_migration,"The lvm driver uses positional arguments to create a new instance for the LVM brick. Some of those arguments are being passed in the wrong position. Kwargs should be used in this case instead.
http://git.openstack.org/cgit/openstack/cinder/tree/cinder/volume/drivers/lvm.py#n711"
553,1258150,neutron,2aaec3a81148e6c0cefd8f5b989dbcecf872a680,1,1,deadlock,Bug #1258150 “nicira,"when parallel tempest tests are enabled, the nicira plugin shows erros when deleting routers.
Parallel operations indeed cause the usual eventlet/mysql deadlock in delete_router as the nvp operation is nested within the db transaction.
The root cause for the deadlock is that the nvp api client uses eventlet to dispatch requests.
while a solution might be to rework the API client, an easier, backportable solution would be to move the NVP operation out of the transaction and ensuring consistency in case of failure.
note: in the same delete_router routine also the metada access network handling should be moved out of the transaction."
554,1258166,nova,7dd4d98c2873e1868296ef973f0be42cadcb6d6d,0,0,Recomendation,N310 check recommends function that doesn't exist,"Check N310 can return ""N310  timeutils.now() must be used instead of datetime.now()"", but timeutils.now() does not exist. Only utcnow() does."
555,1258179,nova,933603ed8523493d0693f02f62fef6d427de421f,1,1,Scalability scenario,Bug #1258179 “VMware,"When there are 100's of VM deployed there are problems with nova compute. This is due to the fact that each interaction with the VM;s via get_vm_ref reads all of the VM's ont he system and then filters by the UUID. The filtering is done on the client side.
There are specific API's that optimize this search - http://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.wssdk.apiref.doc%2Fvim.SearchIndex.html more specifically FindAllByUuid"
556,1258203,cinder,17912f4c4fba270a15cab47bb68d9e30c34a69b0,1,1,wrong check,migrate_volume dest_vg existence is wrong,"if dest_vg != self.vg.vg_name:
        vg_list = volutils.get_all_volume_groups()
            vg_dict = \
                (vg for vg in vg_list if vg['name'] == self.vg.vg_name).next()
`vg['name'] == self.vg.vg_name` should be `vg['name'] == dest_vg`
https://git.openstack.org/cgit/openstack/cinder/tree/cinder/volume/drivers/lvm.py?h=stable/havana#n703"
557,1258275,nova,16dfff5dedffcf4645df3c13b623d1ecd7560d8b,1,1,,Migration record for resize not cleared if excepti...,"Testing on havana.
prep_resize() calls resource tracker's resize_claim() which creates a migration record. This record is cleared during the rt.drop_resize_claim() from confirm_resize() or revert_resize(), however if an exception is thrown before one of these is called or after, but before they clean up the migration record, then the migration record will hang around in the database indefinitely.
This results in an WARNING being logged every 60 seconds for every resize operation that ended with the instance in ERROR state as part of the update_available_resource period task, like the following:
2013-12-04 17:49:15.247 25592 WARNING nova.compute.resource_tracker [req-75e94365-1cca-4bca-92a7-19b2c62b9551 e4857f249aec4160bfa19c12eb805a96 a42cfb9766bf41869efab25703f5ce7b] [instance: 12d2551a-6403-4100-ba57-0995594c9c93] Instance not resizing, skipping migration.
This message is because the resource tracker's _update_usage_from_migrations() logs this warning if a migration record for an instance is found, but the instance's current state is not in a resize state.
These messages will be permanent in the logs even after the instance in question's state is reset, and even after a successful resize has occurred on that instance. There is no way to clean up the old migration record at this point.
It seems like there should be some handling when an exception occurs during resize, finish_resize, confirm_resize, revert_resize, etc. that will drop the resize claim, so the claim and migration record do not persist indefinitely."
558,1258331,glance,0d95e5316a99b78b4f629a29d862548f87671b43,0,0,New scenario. Evolution,Bug #1258331 “Glance v2,"Glance v2 image property quota enforcement can be unintuitive.
There are a number of reasons an image may have more properties than the image_propery_quota allows. E.g. if the quota is lowered or when it is first created. If this happens then any request to modify an image must result in the image being under the new quota. This means that even if the user is removing quotas they can still get an 413 overlimit from glance if the result would still be over the limit.
This is not a great user experience and is unintuitive. Ideally a user should be able to remove properties or any other action except for adding a property when they are over their quota for a given image."
559,1258360,nova,10004672ad1476c55deaad53684a50358da6f656,0,0,removes an unneeded call. Refactoring,Remove unneeded call to conductor in network inter...,Remove unneeded call to conductor in network interface
560,1258379,neutron,3e263ba6c58c43f71c02417f8948300e8cb5c462,0,0,Evolution: router must have gateway interface set ,vpnservice's router must have gateway interface se...,"at line
https://github.com/openstack/neutron/blob/master/neutron/services/vpn/service_drivers/ipsec.py#L172
it is obvious the router must have gateway interface set  then it can be used as vpnservce router."
561,1258438,neutron,6c2bec5d9e40c68c75cc56e077e2af97d1359d20,1,1,,Can't create a firewall for admin tenant when at l...,"Only one firewall is allowed per tenant. This works as expected for non-admin tenants.
When a new firewall is added in the context of admin, this fails if some other tenant already has a firewall. This is because 'get_firewall_count' returns sum of all firewalls in the system. Addition of a new firewall for admin fails with the following error message.
500-{u'NeutronError': {u'message': u'Exceeded allowed count of firewalls for tenant tenant-2. Only one firewall is supported per tenant.', u'type': u'FirewallCountExceeded', u'detail': u''}}
fwaas_plugin.py
----------------
def create_firewall(self, context, firewall):
        LOG.debug(_(""create_firewall() called""))
        tenant_id = self._get_tenant_id_for_create(context,
                                                   firewall['firewall'])
        fw_count = self.get_firewalls_count(context)
        if fw_count:
            raise FirewallCountExceeded(tenant_id=tenant_id)
----------------
=> fw_count = self.get_firewalls_count(context)
In the context of admin, the function counts other tenant's firewall."
562,1258620,nova,46922068ac167f492dd303efb359d0c649d69118,0,0,Evolution: Make network_cache more robust with neutron ,Make network_cache more robust with neutron,"Currently, the network cache assumes neutron is the source of truth for which interfaces are actually attached to an instance. This is not actually correct as nova is really the source of truth here. In order to demonstrate this issue  if one creates multiple ports in neutron that match the same device_id/instance_id as instances in nova those ports will show up in nova list  even though they are not part of the instance."
563,1258629,neutron,b67b20832a5bfccd1bbf8d1e63ebcd7061856881,0,0,Obsolete code,remove dead code  _arp_spoofing_rule(),remove dead code  _arp_spoofing_rule()
564,1259088,neutron,f691ebe03916a78cbf18017d628a28b17f147700,0,0,the dispatch MAYBE dispatch the rpc message to an unready agent,setup_rpc should be the last thing in __init__ met...,"if setup_rpc is too early, the dispatch maybe dispatch the rpc message to an unready agent.  take ovs plugin agent for instance,
after setup_rpc is called, many of the initialization work are still needed to be done. If the message is coming during this time, the instance will  not be fully initialized:
    def __init__(self, integ_br, tun_br, local_ip,
                 bridge_mappings, root_helper,
                 polling_interval, tunnel_types=None,
                 veth_mtu=None, l2_population=False,
                 minimize_polling=False,
                 ovsdb_monitor_respawn_interval=(
                     constants.DEFAULT_OVSDBMON_RESPAWN)):
        '''Constructor.
        :param integ_br: name of the integration bridge.
        :param tun_br: name of the tunnel bridge.
        :param local_ip: local IP address of this hypervisor.
        :param bridge_mappings: mappings from physical network name to bridge.
        :param root_helper: utility to use when running shell cmds.
        :param polling_interval: interval (secs) to poll DB.
        :param tunnel_types: A list of tunnel types to enable support for in
               the agent. If set, will automatically set enable_tunneling to
               True.
        :param veth_mtu: MTU size for veth interfaces.
        :param minimize_polling: Optional, whether to minimize polling by
               monitoring ovsdb for interface changes.
        :param ovsdb_monitor_respawn_interval: Optional, when using polling
               minimization, the number of seconds to wait before respawning
               the ovsdb monitor.
        '''
        self.veth_mtu = veth_mtu
        self.root_helper = root_helper
        self.available_local_vlans = set(xrange(q_const.MIN_VLAN_TAG,
                                                q_const.MAX_VLAN_TAG))
        self.tunnel_types = tunnel_types or []
        self.l2_pop = l2_population
        self.agent_state = {
            'binary': 'neutron-openvswitch-agent',
            'host': cfg.CONF.host,
            'topic': q_const.L2_AGENT_TOPIC,
            'configurations': {'bridge_mappings': bridge_mappings,
                               'tunnel_types': self.tunnel_types,
                               'tunneling_ip': local_ip,
                               'l2_population': self.l2_pop},
            'agent_type': q_const.AGENT_TYPE_OVS,
            'start_flag': True}
        # Keep track of int_br's device count for use by _report_state()
        self.int_br_device_count = 0
        self.int_br = ovs_lib.OVSBridge(integ_br, self.root_helper)
        self.setup_rpc()
        self.setup_integration_br()
        self.setup_physical_bridges(bridge_mappings)
        self.local_vlan_map = {}
        self.tun_br_ofports = {constants.TYPE_GRE: {},
                               constants.TYPE_VXLAN: {}}
        self.polling_interval = polling_interval
        self.minimize_polling = minimize_polling
        self.ovsdb_monitor_respawn_interval = ovsdb_monitor_respawn_interval
        if tunnel_types:
            self.enable_tunneling = True
        else:
            self.enable_tunneling = False
        self.local_ip = local_ip
        self.tunnel_count = 0
        self.vxlan_udp_port = cfg.CONF.AGENT.vxlan_udp_port
        self._check_ovs_version()
        if self.enable_tunneling:
            self.setup_tunnel_br(tun_br)
        # Collect additional bridges to monitor
        self.ancillary_brs = self.setup_ancillary_bridges(integ_br, tun_br)
        # Security group agent supprot
        self.sg_agent = OVSSecurityGroupAgent(self.context,
                                              self.plugin_rpc,
                                              root_helper)
        # Initialize iteration counter
        self.iter_num = 0"
565,1259144,neutron,1763c80711993c55f4f13afe56f449b1dd6d3d3a,0,0,Delete duplicated code,Bug #1259144 “bigswitch,"Check if network is in use is not needed in bigswitch plugin:
https://github.com/openstack/neutron/blob/master/neutron/plugins/bigswitch/plugin.py#L587-L599
as it is done by db_base_plugin, which is called right after:
https://github.com/openstack/neutron/blob/master/neutron/plugins/bigswitch/plugin.py#L601"
566,1259183,nova,6037b9cc20a8ab379855370394e108912ea3a0f2,1,1,Bug…  Solution: Ensure api_paste_conf is an absolute path,wsgi.Loader should ensure the config_path is absol...,"nova-api service will fail to start when the nova-api command is invoked from a directory containing a file with the same name as the one specified in the configuration key 'api_paste_config' if it is not an absolute path (the default is 'api-paste.ini').
[fedora@devstack1 devstack]$ pwd
/home/fedora/devstack
[fedora@devstack1 devstack]$ grep api_paste_conf /etc/nova/nova.conf
api_paste_config = api-paste.ini
[fedora@devstack1 devstack]$ touch api-paste.ini
[fedora@devstack1 devstack]$ nova-api
2013-12-09 09:18:40.082 DEBUG nova.wsgi [-] Loading app ec2 from api-paste.ini from (pid=4817) load_app /opt/stack/nova/nova/wsgi.py:485
2013-12-09 09:18:40.083 CRITICAL nova [-] Cannot resolve relative uri 'config:api-paste.ini'; no relative_to keyword argument given
2013-12-09 09:18:40.083 TRACE nova Traceback (most recent call last):
2013-12-09 09:18:40.083 TRACE nova   File ""/usr/bin/nova-api"", line 10, in <module>
2013-12-09 09:18:40.083 TRACE nova     sys.exit(main())
2013-12-09 09:18:40.083 TRACE nova   File ""/opt/stack/nova/nova/cmd/api.py"", line 49, in main
2013-12-09 09:18:40.083 TRACE nova     max_url_len=16384)
2013-12-09 09:18:40.083 TRACE nova   File ""/opt/stack/nova/nova/service.py"", line 308, in __init__
2013-12-09 09:18:40.083 TRACE nova     self.app = self.loader.load_app(name)
2013-12-09 09:18:40.083 TRACE nova   File ""/opt/stack/nova/nova/wsgi.py"", line 486, in load_app
2013-12-09 09:18:40.083 TRACE nova     return deploy.loadapp(""config:%s"" % self.config_path, name=name)
2013-12-09 09:18:40.083 TRACE nova   File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 247, in loadapp
2013-12-09 09:18:40.083 TRACE nova     return loadobj(APP, uri, name=name, **kw)
2013-12-09 09:18:40.083 TRACE nova   File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 271, in loadobj
2013-12-09 09:18:40.083 TRACE nova     global_conf=global_conf)
2013-12-09 09:18:40.083 TRACE nova   File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 296, in loadcontext
2013-12-09 09:18:40.083 TRACE nova     global_conf=global_conf)
2013-12-09 09:18:40.083 TRACE nova   File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 308, in _loadconfig
2013-12-09 09:18:40.083 TRACE nova     ""argument given"" % uri)
2013-12-09 09:18:40.083 TRACE nova ValueError: Cannot resolve relative uri 'config:api-paste.ini'; no relative_to keyword argument given
2013-12-09 09:18:40.083 TRACE nova"
567,1259267,nova,709410d243a97d35c3da314b41bab039eac75736,1,1,,Bug #1259267 “Nova Docker,"I was playing around with cloud-init. I wanted to use cloud init as substitute for the missing environment variables feature.
The basic idea is to define variables in the user data and inject the them before starting the service.
Following:
docker run -e ""MY_Variable=MyValue"" -d centos
would look like in openstack:
nova boot --image centos:latest --user-data ""MY_Variable=MyValue"" myinstance
Unfortunately does the metadata service not work inside of a docker container. After some testing I figured out that the reason for that is that a container uses as default gateway the docker network (docker ip address). The metadata service simple rejects the call since the IP address of docker container is not associated with the nova instance.
Note: The metadata service itself can be accessed (http://169.254.169.254) but it is not possible to access the actual data (http://169.254.169.254/2009-04-04 - Status 404)
I was able to work around the issue by simply changing the route inside the container:
# Hack: In order to receive data from the metadata service we must make sure we resolve the data via our nova network.
#
# A docker container in openstack has two NICs.
# - eth0 has a IP address on the docker0 bridge which is usually an e.g. 172.0.0.0 IP address.
# - pvnetXXXX is a IP address assigned by nova.
#
# Extract the NIC name of the nova network.
#
NOVA_NIC=$(ip a | grep pvnet | head -n 1 | cut -d: -f2)
while [ ""$NOVA_NIC"" == """" ] ; do
   echo ""Find nova NIC...""
   sleep 1
   NOVA_NIC=$(ip a | grep pvnet | head -n 1 | cut -d: -f2)
done
echo ""Device $NOVA_NIC found. Wait until ready.""
sleep 3
# Setup a network route to insure we use the nova network.
#
echo ""[INFO] Create default route for $NOVA_NIC. Gateway 10.0.0.1""
ip r r default via 10.0.0.1 dev $NOVA_NIC
# Shutdown eth0 since icps will fetch enabled enterface for streaming.
ip l set down dev eth0
This approach is obviously a poor solution since it has certain expectation of the network.
Another solution might be extend the docker driver to add a firewall rule which will masquerade requests on 169.254.169.254 with the actual nova network IP address
I third solution would need improvements in docker. If docker would have a network mode which allows to assign the IP from outside this issue would be solved. That of course is a just which must be accepted by the docker community.
Network Threads:
- https://groups.google.com/forum/#!topic/docker-dev/YfCeX8TBweA
Simple script to test metadata inside the container:
#!/bin/bash
status=$(curl -I -s -o /dev/null -w ""%{http_code}"" http://169.254.169.254/2009-04-04)
while [ $status != '200' ]; do
   echo ""Cannot access metadata, status: '$status', try again...""
   date # easier to see in docker logs that loop is still running.
   status=$(curl -I -s -o /dev/null -w ""%{http_code}"" http://169.254.169.254/2009-04-04)
   sleep 1
done
echo ""Yes we got some user data:""
curl http://169.254.169.254/2009-04-04"
568,1259279,cinder,a83506d170e90b111b7e63fab9f585883971f316,0,0,Improve the logs,Failed stats update doesn't include driver name,"When a driver fails to initialize at startup, the get_volume_stats() will always fail.  The problem is that the log file doesn't include the driver name in the update.
""WARNING cinder.volume.manager [-] Unable to update stats, driver is uninitialized"""
569,1259336,cinder,bc16517ac7351b715d0202f7beb5a5e0d7ff8f97,1,1,Missing exception,Unhandled exception when “None,"Currently the extend method in the cinder volume_actions extension checks for KeyError and ValueError, however the negative tests in Tempest include passing None in to the new_size which results in an unhandled trace in the test logs.
Add TypeError to the list of exceptions we look for."
570,1259431,neutron,ef42f7c3f982092be6b5199bd9c2ade69e0446f3,0,0,Variable for agent instance should be named as 'agent' instead of plugin,plugin variable name should be agent,"plugin = LinuxBridgeNeutronAgentRPC(interface_mappings,    ------> agent =
                                        polling_interval,
                                        root_helper)
    LOG.info(_(""Agent initialized successfully, now running... ""))
    plugin.daemon_loop()    -----> agent.daemon_loop()"
571,1259646,neutron,ca668a0b9ec5f6412cd842b89d523ba1a8369782,0,0,Refactoring code,Clean up ML2 Manager,"Some things need cleanup in the ML2Manager.
1.) In the current ML2 Manager, we are using sys.exit(1) if the network_type isn't found in self.drivers:
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/managers.py#L70
Here we should probably throw an exception. When running unit test, if we hit this condition the unit tests will exit as well.
2.) We should also be mindful of using the reserved keyword 'type' and rename type in this case to something else:
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/managers.py#L49"
572,1259796,nova,5228d01bcde531cc2c141f38a150dbb2daef1c07,1,1,,nova-compute failed to start because get_host_capa...,"When starting nova-compute service on node with lxc hypervisor, it failed with the following error:
 2013-12-11 03:04:41.439 ERROR nova.openstack.common.threadgroup [-] can only parse strings
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 117, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     x.wait()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 49, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     return self.thread.wait()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     return self._exit_event.wait()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     return hubs.get_hub().switch()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 448, in run_service
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     service.start()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/service.py"", line 164, in start
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self.manager.pre_start_hook()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 822, in pre_start_hook
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self.update_available_resource(nova.context.get_admin_context())
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 4971, in update_available_resource
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     nodenames = set(self.driver.get_available_nodes())
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/driver.py"", line 980, in get_available_nodes
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     stats = self.get_host_stats(refresh=refresh)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4569, in get_host_stats
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     return self.host_state.get_host_stats(refresh=refresh)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 429, in host_state
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self._host_state = HostState(self)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4960, in __init__
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self.update_status()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4999, in update_status
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self.driver.get_instance_capabilities()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3702, in get_instance_capabilities
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     caps = self.get_host_capabilities()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2739, in get_host_capabilities
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self._caps.host.cpu.parse_str(features)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/config.py"", line 61, in parse_str
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self.parse_dom(etree.fromstring(xmlstr))
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""lxml.etree.pyx"", line 2993, in lxml.etree.fromstring (src/lxml/lxml.etree.c:62980)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""parser.pxi"", line 1614, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:92786)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup ValueError: can only parse strings
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup
This is because no cpu model info can be gotten from libvirt getCapabilities interface for lxc hypervisor."
573,1259818,glance,130781cb7b6fc1635838a7c3f19d340a0a846637,0,0,Remove unused exceptions ,Remove unused exceptions,"Removed the exception classes in the following because not referenced
anywhere.
-MissingArgumentError
-NotAuthorized"
574,1259867,cinder,92e058d6800e1cb33a0359cdc5c079b55612525c,1,1,,Raise KeyError exception while generating a WSGI r...,"1. Create snapshot metadata with a too long key('a'*260)
    On the _update_snapshot_metadata() in snapshot_metadata.py, it will raise HTTPRequestEntityTooLarge(413) exception in this case. So  the 413 exception expected to raise.  But we got the server fault(500)
2. The KeyError exception raised while generating a WSGI response based on the exception, because headers no 'Retry-After' attribute in this case.
   if code == 413:
            retry = self.wrapped_exc.headers['Retry-After']
            fault_data[fault_name]['retryAfter'] = retry"
575,1259965,neutron,c8ff2a10697303d8a04980d258a12449d12d587a,0,0,Feature: Need to make server return also members and monitors which are in pending states.,[LBaaS] Creation of a health monitor after creatin...,"Description
===========
After making the following steps:
1. Create a pool
2. Create members
3. Create a VIP
The attempt to create a health monitor and associate it with the created pool
leads to the situation when the health monitor is shown as active
but it is not being added to /opt/stack/data/neutron/lbaas/<pool_id>/conf:
# neutron lb-healthmonitor-show c6ddc6e3-4a91-4a81-a110-fc3a44f311a0
+----------------+------------------------------------
| Field          | Value
+----------------+------------------------------------
| admin_state_up | True
| delay          | 3
| expected_codes | 200
| http_method    | GET
| id             | c6ddc6e3-4a91-4a81-a110-fc3a44f311a0
| max_retries    | 3
| pools          | {""status"": ""ACTIVE"", ""status_description"": null, ""pool_id"": ""50497a9f-1439-431b-ac50-dd4cca0216fa""}
| tenant_id      | dcf66bfd01aa4b82b10d8fb263ef375d
| timeout        | 3
| type           | HTTP
| url_path       | /
+----------------+------------------------------------
# cat /opt/stack/data/neutron/lbaas/50497a9f-1439-431b-ac50-dd4cca0216fa/conf
global
        daemon
        user nobody
        group nogroup
        log /dev/log local0
        log /dev/log local1 notice
        stats socket /opt/stack/data/neutron/lbaas/50497a9f-1439-431b-ac50-dd4cca0216fa/sock mode 0666 level user
defaults
        log global
        retries 3
        option redispatch
        timeout connect 5000
        timeout client 50000
        timeout server 50000
frontend 5842ed27-02b9-499b-8512-bcf071a81ab2
        option tcplog
        bind 10.0.0.4:80
        mode http
        default_backend 50497a9f-1439-431b-ac50-dd4cca0216fa
        option forwardfor
backend 50497a9f-1439-431b-ac50-dd4cca0216fa
        mode http
        balance roundrobin
        option forwardfor
        server 78bbb52b-06e9-4eb6-9af9-95ecdf41c83d 10.0.0.3:80 weight 1"
577,1260075,nova,c47b4f388cb40d5792a206ac42ebdaebd5bde0d1,1,1,,Bug #1260075 “VMware,"The VMware Minesweeper CI occasionally runs into this error when trying to boot an instance:
2013-12-11 04:50:15.048 20785 DEBUG nova.virt.vmwareapi.driver [-] Task [ReconfigVM_Task] (returnval){
   value = ""task-322""
   _type = ""Task""
 } status: success _poll_task /opt/stack/nova/nova/virt/vmwareapi/driver.py:926
Reconfigured VM instance to enable vnc on port - 5986 _set_vnc_config /opt/stack/nova/nova/virt/vmwareapi/vmops.py:1461
Instance failed to spawn
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1461, in _spawn
    block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 628, in spawn
    admin_password, network_info, block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 435, in spawn
    upload_folder, upload_name + "".vmdk"")):
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1556, in _check_if_folder_file_exists
    ""browser"")
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim_util.py"", line 173, in get_dynamic_property
    property_dict = get_dynamic_properties(vim, mobj, type, [property_name])
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim_util.py"", line 179, in get_dynamic_properties
    obj_content = get_object_properties(vim, None, mobj, type, property_names)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim_util.py"", line 168, in get_object_properties
    options=options)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 187, in vim_request_handler
    fault_checker(response)
  File ""/opt/stack/nova/nova/virt/vmwareapi/error_util.py"", line 99, in retrievepropertiesex_fault_checker
    exc_msg_list))
VimFaultException: Error(s) NotAuthenticated occurred in the call to RetrievePropertiesEx
Full logs here for a CI build where this occurred are available here: http://162.209.83.206/logs/35303/31/"
578,1260123,nova,e3ac20fca7494122a4c0ed2edc00377aa92cb49d,1,1,Infinite loop,libvirt wait_for_block_job_info can infinitely loo...,"Callers of wait_for_block_job_info may loop infinitely if the job doesn't exist, since libvirt returns an empty dict which this function interprets as cur=0 and end=0 => return True.
I think it should do:
if not any(status):
    return False
Affects online deletion of Cinder GlusterFS snapshots, and possibly other callers of this (live_snapshot).
See http://libvirt.org/git/?p=libvirt-python.git;a=commit;h=f8bc3a9ccc
(Encountered issue on Fedora 19 w/ virt-preview repo, libvirt 1.1.3.)"
579,1260178,nova,09cca5fc0c9dd02832421b26acbf691df0c39936,1,0,Error getting version. The code evolved,Error in getting the major and minor version of hy...,"The utils method get_major_minor_version was being used to get the major and minor versions from the hypervisor version. This was used to determine whether the version is compatible with the required version, to then set the device_id on a vm record during vm creation.
This method was required by the feature: https://review.openstack.org/#/c/55117/
The method is not really required, because the hypervisor version in xen is always in the form of a tuple. This can directly be used, instead of using a utils method.
This fix will include:
- Removing the utils get_major_minor_version method
- Correctly handling the hypervisor version as a tuple"
580,1260249,nova,4963fa0fa0d96d8641339614438d42c81958f728,1,1,fdf248652a38eff287d6cabbac1a260151fecd40,Bug #1260249 “migration-list,"There is an AttributeError when we try to use the command ""nova migration-list""
Traceback (most recent call last):
  File ""/opt/stack/python-novaclient/novaclient/shell.py"", line 721, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/opt/stack/python-novaclient/novaclient/shell.py"", line 657, in main
    args.func(self.cs, args)
  File ""/opt/stack/python-novaclient/novaclient/v1_1/contrib/migrations.py"", line 71, in do_migration_list
    args.cell_name))
  File ""/opt/stack/python-novaclient/novaclient/v1_1/contrib/migrations.py"", line 53, in list
    return self._list(""/os-migrations%s"" % query_string, ""migrations"")
  File ""/opt/stack/python-novaclient/novaclient/base.py"", line 80, in _list
    for res in data if res]
  File ""/opt/stack/python-novaclient/novaclient/base.py"", line 426, in __init__
    self._add_details(info)
  File ""/opt/stack/python-novaclient/novaclient/base.py"", line 449, in _add_details
    for (k, v) in six.iteritems(info):
  File ""/usr/local/lib/python2.7/dist-packages/six.py"", line 439, in iteritems
    return iter(getattr(d, _iteritems)(**kw))
AttributeError: 'unicode' object has no attribute 'iteritems'
ERROR: 'unicode' object has no attribute 'iteritems'"
581,1260265,nova,ce3f9e5fa9cd05f3ee3bb0cc7d06521d05901cf4,0,0,Evolution: Remove traces of now unused host capabilities from scheduler ,BaremetalHostManager cannot distinguish baremetal ...,"BaremetalHostManager could distinguish baremetal hosts by checking ""baremetal_driver"" exists in capabilities or not. However, now BaremetalHostManager cannot, because capabilities are not reported to scheduler and BaremetalHostManager always receives empty capabilities. As a result, BaremetalHostManager just does the same thing as the original HostManager."
582,1260314,glance,a7e977bfd46ffc932f32f7e4e5a60ea3db4ed574,1,1,,glance image-create with invalid store fails but s...,"glance checks whether or not a specified store is valid, but if it is invalid the image has already been created.
I pulled the latest  devstack code and then ran these commands after sourcing openrc:
ubuntu@devstack-glance:/mnt/devstack$ glance index
ID                                   Name                           Disk Format          Container Format     Size
------------------------------------ ------------------------------ -------------------- -------------------- --------------
6792e9a7-f4f8-48cb-b407-80e360b8a773 cirros-0.3.1-x86_64-uec        ami                  ami                        25165824
7808c034-3fdd-4975-af26-e7d5a15d2113 cirros-0.3.1-x86_64-uec-ramdis ari                  ari                         3714968
4efcddb2-9f20-413f-86a3-3bf69455e09b cirros-0.3.1-x86_64-uec-kernel aki                  aki                         4955792
ubuntu@devstack-glance:/mnt/devstack$
ubuntu@devstack-glance:/mnt/devstack$ glance -d image-create --store s3e --disk-format raw --container-format bare --name complete_gibberish </etc/hosts
curl -i -X POST -H 'x-image-meta-container_format: bare' -H 'Transfer-Encoding: chunked' -H 'x-image-meta-store: s3e' -H 'User-Agent: python-glanceclient' -H 'x-image-meta-size: 221' -H 'x-image-meta-is_public: False' -H 'X-Auth-Token: <redacted_token>' -H 'Content-Type: application/octet-stream' -H 'x-image-meta-disk_format: raw' -H 'x-image-meta-name: complete_gibberish' -d '<open file '<stdin>', mode 'r' at 0x7f16181b6150>' http://10.4.36.1:9292/v1/images
HTTP/1.1 400 Bad Request
date: Thu, 12 Dec 2013 12:47:37 GMT
content-length: 52
content-type: text/plain; charset=UTF-8
x-openstack-request-id: req-c9bad6ee-d79c-41f3-bd96-d3929afd742c
400 Bad Request
Store for scheme s3e not found
Request returned failure status.
400 Bad Request
Store for scheme s3e not found
    (HTTP 400)
ubuntu@devstack-glance:/mnt/devstack$ glance index
ID                                   Name                           Disk Format          Container Format     Size
------------------------------------ ------------------------------ -------------------- -------------------- --------------
b26c03e4-7cdf-44fe-9187-7de315c9b38b complete_gibberish             raw                  bare                            221
6792e9a7-f4f8-48cb-b407-80e360b8a773 cirros-0.3.1-x86_64-uec        ami                  ami                        25165824
7808c034-3fdd-4975-af26-e7d5a15d2113 cirros-0.3.1-x86_64-uec-ramdis ari                  ari                         3714968
4efcddb2-9f20-413f-86a3-3bf69455e09b cirros-0.3.1-x86_64-uec-kernel aki                  aki                         4955792
This problem occurs using the v1 API. If using the V2 API the '--store' option does not seem to be present."
583,1260322,cinder,783e3243f98a90550a8b7ec8ce279709ab8db372,0,0,Implementation/Evolution,Cinder sample config does not contain groups other...,"The sample configuration file of cinder shipped with Havana and master lacks config groups other than [DEFAULT], which makes the configuration not actually work.
One example is the key ""connection="", which is in the sample configuration in the group [DEFAULT], but actually needs to be in the group [database] in order to be found by the code. But there are dozens of examples like that."
584,1260333,glance,5ecd5bc3db0ecc8716baa03a6e34f08e1c87fd7e,1,1, Malformed property protection rules return error,Malformed property protection rules return error t...,"Using a property protections file such as:
[.*]
create = @,!
read = @
update = @
delete = @
The create operation has an invalid rule, duplicate values are not allowed. This should probably result in the service refusing to start, however currently the service will start and operations touching this value will return:
500 Internal Server Error
Malformed property protection rule 'some_property': '@' and '!' are mutually exclusive   (HTTP 500)
to the end user. My feeling is that the end user should not receive any information about the cause of the error, just the 500 status."
585,1260538,nova,62c869d2375009190c5358636fd7de2ab9d194a5,0,0, Remove action-args from nova-manage help ,nova-manage useage exposes action-args,"The nova-manage command exposes the action_args options during the usage output for command.
E.g.
$ nova-manage network modify -h
usage: nova-manage network modify [-h] [--fixed_range <x.x.x.x/yy>]
                                  [--project <project name>] [--host <host>]
                                  [--disassociate-project]
                                  [--disassociate-host]
                                  [action_args [action_args ...]]
positional arguments:
  action_args
<snip>
This can cause confusion as users naturally expect there to be more ""actions"" on commands like ""modify"". Even in straightforward cases, this positional argument leaks into usage.
$ nova-manage db version -h
usage: nova-manage db version [-h] [action_args [action_args ...]]
positional arguments:
  action_args
Please consider suppressing documentation on action_args. In addition, expose the __doc__ strings for these functions, which is done in the nova command."
586,1260682,neutron,c15a794a832e0453075d363df949684c0fb86657,0,0,Handle a new exception,Bug #1260682 “LBaaS,"Followin stacktraces appeared after merging https://review.openstack.org/#/c/40381:
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     **args)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/haproxy/plugin_driver.py"", line 171, in update_status
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     context, obj_id['monitor_id'], obj_id['pool_id'], status)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 651, in update_pool_health_monitor
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     assoc = self._get_pool_health_monitor(context, id, pool_id)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 635, in _get_pool_health_monitor
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     monitor_id=id, pool_id=pool_id)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp PoolMonitorAssociationNotFound: Monitor 7cea505d-d5cb-4d3f-958a-6dd14edb65e1 is not associated with Pool 2ce72926-fcbb-442e-b9e6-7724ec6c472c
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp
2013-12-12 14:34:39.963 6522 ERROR neutron.openstack.common.rpc.common [req-bc48bc7a-b9dd-429f-a522-ab6ff56adfc9 None None] Returning exception Monitor 7cea505d-d5cb-4d3f-958a-6dd14edb65e1 is not associated with Pool 2ce72926-fcbb-442e-b9e6-7724ec6c472c to caller
2013-12-12 14:34:39.963 6522 ERROR neutron.openstack.common.rpc.common [req-bc48bc7a-b9dd-429f-a522-ab6ff56adfc9 None None] ['Traceback (most recent call last):\n', '  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data\n    **args)\n', '  File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch\n    neutron_ctxt, version, method, namespace, **kwargs)\n', '  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n    result = getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/haproxy/plugin_driver.py"", line 171, in update_status\n    context, obj_id[\'monitor_id\'], obj_id[\'pool_id\'], status)\n', '  File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 651, in update_pool_health_monitor\n    assoc = self._get_pool_health_monitor(context, id, pool_id)\n', '  File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 635, in _get_pool_health_monitor\n    monitor_id=id, pool_id=pool_id)\n', 'PoolMonitorAssociationNotFound: Monitor 7cea505d-d5cb-4d3f-958a-6dd14edb65e1 is not associated with Pool 2ce72926-fcbb-442e-b9e6-7724ec6c472c\n']
2013-12-12 14:34:40.265 6522 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     **args)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/haproxy/plugin_driver.py"", line 174, in update_status
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     context, model_mapping[obj_type], obj_id, status)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 194, in update_status
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     v_db = self._get_resource(context, model, id)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 212, in _get_resource
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     raise loadbalancer.MemberNotFound(member_id=id)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp MemberNotFound: Member 21f6ce73-fff8-4a7e-b798-527a4bb64b92 could not be found
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp"
587,1260738,glance,f924943f250b52522a9285410d076d3567199c0f,1,1,Images are created with negative sizes :O,image is creating with option size= a negative num...,"I just try to create an image by giving size =-1
and the image is creating succesfully.
 glance image-create --name cirros --is-public true --container-format bare --disk-format qcow2 --location https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk --size -1
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | None                                 |
| container_format | bare                                 |
| created_at       | 2013-12-13T13:48:07                  |
| deleted          | False                                |
| deleted_at       | None                                 |
| disk_format      | qcow2                                |
| id               | 2da4e4f9-5f1a-4c8d-a67c-272588e2efbc |
| is_public        | True                                 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | cirros                               |
| owner            | 6a2db75adb964c5b84010fa22b464715     |
| protected        | False                                |
| size             | -1                                   |
| status           | active                               |
| updated_at       | 2013-12-13T13:48:38                  |
+------------------+--------------------------------------+"
588,1260771,nova,bdecc8d2339e3e0dd87c7258244ac8568b5b965e,0,0,"Implementation: Define ""supported_instances""",Fake compute driver cannot deploy image with hyper...,"The fake compute driver does not provide the attribute supported_instances to ImagePropertiesFilter scheduler filter.
So ImagePropertiesFilter refuses to deploy images with hypervisor_type=fake property on fake computes.
Consequently, fake computes can not be used in multi hypervisor_types deployments because in this case hypervisor_type property on image is mandatory to avoid mixing one hypervisor_type image with another hypervisor_type compute."
589,1260773,cinder,6ccc654a0509c4e308713f9e6fd95266cd112c9d,1,1,,LVM Thin pool free space calculation fails if pool...,"We should always activate the pool at initialization time since we are going to use it anyway.
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/service.py"", line 205, in _child_process
2013-12-13 10:05:14.095 TRACE cinder.service     launcher.run_server(server)
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/service.py"", line 96, in run_server
2013-12-13 10:05:14.095 TRACE cinder.service     server.start()
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/service.py"", line 388, in start
2013-12-13 10:05:14.095 TRACE cinder.service     self.manager.init_host()
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 286, in init_host
2013-12-13 10:05:14.095 TRACE cinder.service     self.publish_service_capabilities(ctxt)
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 912, in publish_service_capabilities
2013-12-13 10:05:14.095 TRACE cinder.service     self._report_driver_status(context)
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 904, in _report_driver_status
2013-12-13 10:05:14.095 TRACE cinder.service     volume_stats = self.driver.get_volume_stats(refresh=True)
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/volume/drivers/lvm.py"", line 345, in get_volume_stats
2013-12-13 10:05:14.095 TRACE cinder.service     self._update_volume_stats()
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/volume/drivers/lvm.py"", line 358, in _update_volume_stats
2013-12-13 10:05:14.095 TRACE cinder.service     self.vg.update_volume_group_info()
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/brick/local_dev/lvm.py"", line 396, in update_volume_group_info
2013-12-13 10:05:14.095 TRACE cinder.service     self.vg_thin_pool)
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/brick/local_dev/lvm.py"", line 149, in _get_thin_pool_free_space
2013-12-13 10:05:14.095 TRACE cinder.service     consumed_space = float(data[0]) / 100 * (float(data[1]))
2013-12-13 10:05:14.095 TRACE cinder.service ValueError: empty string for float()
2013-12-13 10:05:14.095 TRACE cinder.service
2013-12-13 10:05:14.109 INFO cinder.service [-] Child 5052 exited with status 2
2013-12-13 10:05:14.110 INFO cinder.service [-] _wait_child 1
2013-12-13 10:05:14.110 INFO cinder.service [-] wait wrap.failed True
$ sudo lvchange -a n stack-volumes/stack-volumes-pool
$ sudo lvs -o size,data_percent --separator : stack-volumes/stack-volumes-pool
  LSize:Data%
  9.00g:"
590,1260806,nova,f6988b3f1ebf56364696ff25448a8018866fd20b,1,1,,Defaulting device names fails to update the databa...,"_default_block_device_names method of the compute manager, would call the conductor block_device_mapping_update method with the wrong arguments, causing a TypeError and ultimately the instance to fail.
This bug happens only when using a driver that does not provid it's own implementation of default_device_names_for_instance, (currently only the libvirt driver does this).
Also affects havana since https://review.openstack.org/#/c/40229/"
591,1261021,nova,709410d243a97d35c3da314b41bab039eac75736,1,1,,Nova Docker network in container has wrong network...,"Network in container has wrong network mask.
In devstack you have a default network defined like following:
    nova network-list
    +--------------------------------------+---------+-------------+
    | ID                                   | Label   | Cidr        |
    +--------------------------------------+---------+-------------+
    | f7c6e98d-d900-4df2-8523-0c8dd3a4ad7f | private | 10.0.0.0/24 |
    +--------------------------------------+---------+-------------+
If I start up a new container via nova and look at the created network device:
    sudo ip netns exec <container-id> ip a | grep pvnetr
    66: pvnetr70121: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
        inet 10.0.0.2/8 brd 10.255.255.255 scope global pvnetr70121
Then you can see that it has a 10.0.0.2/8 network.
I would expect to find a 10.0.0.2/24"
592,1261442,nova,52f9994aab8b60de2ba22579f923beb050e92bca,1,1,Fix image cache,Spurious error from ComputeManager._run_image_cach...,"I got the following errors in a tempest test at http://logs.openstack.org/97/62397/1/check/check-tempest-dsvm-full/3f7b8c3:
2013-12-16 16:07:52.189 27621 DEBUG nova.openstack.common.processutils [-] Result was 1 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:172
2013-12-16 16:07:52.189 27621 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager._run_image_cache_manager_pass: Unexpected error while running command.
Command: env LC_ALL=C LANG=C qemu-img info /opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk
Exit code: 1
Stdout: ''
Stderr: ""qemu-img: Could not open '/opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk': No such file or directory\n""
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     task(self, context)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 5210, in _run_image_cache_manager_pass
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     self.driver.manage_image_cache(context, filtered_instances)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 4650, in manage_image_cache
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     self.image_cache_manager.verify_base_images(context, all_instances)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/imagecache.py"", line 603, in verify_base_images
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     inuse_backing_images = self._list_backing_images()
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/imagecache.py"", line 345, in _list_backing_images
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     backing_file = virtutils.get_disk_backing_file(disk_path)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/utils.py"", line 442, in get_disk_backing_file
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     backing_file = images.qemu_img_info(path).backing_file
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/images.py"", line 56, in qemu_img_info
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     'qemu-img', 'info', path)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/utils.py"", line 175, in execute
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     return processutils.execute(*cmd, **kwargs)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/openstack/common/processutils.py"", line 178, in execute
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     cmd=' '.join(cmd))
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task ProcessExecutionError: Unexpected error while running command.
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Command: env LC_ALL=C LANG=C qemu-img info /opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Exit code: 1
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Stdout: ''
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Stderr: ""qemu-img: Could not open '/opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk': No such file or directory\n""
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task"
593,1261475,nova,02abbef960b12deaa7a911788dbc56f3a0bd555a,1,1,Aren’t safe… Bug I think. ,Nova should disable libguestfs' automatic cleanup,"By default libguestfs will register an atexit() handler to cleanup any open libguestfs handles when the process exits. Since libguestfs does not provide any mutex locking in its APIs, the atexit handlers are not safe in multi-threaded processes. If they run they are liable to cause memory corruption as multiple threads access the same libguestfs handle. As such at atexit handlers should be disabled in any multi-threaded program using libguestfs. eg by using
  guestfs.GuestFS (close_on_exit = False)
instead of
  guestfs.GuestFS()"
594,1261585,nova,1fc9eafab43accc127044351106983e2cb215520,1,0,For windows compatibility,ConfigDrive metadata is incorrectly generated on W...,"Files must be written with ""wb"" instead of ""w"" in order to support multiple platforms:
https://github.com/openstack/nova/blob/b823db737855149ba847e5b19df9232f109f6001/nova/virt/configdrive.py#L92"
595,1261621,nova,eb3db3be60f22a462f30004c55000e26545748cd,1,1,,nova api value error is not right,"I was trying to add a json field to DB but forget to dumps the json to string, and nova api report the following error.
2013-12-17 12:37:51.615 TRACE object Traceback (most recent call last):
2013-12-17 12:37:51.615 TRACE object   File ""/opt/stack/nova/nova/objects/base.py"", line 70, in setter
2013-12-17 12:37:51.615 TRACE object     field.coerce(self, name, value))
2013-12-17 12:37:51.615 TRACE object   File ""/opt/stack/nova/nova/objects/fields.py"", line 166, in coerce
2013-12-17 12:37:51.615 TRACE object     return self._type.coerce(obj, attr, value)
2013-12-17 12:37:51.615 TRACE object   File ""/opt/stack/nova/nova/objects/fields.py"", line 218, in coerce
2013-12-17 12:37:51.615 TRACE object     raise ValueError(_('A string is required here, not %s'),
2013-12-17 12:37:51.615 TRACE object ValueError: (u'A string is required here, not %s', 'dict') <<<<<<<<<<<
The error should be <string is required here, not dict>"
596,1261652,neutron,44b3783ceb94934bf8d4d62f704d42b49e52866e,1,0,For postgresql specific,Internal server error when deleting empty network ...,"Deleting a network which has no ports and no subnets on it produces the following stack trace in neutron-server logs when using PostgreSql:
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/openvswitch/ovs_neutron_plugin.py"", line 526, in delete_network
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     super(OVSNeutronPluginV2, self).delete_network(context, id)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 999, in delete_network
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     for p in ports)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2227, in __iter__
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     return self._execute_and_instances(context)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2242, in _execute_and_instances
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     result = conn.execute(querycontext.statement, self._params)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1449, in execute
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     params)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1584, in _execute_clauseelement
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     compiled_sql, distilled_params
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     context)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     context)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     cursor.execute(statement, parameters)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource NotSupportedError: (NotSupportedError) SELECT FOR UPDATE/SHARE cannot be applied to the nullable side of an outer join
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource  'SELECT ports.tenant_id AS ports_tenant_id, ports.id AS ports_id, ports.name AS ports_name, ports.network_id AS ports_network_id, ports.mac_address AS ports_mac_address, ports.admin_state_up AS ports_admin_state_up, ports.status AS ports_status, ports.device_id AS ports_device_id, ports.device_owner AS ports_device_owner, ipallocations_1.port_id AS ipallocations_1_port_id, ipallocations_1.ip_address AS ipallocations_1_ip_address, ipallocations_1.subnet_id AS ipallocations_1_subnet_id, ipallocations_1.network_id AS ipallocations_1_network_id, allowedaddresspairs_1.port_id AS allowedaddresspairs_1_port_id, allowedaddresspairs_1.mac_address AS allowedaddresspairs_1_mac_address, allowedaddresspairs_1.ip_address AS allowedaddresspairs_1_ip_address, extradhcpopts_1.id AS extradhcpopts_1_id, extradhcpopts_1.port_id AS extradhcpopts_1_port_id, extradhcpopts_1.opt_name AS extradhcpopts_1_opt_name, extradhcpopts_1.opt_value AS extradhcpopts_1_opt_value, portbindingports_1.port_id AS portbindingports_1_port_id, portbindingports_1.host AS portbindingports_1_host, securitygroupportbindings_1.port_id AS securitygroupportbindings_1_port_id, securitygroupportbindings_1.security_group_id AS securitygroupportbindings_1_security_group_id \nFROM ports LEFT OUTER JOIN portbindingports ON ports.id = portbindingports.port_id LEFT OUTER JOIN ipallocations AS ipallocations_1 ON ports.id = ipallocations_1.port_id LEFT OUTER JOIN allowedaddresspairs AS allowedaddresspairs_1 ON ports.id = allowedaddresspairs_1.port_id LEFT OUTER JOIN extradhcpopts AS extradhcpopts_1 ON ports.id = extradhcpopts_1.port_id LEFT OUTER JOIN portbindingports AS portbindingports_1 ON ports.id = portbindingports_1.port_id LEFT OUTER JOIN securitygroupportbindings AS securitygroupportbindings_1 ON ports.id = securitygroupportbindings_1.port_id \nWHERE ports.tenant_id = %(tenant_id_1)s AND ports.network_id IN (%(network_id_1)s) FOR UPDATE' {'network_id_1': u'f8344378-ed58-4ce3-99ae-408c8591cda9', 'tenant_id_1': u'df3896201a174787b65b2b098aad2968'}
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource
2013-12-17 12:43:49.920 7347 INFO neutron.wsgi [req-b8b4b523-cf50-4bcb-8356-19bbc4b9d057 8549dfbbb00940b9b3659230186bd26d df3896201a174787b65b2b098aad2968] 192.168.1.51 - - [17/Dec/2013 12:43:49] ""DELETE /v2.0/networks/f8344378-ed58-4ce3-99ae-408c8591cda9.json HTTP/1.1"" 500 230 0.063668"
597,1261665,neutron,7dc3c671656974596b9a373c911e5b786d57be35,1,1, was being applied on the incorrect port.,Bug #1261665 “Midonet plugin,"Steps:
Using the MidoNet plugin:
* Create a private network
* Create a VM on the private network
* Create a router
* Create an interface for the router on the private network
* Create an external network
* Set a router's gateway to the external network
* Ping a publicly routable address from a VM on a network attached the router
Observed:
Source NAT is not applied to the outbound traffic
Expected:
Outbound traffic should be source NATted at the router"
598,1261675,nova,9e7e8daa66f0c05ac7ce3927562d246baf70067f,1,1, Unable to snapshot an instance with ephemeral RBD ,Unable to snapshot an instance with ephemeral RBD,"When creating a snapshot from a VM instance, following error appears in Nova compute log:
 2013-12-17 10:39:47.943 28210 ERROR nova.openstack.common.rpc.amqp [req-3a82cad9-3213-47e8-8a42-0f6e2a75008a a2eecc1caf5f40c5ab50405b68730c20 e255029e4a614ed1a5412192db588e74] Exception during message handling
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     **args)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 353, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     payload)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     pass
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 319, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     % image_id, instance=instance)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 309, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2293, in snapshot_instance
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     task_states.IMAGE_SNAPSHOT)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2324, in _snapshot_instance
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     update_task_state)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1399, in snapshot
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     snapshot_backend.snapshot_extract(out_path, image_format)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/imagebackend.py"", line 531, in snapshot_extract
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     images.convert_image(snap, target, out_format)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/images.py"", line 179, in convert_image
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     utils.execute(*cmd, run_as_root=run_as_root)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/utils.py"", line 177, in execute
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     return processutils.execute(*cmd, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/processutils.py"", line 178, in execute
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     cmd=' '.join(cmd))
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp ProcessExecutionError: Unexpected error while running command.
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Command: qemu-img convert -O qcow2 rbd:instances/instance-0000000f_disk /var/lib/nova/instances/snapshots/tmp63tXq0/b916d3ca7fbe46cba4ca7ce5e0138ee9
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Exit code: 1
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Stdout: ''
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Stderr: ""qemu-img: error connecting\nqemu-img: Could not open 'rbd:instances/instance-0000000f_disk': Operation not supported\nqemu-img: Could not open 'rbd:instances/instance-0000000f_disk'\n"""
599,1261692,swift,cd4b4da8b6362e6621602a8b815bbeeea2000ff7,1,1,but the fixing commit solves many more issues!,"Bug #1261692 “swift-recon ignores zone filter if set to “0"""" ","swift-recon has a filter to query only servers in a specific zone:
  -z ZONE, --zone=ZONE  Only query servers in specified zone
If zone is set to ""0"", the filter is not applied."
600,1261728,nova,78d62186e5b0388f740d42cb8da5798cd67d7880,0,0,tests,Interprocess file locks aren't usable in unit test...,"Base test case class has a fixture, that overrides CONF.lock_path value, which means that every test case will have CONF.lock_path set to its own temporary dir path. This makes interprocess locks unusable in unit tests, which is likely to break tests when they are run concurrently using testr (e.g. tests using MySQL/PostgreSQL might want to be run exclusively)."
601,1261731,cinder,99f3ce38e9ad1e272a4a04858ff1acc709e7c9cb,1,1,"One LOG is just a debug, no error or warning. Typo?",Unable to deactivate LVM snapshot,"One of the new error conditions that shows up in the gate now that we are failing on unknown errors is a failure by c-vol to deactivate the LVM snapshot.
The c-vol error condition is the following:
[req-798a0f63-4a42-4f68-9087-54a90ec50376 1b40a47efb8243579e1830d573ac30ed f6fcf023ac3c4fa0a670e28917331b3f] Error reported running lvremove: CMD: sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvremove -f stack-volumes/_snapshot-8a0e5904-d394-4193-97fe-3658e0964c22, RESPONSE:   Unable to deactivate open stack--volumes-_snapshot--8a0e5904--d394--4193--97fe--3658e0964c22 (252:4)
  Unable to deactivate logical volume ""_snapshot-8a0e5904-d394-4193-97fe-3658e0964c22""
http://logs.openstack.org/14/62514/1/check/check-tempest-dsvm-postgres-full/18e9cf9/logs/screen-c-vol.txt.gz#_2013-12-17_02_10_46_029
It looks like it actually succeeds on attempt 2 after the quiece, so the the fix is probably to reduce the level on these to make them not error/warn unless there is a real failure."
602,1261738,glance,b35728019e0eb89c213eed7bc35a1f062c99dcca,1,1, user_total_quota calculated incorrectly ,Bug #1261738 “Openstack Glance,"Description of problem: Bug in quota calculation, if an image upload fails due to quota limit, the failed image size is still added to total storage sum figure! Thus future images may fail to upload even if it looks as quota hasn’t been reached yet.
Version-Release number of selected component (if applicable):
RHEL 6.5
python-glanceclient-0.12.0-1.el6ost.noarch
openstack-glance-2013.2-5.el6ost.noarch
python-glance-2013.2-5.el6ost.noarch
How reproducible:
Steps to Reproduce:
1. vim /etc/glance/glance-api.conf  user_storage_quota = 250mb (in byets)
2. service glance-api restart
3. Upload test small image - would be ok
4. Upload large image say 4Giga, - should fail with ""Error unable to create new image""
5. Try to upload another small file say 49MB.
Actual results:
If the large i,age file or sum of failed uploaded images are more than the quota, any image size will fail to upload.
Expected results:
I should be able to upload as long as the sum of all my images is less than configured qouta.
Additional info:
Mysql show databases;
connect glance;
SELECT * FROM images;
Noticed all the images i tired, initial successful uploaded image status=”active”, images that i deleted status=”deleted”, images that failed to upload due to quota status=”killed”
I than calculated the sum of all the “killed” images.
Set a new quota of the above calculated value + 100MB, restarted glance-api service.
Only than i was able to upload another image of 49MB.
When i set a lower quota value (below the calculated sum of all the killed images) wasn’t able to upload any image.
Images of status killed, which fail upload for any reason, should not be added to total storage sum calcualtion or quota."
603,1261747,glance,cff35c1999997b6af62f4ef285d66f38b19433af,0,0, user_storage_quota now accepts units with value,Bug #1261747 “Openstack Glance,"Description of problem:
When you setting user_total_quota size is in bytes, unit should be in KB\MB\GB
Version-Release number of selected component (if applicable):
RHEL 6.5
python-glanceclient-0.12.0-1.el6ost.noarch
openstack-glance-2013.2-5.el6ost.noarch
python-glance-2013.2-5.el6ost.noarch"
604,1261849,cinder,edc93383f7cd9d055d655a8e345a3efa81b7de5b,1,0,Wrong href because evolution,version list shows wrong hrefs,"If you hit the cinder endpoint directly, you get the version list. The generated hrefs all point to /v1.
cory@cfsyn28:~/devstack$ curl localhost:8776 | python -m json.tool
{
    ""versions"": [
        {
            ""id"": ""v1.0"",
            ""links"": [
                {
                    ""href"": ""http://localhost:8776/v1/"",
                    ""rel"": ""self""
                }
            ],
            ""status"": ""CURRENT"",
            ""updated"": ""2012-01-04T11:33:21Z""
        },
        {
            ""id"": ""v2.0"",
            ""links"": [
                {
                    ""href"": ""http://localhost:8776/v1/"",
                    ""rel"": ""self""
                }
            ],
            ""status"": ""CURRENT"",
            ""updated"": ""2012-11-21T11:33:21Z""
        }
    ]
}"
605,1261978,nova,8727745562d63149ad16f1ef07cbedcf506076da,1,0,specific machine type ,Default machine type for libvirt does not work wit...,"Today Nova does not pass a specific machine type in the libvirt XML configuration file, resulting in the use of a default machine type. In some cases the operator may want to use a non-default machine type. For example, with ARM by default the machine type is integratorcp, but users need to use the virt model or others such as vexpress-a15 with KVM"
606,1262039,nova,48a20d85cb9ef91c48f681c76fc6ca7512eccfbc,0,0,"We should modify the ""image"" to empty dictionary",Inconsistent “image,"1. Create a server from a image, then show the server details, the ""image"" is a dict, such as:
          {
                ""id"": image_id,
                ""links"": [{
                    ""rel"": ""bookmark"",
                    ""href"": bookmark,
                }],
            }
2. Create a server from a volume, then show the server details, the ""image"" is a dict, such as:
        """"
3. It's inconsistent ""image"" value, i think it's a bug."
607,1262051,glance,16d48ca756570ff9b10d5102544d1d5eb0020f29,0,0,tests,Bug #1262051 “Cleanup,"The following methods contained in glance/tests/functional/store_utils.py  are not used anywhere:
. setup_swift,
. teardown_swift,
. get_swift_uri,
. setup_s3,
. teardown_s3,
. get_s3_uri
Let's get rid of it."
608,1262089,neutron,4ded12bef837e707c76af87d71155f04534dcb0d,0,0,tests,Tracebacks in py26 unit test console logs related ...,"After adding state reporting to metadata agent following tracebacks appeared in py26 unit test console logs:
2013-11-22 15:08:33.914 | ERROR:neutron.agent.metadata.agent:Failed reporting state!
2013-11-22 15:08:33.914 | Traceback (most recent call last):
2013-11-22 15:08:33.914 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/agent/metadata/agent.py"", line 258, in _report_state
2013-11-22 15:08:33.914 |     use_call=self.agent_state.get('start_flag'))
2013-11-22 15:08:33.914 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/agent/rpc.py"", line 74, in report_state
2013-11-22 15:08:33.914 |     return self.cast(context, msg, topic=self.topic)
2013-11-22 15:08:33.915 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/proxy.py"", line 171, in cast
2013-11-22 15:08:33.915 |     rpc.cast(context, self._get_topic(topic), msg)
2013-11-22 15:08:33.915 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/__init__.py"", line 158, in cast
2013-11-22 15:08:33.915 |     return _get_impl().cast(CONF, context, topic, msg)
2013-11-22 15:08:33.915 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/impl_fake.py"", line 166, in cast
2013-11-22 15:08:33.915 |     check_serialize(msg)
2013-11-22 15:08:33.916 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/impl_fake.py"", line 131, in check_serialize
2013-11-22 15:08:33.916 |     json.dumps(msg)
2013-11-22 15:08:33.916 |   File ""/usr/lib64/python2.6/json/__init__.py"", line 230, in dumps
2013-11-22 15:08:33.916 |     return _default_encoder.encode(obj)
2013-11-22 15:08:33.916 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 367, in encode
2013-11-22 15:08:33.916 |     chunks = list(self.iterencode(o))
2013-11-22 15:08:33.917 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.917 |     for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.917 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.917 |     for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.917 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.917 |     for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.918 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.918 |     for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.918 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.918 |     for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.918 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.918 |     for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.919 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.919 |     for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.919 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.919 |     for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.919 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 317, in _iterencode
2013-11-22 15:08:33.919 |     for chunk in self._iterencode_default(o, markers):
2013-11-22 15:08:33.920 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 323, in _iterencode_default
2013-11-22 15:08:33.920 |     newobj = self.default(o)
2013-11-22 15:08:33.920 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 344, in default
2013-11-22 15:08:33.920 |     raise TypeError(repr(o) + "" is not JSON serializable"")
2013-11-22 15:08:33.920 | TypeError: <MagicMock name='cfg.CONF.host' id='989177040'> is not JSON serializable
2013-11-22 15:08:33.921 | WARNING:neutron.openstack.common.loopingcall:task run outlasted interval by <MagicMock name='cfg.CONF.AGENT.report_interval.__sub__().__neg__()' id='989263056'> sec
this can be observed in jenkins results for any patch on review.
Need to mock loopingcall in metadata agent tests in order to fix this"
609,1262124,nova,6478554f531f6ee2fa86226fbc79dd31e556bc06,0,0,Evolution: we should allow an admin user to…,Ceilometer cannot poll and publish floatingip samp...,"The ceilometer central agent pull and pubulish floatingip samples or other types of samples .but it cannot get valid samples of floatingip.
The reason is ceilometer floatingip poster call nova API  ""list"" metod of nova.api.openstack.compute.contrib.floating_ips.FloatingIPController, this API get floatingips filtered by context.project_id.
The current context.project_id is the id of tenant ""service"".So,the result is {""floatingips"": []}
the logs of nova-api-os-compute is:
http://paste.openstack.org/show/55285/
Here,ceilometer invoke novaclient to list floatingips,and novaclient call nova API,then,the nova API will call nova network API or neutron API with:
    client.list_floatingips(tenant_id=project_id)['floatingips']
Novaclient can not list other tenant's floatingip but only the tenant of current context.
So, I think we should modify the nova API with adding a parameter like ""all_tenant"" which accessed by admin role.
This should be confirmed?"
610,1262206,nova,d54041f0b4763c00f661088f113e26e38c747730,1,1,,Bug #1262206 “xenapi,"Any errors after the XenServer migrate command completes currently can cause the users VM to be deleted.
While there should be some cleanup performed, deleting the VM does not make sense for the XenAPI driver."
611,1262461,nova,5ebc60c48d4a8b6c7bac96d923626df9ae67a2b2,1,0,Evolution: various resize methods are using the old-style quota class to reserve and commit quota changes directly to the database,resize auto-confirmation failed with nova-conducto...,"I can reproduce this bug in master and stable havana.
this bug is similar to https://bugs.launchpad.net/nova/+bug/1158897
but this bug is cause by _reserve_quota_delta() method, which will need to access DB if quota delta is not empty,
the trace log is:
2013-12-19 01:39:40.879 ERROR nova.compute [-] No db access allowed in nova-compute:
File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/loopingcall.py"", line 125, in _inner
    idle = self.f(*self.args, **self.kw)
  File ""/opt/stack/nova/nova/service.py"", line 314, in periodic_tasks
    return self.manager.periodic_tasks(ctxt, raise_on_error=raise_on_error)
  File ""/opt/stack/nova/nova/manager.py"", line 101, in periodic_tasks
    return self.run_periodic_tasks(context, raise_on_error=raise_on_error)
  File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
    task(self, context)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 4535, in _poll_unconfirmed_resizes
    migration=migration)
  File ""/opt/stack/nova/nova/compute/api.py"", line 199, in wrapped
    return func(self, context, target, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/api.py"", line 189, in inner
    return function(self, context, instance, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/api.py"", line 216, in _wrapped
    return fn(self, context, instance, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/api.py"", line 170, in inner
    return f(self, context, instance, *args, **kw)
  File ""/opt/stack/nova/nova/compute/api.py"", line 2158, in confirm_resize
    reservations = self._reserve_quota_delta(context, deltas)
  File ""/opt/stack/nova/nova/compute/api.py"", line 2238, in _reserve_quota_delta
    return QUOTAS.reserve(context, project_id=project_id, **deltas)
  File ""/opt/stack/nova/nova/quota.py"", line 1272, in reserve
    user_id=user_id)
  File ""/opt/stack/nova/nova/quota.py"", line 487, in reserve
    has_sync=True, project_id=project_id)
  File ""/opt/stack/nova/nova/quota.py"", line 354, in _get_quotas
    usages=False)
  File ""/opt/stack/nova/nova/quota.py"", line 264, in get_project_quotas
    project_quotas = db.quota_get_all_by_project(context, project_id)
  File ""/opt/stack/nova/nova/db/api.py"", line 1023, in quota_get_all_by_project
    return IMPL.quota_get_all_by_project(context, project_id)
  File ""/opt/stack/nova/nova/cmd/compute.py"", line 48, in __call__
    stacktrace = """".join(traceback.format_stack())
2013-12-19 01:39:40.879 ERROR nova.compute.manager [-] [instance: 4ffaabaf-a480-421f-a7a7-efd148d7c956] Error auto-confirming resize: nova-compute. Will retry later."
612,1262488,neutron,5abac020f459045ceee75c971e179d9dbce8eac8,1,1,,bigswitch multi-tenant floating IP loss,"The bigswitch plugin fails to use the admin context to update the floating IPs on the backend controller.
When one tenant assigns a floating IP, the update to the controller doesn't contain the floating IPs of the other tenants so the controller thinks they have been removed."
613,1262858,nova,3ccb676dffea46b4ecfab35e16c0e96642353ee7,1,1,"It says deprecated, but its a typo of a deprecated typo",Wrong deprecated name for images_rbd options,"In nova/virt/libvirt/imagebackend.py, the images_rbd_pool and images_rbd_ceph_conf have deprecated_name set to libvirt_images_rdb_pool and libvirt_images_rdb_ceph_conf, respectively, but the actual option names prior to commit 25a7de2054ba6ae5eb318c86fe165f4302fbfff8 are libvirt_images_rbd_pool and libvirt_images_rbd_ceph_conf. (Note the transposition of the d and b in rbd.)"
614,1262880,cinder,186221779a92002ff9fa13c254710c0abb3803be,1,1,Using wrong value,Bug #1262880 “GlusterFS,"The base= field is not using the correct value when a libvirt snapshot_delete is issued on the Nova side -- pass correct values for ""file_to_merge"" in the delete_info dict depending on whether or not other snapshots exist.
This fixes an issue where attached snapshot deletion results in an inconsistent qcow2 chain."
615,1262885,neutron,d00e0b839378aed4c337efa1a33f6abbb384c149,1,1,Forgot to clean up? Bug: goes out of sync with db after being down ,haproxy-lbaas-agent goes out of sync with db after...,"How to reproduce:
1) create a pool with a VIP
2) check which lbaas-agent that pool is scheduled to
3) shut down lbaas-agent on the node
4) remove the pool/VIP via the API
5) restart lbaas-agent
It turns out the VIP would remain on the lbaas-agent host forever.
Looks like the instance_mapping variable is reset to empty restart the agent restarted, so it essentially lost track of that removed pool/vip."
616,1263204,nova,8cc59d2084a8b1129929a162b753d4ca407018e7,0,0,Refactoring code. Friendly messages,Exceptions that get returned via REST api should h...,"Exceptions in nova/exception.py that get returned via REST api should have more user friendly (non-admin and non-dev) messages.
For example
 class InstanceRecreateNotSupported(Invalid):
     msg_fmt = _('Instance recreate is not implemented by this virt driver.')
Assuming that exception gets returned via the REST API, a user shouldn't have to know what a 'virt driver' is, that is a backend concept that we should be hiding.
Instead this exception should say something like 'Instance recreate is not supported'"
617,1263217,neutron,f0921a4bbad5946abb6d9679867928a970cf5820,0,0, Remove unnecessary call,Unnecessary call to get_dhcp_port from DeviceManag...,"In the file neutron/agent/linux/dhcp.py, the DeviceManager setup method calls get_device which calls get_dhcp_port.  This results in an RPC call.  But, we already had the port in the setup method.
I discovered this as I was trying to optimize the number of these RPC calls."
618,1263258,cinder,65c635d6a5be0b4afaa9289c901f90275a0be102,1,0,Specific of Nexenta iSCSI driver,nexenta-iscsi-volume-migrate-update-provider-locat...,"After Nexenta iSCSI volume driver migrate volume provider_location of volume doesn't updated.
And also volume migrate method doesn't delete temporal snapshot on destination host."
619,1263569,nova,4b4f0d61836e527340511f6198c2f2c0bdf70888,0,0,delete obsolete code,Remove update_service_capabilities from nova,"From the comments of update_service_capabilities, it is said that once publish_service_capabilities was removed, then we can begin the process of  its removal.
Now publish_service_capabilities has been removed, so we can remove update_service_capabilities now as no one is calling it
def update_service_capabilities(self, context, service_name,
                                    host, capabilities):
        """"""Process a capability update from a service node.""""""
        #NOTE(jogo) This is deprecated, but is used by the deprecated
        # publish_service_capabilities call. So this can begin its removal
        # process once publish_service_capabilities is removed.
        if not isinstance(capabilities, list):
            capabilities = [capabilities]
        for capability in capabilities:
            if capability is None:
                capability = {}
            self.driver.update_service_capabilities(service_name, host,
                                                    capability)"
620,1263602,nova,8c359e0d0cf40b82b07ed1a30f6bfc00369892a6,0,0,tests,Do not use contextlib.nested if only mock one func...,"There are some test cases in test_compute_mgr.py using contextlib.nested to mock up functions even there is only one function.
We should use mock.patch.object directly if only mock one function.
def test_init_instance_sets_building_error(self):
        with contextlib.nested(  <<<<< No need use nested here
            mock.patch.object(self.compute, '_instance_update')
          ) as (
            _instance_update,
          ):
            instance = instance_obj.Instance(self.context)
            instance.uuid = 'foo'
            instance.vm_state = vm_states.BUILDING
            instance.task_state = None
            self.compute._init_instance(self.context, instance)
            call = mock.call(self.context, 'foo',
                             task_state=None,
                             vm_state=vm_states.ERROR)
            _instance_update.assert_has_calls([call])"
621,1263647,glance,c1c72d1694927e4dea1c6f09bd0cea67c2643e7b,1,1, Fix the incorrect log message when creating images ,"Bug #1263647 “log image id incorectly when creating image "" ","when I use ""nova image-create my_server_id  name"" to create an image, the log /var/log/glance/api.log record the below texts:
<glance.common.wsgi.Resource object at 0x38a7150>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-12-23 20:24:19.168 20884 INFO glance.registry.api.v1.images [0f0f1d19-9d63-4f2a-88d1-f48c0e28f6f4 e0f87f6761614850b064f9717ac83a36 3b520afd03d94532b64fef4d863230f6] Successfully created image None
but the image id in DB is not None but an uuid."
622,1263684,glance,5de69badfc44dd03e5fa21577a66daabc3ade5a0,1,1,Bad eror code,Bug #1263684 “DB2,"GET:
http://9.12.27.148:9292/v1/images/82ff46a0-f49b-4279-bdad-e665c203444477777777777w232323232eer34r3r3r3qer3r3r3r
would return 500 instead of 400.
The root cause is as below:
2013-12-23 07:43:15.387 27806 INFO glance.wsgi.server [687bccf6-ed9e-4efa-988e-a05faafb6c4b 51336fdd98e449ef911f57ad3d03c818 d9f056c22dac4022bf2ebb1ed70fd25e] Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/eventlet/wsgi.py"", line 384, in handle_one_response
    result = self.application(self.environ, start_response)
  File ""/usr/lib/python2.6/site-packages/keystoneclient/middleware/auth_token.py"", line 571, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/glance/common/wsgi.py"", line 368, in __call__
    response = req.get_response(self.application)
  File ""/usr/lib/python2.6/site-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/lib/python2.6/site-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/lib/python2.6/site-packages/routes/middleware.py"", line 131, in __call__
    response = self.app(environ, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/glance/common/wsgi.py"", line 620, in __call__
    request, **action_args)
  File ""/usr/lib/python2.6/site-packages/glance/common/wsgi.py"", line 646, in dispatch
    return method(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/glance/registry/api/v1/images.py"", line 312, in show
    image = self.db_api.image_get(req.context, id)
  File ""/usr/lib/python2.6/site-packages/glance/db/sqlalchemy/api.py"", line 315, in image_get
    force_show_deleted=force_show_deleted)
  File ""/usr/lib/python2.6/site-packages/glance/db/sqlalchemy/api.py"", line 343, in _image_get
    raise e
DataError: (DataError) ibm_db_dbi::DataError: Statement Execute Failed: [IBM][CLI Driver] CLI0109E  String data right truncation. SQLSTATE=22001 SQLCODE=-99999 'SELECT images.created_at AS images_created_at, images.updated_at AS images_updated_at, images.deleted_at AS images_deleted_at, images.deleted AS images_deleted, images.id AS images_id, images.name AS images_name, images.disk_format AS images_disk_format, images.container_format AS images_container_format, images.size AS images_size, images.status AS images_status, images.is_public AS images_is_public, images.checksum AS images_checksum, images.min_disk AS images_min_disk, images.min_ram AS images_min_ram, images.owner AS images_owner, images.protected AS images_protected, image_properties_1.created_at AS image_properties_1_created_at, image_properties_1.updated_at AS image_properties_1_updated_at, image_properties_1.deleted_at AS image_properties_1_deleted_at, image_properties_1.deleted AS image_properties_1_deleted, image_properties_1.id AS image_properties_1_id, image_properties_1.image_id AS image_properties_1_image_id, image_properties_1.name AS image_properties_1_name, image_properties_1.""value"" AS image_properties_1_value, image_locations_1.created_at AS image_locations_1_created_at, image_locations_1.updated_at AS image_locations_1_updated_at, image_locations_1.deleted_at AS image_locations_1_deleted_at, image_locations_1.deleted AS image_locations_1_deleted, image_locations_1.id AS image_locations_1_id, image_locations_1.image_id AS image_locations_1_image_id, image_locations_1.""value"" AS image_locations_1_value, image_locations_1.meta_data AS image_locations_1_meta_data \nFROM images LEFT OUTER JOIN image_properties AS image_properties_1 ON images.id = image_properties_1.image_id LEFT OUTER JOIN image_locations AS image_locations_1 ON images.id = image_locations_1.image_id \nWHERE images.id = ?' ('82ff46a0-f49b-4279-bdad-e665c203444477777777777w232323232eer34r3r3r3qer3r3r3r',)"
623,1263729,glance,a8ef64c4eccbc0639e0e7a81ff584287a4f9ddd0,1,1, Image size won't be updated if locations are updated to empty ,Image size won't be updated if locations are updat...,"See https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L225
Based on current design, the image status will be updated to 'queued' if all locations are removed from the target image, but for now, the image size won't be updated. It doesn't make sense and will confuse the end user."
624,1263813,neutron,f778931252fbee34980e32289e96534eebb89c2b,1,1, Corrects broken format strings ,TypeError - not enough args for format string in i...,"jpipes@uberbox:~/repos/openstack/neutron$ git log --oneline | head -1
84aeb9a Merge ""Imported Translations from Transifex""
Running tox -eALL resulted in:
i18n runtests: commands[0] | python ./tools/check_i18n.py ./neutron ./tools/i18n_cfg.py
Traceback (most recent call last):
  File ""./tools/check_i18n.py"", line 151, in <module>
    debug):
  File ""./tools/check_i18n.py"", line 112, in check_i18n
    ASTWalker())
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 106, in walk
    walker.preorder(tree, visitor)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 63, in preorder
    self.dispatch(tree, *args) # XXX *args make sense?
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 62, in visitConst
    self.lines[node.lineno - 1][:-1], msg),
TypeError: not enough arguments for format string
ERROR: InvocationError: '/home/jpipes/repos/openstack/neutron/.tox/i18n/bin/python ./tools/check_i18n.py ./neutron ./tools/i18n_cfg.py'"
625,1263820,cinder,1e73e64e7407294dc68af9c09323e2b396335c4a,0,0,Handle a new exception,Handle terminate_connection() exception in volume ...,"Due to the fact that we sometimes need to manually terminate a volume's connection through volume api, it's possile that these  backend drivers throw exceptions while doing that.  Currently exceptions are bubbled up to volume API and are not being handled.  This patch logs exception in volume manager and then raises VolumeBackendAPIException to caller."
626,1263866,neutron,501213686886baccd3280e10b8856a25d3517519,1,1,I see a race condition. Bug,OVS lib deferred apply cannot handle concurrency,"OVS lib propose a deferred apply methods to save system calls to 'ovs-ofctl' binaries ('ovs-ofctl' can apply flow from file or stdin if file is '-').
This method use a dict for 'add', mod' or 'del' flow actions that contain a concatenated string flows. This dict is purge after all flows are applied at the end of 'deferred_apply_off' method.
If another call is made on that dict during the 'deferred_apply_off', some flows could be deleted a the end when they have not been applied.
I can see that on ML2 plugin with l2-pop mechanism driver. If I delete more than one port at a time, some flooding flow rules could be not deleted on the br-tun bridge."
627,1263881,neutron,7e91362dbb974df5ee44d346e1d0971479510d4b,1,1, fails when more than one port is added/deleted simultaneously ,ML2 l2-pop Mechanism driver fails when more than o...,"If I create more than one VM at a time (with nova boot option --num-instances), sometimes the flooding flow for broadcast, multicast and unknown unicast are not added on certain l2 agents.
And conversely, when I delete more than one VM at a time, the flooding rule are not purge on certain l2 agents when it's necessary.
I made this test on the trunk version with OVS agent."
628,1263945,glance,7e13cf73f1869599c1218cf58468c21bb22dd5bf,1,0, Backwards compat support breaks transport_url,Backwards compat support breaks transport_url,"Passing a fake url to `get_transport` makes it ignore the configured `transport_url`. This blocks any chance of using the new transport_url configuration.
https://github.com/openstack/glance/blob/master/glance/notifier.py#L81"
629,1264053,cinder,5fe07c1be34a73c457bd8e6b8a86bfc6e53567d5,0,0,Remove redundant check,Redundant size check in volume restore api,"In cinder  volume restore api, we do twice size check when restore volume with a given volume uuid. It's necessary to remove the redundant check."
630,1264193,glance,1a9f95329f269673efffb35ca403292f7d404333,1,1,Fix inconsistent doc string and code,wrong doc string for glance.db.sqlalchemy.migratio...,"When review patch: https://review.openstack.org/#/c/64076/
I found that:
def db_sync(version=None, current_version=None):
    """"""
Place a database under migration control and perform an upgrade
:retval version number
""""""
    sql_connection = CONF.sql_connection
    try:
        _version_control(current_version or 0)
    except versioning_exceptions.DatabaseAlreadyControlledError as e:
        pass
    if current_version is None:
        current_version = int(db_version())
    if version is not None and int(version) < current_version:
        downgrade(version=version)
    elif version is None or int(version) > current_version:
        upgrade(version=version)
db_sync doesn't return anything, the doc string and code are broken. Since db_sync() accept empty arguments list, so a reasonable version number should be returned instead of remove the incorrect doc string."
631,1264204,glance,2bebf1090156da5a773df723026c16bc40d8a138,0,0,Tests,Bug #1264204 “redundant code in glance.tests.unit.utils,"glance.tests.unit.utils:FakeDB uses glance.db.simple.api
FakeDB  implements reset with:
    @staticmethod
    def reset():
        simple_db.DATA = {
            'images': {},
            'members': [],
            'tags': {},
            'locations': [],
            'tasks': {},
            'task_info': {}
        }
while api already has:
def reset():
    global DATA
    DATA = {
        'images': {},
        'members': [],
        'tags': {},
        'locations': [],
        'tasks': {},
        'task_info': {}
    }
the redundant code in glance.tests.unit.utils:FakeDB.reset should be replaced with simple_db.reset()"
632,1264220,nova,8010c8faf9f030d2c0264189f9e6c70e10a093f2,1,1,(ef3b1385cb3708aa4993a149ce24082b4082e347),An internal error happens if passing invalid param...,"If passing invalid parameter to ""create flavor_extraspecs"" API, an internal error happens and Traceback is written in log file.
Nova should return BadRequest response instead of internal error.
$ curl -i 'http://10.21.42.109:8774/v2/fd283c7ef47b4f46899403e9ebb1e2ed/flavors/6e05eb08-ef1a-4183-9eb9-5c175060247a/os-extra_specs' [..] -d '{""foo"": {""key01"": ""value01""}}'
HTTP/1.1 500 Internal Server Error
Content-Length: 128
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-e8b5120c-e585-4095-a387-d3eed52b7415
Date: Thu, 26 Dec 2013 11:06:46 GMT
{""computeFault"": {""message"": ""The server has either erred or is incapable of performing the requested operation."", ""code"": 500}}
$
** The log of nova-api **
2013-12-26 20:06:46.026 ERROR nova.api.openstack [req-e8b5120c-e585-4095-a387-d3eed52b7415 admin demo] Caught error: 'NoneType' object has no attribute 'keys'
2013-12-26 20:06:46.026 TRACE nova.api.openstack Traceback (most recent call last):
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/__init__.py"", line 121, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return req.get_response(self.application)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-12-26 20:06:46.026 TRACE nova.api.openstack     application, catch_exc_info=False)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-12-26 20:06:46.026 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 581, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return self.app(env, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     response = self.app(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 930, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     content_type, body, accept)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 992, in _process_stack
2013-12-26 20:06:46.026 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 1073, in dispatch
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return method(req=request, **action_args)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/compute/contrib/flavorextraspecs.py"", line 76, in create
2013-12-26 20:06:46.026 TRACE nova.api.openstack     specs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/api.py"", line 1487, in flavor_extra_specs_update_or_create
2013-12-26 20:06:46.026 TRACE nova.api.openstack     extra_specs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 130, in wrapper
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return f(*args, **kwargs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 4483, in flavor_extra_specs_update_or_create
2013-12-26 20:06:46.026 TRACE nova.api.openstack     filter(models.InstanceTypeExtraSpecs.key.in_(specs.keys())).\
2013-12-26 20:06:46.026 TRACE nova.api.openstack AttributeError: 'NoneType' object has no attribute 'keys'"
633,1264223,cinder,534ce5dde117a61abbab49e8b1bc34b5fcbfc095,1,1,typo in code,webob.exc.HTTPBadRequest doesn't accept parameter ...,"If the specific error message is required, we must set explanation='message' to initialize subclass of webob.exc.WSGIHTTPException(). There is some code specify the message to 'reason' incorrectly.
Ex.
OK: raise webob.exc.HTTPBadRequest(explanation=_('What?'))
WRONG: raise webob.exc.HTTPBadRequest(reason=_(""What's up?""))"
634,1264360,cinder,8328fc46d783f4ec9286eededafa91afae89cba0,1,0,Mysql specific,Downgrading cinder schema fails when running 018_a...,"Downgrading the cinder schema fails when running  018_add_qos_specs.py under MySQL.  The upgrade path of this schema patch adds the foreign key volume_types_ibfk_1 on table volume_types, and the downgrade does not correspondingly remove it before attempting to drop the qos_specs_id column."
635,1264424,cinder,57fd92a3943aa4856de6ceb18a7b704ed7fae7c6,0,0,Feature: Some exc.HTTPError subclass needs keyword argument ``explanation`` ,Some exc.HTTPError subclass needs keyword argument...,"Ex.
  NG: raise webob.exc.HTTPBadRequest('What?')
  OK: raise webob.exc.HTTPBadRequest(explanation='What?')
If there is no ``explanation=``, the message of the argument is
recognized as ``detail``."
636,1264428,glance,1864939720c94b602699f69002c6467fda247deb,0,0,tests,Incorrect URL requested in Glance v1 API test_get_...,"The test code in glance/tests/unit/v1/test_api.py:TestGlanceAPI.test_get_images_unauthorized requests for the wrong URL - instead of requesting for ""/images"" it requests for ""/images/detail"" (making it identical to the test_get_images_unauthorized test)."
638,1265081,neutron,de4b5a2bba57cbd5294fb6f9527d64b8eeddf901,1,1,Fix integrity,Bug #1265081 “nicira,"This is a stacktrace experienced during a test on trunk:
2013-12-30 13:42:39.606 29118 DEBUG routes.middleware [-] Matched DELETE /ports/a42a9719-8416-4c44-a4f3-b3861648281f __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-12-30 13:42:39.606 29118 DEBUG routes.middleware [-] Route path: '/ports/{id}{.format}', defaults: {'action': u'delete', 'controller': <wsgify at 72315472 wrapping <function resource at 0x44f2500>>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-12-30 13:42:39.606 29118 DEBUG routes.middleware [-] Match dict: {'action': u'delete', 'controller': <wsgify at 72315472 wrapping <function resource at 0x44f2500>>, 'id': u'a42a9719-8416-4c44-a4f3-b3861648281f', 'format': None} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-12-30 13:42:39.606 29118 DEBUG neutron.openstack.common.rpc.amqp [req-d8b115ac-38c3-4690-b670-7c81a9d9ca15 663434b6088b4984991d07a858d6f6bc 4a9f94ca2c674047b8e86fae8eefc9a4] Sending port.delete.start on notifications.info notify /opt/stack/neutron/neutron/openstack/common/rpc/amqp.py:598
2013-12-30 13:42:39.607 29118 DEBUG neutron.openstack.common.rpc.amqp [req-d8b115ac-38c3-4690-b670-7c81a9d9ca15 663434b6088b4984991d07a858d6f6bc 4a9f94ca2c674047b8e86fae8eefc9a4] UNIQUE_ID is 3f460839b3144eaa911c04de8725eb06. _add_unique_id /opt/stack/neutron/neutron/openstack/common/rpc/amqp.py:339
2013-12-30 13:42:39.610 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3681] Acquired connection https://192.168.1.8:443. 9 connection(s) available. acquire_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:134
2013-12-30 13:42:39.611 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3681] Issuing - request POST https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:99
2013-12-30 13:42:39.611 29118 DEBUG neutron.plugins.nicira.api_client.request [-] Setting X-Nvp-Wait-For-Config-Generation request header: '96230' _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:124
2013-12-30 13:42:39.647 29118 DEBUG neutron.plugins.nicira.nvplib [req-d8b115ac-38c3-4690-b670-7c81a9d9ca15 663434b6088b4984991d07a858d6f6bc 4a9f94ca2c674047b8e86fae8eefc9a4] Looking for port with q_port_id tag 'a42a9719-8416-4c44-a4f3-b3861648281f' on: '6d795954-a836-47f2-b2e3-2438e0da7f80' get_port_by_neutron_tag /opt/stack/neutron/neutron/plugins/nicira/nvplib.py:743
2013-12-30 13:42:39.648 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3682] Acquired connection https://192.168.1.8:443. 8 connection(s) available. acquire_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:134
2013-12-30 13:42:39.648 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3682] Issuing - request GET https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport?fields=uuid&tag_scope=q_port_id&tag=a42a9719-8416-4c44-a4f3-b3861648281f _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:99
2013-12-30 13:42:39.648 29118 DEBUG neutron.plugins.nicira.api_client.request [-] Setting X-Nvp-Wait-For-Config-Generation request header: '96230' _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:124
2013-12-30 13:42:39.670 29118 DEBUG neutron.openstack.common.rpc.amqp [-] received {u'_context_roles': [u'admin'], u'_context_request_id': u'req-0f858449-8d08-4c71-af5c-3fa7b4470f20', u'_context_tenant_name': None, u'_context_project_name': None, u'_context_read_deleted': u'no', u'args': {u'agent_state': {u'agent_state': {u'topic': u'dhcp_agent', u'binary': u'neutron-dhcp-agent', u'host': u'cidevstack', u'agent_type': u'DHCP agent', u'configurations': {u'subnets': 3, u'use_namespaces': True, u'dhcp_lease_duration': 86400, u'dhcp_driver': u'neutron.agent.linux.dhcp.Dnsmasq', u'networks': 3, u'ports': 5}}}, u'time': u'2013-12-30T21:42:39.665454'}, u'_context_tenant': None, u'method': u'report_state', u'_unique_id': u'49ca1db3e75247e594c8c4005b4b262a', u'_context_timestamp': u'2013-12-30 21:42:39.665074', u'_context_is_admin': True, u'version': u'1.0', u'_context_project_id': None, u'_context_tenant_id': None, u'_context_user': None, u'_context_user_id': None, u'namespace': None, u'_context_user_name': None} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276
2013-12-30 13:42:39.670 29118 DEBUG neutron.openstack.common.rpc.amqp [-] unpacked context: {'project_name': None, 'user_id': None, 'roles': [u'admin'], 'tenant_id': None, 'request_id': u'req-0f858449-8d08-4c71-af5c-3fa7b4470f20', 'is_admin': True, 'tenant': None, 'timestamp': u'2013-12-30 21:42:39.665074', 'tenant_name': None, 'project_id': None, 'user_name': None, 'read_deleted': u'no', 'user': None} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276
2013-12-30 13:42:39.671 29118 DEBUG neutron.context [req-0f858449-8d08-4c71-af5c-3fa7b4470f20 None None] Arguments dropped when creating context: {'project_name': None, 'tenant': None} __init__ /opt/stack/neutron/neutron/context.py:84
2013-12-30 13:42:39.681 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3681] Completed request 'POST https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport': 201 (0.07 seconds) _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:141
2013-12-30 13:42:39.682 29118 DEBUG neutron.plugins.nicira.api_client.request [-] Reading X-Nvp-config-Generation response header: '96231' _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:146
2013-12-30 13:42:39.682 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3681] Released connection https://192.168.1.8:443. 9 connection(s) available. release_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:189
2013-12-30 13:42:39.684 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3682] Completed request 'GET https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport?fields=uuid&tag_scope=q_port_id&tag=a42a9719-8416-4c44-a4f3-b3861648281f': 200 (0.04 seconds) _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:141
2013-12-30 13:42:39.684 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3682] Released connection https://192.168.1.8:443. 10 connection(s) available. release_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:189
2013-12-30 13:42:39.685 29118 DEBUG neutron.plugins.nicira.api_client.request_eventlet [-] [3681] Completed request 'POST /ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport': 201 _handle_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request_eventlet.py:156
2013-12-30 13:42:39.685 29118 DEBUG neutron.plugins.nicira.api_client.request_eventlet [-] [3682] Completed request 'GET /ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport?fields=uuid&tag_scope=q_port_id&tag=a42a9719-8416-4c44-a4f3-b3861648281f': 200 _handle_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request_eventlet.py:156
2013-12-30 13:42:39.686 29118 DEBUG neutron.plugins.nicira.nvplib [-] Created logical port d612afd7-339d-4f25-97d2-e6f6c966551e on logical switch 6d795954-a836-47f2-b2e3-2438e0da7f80 create_lport /opt/stack/neutron/neutron/plugins/nicira/nvplib.py:856
2013-12-30 13:42:39.696 29118 ERROR neutron.api.v2.resource [-] delete failed
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 1342, in delete_port
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     port_delete_func(context, neutron_db_port)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 498, in _nvp_delete_port
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     port_data)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 750, in _nvp_get_port_id
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     nvp_port['uuid'])
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/dbexts/nicira_db.py"", line 54, in add_neutron_nvp_port_mapping
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     return mapping
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     self.commit()
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     self._prepare_impl()
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     self.session.flush()
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 541, in _wrap
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     _raise_if_duplicate_entry_error(e, get_engine().name)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 492, in _raise_if_duplicate_entry_error
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     raise exception.DBDuplicateEntry(columns, integrity_error)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource DBDuplicateEntry: (IntegrityError) (1062, ""Duplicate entry 'a42a9719-8416-4c44-a4f3-b3861648281f' for key 'PRIMARY'"") 'INSERT INTO quantum_nvp_port_mapping (quantum_id, nvp_id) VALUES (%s, %s)' ('a42a9719-8416-4c44-a4f3-b3861648281f', 'd612afd7-339d-4f25-97d2-e6f6c966551e')
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource
2013-12-30 13:42:39.700 29118 INFO neutron.wsgi [-] 127.0.0.1 - - [30/Dec/2013 13:42:39] ""DELETE //v2.0/ports/a42a9719-8416-4c44-a4f3-b3861648281f HTTP/1.1"" 500 249 0.097381
It is unclear what the root cause is at the moment."
639,1265132,neutron,02f4516ad38c5deeefd686b87edf4bf06cffe5af,0,0,redundant code,neutron code redundancy,"I found the code in l3_db.py is redundant,the code is below:
in function _update_router_gw_info:
        if gw_port and gw_port['network_id'] != network_id:
            fip_count = self.get_floatingips_count(context.elevated(),
                                                   {'router_id': [router_id]})
            if fip_count:
                raise l3.RouterExternalGatewayInUseByFloatingIp(
                    router_id=router_id, net_id=gw_port['network_id'])
            if gw_port and gw_port['network_id'] != network_id:
                with context.session.begin(subtransactions=True):
                    router.gw_port = None
                    context.session.add(router)
                self._core_plugin.delete_port(context.elevated(),
                                              gw_port['id'],
                                              l3_port_check=False)
it does not need the third if!"
640,1265148,cinder,a4c750f3027977551c1fdab430040689a620edb0,0,0,redundant variable creation,Redundant variable declarations  in create(),"The method create() in /cinder/api/v2/volumes.py has two redundant variable declarations:
 image_href = None
 image_uuid = None
these should be reduced."
641,1265264,nova,608a710db5bcf4359e1e445553143cd39ffe194c,0,0,unused import,Remove unused import,"************* Module destroy_cached_images
W: 31, 0: Unused import logging (unused-import)"
642,1265267,neutron,ab97446132da9b0aff4d13ac01591f86aa066d48,0,0,unused import,Remove unused imports,"************* Module neutron.plugins.plumgrid.plumgrid_plugin.plumgrid_plugin
W: 33, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.metaplugin.meta_neutron_plugin
W: 30, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.nec.db.api
W: 27, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.nec.nec_plugin
W: 36, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.nec.common.config
W: 21, 0: Unused import rpc (unused-import)
--
************* Module neutron.plugins.ryu.ryu_neutron_plugin
W: 43, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ryu.agent.ryu_neutron_agent
W: 46, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.midonet.plugin
W: 51, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.midonet.agent.midonet_driver
W: 24, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.db.mlnx_db_v2
W: 26, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.agent.eswitch_neutron_agent
W: 39, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.common.comm_utils
W: 23, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.mlnx_plugin
W: 37, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.hyperv.hyperv_neutron_plugin
W: 29, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.nicira.check_nvp_config
W: 28, 0: Unused import nvp_cfg (unused-import)
************* Module neutron.plugins.nicira.NeutronServicePlugin
W: 32, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.nicira.vshield.vcns_driver
W: 22, 0: Unused import nicira_cfg (unused-import)
--
************* Module neutron.plugins.nicira.NeutronPlugin
W: 48, 0: Unused import quota_db (unused-import)
W: 62, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.plugin
W: 31, 0: Unused import quota_db (unused-import)
W: 46, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.drivers.mech_arista.mechanism_arista
W: 26, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.drivers.l2pop.mech_driver
W: 27, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.drivers.cisco.network_db_v2
W: 24, 0: Unused import nexus_models_v2 (unused-import)
--
************* Module neutron.plugins.linuxbridge.db.l2network_db_v2
W: 25, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.linuxbridge.lb_neutron_plugin
W: 40, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent
W: 50, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.openvswitch.agent.ovs_neutron_agent
W: 51, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.openvswitch.ovs_neutron_plugin
W: 49, 0: Unused import quota_db (unused-import)
W: 62, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.embrane.base_plugin
W: 33, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.cisco.db.network_db_v2
W: 29, 0: Unused import nexus_models_v2 (unused-import)
--
************* Module neutron.tests.unit.nec.test_db
W: 25, 0: Unused import nmodels (unused-import)
--
************* Module neutron.tests.unit.nec.test_security_group
W: 25, 0: Unused import ndb (unused-import)
--
************* Module neutron.tests.unit.nec.test_ofc_manager
W: 24, 0: Unused import nmodels (unused-import)
--
************* Module neutron.tests.unit.ryu.test_defaults
W: 20, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.ryu.test_ryu_plugin
W: 19, 0: Unused import ryu_models_v2 (unused-import)
--
************* Module neutron.tests.unit.ryu.test_ryu_db
W: 24, 0: Unused import ryu_models_v2 (unused-import)
W: 22, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.mlnx.test_defaults
W: 19, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.mlnx.test_mlnx_comm_utils
W: 20, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.nicira.test_nvplib
W: 25, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.openvswitch.test_ovs_defaults
W: 18, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.embrane.test_embrane_l3_plugin
W: 26, 0: Unused import config (unused-import)
************* Module neutron.tests.unit.embrane.test_embrane_defaults
W: 25, 0: Unused import config (unused-import)
************* Module neutron.tests.unit.embrane.test_embrane_neutron_plugin
W: 26, 0: Unused import config (unused-import)
--
************* Module install_venv
W: 29, 0: Unused import subprocess (unused-import)"
643,1265408,nova,5930e5f9acad1e42b3eb32f62b8cbc1d9b4c6384,0,0,unused code,Remove unused code in nova/api/ec2/__init__.py,"""result = None"" in https://github.com/openstack/nova/blob/master/nova/api/ec2/__init__.py#L546  is unused code, remove it"
644,1265446,glance,d37907af5b71024024fb70d9454e4b3e05875105,1,1,,v2 upload returns '500' when quota exceeded on loc...,"Steps to reproduce:
1) Configure glance to use local filesystem backend, make sure the quota is explicitly configured.
2) glance --os-auth-token ""$(cat ~/user-token)"" \
    --os-image-url http://127.0.0.1:9292 \
    --os-image-api-version 2 \
    image-create --container-format bare \
        --disk-format qcow2 --name localfs-image
3) Upload image data. Make sure the image data size > quota
 glance --os-auth-token ""$(cat ~/user-token)"" \
    --os-image-url http://127.0.0.1:9292 \
    --os-image-api-version 2 \
    image-upload --file $HOME/testImg.qcow2 $img_id
Request returned failure status.
HTTPInternalServerError (HTTP 500)
Same thing happens if you use curl:
1) Configure glance to use local filesystem backend
2) glance --os-auth-token ""$(cat ~/user-token)"" \
    --os-image-url http://127.0.0.1:9292 \
    --os-image-api-version 2 \
    image-create --container-format bare \
        --disk-format qcow2 --name localfs-image
3) curl -i -X PUT -H 'Transfer-Encoding: chunked' \
    -H 'X-Auth-Token: $(cat ~/user-token)' \
    -H 'Content-Type: application/octet-stream' \
    -H 'User-Agent: python-glanceclient' \
    --data-binary @$HOME/testImg.qcow2 http://127.0.0.1:9292/v2/images/$img_id/file
HTTP/1.1 100 Continue
HTTP/1.1 500 Internal Server Error
Content-Type: text/plain
Content-Length: 0
Date: Thu, 02 Jan 2014 07:27:26 GMT
Connection: close"
645,1265448,glance,e19e2b812031802dd46f2f29b0a0508b9f7ac470,1,0,Evolution bug,exception.NotFound is parsed wrong at the API leve...,"When user updates an image with location but introduced a typo, it's expected to run into location not found exception. However, because we don't distinguish the image not found and location not found, so Glance will say the image is not found based on current design.
Failed to find image 41a997ac-e647-4b5f-b158-8e6e13875f88 to update
See https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L124 and https://github.com/openstack/glance/blob/master/glance/store/filesystem.py#L156 Though I think the issue is also existed in other backends."
646,1265452,nova,d48dd03e4278ef3d6ed50521f595254167726d3c,1,1,,cache lock for image not consistent,"According to this bug https://bugs.launchpad.net/nova/+bug/1256306
for one image in _base dir 03d8e206-6500-4d91-b47d-ee74897f9b4e
2 locks were created
-rw-r--r-- 1 nova nova 0 Oct 4 20:40 nova-03d8e206-6500-4d91-b47d-ee74897f9b4e
-rw-r--r-- 1 nova nova 0 Oct 4 20:40 nova-_var_lib_nova_instances__base_03d8e206-6500-4d91-b47d-ee74897f9b4e
generally locks are used to protect concurrent data access, so the lock can't work as they expected (mutual access)
in current code fetch_image from glance use lock nova-xxxxx while copy image from _base to target directory use nova_var_lib_xxx
should they use same lock?"
647,1265465,nova,793604653f68e2d7d184182d0a976a7d43c07607,1,1,,Bug #1265465 “xenapi,"When the XenAPI driver resizes a boot partition, it does not take care to add back the boot partition flag.
With PV images, this is not really needed, because Xen doesn't worry about the partition being bootable, but for HVM images, it is stops the image from booting any more."
648,1265494,nova,6abda5b738c5d801ede3328cf0ec1dc9dddc022f,1,1,"It is kept as pause, and shouldn’t",Bug #1265494 “Openstack Nova,"Description of problem:
Unpauseing an instance fails if host has rebooted.
Version-Release number of selected component (if applicable):
RHEL: release 6.5 (Santiago)
openstack-nova-api-2013.2.1-1.el6ost.noarch
openstack-nova-compute-2013.2.1-1.el6ost.noarch
openstack-nova-scheduler-2013.2.1-1.el6ost.noarch
openstack-nova-common-2013.2.1-1.el6ost.noarch
openstack-nova-console-2013.2.1-1.el6ost.noarch
openstack-nova-conductor-2013.2.1-1.el6ost.noarch
openstack-nova-novncproxy-2013.2.1-1.el6ost.noarch
openstack-nova-cert-2013.2.1-1.el6ost.noarch
How reproducible:
Every time
Steps to Reproduce:
1. Boot an instance
2. Pause that instance
3. Reboot host
4. Unpause instance
Actual results:
can't unpause instance stuck in status paused, power state - shutdown
Expected results:
Instance should unpause, return to running state
Additional info:
virsh list -all --managed-save
ID is missing from paused instance ""-"" (pausecirros), state -> shut off.
[root@orange-vdse ~(keystone_admin)]# virsh list --all --managed-save
 Id    Name                           State
----------------------------------------------------
 1     instance-00000003              running
 2     instance-00000002              running
 -     instance-00000001              shut off
[root@orange-vdse ~(keystone_admin)]# nova list  (notice nova status paused)
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ID                                   | Name          | Status | Task State | Power State | Networks        |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ebe310c2-d715-45e5-83b6-32717af1ac90 | cirros        | ACTIVE | None       | Running     | net=192.168.1.4 |
| 3ef89feb-414f-4524-b806-f14044efdb14 | pausecirros   | PAUSED | None       | Shutdown    | net=192.168.1.5 |
| 8bcae041-2f92-4ae2-a2c2-ee59b067ac76 | suspendcirros | ACTIVE | None       | Running     | net=192.168.1.2 |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
Testing without rebooting host, ID/state (""1""/paused) instance (cirros) are ok and it unpauses ok.
[root@orange-vdse ~(keystone_admin)]# virsh list --all --managed-save
 Id    Name                           State
----------------------------------------------------
 1     instance-00000003              paused
 2     instance-00000002              running
 -     instance-00000001              shut off
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ID                                   | Name          | Status | Task State | Power State | Networks        |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ebe310c2-d715-45e5-83b6-32717af1ac90 | cirros        | PAUSED | None       | Paused      | net=192.168.1.4 |
| 3ef89feb-414f-4524-b806-f14044efdb14 | pausecirros   | PAUSED | None       | Shutdown    | net=192.168.1.5 |
| 8bcae041-2f92-4ae2-a2c2-ee59b067ac76 | suspendcirros | ACTIVE | None       | Running     | net=192.168.1.2 |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+"
649,1265512,nova,55296828a7d7a03736a21d846da15571ca22ec9e,1,1,,Bug #1265512 “VMware,"In some cases, the session with the VC is terminated and restarted again. This can happen for example when the user does:
nova list (and there are no running VMs)
In addition to the restart of the session the operation also waits 2 seconds."
650,1265561,glance,5f56945157f94e71ca02a4b86a2d6528afc99058,0,0,More helpful logs,Log message printed for unhandled exception is not...,"Currently, on an unhandled exception, the error message logged through sys.excepthook is not very helpful.
Currently it prints only the exception_value.
https://github.com/openstack/oslo-incubator/blob/master/openstack/common/log.py#L396
Fix:
Make the log message print both the exception_type and exception_value of the unhandled exception.
PS: Currently the traceback is printed only when the VERBOSE is ON."
651,1265607,nova,e73072e76c2aedfc52cd2de480b2f41b60769acc,1,1,,Instance.refresh() sends new info_cache objects,If an older node does an Instance.refresh() it will fail because conductor will overwrite the info_cache field with a new InstanceInfoCache object. This happens during the LifecycleEvent handler in nova-compute.
652,1265711,glance,a5b78ae0b502ad0ee95037f5b1d9619dba3dab81,1,1,Negative numbers raise a bug,image is creating with option min_disk and min_ram...,"I try to create a image with the min_ram and min_disk as negative number, and create image operation is successful.
I use the glance api V2 to create  image.
curl -i -X POST  -H ""X-Auth-Token: $token"" -H ""content-type: application/json"" -d '{""name"": ""image-7"", ""type"": ""kernel"", ""foo"": ""bar"", ""disk_format"": ""aki"", ""container_format"": ""aki"", ""protected"": false, ""tags"": [""test"",""image""], ""visibility"": ""public"", ""min_ram"":-1, ""min_disk"":-1' \
http://192.168.0.100:9292/v2/images
glance image-show image-7
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| Property 'foo'   | bar                                  |
| Property 'type'  | kernel                               |
| container_format | aki                                  |
| created_at       | 2014-01-03T06:01:14                  |
| deleted          | False                                |
| disk_format      | aki                                  |
| id               | aaa8e463-10aa-4518-a590-52feebaabcb5 |
| is_public        | True                                 |
| min_disk         | -1                                   |
| min_ram          | -1                                   |
| name             | image-7                              |
| owner            | 25adaa8f93ee4199b6a362c45745231d     |
| protected        | False                                |
| status           | queued                               |
| updated_at       | 2014-01-03T06:01:14                  |
+------------------+--------------------------------------+
I think glance need to check the value of min_ram and  min_disk. They need to be greater than or equal 0."
653,1265725,cinder,3c876c67eb32ecffb58a2234c828e5cfee70ec90,0,0,Remoe unused parameters,remove unused context parameter in check_*** metho...,"Input parameter 'context' of Check_*** methods(check_attach, check_detach, _check_metadata_properties, _check_volume_availability) in cinder/volume/api.py are not being used, so remove this 'context' parameter."
654,1265893,cinder,c32302c5d5f35df39b49d32fcba47a4d36b3783a,0,0,Update V2 API,Update V2 API to return detailed volume informatio...,"Currently the V2 API returns volume summary after volume create which requires the cinderclient to invoke additional GET request to get additional details.
This bug is to update the summary method to detail and remove the additional get from cinderclient."
655,1266048,cinder,3e4f554f614c8cb6d5f014c72f3c635184a4dec2,1,1,,Copy image to volume iSCSI multipath doesn't work ...,"The connect_volume and disconnect_volume code in brick assumes that the targets for different portals are the same for the same multipath device.  This is true for some arrays but not for others.  When there are different targets associated with different portals for the same multipath device, multipath doesn't work properly during copy image to volume and copy volume to image operations."
656,1266051,nova,429ac4dedd617f8c1f7c88dd8ece6b7d2f2accd0,1,1,,Attach/detach volume iSCSI multipath doesn't work ...,"The connect_volume and disconnect_volume code in LibvirtISCSIVolumeDriver assumes that the targets for different portals are the same for the same multipath device. This is true for some arrays but not for others. When there are different targets associated with different portals for the same multipath device, multipath doesn't work properly during attach/detach volume operations."
657,1266334,neutron,bf8bf46cbbe7428c9a12886604fe5aa10019aaec,0,0, Quota extension is not enabled,Quota extension is not enabled for bigswitch plugi...,Any quota request made to neutron fails with the bigswitch plugin. Enabling the quota extension will fix this.
658,1266344,nova,521a03bb67724ab545430ea891eb6d6d90b992f5,0,0,Remove duplicate code,duplicate __init__() in ExtensionResource,"Let's go in codes directly, to get resources from extension, the child class[0] has an __init__() to register itself, and the farther class [1]'s __init__() has already did this. If there are no some specified variables needed by child class, the child's __init__ could be removed. In <nova>/nova/api/openstack/compute/contrib/, nearly all of extensions class don't have such a duplicate __init__(). Removing the __init__() in child class could help keeping codes consistent and clean.
[0] https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/contrib/fixed_ips.py#L88
[1] https://github.com/openstack/nova/blob/master/nova/api/openstack/extensions.py#L63"
659,1266450,nova,b9a2ad044c68212665c15aa49f1257cd4168c738,0,0,unused variables,Remove unused variables in imagebackend.py,"The method create() in /nova/virt/libvirt/ imagebackend.py has two unused
variable: old_format and features.These should be removed."
660,1266534,nova,cd0371280dd204a10673d5d1b70ba0b9f2757a22,0,0,Fixing tests,baselineCPU parser break unit tests in nova,"The fix for https://bugs.launchpad.net/nova/+bug/1259796 break nova unit tests. The original test nova.tests.virt.libvirt.test_libvirt.LibvirtConnTestCase.test_cpu_features_bug_1217630 and more tests that parse xml are broken. Details here:
http://paste.openstack.org/show/60471/"
661,1266537,neutron,d9bcd597c67900472709cd11604afe1616e0af4e,1,1,Race condition,internal neutron server error on tempest VolumesAc...,"The following traces were found when  VolumesActionsTest has failed:
http://logs.openstack.org/18/63918/5/gate/gate-tempest-dsvm-neutron-pg-isolated/5a67370/console.html
http://logs.openstack.org/18/63918/5/gate/gate-tempest-dsvm-neutron-pg-isolated/5a67370/logs/screen-q-svc.txt.gz?level=TRACE#_2014-01-04_11_56_37_489"
662,1266579,nova,743b74d59d3279478c512a0a559642b1d05284bc,1,1,Race condition,Bug #1266579 “VMware,"When running the Tempest tests in parallel, 2 tests are failing due to instances spawning with ERROR:
  setUpClass (tempest.api.compute.admin.test_fixed_ips.FixedIPsTestXml)
  setUpClass (tempest.api.compute.admin.test_fixed_ips_negative.FixedIPsNegativeTestJson)
The VimFaultException seen in the nova scheduler log is:
  Cannot complete the operation because the file or folder [datastore1] vmware_base already exists
Full Traceback here (non-wrapped version here: http://paste.openstack.org/show/60502/):
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1053, in _build_instance
    set_access_ip=set_access_ip)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 356, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1456, in _spawn
    LOG.exception(_('Instance failed to spawn'), instance=instance)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1453, in _spawn
    block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 587, in spawn
    admin_password, network_info, block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 437, in spawn
    upload_folder, upload_name + "".vmdk"")):
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1549, in _check_if_folder_file_exists
    ds_ref)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1534, in _mkdir
    createParentDirectories=False)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 795, in _call_method
    return temp_module(*args, **kwargs)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 200, in vim_request_handler
    raise error_util.VimFaultException(fault_list, excep)
VimFaultException: Server raised fault: 'Cannot complete the operation because the file or folder [datastore1] vmware_base already exists'"
663,1266611,nova,aa1792eb4c1d10e9a192142ce7e20d37871d916a,1,1,a fix commit add a bug,test_create_image_with_reboot fails with InstanceI...,"Looks like an intermittent failure:
http://logs.openstack.org/25/64725/4/check/gate-nova-python27/e603e9e/testr_results.html.gz
2014-01-06 21:49:45.870 | Traceback (most recent call last):
2014-01-06 21:49:45.870 |   File ""nova/tests/api/ec2/test_cloud.py"", line 2343, in test_create_image_with_reboot
2014-01-06 21:49:45.870 |     self._do_test_create_image(False)
2014-01-06 21:49:45.871 |   File ""nova/tests/api/ec2/test_cloud.py"", line 2316, in _do_test_create_image
2014-01-06 21:49:45.871 |     no_reboot=no_reboot)
2014-01-06 21:49:45.871 |   File ""nova/api/ec2/cloud.py"", line 1709, in create_image
2014-01-06 21:49:45.872 |     name)
2014-01-06 21:49:45.872 |   File ""nova/compute/api.py"", line 161, in inner
2014-01-06 21:49:45.872 |     method=f.__name__)
2014-01-06 21:49:45.873 | InstanceInvalidState: Instance b1d4d924-069c-409c-bbdb-4f0478526057 in task_state powering-off. Cannot snapshot_volume_backed while the instance is in this state."
664,1266617,nova,20fa2a35c843afe409ad347a40751bb59f9ecf9e,0,0,unused code,Remove unused code in test_attach_interfaces.py,"Remove unused code in test_attach_interfaces.py:
body = jsonutils.dumps({'port_id': FAKE_PORT_ID1}) is unused in test_attach_interface_without_network_id().
We should remove it."
665,1266620,nova,c167db9536c2d1ad22ac7463d448d092ca5b530e,1,1,Wrong message,Bug #1266620 “bad english in string,"This string: ""Disabled reason contains invalid characters or is too long""
found in
nova/api/openstack/compute/contrib/services.py:177, nova/api/openstack/compute/plugins/v3/services.py:159
should be fixed.
From the spanish translation team:
Se recomienda que esta cadena sea reestructurada en su versión original, particularmente ""Disabled reason"".
Essentially, the string isn't the best English and should be re-written so it can be translated properly - particularly the use of ""Disabled reason""."
666,1266634,nova,c199a14039ddcde63896456ee25db68da1afcc8f,0,0,tests,unnecessary parameter is found in quota-classes re...,"Nova quota-classes request should be in the format like:
body = {'quota_class_set': {'instances': 50, 'cores': 50,
                            'ram': 51200, 'floating_ips': 10,
                            'fixed_ips': -1, 'metadata_items': 128,
                            'security_groups': 10,
                            'security_group_rules': 20,
                            'key_pairs': 100,
                            }}
But in some unit test cases, the id parameter is found in the body, it should be deleted.
 def test_quotas_update_as_admin(self):
     body = {'quota_class_set': {'instances': 50, 'cores': 50,
                                 'ram': 51200, 'floating_ips': 10,
                                 'fixed_ips': -1, 'metadata_items': 128,
                                 ★'id': 'test_class', ★
                                 'security_groups': 10,
                                 'security_group_rules': 20,
                                 'key_pairs': 100}}"
667,1266650,neutron,d4ec8e13d7b43a21443f5b67cc16db68bded9bba,0,0, should support Quota extension ,Ryu plugin should support Quota extension,"Now, start of VM fails because Ryu plugin does not  support Quota extension.
This problem and other problems due to Quota request will be solved by adding Quota support."
668,1266711,nova,85068cc9f68aa704895ffe5ac185bb9cf7d05e2d,1,1,,Bug #1266711 “AttributeError,"During Jenkins tests I got this error two times with a different patchset
ft1.8205: nova.tests.virt.libvirt.test_libvirt.LibvirtNonblockingTestCase.test_connection_to_primitive_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{WARNING [nova.virt.libvirt.driver] URI test:///default does not support events: internal error: could not initialize domain event timer}}}
Traceback (most recent call last):
  File ""nova/tests/virt/libvirt/test_libvirt.py"", line 7570, in test_connection_to_primitive
    jsonutils.to_primitive(connection._conn, convert_instances=True)
  File ""nova/virt/libvirt/driver.py"", line 678, in _get_connection
    wrapped_conn = self._get_new_connection()
  File ""nova/virt/libvirt/driver.py"", line 664, in _get_new_connection
    wrapped_conn.registerCloseCallback(
  File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/tpool.py"", line 172, in __getattr__
    f = getattr(self._obj,attr_name)
AttributeError: virConnect instance has no attribute 'registerCloseCallback'"
669,1266804,cinder,d64478fd35a7024f46fa8dbf54103a7a0e30be5a,0,0,feature for postgre,User/pass logged in PostgreSQL connection string,PostgreSQL connection strings may contain passwords too which need to be filtered out (like it's done for MySQL).
670,1266906,cinder,a7b6d82fd170bc594ceff791eb10b900a61c9175,1,1,,Volume type defaults remain in Defaults tab even a...,"After creating a volume type 'abc' a number of default quotas are added to the Default tab's quota table:
- Volumes Abc
- Snapshots Abc
- Gigabytes Abc
I expected these defaults to be removed from the table once I remove the volume type. However, after removing the volume type they are still there. I verified that volume type was removed from the Volumes tab, and also marked deleted in Cinder database."
671,1266919,nova,6438d2587e1b93c73b8461b0e7dbd7bcd5243aea,1,1,There is a bug and add an exception for that,Missing InstanceInfoCache entry prevents delete,"If you're trying to delete an instance which is out-of-sync such that it's missing a InstanceInfoCache entry, you'll will receive a traceback: http://paste.openstack.org/show/60685/
Delete in this case should be allowed so that you can cleanup these 'broken' instances.
The solution is to catch the InstanceInfoCacheNotFound exception (like we do with other NotFound exceptions around this code), and continue on."
672,1266986,glance,ec979d5b112e2d23eb428cdfaf467d64cfa79499,1,1,typo in a message ,Fix typo in gridfs store,"Referring commit 61a715e17e8d6e6dbe60d35447c70fba990bd2e9
https://github.com/openstack/glance/blob/master/glance/store/gridfs.py#L99
Fix typo:
msg = (""Missing dependecies: pymongo"")
It should be,
msg = (""Missing dependencies: pymongo"")"
673,1267074,glance,ff1f6607e71ee261a9499b4a11de34e3648c570b,0,0,It’s better to use…,type() method should be replaced with isinstance()...,"In store/__init__.py, there is two places use the ""type"" method to determine the type. It's bertter to use the ""isinstance"" method instead.
The code is:
def check_location_metadata(val, key=''):
    t = type(val)
    if t == dict:
        for key in val:
            check_location_metadata(val[key], key=key)
    elif t == list:
        ndx = 0
        for v in val:
            check_location_metadata(v, key='%s[%d]' % (key, ndx))
            ndx = ndx + 1
    elif t != unicode:
        raise BackendException(_(""The image metadata key %s has an invalid ""
                                 ""type of %s.  Only dict, list, and unicode ""
                                 ""are supported."") % (key, str(t)))
def store_add_to_backend(image_id, data, size, store):
    (location, size, checksum, metadata) = store.add(image_id, data, size)
    if metadata is not None:
        if type(metadata) != dict:
            msg = (_(""The storage driver %s returned invalid metadata %s""
                     ""This must be a dictionary type"") %
                   (str(store), str(metadata)))
            LOG.error(msg)
            raise BackendException(msg)"
674,1267101,neutron,85ddbde058d8bda0b938eb7a45ef73519a831b3b,1,1, request timeout error,Bug #1267101 “nicira,"Stacktrace here:
http://paste.openstack.org/show/60785/
Occurence here:
http://162.209.83.206/logs/58017/16/logs/screen-q-svc.txt.gz"
676,1267246,neutron,a9c6bb4647afd3df985cf40531d48957ad5d9580,0,0,tests,missing test coverage in port_security tests,missing code coverage in port_security tests
677,1267290,neutron,a1e1b0a6251c9b0838f871591316c41f37b4b3e0,1,1,performance bug,Net-list is very slow under metaplugin,"If there are many networks when using metaplugin, net-list (GET networks API)
takes very long time.
For example: (showing hardware spec etc. is omitted since it is relative comparison.)
--- 200 networks, openvswitch plugin used natively
$ time neutron net-list
...snip
real    0m2.007s
user    0m0.428s
sys     0m0.100s
---
--- 200 openvswitch networks, under metaplugin
$ time neutron net-list
...snip
real    0m7.700s
user    0m0.472s
sys     0m0.072s
---
Note that the quantum-server wastes a lot of cpu usage too."
678,1267291,neutron,df315513efe62667220562dd2edb5401b15ab2ba,0,0,This possibly causes,DB lock wait timeout is possible for metaplugin,"There are some places that a target plugin's method is called in
'with context.session.begin(subtransaction=True)'.
This causes 'lock wait timeout' error potentially.
--- error is like:
OperationalError: (OperationalError) (1205, 'Lock wait timeout exceeded; try restarting transaction') ...snip
---
For example say ml2 is target plugin. Ml2's mechanism drivers separates precommit and postcommit
method so that 'lock wait timeout' error does not occur.
But it is meaningless if ml2 is used under metaplugin."
679,1267300,nova,9f7c5bf5ad38237f2adac980afb69fcdb26678a3,0,0,It’s better to use…,type() method should be replaced with isinstance()...,"In nova/utils.py, a func use the ""type"" method to determine the type. It's bertter to use the ""isinstance"" method instead.
The code is:
def convert_version_to_int(version):
    try:
        if type(version) == str:
            version = convert_version_to_tuple(version)
       if type(version) == tuple:
           return reduce(lambda x, y: (x * 1000) + y, version)
    except Exception:
           raise exception.NovaException(message=""Hypervisor version invalid."")
this bug is fixed in glance:
https://review.openstack.org/#/c/65611/4"
680,1267423,cinder,f78746062f942b30d73f2575c41dee26b3619f2f,0,0,unused code,Remove unused variable in os-extend api,"Temp variable _val  in _extend  method is  not being used, so remove this '_val' variable."
681,1267468,neutron,7b9daa829fe137cb3da377c89b293f01e5fa0871,0,0,add sanity checks ,Add more sanity checks to VMware NSX config/backen...,neutron-check-nsx-config test the configuration for Neutron and VMware NSX. Extend this tool to check the backend even further.
682,1267619,neutron,251159c90a69dfb61d9927093d5eae41cd99e2a7,1,0,Issues by evolution,Fix Migration 50e86cb2637a,"The following migration 50e86cb2637a called
op.rename_table('neutron_nvp_port_mapping', 'neutron_nsx_port_mappings')
though the table name was actually quantum_nvp_port_mapping. Because of this
the quantum_id->nvp_id mapping was never migrated over to the new table and
you would be left with a quantum_nvp_port_mapping table hanging around.
In addition, the downgrade would rename the table to neutron_nvp_port_mapping
instead of quantum_nvp_port_mapping. This patch addresses this issues."
683,1267664,nova,58d7eeede1e2912ab250fd306c85ca1a05de2fbc,0,0,No bug: Private methods should be prefixed with a _ for Clarity,Bug #1267664 “docker driver exposes public methods outside of dr... ,"The following methods are public and should be prefixed with a _ to indicate that they are private:
     is_daemon_running
     find_container_by_name
     get_available_resource"
684,1267682,neutron,d583d55c6a4e6c01c49411b7e5b519031bce0699,1,0,On some linux distribution,check_vxlan_support works incorrect on RHEL6.5,RHEL6.5 has back ported vxlan support feature into its kernel 2.6 but check_vxlan_support still give NOT result.
685,1267685,nova,44545eab5e86d0837f4032759f2d8e8e6f9846b0,0,0,Support IPv6,boot vm don't support ipv6,"when boot vm, it can use '--nic <net-id=net-uuid,v4-fixed-ip=ip-addr, port-id=port-uuid>' to set network info, if it want to use ipv6,it hase to use port-id which hase a ipv6 address, I think it should can use '--nic net-id=net-uuid, fixed-ip=ip-addr' which include ipv4 and ipv6 address.Currently,nova already prevent that:
if address is not None and not utils.is_valid_ipv4(address):
   msg = _(""Invalid fixed IP address (%s)"") % address
    raise exc.HTTPBadRequest(explanation=msg)"
686,1267790,neutron,5b5ba869aa4680bcfa7544c4e718cb5a5dc9c168,1,1,,Fixed metric_index causes ValueError in ip_lib,"ip_lib.IpRouteCommand.get_gateway() raises ValueError to default_route_line like
default via 192.168.99.1 proto static
since it wrongly assumes that the 5th word is a value metric (in the example above, it is ""static"")."
687,1267841,neutron,2779fdd14279e017f35780fe343b1c7243898397,1,1,Fix description,ext_gw_mode refers to DNAT,"The extension module for the L3_ext_gw_mode extension has:
- a RouterDNATdisabled exception which is not used anywhere
- a description which refers to disabling DNAT, which is not allowed by the extension
The above are 'vestigial' items from a previous release of the extension which allowed to disable DNAT (floating IPs)"
688,1268294,cinder,671cd61704551a5d1fd753bb890650edd6dcd152,0,0,"Imrpove?: If the driver raises an exception, make sure it is printed",driver exception swallowed during retype,"If a driver raises an exception during retype, only a generic message is printed by the manager.  It would help debugging if the original exception was printed as well."
689,1268460,neutron,5fae0fae2164f4fab64e3eff01c9c644d861946a,0,0,Improve?: Report proper error message ,PLUMgrid plugin should report proper error message...,PLUMgrid Director error messages should be reported at plugin level as well.
691,1268513,cinder,7d028b77a4bde48ed2729f370773c9d510156686,0,0,Add support for special char,Can not translate special char “&,"1. I add special char like """"!@#$%^&*"" in volume ""metadata = {""key1"": ""value1"",
                    : ""!@#$%^&*""} .Then try to insert it into volume.
2. I test it successfully in  volumes_client.create_volume_metadata Json function.
3. When I test it in XML function, it report error like below
Traceback (most recent call last):
  File ""/tempest/tempest/api/volume/test_volume_metadata.py"", line 130, in test_create_volume_metadata_specialchart_blank
    metadata)
  File ""/tempest/tempest/services/volume/xml/volumes_client.py"", line 381, in create_volume_metadata
    self.headers)
  File ""/tempest/tempest/common/rest_client.py"", line 302, in post
    return self.request('POST', url, headers, body)
  File ""/tempest/tempest/common/rest_client.py"", line 436, in request
    resp, resp_body)
  File ""/tempest/tempest/common/rest_client.py"", line 530, in _error_checker
    raise exceptions.ServerFault(message)
ServerFault: Got server fault
Details: The server has either erred or is incapable of performing the requested operation.
4. After our investigation, we find when we remove special char ""&"" , it will be ok. So I think in XML function, it can not translate these special char to correct format.
I also run it in tempest, it also run same error
FAIL: tempest.api.volume.test_volume_metadata.VolumeMetadataTestXML.test_create_volume_metadata_specialchart_blank[gate
2014-01-13 11:13:15.079 | Traceback (most recent call last):
2014-01-13 11:13:15.079 | File ""tempest/api/volume/test_volume_metadata.py"", line 130, in test_create_volume_metadata_specialchart_blank
2014-01-13 11:13:15.079 | metadata)
2014-01-13 11:13:15.079 | File ""tempest/services/volume/xml/volumes_client.py"", line 380, in create_volume_metadata
2014-01-13 11:13:15.079 | self.headers)
2014-01-13 11:13:15.080 | File ""tempest/common/rest_client.py"", line 302, in post
2014-01-13 11:13:15.080 | return self.request('POST', url, headers, body)
2014-01-13 11:13:15.080 | File ""tempest/common/rest_client.py"", line 436, in request
2014-01-13 11:13:15.080 | resp, resp_body)
2014-01-13 11:13:15.080 | File ""tempest/common/rest_client.py"", line 530, in _error_checker
2014-01-13 11:13:15.080 | raise exceptions.ServerFault(message)
2014-01-13 11:13:15.080 | ServerFault: Got server fault
2014-01-13 11:13:15.080 | Details: The server has either erred or is incapable of performing the requested operation"
692,1268561,neutron,45557c43e4f8e42a98c09deb45739bebaa5b8507,0,0,tests,test_create_security_group_rule_bad_tenant succeed...,"https://github.com/openstack/neutron/blob/master/neutron/tests/unit/test_extension_security_group.py#L760
When the security group is created, the security group has already been deleted.
This means that the notfound error is raised because the security group has been removed, and not because the rule is being created with the wrong tenant id."
693,1268569,nova,cbbb9de51b52793f6d424c58199b22a422d39aed,1,1, race condition,Double removal of floating IP in nova-network,"It is possible to send two DELETE requests via api and in_use.floating_ips will be decreased by two. It can be changed even to value below zero:
Logs:
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: 172.16.0.2 ""DELETE /v2/46f4516b6fbd461eb30f17409
36c2167/os-floating-ips/1 HTTP/1.1"" status: 202 len: 209 time: 0.0797279
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: (19517) accepted ('172.16.0.2', 47613)
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: 172.16.0.2 ""DELETE /v2/46f4516b6fbd461eb30f17409
36c2167/os-floating-ips/1 HTTP/1.1"" status: 202 len: 209 time: 0.0804379
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: (19517) accepted ('172.16.0.2', 47615)
<0>Dec 19 17:52:08 node-2 <BF><180>nova-nova.db.sqlalchemy.api WARNING: Change will make usage less than 0 for the fol
lowing resources: ['floating_ips']
Database:
mysql> select resource,in_use from quota_usages;
+-----------------+--------+
| resource | in_use |
+-----------------+--------+
| security_groups | 0 |
| instances | 0 |
| ram | 0 |
| cores | 0 |
| fixed_ips | 0 |
| floating_ips | -1 |
+-----------------+--------+
6 rows in set (0.00 sec)"
694,1268711,neutron,9d3a07ed04952c1fd8e65b5f46671208b793d659,0,0,No bug. Just Remove psutil dependency ,Unlock the psutil version requirement or use an al...,"psutil is currently version locked at psutil>=0.6.1,<1.0
These versions are not in PyPI, which is not liked by the new pip.
For further information see the comments in https://review.openstack.org/65209"
695,1268762,neutron,b78eea6146145793a7c61705a1602cf5e9ac3d3a,1,1,"Unfortunately, ovs does not reinitialize the interfaces",Remove and recreate interfacein ovs  if already ex...,"If the dhcp-agent machine restarts and openvswitch logs the following
warning message for all tap interfaces that have not been recreated yet:
bridge|WARN|could not open network device tap2cf7dbad-9d (No such device)
Once the dhcp-agent starts he recreates the interfaces and readds them to the
ovs-bridge. Unfortinately, ovs does not reinitalize the interface as its
already in ovsdb and does not assign it an ofport number.
In order to correct this we should first remove interfaces that exist and
then readd them.
root@arosen-desktop:~# ovs-vsctl  -- --may-exist add-port br-int fake1
# ofport still -1
root@arosen-desktop:~# ovs-vsctl  list inter | grep -A 2 fake1
name                : ""fake1""
ofport              : -1
ofport_request      : []
root@arosen-desktop:~# ip link add fake1 type veth peer name fake11
root@arosen-desktop:~# ifconfig fake1
fake1     Link encap:Ethernet  HWaddr 56:c3:a1:2b:1f:f4
          BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
root@arosen-desktop:~# ovs-vsctl  list inter | grep -A 2 fake1
name                : ""fake1""
ofport              : -1
ofport_request      : []
root@arosen-desktop:~# ovs-vsctl  -- --may-exist add-port br-int fake1
root@arosen-desktop:~# ovs-vsctl  list inter | grep -A 2 fake1
name                : ""fake1""
ofport              : -1
ofport_request      : []"
696,1269152,neutron,47871570eb7ed351738828bc233343520283de0d,0,0,"Missing LOG,exception to show the error",Missing log.exception in nvp plugin for create_rou...,Missing log.exception in nvp plugin for create_router
698,1269445,cinder,193096a476ca3da2ef7f36f89fdbd897c5bcc320,1,0,Check newer version,Issues in volume creation on unsupported skipactiv...,"Cinder volume service traces with the following error when lvchange is not in $PATH. As lvchange is in /sbin on SLE11, and /sbin is not part of the path for non-root, the test aborts with this trace:
2014-01-15 13:55:39.292 9325 ERROR cinder.volume.flows.create_volume [req-04565125-c12a-4081-b7dd-93fe0afc7e17 a0f6c8d7d68241f3bd6d1768775dc239 9d341e8aeb794086894e49e132766cd0] Unexpected build error:
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume Traceback (most recent call last):
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 172, in run_it
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     result = runner(context, *args, **kwargs)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/taskflow/utils.py"", line 260, in __call__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     self.result = self.task(*args, **kwargs)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1499, in __call__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     **volume_spec)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1300, in _create_from_snapshot
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     snapshot_ref)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/volume/drivers/lvm.py"", line 176, in create_volume_from_snapshot
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     self.vg.activate_lv(snapshot['name'], is_snapshot=True)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/brick/local_dev/lvm.py"", line 487, in activate_lv
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     if self.supports_lvchange_ignoreskipactivation:
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/brick/local_dev/lvm.py"", line 190, in supports_lvchange_ignoreskipactivation
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     (out, err) = self._execute(*cmd)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/utils.py"", line 142, in execute
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     return processutils.execute(*cmd, **kwargs)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/openstack/common/processutils.py"", line 158, in execute
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     shell=shell)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/eventlet/green/subprocess.py"", line 44, in __init__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     subprocess_orig.Popen.__init__(self, args, 0, *argss, **kwds)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/subprocess.py"", line 623, in __init__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     errread, errwrite)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/subprocess.py"", line 1141, in _execute_child
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     raise child_exception
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume OSError: [Errno 13] Permission denied
Solution/workaround is to run it via the root wrapper, which has /sbin in its trusted paths."
699,1269501,neutron,212c755c34c195148c668b553ccd9f371df63420,0,0,Refactor to remove _recycle_ip ,"With removal of explicit _recycle_ip, the method s...","https://review.openstack.org/#/c/58017 removes the need to explicitly call _recycle_ip.  More specifically, it makes _recycle_ip a simple pass-through to _delete_ip_allocation.  A follow-on needs to remove _recycle_ip and replace calls with _delete_ip_allocation."
700,1269505,neutron,0d111c97334c28444708e623a3fde912bc7c25d7,0,0,delete code,Remove release_lease from DhcpBase,https://review.openstack.org/#/c/56263 removes the need to explicitly call release_lease on this class.  It should be removed.
702,1269567,neutron,702e1fbf7ad5dd961dfd35cd6a0e54d4d6da5e34,1,1,performance issue,L3 agent making RPC calls to get external network ...,"In _process_routers, the L3 agent makes an RPC call each time that _process_routers is called to get the external network id as long as it was not configured using the gateway_external_network_id configuration option.
This adds some time process a router.  Since the external id will not be changing, we should be able to get away with fetching and saving this value once."
703,1269633,cinder,f1d30c9bc4ae626883d827d3b0a0458aa9f70820,0,0,missing method,FibreChannelDriver is missing accept_transfer,The base FibreChannelDriver is missing the new accept_transfer method.
704,1269684,nova,fd901be0c6718c26c078bb5ae85ba804b153833c,1,1,missing information in the payload.. bug i think in this case,payload is empty when remove metadata with event u...,"liugya@liugya-ubuntu:~/src/nova-ce$ nova  aggregate-set-metadata 1 a=a1
Aggregate 1 has been successfully updated.
+----+------+-------------------+-------+----------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+------+-------------------+-------+----------+
| 1  | agg1 | None              |       | 'a=a1'   |
+----+------+-------------------+-------+----------+
liugya@liugya-ubuntu:~/src/nova-ce$ nova  aggregate-set-metadata 1 a
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(100)update_metadata()
-> compute_utils.notify_about_aggregate_update(context,
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(101)update_metadata()
-> ""updatemetadata.start"",
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(102)update_metadata()
-> payload)
(Pdb) payload
{'meta_data': {u'a': None}, 'aggregate_id': 1}
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(118)update_metadata()
-> payload['meta_data'] = to_add
(Pdb)
> /opt/stack/nova/nova/objects/aggregate.py(119)update_metadata()
-> compute_utils.notify_about_aggregate_update(context,
(Pdb)
> /opt/stack/nova/nova/objects/aggregate.py(120)update_metadata()
-> ""updatemetadata.end"",
(Pdb)
> /opt/stack/nova/nova/objects/aggregate.py(121)update_metadata()
-> payload)
(Pdb) payload
{'meta_data': {}, 'aggregate_id': 1} <<< meta_data is empty, this caused 3rd party do not know which meta_data was now removed after get notification of updatemetadata.end"
705,1269958,cinder,9c1ad54e7815d50d9561d88337724ad2042c92a2,0,0,"I dont see the bug, just a report that shouldnt use env",cinder allows 'env' as commandfilter in rootwrap,"cinder/image/image_utils.py uses
  def qemu_img_info(path):
      """"""Return a object containing the parsed output from qemu-img info.""""""
      out, err = utils.execute('env', 'LC_ALL=C', 'LANG=C',
                               'qemu-img', 'info', path,
                               run_as_root=True)
      return QemuImgInfo(out)
This was added as part of I849b04b8aae76da068abcd2a20c1fcecca8a5caa
There is nothing wrong with that per se, however the rootwrap filters were updated with:
+ env: CommandFilter, /usr/bin/env, root
env is a wrapper that allows to run any command in the $PATH, so this is more or less equivalent to allowing bash in commandfilter. As a hardening precaution, env should not be allowed in CommandFilter.
The code in question can be easily reworked and EnvFilter can be used instead to harden the check."
706,1269990,nova,cc134ab2dca6c28e75d4530517f309c3fabc969a,1,1,LXC I think is OS code,LXC volume issues,"Few issues with LXC and volumes that relate to the same code.
* Hard rebooting a volume will make attached volumes disappear from libvirt xml
* Booting an instance specifying an extra volume (passing in block_device_mappings on server.create) will result in the volume not being in the libvirt xml
This is due to 2 places in the code where LXC is treated differently
1. nova.virt.libvirt.blockinfo  get_disk_mapping
2. nova.virt.libvirt.driver get_guest_storage_config"
707,1270008,nova,844df860c38ac38550b8d1739fd53131cd7fd864,1,1,Add a exception to handle a bug,periodic tasks will be invalid if a qemu process b...,"I am using stable havana nova.
I got this exception while I delete my kvm instance, but the qemu process of this instance become to 'defunct' status by some unknown reason(may be a qemu/kvm bug), and then the periodic task stopped unexpectly everytime, then the resources of this compute node will never be reported, because of this exception below, I think we should handle this exception while running periodic task.
2014-01-16 15:53:28.421 47954 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: cannot get CPU affinity of process 62279: No such process
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     task(self, context)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 5617, in update_available_resource
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     rt.update_available_resource(context)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     return f(*args, **kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 281, in update_available_resource
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     resources = self.driver.get_available_resource(self.nodename)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4275, in get_available_resource
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     stats = self.host_state.get_host_stats(refresh=True)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 5350, in get_host_stats
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     self.update_status()
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 5386, in update_status
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     data[""vcpus_used""] = self.driver.get_vcpu_used()
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 3949, in get_vcpu_used
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     vcpus = dom.vcpus()
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 179, in doit
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     result = proxy_call(self._autowrap, f, *args, **kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 139, in proxy_call
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     rv = execute(f,*args,**kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77, in tworker
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     rv = meth(*args,**kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 2222, in vcpus
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     if ret == -1: raise libvirtError ('virDomainGetVcpus() failed', dom=self)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task libvirtError: cannot get CPU affinity of process 62279: No such process
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task
and the exception while I delete this instance:
2014-01-16 15:13:26.640 47954 ERROR nova.openstack.common.rpc.amqp [req-03ed9463-0740-4423-bf1e-2334ed29ee5c 9537af4d80e546409b670673f9a81388 3179fc9d69d747b4a06f27a6d2334050] Exception during message handling
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     **args)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 400, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     payload)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 290, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     pass
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 276, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 341, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 318, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 305, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2128, in terminate_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     do_terminate_instance(instance, bdms, clean_shutdown)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     return f(*args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2120, in do_terminate_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     reservations=reservations)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/hooks.py"", line 105, in inner
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     rv = f(*args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2091, in _delete_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     user_id=user_id)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2063, in _delete_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     clean_shutdown=clean_shutdown)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1984, in _shutdown_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     requested_networks)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1974, in _shutdown_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     context=context, clean_shutdown=clean_shutdown)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 883, in destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     self._destroy(instance)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 838, in _destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     instance=instance)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 810, in _destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     virt_dom.destroy()
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 179, in doit
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     result = proxy_call(self._autowrap, f, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 139, in proxy_call
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     rv = execute(f,*args,**kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77, in tworker
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     rv = meth(*args,**kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 760, in destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     if ret == -1: raise libvirtError ('virDomainDestroy() failed', dom=self)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp libvirtError: Failed to terminate process 61945 with SIGKILL: Device or resource busy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp"
709,1270034,cinder,68ed1bcec81e173648a4a86e5f4e21abc6dcf539,0,0,Unused variable,Remove unused variable in restore_backup method,"Variable err in restore_backup method is not being used, so remove this 'err ' variable."
710,1270088,nova,da84cb078e9820c7c9d64bdccd904e1b257ab96e,0,0,No bug: needs tests + better log ,Bug #1270088 “disk/api.py,"In disk/api.py the method resize2fs does not have a test.
Also, the method firstly use e2fsck to check if the filesystem is correct. if the program failed no information was logged and the actual algorithm try to do the resize anyway. Same with e2fsck, if resize2fs failed not information are logged.
We need to add tests for this function and log every error returned."
711,1270178,cinder,6c9f81b97ededf2b6401861f7d876851ff07b34b,0,0,No bug: code needs update from Oslo,common/rpc code needs update from Oslo,"The rpc code from Oslo is quite out of date, and missing at least one bugfix of interest (bug 1189711)."
712,1270192,cinder,3764cecfc3b0a5b35634b15a4b049f433a8a22de,1,1, cinder volume hanging,cinder volume hanging on removing snapshots,"On any lvm2 version without lvmetad running, I can get cinder-volume to hang on issuing lvm related commands after deleting snapshots (that are non-thin provisioned LVM snapshots with clear_volume set to zero).
the issue is that lvm locks up due to trying to access suspended device mapper entries, and at some point cinder-volume does a lvm related command and hangs on that. a setting of ignore_suspended_devices = 1 in lvm.conf helps with that, as lvremove hangs on scanning the device state (which it
needs to do because it doesn't have current information available via lvmetad).
I can use this script to trigger the issue:
=== cut hang.sh ===
enable_fix=1
#enable_fix=0
vg=cinder-volumes
v=testvol.$$
lvcreate --name $v $vg -L 1g
sleep 2
lvcreate --name snap-$v --snapshot $vg/$v -L 1g
vgp=/dev/mapper/${vg/-/--}-snap--${v/-/--}
sleep 2
( sleep 10 < $vgp-cow ) &
test ""$enable_fix"" -eq ""1"" && lvchange -y -an $vg/snap-$v
lvremove -f $vg/snap-$v
sleep 1
lvremove -f $vg/$v
=== cut hang.sh ===
vg needs to be set to a lvm VG that exists and can take a few gig of space. whenever enable_fix is set to 0, lvremove -f ends with :
  Unable to deactivate open cinder--volumes-snap--testvol.27700-cow (252:5)
  Failed to resume snap-testvol.27700.
  libdevmapper exiting with 1 device(s) still suspended.
this is because the sleep command before keeps a fd open on the -cow. The script then also never finishes and any other lvm command hangs as well.
apparently in real-life this is either udev or the dd command still having the fd open for some reason I have not yet understood.
The deactivation before removing seems to help."
713,1270204,cinder,e62b70562dd8d2d94b526f3f7a52ef4aff9a3f40,1,1,Bug with spaces,storwize driver doesn't handle spaces in server na...,"If there's a space in the server's name, the storwize driver's SSH command will get caught by the SSH injection check in cinder."
714,1270624,cinder,82283790b7c5ba6ae0e561ee3302d7fed7983267,1,1,Bug in log,UnboundLocalError in TgtAdm.update_iscsi_target in...,"If some error occurs on tgt-admin call, update_iscsi_target fails with UnboundLocalError instead of expected ISCSITargetUpdateFailed because of debug output.
Traceback (most recent call last):
  File ""/opt/stack/new/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
    **args)
  File ""/opt/stack/new/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
    return getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 345, in create_volume
    _run_flow_locked()
  File ""/opt/stack/new/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
    retval = f(*args, **kwargs)
  File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 340, in _run_flow_locked
    _run_flow()
  File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 336, in _run_flow
    flow_engine.run()
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/lock_utils.py"", line 51, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 118, in run
    self._run()
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 128, in _run
    self._revert(misc.Failure())
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 81, in _revert
    misc.Failure.reraise_if_any(failures.values())
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 487, in reraise_if_any
    failures[0].reraise()
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 494, in reraise
    six.reraise(*self._exc_info)
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 36, in _execute_task
    result = task.execute(**arguments)
  File ""/opt/stack/new/cinder/cinder/volume/flows/create_volume/__init__.py"", line 1450, in execute
    model_update = self.driver.create_export(context, volume_ref)
  File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 595, in create_export
    return self._create_export(context, volume)
  File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 626, in _create_export
    volume_path, chap_auth)
  File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 447, in _create_tgtadm_target
    old_name=old_name)
  File ""/opt/stack/new/cinder/cinder/brick/iscsi/iscsi.py"", line 187, in create_iscsi_target
    self.update_iscsi_target(name)
  File ""/opt/stack/new/cinder/cinder/brick/iscsi/iscsi.py"", line 253, in update_iscsi_target
    LOG.debug(""StdOut from tgt-admin --update: %s"" % out)
UnboundLocalError: local variable 'out' referenced before assignment"
715,1270680,nova,119e2a690a64fdfd541010590285f754b3393653,1,0,Evolution bug: this race may have been introduced by the use of objects,v3 extensions api inherently racey wrt instances,"The pci extension for the v3 API does another instance lookup back to the database for instance objects. The issue being that when you are doing something like a list_* operation on instances, this means that we're making a second trip to the database that's distinct from the first lookup in the request handling. If an instance got deleted between the request and the extension hook running, this will generate a database exception, which turns into an InstanceNot found, and 404s the list operation *if any instance was deleted during the request*
We are managing to hit this quite frequently in tempest with our test_list_servers_by_admin_with_all_tenants (even at only concurency 2)  - http://logs.openstack.org/80/67480/1/gate/gate-tempest-dsvm-full/24f9aab/console.html#_2014-01-20_01_18_11_102
The explosion looks like this - http://logs.openstack.org/80/67480/1/gate/gate-tempest-dsvm-full/24f9aab/logs/screen-n-api.txt.gz?level=INFO#_2014-01-20_00_57_44_352
Logstash picks up these tracebacks really easily. This kind of explosion doesn't always trigger a Tempest failure, because some times this might be in cleanup code, where we protect against 404s (though it probably means we are leaking resources a lot on a normal run).
Logstash query - http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVFJBQ0Ugbm92YS5hcGkub3BlbnN0YWNrXCIgQU5EIG1lc3NhZ2U6XCJJbnN0YW5jZU5vdEZvdW5kOiBJbnN0YW5jZVwiIEFORCBmaWxlbmFtZTpcImxvZ3Mvc2NyZWVuLW4tYXBpLnR4dFwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzkwMTgzNzk1ODI1fQ=="
716,1270693,nova,7e18d67cc9c96a14e0ca24a41820441e41457738,0,0,tests,_last_vol_usage_poll was not properly updated,"I found this error in jinkins's log: http://logs.openstack.org/02/67402/4/check/gate-nova-python27/b132ac8/console.html
2014-01-20 02:36:59.295 | ======================================================================
2014-01-20 02:36:59.295 | FAIL: nova.tests.compute.test_compute.ComputeVolumeTestCase.test_poll_volume_usage_with_data
2014-01-20 02:36:59.295 | tags: worker-0
2014-01-20 02:36:59.296 | ----------------------------------------------------------------------
2014-01-20 02:36:59.296 | Empty attachments:
2014-01-20 02:36:59.296 |   stderr
2014-01-20 02:36:59.296 |   stdout
2014-01-20 02:36:59.297 |
2014-01-20 02:36:59.297 | pythonlogging:'': {{{
2014-01-20 02:36:59.297 | INFO [nova.virt.driver] Loading compute driver 'nova.virt.fake.FakeDriver'
2014-01-20 02:36:59.298 | AUDIT [nova.compute.resource_tracker] Auditing locally available compute resources
2014-01-20 02:36:59.298 | AUDIT [nova.compute.resource_tracker] Free ram (MB): 7680
2014-01-20 02:36:59.298 | AUDIT [nova.compute.resource_tracker] Free disk (GB): 1028
2014-01-20 02:36:59.299 | AUDIT [nova.compute.resource_tracker] Free VCPUS: 1
2014-01-20 02:36:59.299 | INFO [nova.compute.resource_tracker] Compute_service record created for fake-mini:fakenode1
2014-01-20 02:36:59.299 | AUDIT [nova.compute.manager] Deleting orphan compute node 2
2014-01-20 02:36:59.300 | }}}
2014-01-20 02:36:59.300 |
2014-01-20 02:36:59.300 | Traceback (most recent call last):
2014-01-20 02:36:59.300 |   File ""nova/tests/compute/test_compute.py"", line 577, in test_poll_volume_usage_with_data
2014-01-20 02:36:59.301 |     self.compute._last_vol_usage_poll)
2014-01-20 02:36:59.301 |   File ""/usr/lib/python2.7/unittest/case.py"", line 420, in assertTrue
2014-01-20 02:36:59.301 |     raise self.failureException(msg)
2014-01-20 02:36:59.302 | AssertionError: _last_vol_usage_poll was not properly updated <1390185067.18>"
717,1270811,nova,861d67120227f7a52ff3f9ef7225b84a9ac26647,0,0,tests,Wrong option is used in test_availability.py,"I found a testcase bug in nova/tests/api/openstack/compute/plugins/v3/test_availability_zone.py
     def test_create_instance_with_availability_zone(self):
         def create(*args, **kwargs):
             self.assertIn('availability_zone', kwargs)
             return old_create(*args, **kwargs)
         old_create = compute_api.API.create
         self.stubs.Set(compute_api.API, 'create', create)
         image_href = '76fa36fc-c930-4bf3-8c8a-ea2a2420deb6'
         flavor_ref = 'http://localhost/v3/flavors/3'
         body = {
             'server': {
                 'name': 'config_drive_test',
                 'image_ref': image_href,
                 'flavor_ref': flavor_ref,
                 'metadata': {
                     'hello': 'world',
                     'open': 'stack',
                 },
               ★  'availability_zone': ""nova"",★
             },
         }
         req = fakes.HTTPRequestV3.blank('/v3/servers')
         req.method = 'POST'
         req.body = jsonutils.dumps(body)
         req.headers[""content-type""] = ""application/json""
         admin_context = context.get_admin_context()
         service1 = db.service_create(admin_context, {'host': 'host1_zones',
                                          'binary': ""nova-compute"",
                                          'topic': 'compute',
                                          'report_count': 0})
         agg = db.aggregate_create(admin_context,
                 {'name': 'agg1'}, {'availability_zone': 'nova'})★
         db.aggregate_host_add(admin_context, agg['id'], 'host1_zones')
The correct request for availability-zone option is 'os-availability-zone:availability_zone'."
718,1270863,neutron,fbf0e621399027d5004bbc8c888d67a3c2851686,0,0,Use of constants instead of variables,Bug #1270863 “db_base_plugin_v2 AUTO_DELETE_PORT_OWNERS doesn't ... ,db_base_plugin_v2 AUTO_DELETE_PORT_OWNERS should use the constant defined for network:dhcp.
719,1270973,nova,33cc64fb81773f0c246073d23c525357c9aa3b08,1,1,,Remove and recreate interface if already exists,"If a nova-compute machine restarts when openvswitch comes up it logs the following warning messages for all tap interfaces that do not exist:
bridge|WARN|could not open network device tap2cf7dbad-9d (No such device)
Once the compute-node  starts it recreates the interfaces and re-adds them to the
ovs-bridge. Unfortunately, ovs does not reinitialize the interfaces as they
are already in ovsdb and does not assign them a ofport number.
This situation corrects itself though the next time a port is added to the
ovs-bridge which is why no one has probably noticed this issue till now.
In order to correct this we should first remove interface that exist and
then readd them.
same bug on neutron-side:  https://bugs.launchpad.net/neutron/+bug/1268762"
720,1271231,neutron,2302ed3f22512590af23a649e09b952189e0fa9d,1,0,Bug by evolution,securitygroups table is created again while migrat...,"There is attempt to create a new table securitygroups by 49f5e553f61f_ml2_security_groups.py while table already exists (created by 3cb5d900c5de_security_groups.py)
INFO  [alembic.migration] Context impl MySQLImpl.
INFO  [alembic.migration] Will assume non-transactional DDL.
INFO  [alembic.migration] Running upgrade havana -> e197124d4b9, add unique constraint to members
INFO  [alembic.migration] Running upgrade e197124d4b9 -> 1fcfc149aca4, Add a unique constraint on (agent_type, host) columns to prevent a race
condition when an agent entry is 'upserted'.
INFO  [alembic.migration] Running upgrade 1fcfc149aca4 -> 50e86cb2637a, nsx_mappings
INFO  [alembic.migration] Running upgrade 50e86cb2637a -> ed93525fd003, bigswitch_quota
INFO  [alembic.migration] Running upgrade ed93525fd003 -> 49f5e553f61f, security_groups
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/new/neutron/neutron/db/migration/cli.py"", line 143, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/new/neutron/neutron/db/migration/cli.py"", line 80, in do_upgrade_downgrade
    do_alembic_command(config, cmd, revision, sql=CONF.command.sql)
  File ""/opt/stack/new/neutron/neutron/db/migration/cli.py"", line 59, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 124, in upgrade
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 199, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 199, in load_python_file
    module = load_module(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 55, in load_module
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/new/neutron/neutron/db/migration/alembic_migrations/env.py"", line 105, in <module>
    run_migrations_online()
  File ""/opt/stack/new/neutron/neutron/db/migration/alembic_migrations/env.py"", line 89, in run_migrations_online
    options=build_options())
  File ""<string>"", line 7, in run_migrations
  File ""/usr/local/lib/python2.7/dist-packages/alembic/environment.py"", line 652, in run_migrations
    self.get_context().run_migrations(**kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/migration.py"", line 225, in run_migrations
    change(**kw)
  File ""/opt/stack/new/neutron/neutron/db/migration/alembic_migrations/versions/49f5e553f61f_ml2_security_groups.py"", line 53, in upgrade
    sa.PrimaryKeyConstraint('id')
  File ""<string>"", line 7, in create_table
  File ""/usr/local/lib/python2.7/dist-packages/alembic/operations.py"", line 647, in create_table
    self._table(name, *columns, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/impl.py"", line 149, in create_table
    self._exec(schema.CreateTable(table))
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/impl.py"", line 76, in _exec
    conn.execute(construct, *multiparams, **params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
    compiled
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
    cursor.execute(statement, parameters)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 174, in execute
    self.errorhandler(self, exc, value)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler
    raise errorclass, errorvalue
sqlalchemy.exc.OperationalError: (OperationalError) (1050, ""Table 'securitygroups' already exists"") '\nCREATE TABLE securitygroups (\n\ttenant_id VARCHAR(255), \n\tid VARCHAR(36) NOT NULL, \n\tname VARCHAR(255), \n\tdescription VARCHAR(255), \n\tPRIMARY KEY (id)\n)\n\n' ()"
721,1271249,cinder,28b8ecc15d874644952bb74539c61becaf5f7d20,0,0,tests,Possibly broken unit tests for TGT and LIO iSCSI b...,"I'm guessing (though I'm not really sure) that these two items in cinder/tests/test_iscsi.py are inaccurate:
from TgtAdmTestCase.setUp():
        self.script_template = ""\n"".join([
            'tgt-admin --update iqn.2011-09.org.foo.bar:blaa',
            'tgt-admin --force '
            '--delete iqn.2010-10.org.openstack:volume-blaa',
            'tgtadm --lld iscsi --op show --mode target'])
from LioAdmTestCase.setUp():
        self.script_template = ""\n"".join([
            'cinder-rtstool create '
            '/foo iqn.2011-09.org.foo.bar:blaa test_id test_pass',
            'cinder-rtstool delete iqn.2010-10.org.openstack:volume-blaa'])
Note how the IQNs for creation and deletion don't match.
The trouble is that if you replace those hardcoded IQNs with %(target)s, the unit tests break, possibly indicating an issue with the underlying brick implementations themselves."
722,1271331,nova,74ade48634e2b72ec0b09792e97ac77dfb5fd999,0,0,tests,unit test failure in gate nova.tests.db.test_sqlit...,"We are occasionally seeing the test nova.tests.db.test_sqlite.TestSqlite.test_big_int_mapping fail in the gate due to
Traceback (most recent call last):
  File ""nova/tests/db/test_sqlite.py"", line 53, in test_big_int_mapping
    output, _ = utils.execute(get_schema_cmd, shell=True)
  File ""nova/utils.py"", line 166, in execute
    return processutils.execute(*cmd, **kwargs)
  File ""nova/openstack/common/processutils.py"", line 168, in execute
    result = obj.communicate()
  File ""/usr/lib/python2.7/subprocess.py"", line 754, in communicate
    return self._communicate(input)
  File ""/usr/lib/python2.7/subprocess.py"", line 1314, in _communicate
    stdout, stderr = self._communicate_with_select(input)
  File ""/usr/lib/python2.7/subprocess.py"", line 1438, in _communicate_with_select
    data = os.read(self.stdout.fileno(), 1024)
OSError: [Errno 11] Resource temporarily unavailable
logstash query: message:""FAIL: nova.tests.db.test_sqlite.TestSqlite.test_big_int_mapping""
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRkFJTDogbm92YS50ZXN0cy5kYi50ZXN0X3NxbGl0ZS5UZXN0U3FsaXRlLnRlc3RfYmlnX2ludF9tYXBwaW5nXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6ImFsbCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzOTAzMzk1MTU1NDcsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0="
723,1271449,neutron,65dde5624bbe1a1f896fe437fef0ad9bbe168153,1,1,,neutron-dhcp-agent can not be started when end use...,"When we only install neutron-linuxbridge-agent package on RHEL6.5, the dhcp agent can not be started successfully. That is because there is hard code to use openvswitch.common plugin in nuetron.agent.linux.ovs_lib.py.
[root@osee15-control01 neutron]# /usr/bin/neutron-dhcp-agent --log-file /var/log/neutron/dhcp-agent.log --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/dhcp_agent.ini
Traceback (most recent call last):
  File ""/usr/bin/neutron-dhcp-agent"", line 6, in <module>
    from neutron.agent.dhcp_agent import main
  File ""/usr/lib/python2.6/site-packages/neutron/agent/dhcp_agent.py"", line 27, in <module>
    from neutron.agent.linux import interface
  File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/interface.py"", line 26, in <module>
    from neutron.agent.linux import ovs_lib
  File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ovs_lib.py"", line 31, in <module>
    from neutron.plugins.openvswitch.common import constants
ImportError: No module named openvswitch.common"
724,1271568,cinder,65fa80c361f71158cc492dfc520dc4a63ccfa419,1,1,Bug: can't handle rpc message. Need move a method,can't handle rpc message when cinder-volume start ...,"cinder-scheduler send a rpc message to cinder-volume which was killed just now.
Then when cinder-volume restart, it may receive the rpc message and begin to handle it before driver initialized.
Most functions in manager need to judge wether the driver has been initialized, so the rpc request would be reject. But those requests shoud be handle in fact.
The reason is, in service.py, we create consumer before manager call ""init_host()"", and we call ""self.driver.set_initialized()"" in init_host."
725,1271821,nova,951bae39e704b04414ebb711b425631cf2d46be5,0,0,No bug. ‘So we need to add a protection ‘,Target 'host' in 'evacuate' should not be the orig...,"'Evacuate' function aims to help administrator/operator to evacuate servers if this compute node fails.
So we need to add a protection here, the target host should not be the original host."
726,1272128,neutron,609f01a1ddca64ec191cb15a4f6fb93d219c3336,0,0,Support Port Binding Extension in Cisco N1kv plugin,Support Port Binding Extension in Cisco N1kv plugi...,Plugins using libvirt_ovs_bridge config are affected due to changes in nova's VIF plugging code. Fix port crud in the Cisco N1kv Neutron plugin by extending Port Bindings Extension.
727,1272136,glance,8143d814f2b355b1a2a7a1bd92920641e4d0fb43,0,0,tests,Case test_get_index_sort_updated_at_desc failed so...,"I have seen this failure several times. And it doesn't happen always, so it seems like a race condition issue. And until, I just saw it on py27, so not sure if it's existed in py26.
2014-01-23 15:12:33.910 | FAIL: glance.tests.unit.v2.test_registry_client.TestRegistryV2Client.test_get_index_sort_updated_at_desc
2014-01-23 15:12:33.910 | ----------------------------------------------------------------------
2014-01-23 15:12:33.911 | _StringException: Traceback (most recent call last):
2014-01-23 15:12:33.911 |   File ""/home/jenkins/workspace/gate-glance-python27/glance/tests/unit/v2/test_registry_client.py"", line 268, in test_get_index_sort_updated_at_desc
2014-01-23 15:12:33.911 |     unjsonify=False)
2014-01-23 15:12:33.911 |   File ""/home/jenkins/workspace/gate-glance-python27/glance/tests/utils.py"", line 472, in assertEqualImages
2014-01-23 15:12:33.911 |     self.assertEqual(images[i]['id'], value)
2014-01-23 15:12:33.911 |   File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 324, in assertEqual
2014-01-23 15:12:33.911 |     self.assertThat(observed, matcher, message)
2014-01-23 15:12:33.911 |   File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 414, in assertThat
2014-01-23 15:12:33.911 |     raise MismatchError(matchee, matcher, mismatch, verbose)
2014-01-23 15:12:33.911 | MismatchError: !=:
2014-01-23 15:12:33.911 | reference = u'db4ddeb5-edd6-4557-b635-6ecb4e5265a0'
2014-01-23 15:12:33.912 | actual    = '406c995a-70e0-4010-a6eb-9dff61a2d2a7'
http://logs.openstack.org/19/67019/2/check/gate-glance-python27/783d216/console.html"
728,1272182,cinder,04dd95bef7e88c2602cca74d542654ec7bdfc754,0,0,Docstring change and unused variable,Fix docstring and remove unused variable in Volume...,Fix error docstring and remove unused variable 'volume ' in VolumeTransferTestCase.
729,1272195,nova,46922068ac167f492dd303efb359d0c649d69118,0,0,Refactoring: Make network_cache more robust with neutron ,Previous ip show twice after 'interface-attach',"Hi team,
I encounter this problem in the below situation:
OS : Ubuntu
Version : Icehouse
1. Normally boot a vm
xianghui@xianghui:/opt/stack/nova$ nova list
+--------------------------------------+----------+---------+------------+-------------+------------------------------+
| ID                                   | Name     | Status  | Task State | Power State | Networks                     |
+--------------------------------------+----------+---------+------------+-------------+------------------------------+
| 35cc6c7c-31b1-491a-8e0d-766a098fb8d9 | cirros_3 | ACTIVE  | None       | Running     | private=10.0.0.5             |
+--------------------------------------+----------+---------+------------+-------------+------------------------------+
xianghui@xianghui:/opt/stack/nova$ neutron net-list
+--------------------------------------+---------+----------------------------------------------------+
| id                                   | name    | subnets                                            |
+--------------------------------------+---------+----------------------------------------------------+
| 2d6842e2-b82c-4d5c-8601-7928ab85a8fd | private | 44d8d50a-197d-4f52-90c4-487495fdb8b5 10.0.0.0/24   |
+--------------------------------------+---------+----------------------------------------------------+
2. Attach the instance with an interface
xianghui@xianghui:/opt/stack/nova$ nova interface-attach cirros_3 --net-id=2d6842e2-b82c-4d5c-8601-7928ab85a8fd
xianghui@xianghui:/opt/stack/nova$ nova list
+--------------------------------------+----------+---------+------------+-------------+--------------------------------------+
| ID                                   | Name     | Status  | Task State | Power State | Networks                             |
+--------------------------------------+----------+---------+------------+-------------+--------------------------------------+
| 35cc6c7c-31b1-491a-8e0d-766a098fb8d9 | cirros_3 | ACTIVE  | None       | Running     | private=10.0.0.5, 10.0.0.5, 10.0.0.6 |
| fa79e7a9-a838-484b-b8a2-4447d4f5d6a0 | fedora-1 | SHUTOFF | None       | Shutdown    | private=10.0.0.3, 172.24.0.2         |
| 97a3758f-9777-44cc-9035-ac95e57f8304 | fedora-2 | SHUTOFF | None       | Shutdown    | private=10.0.0.4                     |
+--------------------------------------+----------+---------+------------+-------------+--------------------------------------+
3. Above shows the previous ip twice until next update_info_cache() happens."
730,1272286,nova,d46f4a5b6d17f20a9c1de9367af11c9f3e7a7ef0,1,1,There is a bug. But they only remove code,Bug #1272286 “VMware driver,"Nova Compute fails to start when there are multiple nova compute services  running on different VMs(nova compute VMs) and each Vm managing multiple cluster in a vCenter and instances are provisioned on them.
Explanation:
Lets say, one nova compute vm(C1) is managing 5 clusters, and another(C2) managing 5 clusters. C1 manages n number of instances. Suppose in C2 compute service gets restarted, it fails to start.
Reason:
on the start up of the nova-compute,  it checks the instances reported by the driver are still associated with this host.  If they are  not, it destroys them.
method _destroy_evacuated_instances calls the driver list_instances, which lists all the instances in the vCenter, though they are managed by different compute. Instead it should return only the vms which are managed by c1/c2.
log file attached."
731,1272365,cinder,b1e7d0ad220d869323754683dbb447f6821ef705,0,0,Just because of pylinter: Ensure return,QoS specs update return value,"In db/api.py, qos_specs_update calls ""IMPL.qos_specs_update()"" instead of ""return IMPL.qos_specs_update()"".
This generates a pylint lintstack error:
[""Assigning to function call which doesn't return"", ""res = db.qos_specs_update(context, qos_specs_id, specs)""]
This doesn't look right -- I assume the result should be returned from db.qos_specs_update."
732,1272490,neutron,1027d02977812064b688fe0748c46663c1627cf2,1,0, some drivers do not support bulk operations,ML2 uses bulk support with drivers that don't supp...,"ML2 statically sets __native_bulk_support to True. However, some drivers do not support bulk operations so there should be a way to turn off bulk support when using those drivers."
733,1272500,nova,9271e300a55f8aa16ff867fff71adadb383849b8,1,0,newer version,Newer version of SQLite (>=3.7.16) cause some of o...,"After upgrading my machine to fedora 20 I noticed that some of our tests were failing. Then with a bit more investigation I saw that it was related to the version of the sqlite.
The problem seems to be already fixed in oslo by the review https://review.openstack.org/#/c/61405 . We need to sync the new version of the db.sqlalchemy module from oslo to ironic in order to get it fixed in Ironic as well.
LOG:
(venv)[lucasagomes@lucasagomes ironic]$ rpm -q sqlite
sqlite-3.8.2-1.fc20.x86_64
Example of broken test:
FAIL: ironic.tests.conductor.test_manager.ManagerTestCase.test_start_registers_driver_names
tags: worker-5
----------------------------------------------------------------------
Empty attachments:
  pythonlogging:''
  stdout
stderr: {{{
ironic/openstack/common/db/sqlalchemy/session.py:486: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6
  m = _DUP_KEY_RE_DB[engine_name].match(integrity_error.message)
}}}
Traceback (most recent call last):
  File ""ironic/tests/conductor/test_manager.py"", line 73, in test_start_registers_driver_names
    self.service.start()
  File ""ironic/conductor/manager.py"", line 106, in start
    'drivers': self.drivers})
  File ""ironic/objects/__init__.py"", line 28, in wrapper
    result = fn(*args, **kwargs)
  File ""ironic/db/sqlalchemy/api.py"", line 521, in register_conductor
    conductor.save()
  File ""ironic/openstack/common/db/sqlalchemy/models.py"", line 53, in save
    session.flush()
  File ""ironic/openstack/common/db/sqlalchemy/session.py"", line 551, in _wrap
    raise exception.DBError(e)
DBError: (IntegrityError) UNIQUE constraint failed: conductors.hostname u'INSERT INTO conductors (created_at, updated_at, hostname, drivers) VALUES (?, ?, ?, ?)' ('2014-01-24 19:43:43.362929', '2014-01-24 19:43:43.362563', 'test-host', '[""fake3"", ""fake4""]')"
734,1272503,swift,15c04f6869c3ae4bb8906b3783551df0e718d462,0,0,tests,test_connection_pool_timeout in unit.proxy.test_se...,"Seen here:
http://logs.openstack.org/84/67584/5/gate/gate-swift-python27/cf6c70e/console.html
Just need to evaluate and track."
735,1272518,nova,0b8267aa834e9d86cf2ecd9fd64ccc218df454bd,0,0,Modify some typos in tests and also deletes 2 returns in code that doesnt make sense. No bug,"The _test_{compute,console,network,scheduler}_api ...","The _test_{compute,console,network,scheduler}_api methods, found in nova/tests/{compute,console,network,scheduler}/test_rpcapi.py, all have the following line in the beginning:
    expected_retval = 'foo' if method == 'call' else None
However, the ""method"" parameter is never equal to 'call'. It is probably meant to read:
    expected_retval = 'foo' if rpc_method == 'call' else None
instead."
736,1272565,neutron,bb406281d8e36c29874bee3d9a3d1d5eae4ccdd5,0,0,Bad log severity: Debug severity is more appropriate,Validation should not log bad user input at error ...,"I noticed this while reviewing Ic2c87174.  When I read through log
files, I don't want to see errors like this that come from validating
bad user input.  Debug severity is more appropriate."
737,1272623,nova,6534a89de9cabc274cbdb7d2ecee3d851c456a87,1,1,"Bug that allows to restore, when it shouldn’t ",nova refuses to start if there are baremetal insta...,"This can happen if a deployment is interrupted at just the wrong time.
2014-01-25 06:53:38,781.781 14556 DEBUG nova.compute.manager [req-e1958f79-b0c0-4c80-b284-85bb56f1541d None None] [instance: e21e6bca-b528-4922-9f59-7a1a6534ec8d] Current state is 1, state in DB is 1. _init_instance /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py:720
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/service.py"", line 480, in run_service
    service.start()
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/service.py"", line 172, in start
    self.manager.init_host()
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 805, in init_host
    self._init_instance(context, instance)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 684, in _init_instance
    self.driver.plug_vifs(instance, net_info)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/driver.py"", line 538, in plug_vifs
    self._plug_vifs(instance, network_info)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/driver.py"", line 543, in _plug_vifs
    node = _get_baremetal_node_by_instance_uuid(instance['uuid'])
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/driver.py"", line 85, in _get_baremetal_node_by_instance_uuid
    node = db.bm_node_get_by_instance_uuid(ctx, instance_uuid)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/db/api.py"", line 101, in bm_node_get_by_instance_uuid
    instance_uuid)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 112, in wrapper
    return f(*args, **kwargs)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/db/sqlalchemy/api.py"", line 152, in bm_node_get_by_instance_uuid
    raise exception.InstanceNotFound(instance_id=instance_uuid)
InstanceNotFound: Instance 84c6090b-bf42-4c6a-b2ff-afb22b5ff156 could not be found.
If there is no allocated node, we can just skip that part of delete."
738,1272830,nova,2bfc7171c23d0595aa7f8680271778bc58cb28ba,0,0,Add more documentation,Bug #1272830 “document expected behavior when setting poll confi... ,"We have a whole bunch of config options that control periodic tasks that can be turned off when setting the option to 0 or -1. We should document in  the config option help messages what the expected behavior when setting these options to < 1.
Related patch: https://review.openstack.org/#/c/60641"
740,1273139,nova,a6d28e811ad916d686e350ce2dd8f6b8b0e128d6,1,1,typo in code,HostBinaryNotFound exception isn't caught in servi...,"When I update a service with not existing hostname or binary , I got the 500 error in nova api log.
2014-01-28 03:15:29.829 ERROR nova.api.openstack.extensions [req-5b1f3fc5-349a-4415-a4f5-63eab1c259a0 admin demo] Unexpected exception in API method
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 470, in wrapped
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/services.py"", line 172, in update
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     self.host_api.service_update(context, host, binary, status_detail)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 3122, in service_update
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/objects/base.py"", line 112, in wrapper
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     result = fn(cls, context, *args, **kwargs)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/objects/service.py"", line 105, in get_by_args
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     db_service = db.service_get_by_args(context, host, binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/db/api.py"", line 131, in service_get_by_args
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     return IMPL.service_get_by_args(context, host, binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 112, in wrapper
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 469, in service_get_by_args
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     raise exception.HostBinaryNotFound(host=host, binary=binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions HostBinaryNotFound: Could not find binary nova-cert on host xu-de.
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions"
741,1273259,neutron,9e247277e2d916fa705715e018ef5f3f221f0efa,0,0,tests,Bug #1273259 “Unit test failure,"FAIL: neutron.tests.unit.test_extension_ext_gw_mode.TestL3GwModeMixin.test_update_router_gw_with_gw_info_none
tags: worker-3
----------------------------------------------------------------------
Empty attachments:
pythonlogging:''
stderr
stdout
Traceback (most recent call last):
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/tests/unit/test_extension_ext_gw_mode.py"", line 251, in test_update_router_gw_with_gw_info_none
self._test_update_router_gw(None, True)
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/tests/unit/test_extension_ext_gw_mode.py"", line 238, in _test_update_router_gw
self.context, self.router.id, gw_info)
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/db/l3_gwmode_db.py"", line 62, in _update_router_gw_info
context, router_id, info, router=router)
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/db/l3_db.py"", line 205, in _update_router_gw_info
   l3_port_check=False)
TypeError: delete_port() got an unexpected keyword argument 'l3_port_check'"
742,1273266,nova,11d8dc7b9eed0e293ac5afab4792d8af7b807f4c,1,1,Wrong message log,Error message is malformed when removing a non-exi...,"Trying to remove a security group from an instance which is not actually associated with the instance produces the following:
---
$nova remove-secgroup 71069945-5bea-4d53-b6ab-9026bfeebba4 phil
ERROR: [u'Security group %(security_group_name)s not assocaited with the instance %(instance)s', {u'instance': u'71069945-5bea-4d53-b6ab-9026bfeebba4', u'security_group_name': u'phil'}] (HTTP 404) (Request-ID: req-a334b53d-e7cc-482c-9f1f-7bc61b8367e0)
---
The variables are not being populated correctly, and there is a typo:  "" assocaited"""
743,1273355,neutron,d26bd87919990923b6fc85779f03705514d8d0b3,1,1,fails,Grizzly-havana upgrade fails when migrating from s...,"If grizzly is deployed without using quantum-db-manage and letting neutron to create tables, there are not created tables servicedefinitions and servicetypes. These tables are dropped later when using LoadBalancerPlugin. When creating db scheme with quantum-db-manage, these tables are created and dropped correctly."
744,1273496,nova,bb67b0137e8faebc8892c9176775f02f6dbb920b,1,0,Bad driver name,libvirt iSCSI driver sets is_block_dev=False,"Trying to use iSCSI with libvirt/Xen, attaching volumes to instances was failing. I tracked this down to the libvirt XML looking like:
<disk type=""block"" device=""disk"">
  <driver name=""file"" type=""raw"" cache=""none""/>
  <source dev=""/dev/disk/by-path/ip-192.168.8.11:3260-iscsi-iqn.1986-03.com.sun:02:ecd142ab-b1c7-6bcf-8f91-f55b6c766bcc-lun-0""/>
  <target bus=""xen"" dev=""xvdb""/>
  <serial>e8c640c6-641b-4940-88f2-79555cdd5551</serial>
</disk>
The driver name should be ""phy"", not ""file"".
More digging lead to the iSCSI volume driver in nova/virt/libvirt/volume.py, which does:
class LibvirtISCSIVolumeDriver(LibvirtBaseVolumeDriver):
    """"""Driver to attach Network volumes to libvirt.""""""
    def __init__(self, connection):
        super(LibvirtISCSIVolumeDriver,
              self).__init__(connection, is_block_dev=False)
Surely is_block_dev should be ""True"" for iSCSI?? Changing this makes the problem go away - now pick_disk_driver_name() in nova/virt/libvirt/utils.py does the right thing and my volume attaches successfully.
Am I missing something here... ?"
745,1273803,nova,81f9256747377f1b9488b407a2f33d251ffd1d8f,1,1,Trying to do something wrong,The pci manager try to modify the pci device list,"Currently the ObjectList is mostly immutable, i.e. although the items in the list is changable, but the list itself should not be add or remove.
However the PCI manager use a ObjectList to track all the devices in the host and may add/remove, this is not correct. We should not use the Object List but a simple list to track all the devices."
746,1273852,nova,60ca9548905b58cea67776a71864486bfd1d69c7,0,0,No bug: should be purely DB layer,"Bug #1273852 "" PCI device object should be purely DB layer” ","Currently the PCI device object includes a lot of function like alloc/free/claim etc. However, the NovaObject should not be used this way, and it makes the PCI device object really different with other NovaObject implementation.
We should keep the PCI device object as simple data access, and keep those method to separated functions."
747,1274123,cinder,5ba26c8aa5b295a1202fbc6c01eb050ce3f3af3d,1,1,Race condition,storwize driver is not thread-safe,"when launching multiple VMs at the same time (for example by selecting a higher than one instance count in the dashboard), the action will fail with various errors.
most likely the process goes as follows:
Thread1 + ... + ThreadN at the same time: check if the host is mapped -> it doesn't
Thread1: let's create the host!
...
ThreadN: let's create the host!
Thread1 succeeds
Thread2..N fails
Thread1 doesn't even finish the mapping _most of the time_"
748,1274252,cinder,9afbd3fbba201ee606cf6e3b57675d3fbf9514b2,1,1,,volume_type_encryption extension allows create whe...,"Location: cinder.api.contrib.volume_type_encryption
Method: create
Bug:
When a volume type is made an encrypted volume type, through the volume_type_encryption API extension, the extension does not confirm no volumes exist with the volume type before making the volume type encrypted.  If volumes exist with the volume type before the volume type is made an encrypted volume type, these volumes will not be encrypted, although the user may think they are encrypted because they have an encrypted volume type.
Proposed Fix:
Add a check in the volume_type_encryption extension to stop the creation of an encrypted volume type if there are currently volumes with the volume type.  Also add a unit test confirming functionality."
749,1274294,nova,e498373b26d3fa05274429bcf958ea86eb4e3af2,1,1,Fix help msg,vmware section of nova.conf describes host_ip as u...,"In nova.conf, the host_ip of the vmware section describes it as being a URL.  But the expected input is a hostname or IP address, not a URL.  A URL is composed of a scheme, two colons, etc.  See http://en.wikipedia.org/wiki/Uniform_resource_locator
https://github.com/openstack/nova/blob/master/etc/nova/nova.conf.sample#L3265
[vmware]
#
# Options defined in nova.virt.vmwareapi.driver
#
# URL for connection to VMware ESX/VC host. (string value)
#host_ip=<None>
# Username for connection to VMware ESX/VC host. (string
# value)
#host_username=<None>
# Password for connection to VMware ESX/VC host. (string
# value)
#host_password=<None>
Also here:
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/driver.py#L49"
751,1274334,cinder,dc4655a5ae5a2ad576b68f3bc7d5044e395ee49e,1,1,typo in code,Bug #1274334 “3PAR manually created host detach KeyError,"how to duplicate
Manually creating host in 3PAR IMC with wwn from host
create volume
Attach volume
detach volume
2014-01-29 15:09:47.582 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Got semaphore ""3par"" for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:191
2014-01-29 15:09:47.583 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Attempting to grab file lock ""3par"" for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:202
2014-01-29 15:09:47.584 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Got file lock ""3par"" at /opt/stack/data/cinder/cinder-3par for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/common/loc
kutils.py:232
2014-01-29 15:09:47.584 DEBUG cinder.volume.drivers.san.hp.hp_3par_common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5
657274455fcad] Connecting to 3PAR from (pid=6822) client_login /opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py:178
2014-01-29 15:09:48.021 DEBUG cinder.volume.drivers.san.hp.hp_3par_common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5
657274455fcad] Disconnect from 3PAR from (pid=6822) client_logout /opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py:189
2014-01-29 15:09:48.022 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Released file lock ""3par"" at /opt/stack/data/cinder/cinder-3par for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/commo
n/lockutils.py:239
2014-01-29 15:09:48.022 ERROR cinder.openstack.common.rpc.amqp [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455fc
ad] Exception during message handling
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/utils.py"", line 821, in wrapper
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 660, in terminate_connection
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     self.driver.terminate_connection(volume_ref, connector, force=force)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 226, in terminate_connection
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     wwn=connector['wwpns'])
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1024, in terminate_connec
tion
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     hostname = self._get_3par_hostname_from_wwn_iqn(wwn, iqn)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1010, in _get_3par_hostna
me_from_wwn_iqn
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     if wwn == fc['WWN']:
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp KeyError: 'WWN'
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp
2014-01-29 15:09:48.024 ERROR cinder.openstack.common.rpc.common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455
fcad] Returning exception 'WWN' to caller
2014-01-29 15:09:48.025 ERROR cinder.openstack.common.rpc.common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455
fcad] ['Traceback (most recent call last):\n', '  File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data\n    **args)\n', '  File ""/op
t/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch\n    return getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/opt/stack/cinder/ci
nder/utils.py"", line 821, in wrapper\n    return func(self, *args, **kwargs)\n', '  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 660, in terminate_connection
\n    self.driver.terminate_connection(volume_ref, connector, force=force)\n', '  File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner\n
 retval = f(*args, **kwargs)\n', '  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 226, in terminate_connection\n    wwn=connector[\'wwpns\']
)\n', '  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1024, in terminate_connection\n    hostname = self._get_3par_hostname_from_wwn_iq
n(wwn, iqn)\n', '  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1010, in _get_3par_hostname_from_wwn_iqn\n    if wwn == fc[\'WWN\']:\n'
, ""KeyError: 'WWN'\n""]"
752,1274365,glance,7f3e27ab06a0792181347f0f4feb7d06b546d9ac,0,0,tests,Bug #1274365 “FAIL,"Seeing this in the gate-glance-python27 jobs - 8 hits in the last 7 days.
2014-01-30 01:29:36.436 | ======================================================================
2014-01-30 01:29:36.436 | FAIL: glance.tests.unit.v2.test_tasks_resource.TestTasksController.test_index_with_sort_key
2014-01-30 01:29:36.436 | ----------------------------------------------------------------------
2014-01-30 01:29:36.436 | _StringException: Traceback (most recent call last):
2014-01-30 01:29:36.436 |   File ""/home/jenkins/workspace/gate-glance-python27/glance/tests/unit/v2/test_tasks_resource.py"", line 233, in test_index_with_sort_key
2014-01-30 01:29:36.436 |     self.assertEqual(UUID4, actual[0])
2014-01-30 01:29:36.437 |   File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
2014-01-30 01:29:36.437 |     self.assertThat(observed, matcher, message)
2014-01-30 01:29:36.437 |   File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
2014-01-30 01:29:36.437 |     raise mismatch_error
2014-01-30 01:29:36.437 | MismatchError: !=:
2014-01-30 01:29:36.437 | reference = '6bbe7cc2-eae7-4c0f-b50d-a7160b0c6a86'
2014-01-30 01:29:36.437 | actual    = 'c80a1a6c-bd1f-41c5-90ee-81afedb1d58d'
2014-01-30 01:29:36.437 |
Logstash URL:
http://logstash.openstack.org/#eyJzZWFyY2giOiJcImluIHRlc3RfaW5kZXhfd2l0aF9zb3J0X2tleVwiIEFORCBcInRlc3RfdGFza3NfcmVzb3VyY2UucHlcIiBBTkQgZmlsZW5hbWU6XCJjb25zb2xlLmh0bWxcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNjA0ODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTM5MTA0NzE4ODMyNywibW9kZSI6IiIsImFuYWx5emVfZmllbGQiOiIifQ==
Logstash Query:
message:""in test_index_with_sort_key"" AND message:""test_tasks_resource.py"" AND filename:""console.html"""
753,1274439,nova,73c87a280e77e03d228d34ab4781ca2e3b02e40e,1,0,performance issue. Change in requirements,Bug #1274439 “VMware,"The default values of the task_poll_interval is 5 seconds. This means that any operation against the VC will wait at least 5 seconds.
An example of this - a spawn operation would take on average take 25 seconds. When this parameter what changed to 0.2 - 1.0 seconds the operation would take on average 9 seconds."
754,1274523,nova,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,0,0,Add support ,connection_trace does not work with DB2 backend,"When setting connection_trace=True, the stack trace does not get printed for DB2 (ibm_db).
I have a patch that we've been using internally for this fix that I plan to upstream soon, and with that we can get output like this:
2013-09-11 13:07:51.985 28151 INFO sqlalchemy.engine.base.Engine [-] SELECT services.created_at AS services_created_at, services.updated_at AS services_updated_at, services.deleted_at AS services_deleted_at, services.deleted AS services_deleted, services.id AS services_id, services.host AS services_host, services.""binary"" AS services_binary, services.topic AS services_topic, services.report_count AS services_report_count, services.disabled AS services_disabled, services.disabled_reason AS services_disabled_reason
FROM services WHERE services.deleted = ? AND services.id = ? FETCH FIRST 1 ROWS ONLY
2013-09-11 13:07:51,985 INFO sqlalchemy.engine.base.Engine (0, 3)
2013-09-11 13:07:51.985 28151 INFO sqlalchemy.engine.base.Engine [-] (0, 3)
        File /usr/lib/python2.6/site-packages/nova/servicegroup/drivers/db.py:92 _report_state() service.service_ref, state_catalog)
        File /usr/lib/python2.6/site-packages/nova/conductor/api.py:270 service_update() return self._manager.service_update(context, service, values)
        File /usr/lib/python2.6/site-packages/nova/openstack/common/rpc/common.py:420 catch_client_exception() return func(*args, **kwargs)
        File /usr/lib/python2.6/site-packages/nova/conductor/manager.py:461 service_update() svc = self.db.service_update(context, service['id'], values)
        File /usr/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py:505 service_update() with_compute_node=False, session=session)
        File /usr/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py:388 _service_get() result = query.first()"
755,1274601,cinder,d311066dd43bdc4598fa76837d78ecfac16f5e58,0,0,No BFC for this bug,GlanceMetadataNotFound exception not through durin...,"delete_volume implementation would like to log a warning ""no glance metadata found for volume %s"" in case of lack of metadata by catching a GlanceMetadataNotFound exception.
Nevertheless this exception is never throw by the SQL backend implementation.
So this log cannot be display correctly."
756,1274611,nova,0d447f6edd406547e828c592823ac6624e56e91f,1,1,Bug with the configuration,nova-network bridge setup fails if the interface a...,"While setting the bridge up, if the network interface has a dynamic address, the 'dynamic' flag will be displayed in the ""ip addr show"" command:
[fedora@dev1 devstack]$ ip addr show dev eth0 scope global
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:00:00:01 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.2/24 brd 192.168.122.255 scope global dynamic eth0
       valid_lft 2225sec preferred_lft 2225sec
When latter executing ""ip addr del"" with the IPv4 details, the 'dynamic' flag is not accepted, causes the command to crash and leaves the bridge half configured."
757,1274627,nova,a70c5eb83d14541c2297b6d834dff1c0ac00962e,0,0,"I dont see the bug, feature?. Volume attach /detach should be blocked ",Volume attach /detach should be blocked during som...,"Currently volume attach, detach, and swap check on vm_state but not task_state.  This means that, for example, volume attach is allowed during a reboot, rebuild, or migration.
As with other operations the check should be against a task state of ""None"""
758,1274718,nova,e6ac12041ebe85fe9d46ab9346ae0bc9ef0a3599,0,0,tests,_test_postgresql_opportunistically calls get_mysql...,"Not sure if this is intentional or an oversight, but nova.tests.db.test_migrations.BaseWalkMigrationTestCase._test_postgresql_opportunistically calls get_mysql_connection_info even though there is a get_pgsql_connection_info method that is slightly different and used elsewhere.
Obviously not breaking anything, but should probably be cleaned up."
759,1274758,nova,82821b407ce7a226918758876abca46b727d9706,1,1,,Error during ComputeManager.update_available_resou...,"Error during ComputeManager.update_available_resource: 'NoneType' object has no attribute '__getitem__'
ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: 'NoneType' object has no attribute '__getitem__'
TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/openstack/common/periodic_task.py"", line 182, in run_periodic_tasks
TRACE nova.openstack.common.periodic_task     task(self, context)
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 5049, in update_available_resource
TRACE nova.openstack.common.periodic_task     rt.update_available_resource(context)
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/openstack/common/lockutils.py"", line 249, in inner
TRACE nova.openstack.common.periodic_task     return f(*args, **kwargs)
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/compute/resource_tracker.py"", line 300, in update_available_resource
TRACE nova.openstack.common.periodic_task     resources = self.driver.get_available_resource(self.nodename)
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 3943, in get_available_resource
TRACE nova.openstack.common.periodic_task     stats = self.host_state.get_host_stats(refresh=True)
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 5016, in get_host_stats
TRACE nova.openstack.common.periodic_task     self.update_status()
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 5052, in update_status
TRACE nova.openstack.common.periodic_task     data[""vcpus_used""] = self.driver.get_vcpu_used()
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 3626, in get_vcpu_used
TRACE nova.openstack.common.periodic_task     total += len(vcpus[1])
TRACE nova.openstack.common.periodic_task TypeError: 'NoneType' object has no attribute '__getitem__'
TRACE nova.openstack.common.periodic_task
http://logs.openstack.org/51/63551/6/gate/gate-tempest-dsvm-postgres-full/4860441/logs/screen-n-cpu.txt.gz?level=TRACE#_2014-01-30_22_38_29_401
Seen in the gate
logstash query: message:""TypeError: 'NoneType' object has no attribute '__getitem__'"" AND filename:""logs/screen-n-cpu.txt"""
760,1274924,nova,328beb0433c04a0888d38cbc2927248fdd963318,1,1,Problem updating. Deletes some metadata,Settings the availability zone deletes all attache...,"I'm using latest git with devstack on Ubuntu 12.04.
When I update the availability zone the attached metadata gets deleted. Steps to reproduce the problem:
 1) nova aggregate-create testagg   #assuming that this creates a new metadata entry with the Id 26
 2) nova aggregate-set-metadata 26 x=y
 3) nova aggregate-update 26 testagg zone1
Now the availability zone is set, but the x=y metadata is lost."
761,1274992,nova,2749bb9138f5946d1ef3030c7fe349fbd5920139,1,1,Race condition,Utils.is_neutron can create race conditions,"It appears that using the utils.is_neutron() function can create race conditions, if not used carefully. Review #56381 was attempting to check the presence of Neutron, in order to determine if hairpinning should be enabled on the bridge that Nova creates. Only after creating a new configuration flag in Nova.conf and checking the setting, were the unit tests passing and the race condition resolved.
My hunch is that the global variable that utils.is_neutron( ) creates is the root cause of the issue.
https://review.openstack.org/#/c/56381/"
762,1275126,glance,2c94b11c40433e44c481aed6b590a563471dc358,0,0, Add VMware store the strategy module ,Add VMware store the strategy module,The VMware store needs to be added to strategy module: https://github.com/openstack/glance/blob/9567c2b6a06aa1e8205f9f30beca63d77500dd1d/glance/common/location_strategy/store_type.py#L55
763,1275166,neutron,ebc88e6110660c0d543e75140d2e19ccf4796bea,0,0,remove unused code,Clean up unused RPC calls from Cisco N1kv plugin,Remove unused RPC calls since n1kv plugin does not communicate with l2 agent.
764,1275173,cinder,da13c6285bb0aee55cfbc93f55ce2e2b7d6a28f2,0,0,Refactoring code to make 1 less call. No one speaks about performance,_translate_from_glance() can cause an unnecessary ...,"I noticed when performing a ""nova image-show"" on a current (not deleted) image, two HTTP requests were issued. Why isn't the Image retrieved on the first GET request?
In fact, it is. The problem lies in _extract_attributes(), called by _translate_from_glance(). This function loops through a list of expected attributes, and extracts them from the passed-in Image. The problem is that if the attribute 'deleted' is False, there won't be a 'deleted_at' attribute in the Image. Not finding the attribute results in getattr() making another GET request (to try to find the ""missing"" attribute?). This is unnecessary of course, since it makes sense for the Image to not have that attribute set."
765,1275267,nova,6a0e6209cab2ced467b63ce9ce69a41daae669fe,1,1,,GuestFS fails to mount image for data injection,"A GuestFS error is causing injection to fail. This result in a warning for metadata injection but results in a spawn error for key injection.
This is logged with debug level:
Exception AttributeError: ""GuestFS instance has no attribute '_o'"" in <bound method GuestFS.__del__ of <guestfs.GuestFS instance at 0x3ea9f38>> ignored
febootstrap-supermin-helper: ext2: parent directory not found: /lib: File not found by ext2_lookup
And causes this error: http://paste.openstack.org/show/62293/
Full logs available here:  http://logs.openstack.org/58/63558/8/check/check-tempest-dsvm-neutron-pg/108e4ca/logs
Interestingly, it seems guestfs was not actually used when the relevant patches went throught the gate checks:
https://review.openstack.org/#/c/70237/
https://review.openstack.org/#/c/70354/
This was expected for patch #70354 but sounds strange for patch #70237
Finally, The traceback seeems different from that of bug 1221985"
766,1275352,nova,c283576770b1bb96fd3dd012c6ced22b2a8eb903,0,0,tests,test_migration_utils runs sqlite tests even if no ...,"There are several jobs that only run if you have sqlite configured in nova.tests.db.test_migration_utils but there are two which will run and fail if you don't have sqlite configured:
======================================================================
ERROR: nova.tests.db.test_migration_utils.TestMigrationUtils.test_check_shadow_table_with_unsupported_type
----------------------------------------------------------------------
_StringException: pythonlogging:'': {{{INFO [nova.tests.db.test_migrations] Creating DB2 schema nova.novatstu}}}
Traceback (most recent call last):
  File ""/root/nova-es/nova/tests/db/test_migration_utils.py"", line 391, in test_check_shadow_table_with_unsupported_type
    engine = self.engines['sqlite']
KeyError: 'sqlite'
======================================================================
ERROR: nova.tests.db.test_migration_utils.TestMigrationUtils.test_util_drop_unique_constraint_with_not_supported_sqlite_type
----------------------------------------------------------------------
_StringException: pythonlogging:'': {{{INFO [nova.tests.db.test_migrations] Creating DB2 schema nova.novatstu}}}
Traceback (most recent call last):
  File ""/root/nova-es/nova/tests/db/test_migration_utils.py"", line 215, in test_util_drop_unique_constraint_with_not_supported_sqlite_type
    for i in xrange(0, len(values)):
UnboundLocalError: local variable 'values' referenced before assignment
Those should be skipped if sqlite isn't configured like the other sqlite-specific tests."
767,1275654,neutron,fdbaba877f98bf20c1ff71c9bd0d04956120f845,0,0,tests,neutron.unit.services code duplication,"code duplication between neutron.unit.services.loadbalancer.test_loadbalancer_plugin
and neutron.unit.services.vpn.test_vpnaas_extension"
768,1275682,cinder,86591a24d48fe19be427b5a8981821fcdb85bb37,0,0, volume retype to be supported,Bug #1275682 “vmware,"https://blueprints.launchpad.net/cinder/+spec/volume-retype
Volume retype is a new driver API that is introduced in IceHouse release. This allows the user to manually change the volume type associated with a volume.
The vmdk driver uses a volume-type to associate vSphere storage profile with the volume. The vmdk driver needs to handle a volume type change."
769,1275755,nova,280af85945d7ce8be60135fd364f22e1e8782b87,0,0,Exception with more information,v2 volume api should return volumenotfound,"currently, NotFound exception was raised when no volume found in v2 api
this will not be helpful to user especially there is no 'ec2 error code' returned"
770,1275771,nova,05f688e49fbf543a6c69c0f186279c6732118470,1,1,Notifications do not work,Bug #1275771 “Notifications do not work,"When enabling notification with notification_driver = messaging, I get the following:
2014-02-03 14:20:41.152 ERROR oslo.messaging.notify._impl_messaging [-] Could not send notification to notifications. Payload={'priority': 'INFO', '_unique_id': 'da748b32fd144c25adc45ba5b393339d', 'event_type': 'compute.instance.create.end', 'timestamp': '2014-02-03 14:20:41.151419', 'publisher_id': 'compute.devstack', 'payload': {'node': u'devstack', 'state_description': '', 'ramdisk_id': u'37ad58df-c587-4bed-9062-9428ca14eaf0', 'created_at': '2014-02-03 14:20:33+00:00', 'access_ip_v6': None, 'disk_gb': 0, 'availability_zone': u'nova', 'terminated_at': '', 'ephemeral_gb': 0, 'instance_type_id': 6, 'instance_flavor_id': '42', 'image_name': u'cirros-0.3.1-x86_64-uec', 'host': u'devstack', 'fixed_ips': [FixedIP({'version': 4, 'floating_ips': [], 'label': u'private', 'meta': {}, 'address': u'10.0.0.2', 'type': u'fixed'})], 'user_id': u'6bcbc8f54d65473c9a0c4a55f64fb580', 'message': u'Success', 'deleted_at': '', 'reservation_id': u'r-jycyyveh', 'image_ref_url': u'http://162.209.87.220:9292/images/7b8d712a-fb31-43b8-8a05-a74d70fd8a11', 'memory_mb': 64, 'root_gb': 0, 'display_name': u'dwq', 'instance_type': 'm1.nano', 'tenant_id': u'cda1741ff4ef47f48fb3d9d76e302add', 'access_ip_v4': None, 'hostname': u'dwq', 'vcpus': 1, 'instance_id': '272c2ec6-bb98-4e84-9377-84c63c7a9ce9', 'kernel_id': u'5d1a6130-0e6a-4155-9c05-0174a654da68', 'state': u'active', 'image_meta': {u'kernel_id': u'5d1a6130-0e6a-4155-9c05-0174a654da68', u'container_format': u'ami', u'min_ram': u'0', u'ramdisk_id': u'37ad58df-c587-4bed-9062-9428ca14eaf0', u'disk_format': u'ami', u'min_disk': u'0', u'base_image_ref': u'7b8d712a-fb31-43b8-8a05-a74d70fd8a11'}, 'architecture': None, 'os_type': None, 'launched_at': '2014-02-03T14:20:41.070490', 'metadata': {}}, 'message_id': '03b2985a-6bcd-44ff-8303-29618d3c2b01'}
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging Traceback (most recent call last):
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/opt/stack/oslo.messaging/oslo/messaging/notify/_impl_messaging.py"", line 47, in notify
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     version=self.version)
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/opt/stack/oslo.messaging/oslo/messaging/transport.py"", line 93, in _send_notification
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     self._driver.send_notification(target, ctxt, message, version)
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 393, in send_notification
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     return self._send(target, ctxt, message, envelope=(version == 2.0))
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 362, in _send
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     rpc_amqp.pack_context(msg, context)
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqp.py"", line 299, in pack_context
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     context_d = six.iteritems(context.to_dict())
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/six.py"", line 484, in iteritems
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     return iter(getattr(d, _iteritems)(**kw))
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging AttributeError: 'RequestContext' object has no attribute 'iteritems'
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging"
771,1275772,nova,378c28a8c05d3943324b5a39edef947ed7a9f52a,1,1,missing raise statement,Missing “raise,"There's a missing raise statement when checking the ConfigDrive format:
https://github.com/openstack/nova/blob/master/nova/virt/hyperv/vmops.py#L283"
772,1276011,glance,8496dad34797104482a5a76d809b72569adcd794,0,0,Remove duplicate code,Duplicate type defination in v2 images schema,"For example, see https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L664 and https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L666"
773,1276038,nova,f591eee475df177fef5f10898b2d95c1a75f5563,0,0, Disable IGMP and refactorize tests,Linux bridge create by the OVS hybrid vif driver s...,"The Linux bridge (qbr) created by the OVS hybrid vif driver sends usefulness IGMP queries.
By default, Linux bridge activates IGMP snooping onto a switch.
In the case of the Linux bridge was created by the OVS hybrid vif driver, is usefulness.
Because this bridge is only here to be able to apply security groups through netfilter (OVS bridge doesn't use netfilter hooks) and so it contains only 2 interfaces: the VM tap and a side of the veth patch port.
IGMP snooping on a switch of 2 ports is unnecessarily.
Furthermore, the Linux bridge could (I didn't find why and when) send some IGMP queries when the IGMP snooping is activated.
This IGMP queries could unnecessarily pollute the network and the network equipemnts log."
774,1276142,glance,f50074c0f65fdeae90dcd4b469fa2ac6f306e4b7,1,1,Fix unstable state after image wasnt deleted because an error,Physical image can not be deleted again if deletio...,"If the deletion of physical image failed with OSError for some reason, it is not possible to delete the image using image-delete api call.
In the longevity test we have encountered this issue, IMO there is some issue with OS and it is throwing ""Forbidden"" exception and somehow its not deleting that file, so in order to reproduce this issue please follow the below mentioned steps:
Steps to reproduce:
1. image creation, upload
    openstack@opencloud1:~$ glance image-create
    +------------------+--------------------------------------+
    | Property         | Value                                |
    +------------------+--------------------------------------+
    | checksum         | None                                 |
    | container_format | None                                 |
    | created_at       | 2014-01-29T02:27:07                  |
    | deleted          | False                                |
    | deleted_at       | None                                 |
    | disk_format      | None                                 |
    | id               | 675a82ab-4515-451a-932b-6da7f8bce056 |
    | is_public        | False                                |
    | min_disk         | 0                                    |
    | min_ram          | 0                                    |
    | name             | None                                 |
    | owner            | de4a6631051a4df09bc1b12923244296     |
    | protected        | False                                |
    | size             | 0                                    |
    | status           | queued                               |
    | updated_at       | 2014-01-29T02:27:07                  |
    +------------------+--------------------------------------+
    openstack@opencloud1:~$ glance image-update 675a82ab-4515-451a-932b-6da7f8bce056 --file images/CorePlus4.7.1.qcow2 --disk-format qcow2 --container-format bare
    +------------------+--------------------------------------+
    | Property         | Value                                |
    +------------------+--------------------------------------+
    | checksum         | a5282e9259f822c782bc7aea8a8870c6     |
    | container_format | bare                                 |
    | created_at       | 2014-01-29T02:27:07                  |
    | deleted          | False                                |
    | deleted_at       | None                                 |
    | disk_format      | qcow2                                |
    | id               | 675a82ab-4515-451a-932b-6da7f8bce056 |
    | is_public        | False                                |
    | min_disk         | 0                                    |
    | min_ram          | 0                                    |
    | name             | None                                 |
    | owner            | de4a6631051a4df09bc1b12923244296     |
    | protected        | False                                |
    | size             | 43450368                             |
    | status           | active                               |
    | updated_at       | 2014-01-29T02:27:20                  |
    +------------------+--------------------------------------+
2. write protect the image file
    openstack@opencloud1:~$ sudo chattr +i /opt/stack/data/glance/images/675a82ab-4515-451a-932b-6da7f8bce056
3. delete the image using ""glance image-delete""
    openstack@opencloud1:~$ glance image-delete 675a82ab-4515-451a-932b-6da7f8bce056
    Request returned failure status.
    HTTPForbidden (HTTP 403)
4. Image is logically deleted, but physically remaining
    openstack@opencloud1:~$ glance image-show 675a82ab-4515-451a-932b-6da7f8bce056
    +------------------+--------------------------------------+
    | Property         | Value                                |
    +------------------+--------------------------------------+
    | checksum         | a5282e9259f822c782bc7aea8a8870c6     |
    | container_format | bare                                 |
    | created_at       | 2014-01-29T02:27:07                  |
    | deleted          | True                                 |
    | deleted_at       | 2014-01-29T02:30:49                  |
    | disk_format      | qcow2                                |
    | id               | 675a82ab-4515-451a-932b-6da7f8bce056 |
    | is_public        | False                                |
    | min_disk         | 0                                    |
    | min_ram          | 0                                    |
    | owner            | de4a6631051a4df09bc1b12923244296     |
    | protected        | False                                |
    | size             | 43450368                             |
    | status           | deleted                              |
    | updated_at       | 2014-01-29T02:30:49                  |
    +------------------+--------------------------------------+
    openstack@opencloud1:~$ sudo ls -ls /opt/stack/data/glance/images/
    total 33088
    42432 -rw-r----- 1 glance glance   43450368 Jan 29 11:27 675a82ab-4515-451a-932b-6da7f8bce056
5. Remove the write-protect from image file
    openstack@opencloud1:~$ sudo chattr -i /opt/stack/data/glance/images/675a82ab-4515-451a-932b-6da7f8bce056
6. Try to delete the image again using ""glance image-delete""
    openstack@opencloud1:~$ glance image-delete 675a82ab-4515-451a-932b-6da7f8bce056
    Request returned failure status.
    HTTPForbidden (HTTP 403)
7. Image file will not get deleted
    openstack@opencloud1:~$ sudo ls -ls /opt/stack/data/glance/images/
    total 33088
    42432 -rw-r----- 1 glance glance   43450368 Jan 29 11:27 675a82ab-4515-451a-932b-6da7f8bce056
This issue is reproducible only in v1 version. In case of v2 version, it doesn't delete the meta data associated with the image before actually deleting it from the backend store.
In glance, if you enable delayed_delete = True and run glance-scrubber service, glance api will put the image in the queue when the image is deleted by the user and this image will be deleted asynchronously. If it encounters the above reported issue while deleting the image in the glance-scrubber, then this service will simply log error message and keep on retrying deleting the image until it is deleted finally.
Possible solution:
1. When delayed_delete is enabled.
delete_image_metadata method should be moved to the scrubber and called immediately after setting the image status from ""pending_delete"" to ""deleted"".
2. When delayed_delete is not enabled.
delete_image_metadata method should be called after deleting the actual image from the backend.
i.e. after upload_utils.initiate_deletion(req, image['location'], id, CONF.delayed_delete) is called
Also the status of the image should be set back to the previous state in the forbidden exception block."
775,1276312,nova,60da388de842a0e3b11aed8f4f4e2e21ab270660,0,0,Mak a more generic msg,VirtualInterfaceMacAddressException message mentio...,"The exception ""VirtualInterfaceMacAddressException"" message says
""5 attempts to create virtual interface with unique mac address failed"" when the number of attempts is configurable
by ""create_unique_mac_address_attempts"".
As proposed fix change to a generic message."
777,1276398,nova,c94b71c2d42a651f3b24d5f0ac1819bfa2a79a12,0,0,There is a bug in tests. Change the place of the configuration to satisfy the tests,nova.tests.api.ec2.test_cloud is missing consoleau...,"If you run nova unit tests outside of a virtualenv, like with using nosetests on python 2.6 (which should be supported but not really enforced), then the ec2 test_cloud test fails due to not having the consoleauth_manager config option in scope:
Traceback (most recent call last):
  File ""/root/nova/nova/tests/api/ec2/test_cloud.py"", line 178, in setUp
    self.consoleauth = self.start_service('consoleauth')
  File ""/root/nova/nova/test.py"", line 295, in start_service
    svc = self.useFixture(ServiceFixture(name, host, **kwargs))
  File ""/usr/lib/python2.6/site-packages/testtools/testcase.py"", line 628, in useFixture
    fixture.setUp()
  File ""/root/nova/nova/test.py"", line 174, in setUp
    self.service = service.Service.create(**self.kwargs)
  File ""/root/nova/nova/service.py"", line 272, in create
    manager = CONF.get(manager_cls, None)
  File ""/usr/lib64/python2.6/_abcoll.py"", line 336, in get
    return self[key]
  File ""/usr/lib/python2.6/site-packages/oslo/config/cfg.py"", line 1626, in __getitem__
    return self.__getattr__(key)
  File ""/usr/lib/python2.6/site-packages/oslo/config/cfg.py"", line 1622, in __getattr__
    raise NoSuchOptError(name)
NoSuchOptError: no such option: consoleauth_manager
There is a mailing list thread related to this, but not for ec2:
http://lists.openstack.org/pipermail/openstack-dev/2013-September/014896.html
Simply importing this fixes the problem:
CONF.import_opt('consoleauth_manager', 'nova.consoleauth.manager')"
778,1276409,neutron,844ae433c17ee3956a8c83ed693ca44c8e3c38de,1,1,typo in a message ,Fix error message typo in PLUMgrid Plugin,"PLUMgrid Plugin needs to fix error message typo in ""_network_admin_state"" function where
""Network Admin State Validation Falied"" needs to have typo fixed."
779,1276440,neutron,3faea81c6029033c85cefd6e98d7a3e719e858f5,1,1,dont not send duplicate msg. High load of traffic,Metadata-agent sends a lot of same requests to neu...,"When cloud-init is running metadata-agent sends a lot of same requests to neutron-server. These requests serves for obtaining instance_id and tenant_id of running instance. When higher amount of instances is started at once mentioned requests cause high load for neutron-server.
Requests could be cached with small ttl."
780,1276539,nova,fb12fa03ad21c4c8924175ae77bc6bd30139f089,0,0,Raise more specific exceptions,Bug #1276539 “VMware,"[req-83b880af-7ed2-417f-8d91-ae9b7d624be1 FixedIPsTestJson-1652350262 FixedIPsTestJson-1562534047] In vmwareapi: _call_method (session=52e6b7a6-4745-7303-17f1-52ed4da676a8)
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 817, in _call_method
    return temp_module(*args, **kwargs)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 197, in vim_request_handler
    for child in detail.getChildren():
AttributeError: 'NoneType' object has no attribute 'getChildren'"
781,1276644,nova,acf9fddcf3129ffab9e01aeb1e9991830013bfbd,1,1,Bug not defining a variable in python,Bug #1276644 “VMware,"The following appears in the log files when this happens:
[req-83b880af-7ed2-417f-8d91-ae9b7d624be1 FixedIPsTestJson-1652350262 FixedIPsTestJson-1562534047] In vmwareapi:vmops:destroy, got this exception while deleting the VM contents from the disk: local variable 'datastore_name' referenced before assignment"
782,1276728,nova,3720fd17b060de6f14e537e6c003e923c67c65a0,0,0,'can be wrong’ not a bug still,Cells capacity reports can be wrong when multiple ...,"mysql> select memory_mb,memory_mb_used,free_ram_mb,free_disk_gb from compute_nodes where free_ram_mb > 8000;
+-----------+----------------+-------------+--------------+
| memory_mb | memory_mb_used | free_ram_mb | free_disk_gb |
+-----------+----------------+-------------+--------------+
|    131026 |         121640 |        9386 |          810 |
|    131026 |         121621 |        9405 |          990 |
|    131026 |         121636 |        9390 |          790 |
+-----------+----------------+-------------+--------------+
3 rows in set (0.00 sec)
which indicates three hosts that can handle an 8GB instance, but the capacity reported in the cell log shows:
capacities: {'ram_free': {'units_by_mb': {'8192': 6,
What's happening is that two flavors have memory_mb set to 8192 so the slots are being counted twice."
783,1276731,nova,c9a020f121a85ff88b890e964dc8c1f6d0ff5843,0,0,Suggestion:  should not rely,simple_tenant_usage extension should not rely on l...,"The simple_tenant_usage extension gets the flavor data from the instance and then looks up the flavor from the database to return usage information. Since we now store all of the flavor data in the instance itself, we should use that information instead of what the flavor currently says is right. This both (a) makes it more accurate and (b) avoids us failing to return usage info if a flavor disappears."
784,1276876,neutron,6aa0a44a9d2ed0a4c2d6d515458c94698a1ad3d8,0,0,Add and update information,Add update subnet and pass required attributes in ...,"1) Send dns nameserver and dhcp info to the VSM
2) Update subnet properties to VSM"
786,1277054,nova,ea7242a293abe59dd91add5e44e58a2eb6cd99fc,1,1,,Poll rescued instances fails with key error,"After an instance has been in the rescue state for some time, a periodic task triggers to unrescue the instances:
_poll_rescued_instances
File nova/notifications.py info_from_instance
  instance_type = flavors.extract_flavor(instance_ref)
File ""nova/compute/flavors.py"" in extract_flavor
  instance_type[key] = type_fn(sys_meta[type_key])
KeyError: 'instance_type_memory_mb'
This then continues to happen on every run of the periodic task, and starts to fill up the DB with instance faults."
787,1277155,cinder,dc02810c5dc64ca48a163645588ea6a60367227a,0,0,tests,LVM migrate volume tests are not testing correct c...,"cinder/tests/test_volume.py
LVMISCSIVolumeDriverTestCase.test_lvm_migrate_volume_diff_driver
LVMISCSIVolumeDriverTestCase.test_lvm_migrate_volume_diff_host
The location_info is incorrect, it needs to be 5 sections separated by colons,  which means that the following condition does not get tested
691         if (dest_type != 'LVMVolumeDriver' or dest_hostname != self.hostname):
692             return false_ret"
788,1277168,cinder,ea7d4a599224b5d0c7674d03993bbe72c49f0d51,1,0,Bug because of evolution of the names etc,having oslo.sphinx in namespace package causes iss...,"http://lists.openstack.org/pipermail/openstack-dev/2014-January/023759.html
We've decided to rename oslo.sphinx to oslosphinx. This will require small changes in the doc builds for a lot of the other projects.
The problem seems to be when we pip install -e oslo.config on the system, then pip install oslo.sphinx in a venv. oslo.config is unavailable in the venv, apparently because the namespace package for o.s causes the egg-link for o.c to be ignored."
789,1277222,neutron,056942ad5c0b379ebad06110b45b72f685b6c03d,0,0, Reorganize code,Reorganize code tree for multiple cisco ML2 mech d...,"Currently there is one ML2 driver for cisco nexus in
    neutron/plugins/ml2/drivers/cisco
It needs to go down a level so other cisco drivers can live alongside it:
    neutron/plugins/ml2/drivers/cisco/apic
    neutron/plugins/ml2/drivers/cisco/nexus
    neutron/plugins/ml2/drivers/cisco/ucs
    neutron/plugins/ml2/drivers/cisco/..."
790,1277422,nova,c70749b9c8d9575ba28b2c012615d6bae146ccc3,1,1,wrong quota calculation,Quotas change incorrectly when a resizing instance...,"When deleting a resizing instance that has not yet finished resizing, quotas should be adjusted for the old flavor type. Instead it incorrectly use values from the new flavor.
This was originally reported and fixed in bug 1099729 but has since resurfaced with the move to objects (commit  dce64683291ba2cdb5e6617e01ccc2909254acb4). This was made possible by a prior change (commit a56f0b33069b919ebb24c4afdcc6b6c31592c98e) that accidentally removed the test put in place to guard against this error ever happening again."
791,1277914,neutron,e62e5f3763934bef452d07dafc29ee3709bc2213,1,1,Raise a exception that does not exist,ML2 plugin cannot raise NoResultFound exception,"The ML2 plugin cannot raise NoResultFound exception because it does not use the correct sqlalchemy library.
'from sqlalchemy import exc as ...'  instead of 'from sqlalchemy.orm import exc as ...'"
792,1278028,cinder,a82bae3f7b56fb84377fa6571b9f21ca6809da0e,0,0,Update the default settings to a new value,"Bug #1278028 ""    VMware","https://review.openstack.org/70079
Dear documentation bug triager. This bug was created here because we did not know how to map the project name ""openstack/nova"" to a launchpad project name. This indicates that the notify_impact config needs tweaks. You can ask the OpenStack infra team (#openstack-infra on freenode) for help if you need to.
commit 73c87a280e77e03d228d34ab4781ca2e3b02e40e
Author: Gary Kotton <email address hidden>
Date:   Thu Jan 30 01:44:10 2014 -0800
    VMware: update the default 'task_poll_interval' time
    The original means that each operation against the backend takes at
    least 5 seconds. The default is updated to 0.5 seconds.
    DocImpact
        Updated default value for task_poll_interval from 5 seconds to
        0.5 seconds
    Change-Id: I867b913f52b67fa9d655f58a2e316b8fd1624426
    Closes-bug: #1274439"
793,1278035,cinder,4cac1bd3227fd8b65744e75f0df018fb34bcb1c1,0,0,Feature not a bug,storwize-svc migration/retype may leave extra volu...,"Cinder storage-assisted volume migration and retype work by creating a new copy of the volume, waiting for it to sync, and deleting the original copy.  During the sync time (which may be long), if the Cinder process goes down for any reason, we will end up with a volume that has two copies, which wastes resources."
794,1278104,nova,423f344c59da1eecd7ab8bbb905abc1ff5948b14,1,0,Docker version bug,Docker cannot start a new instance because of an i...,"Cannot create new instances because of an internal error. It seems like that the docker client returns None instead of a empty array if not a single container is started.
Update:
The root cause for the issue is that the docker v1.8 rest API doesn't longer deliver the container list for v1.4 api calls. We must upgrade to >= 1.7 in order to make the docker driver working again.
Additional does docker not set the Content-Type correctly. The docker client expects that the Content-Type is application/json but it is plain/text. The parsing of the content will be skipped in this case.
Please see also: https://github.com/dotcloud/docker/pull/3974
Stacktrace:
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 821, in init_host
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup     self._destroy_evacuated_instances(context)
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 532, in _destroy_evacuated_instances
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup     local_instances = self._get_instances_on_driver(context, filters)
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 511, in _get_instances_on_driver
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup     driver_instances = self.driver.list_instances()
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/docker/driver.py"", line 96, in list_instances
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup     for container in self.docker.list_containers():
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup TypeError: 'NoneType' object is not iterable
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup"
795,1278149,nova,9bb9a77e41b009cb940e98feda666dd8d806c862,1,1,typo in code,Bug #1278149 “VMware,"Minesweeper CI is seeing the following Tempest suites fail due to errors during performing rescue operations.
tempest.api.compute.servers.test_server_rescue.ServerRescueTestJSON
tempest.api.compute.v3.servers.test_server_rescue.ServerRescueTestXML
tempest.api.compute.v3.servers.test_server_rescue.ServerRescueV3Test
Error message seen in the nova cpu log is:
Traceback (most recent call last):
  File ""/opt/stack/oslo.messaging/oslo/messaging/_executors/base.py"", line 36, in _dispatch
    incoming.reply(self.callback(incoming.ctxt, incoming.message))
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 134, in __call__
    return self._dispatch(endpoint, method, ctxt, args)
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 104, in _dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 356, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 240, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 226, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 291, in decorated_function
    function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 2731, in rescue_instance
    reason=_(""Driver Error: %s"") % unicode(e))
InstanceNotRescuable: Instance 3f317fe3-1777-4f0d-a60a-f2370d9b2fb0 cannot be rescued: Driver Error: can't set attribute
Full logs for a run that saw this error is available here: http://208.91.1.172//logs/nova/72125/1/"
796,1278271,neutron,26d8231b0bf4050453bcb6122484c801eff852ca,0,0,I think is not a bug. ,Bug #1278271 “SG rpc,In review https://review.openstack.org/#/c/63100 extra logging is done when no devices match the updated security group.
797,1278279,nova,b699c703e00eda1c8368b2470815b8cfc2fae2e4,1,1,bad attributes,hypervisor and near attributes do not exist in sch...,"The API samples of scheduler-hints contains ""hypervisor"" and ""near"" attributes like the following:
{
    ""server"" : {
        ""name"" : ""new-server-test"",
        ""image_ref"" : ""http://glance.openstack.example.com/openstack/images/70a599e0-31e7-49b7-b260-868f441e862b"",
        ""flavor_ref"" : ""http://openstack.example.com/openstack/flavors/1"",
        ""os-scheduler-hints:scheduler_hints"": {
            ""hypervisor"": ""xen"",
            ""near"": ""48e6a9f6-30af-47e0-bc04-acaed113bb4e""
        }
    }
}
However the attributes do not exist in the scheduler-hints parameter of ""create a server"" API."
798,1278291,nova,1fb1b058211f08c0b993372e734ed62cd9267193,1,1,typo getting the msg,log_handler miss  some log information,"log_handler:PublishErrorsHandler just emit the `msg` attribute of record. But many times we log with extra arguments, like ""LOG.debug('start %s', blabla)"", which will result in only show ""start %s"" in notification payload."
799,1278530,neutron,1c0235fb592d63e03506670e5690ab6c7ecab0a7,1,1,sending wrong id,Bug #1278530 “BigSwitch,"When deleting/updating a port on a shared network, the plugin is currently using the tenant ID of the port owner in the URI rather than the tenant ID of the network owner. This should always be the tenant ID of the network the port is being attached to."
800,1278581,neutron,8f0f6ca82fcbec153f992fd2347b6c4fd9a9eb25,1,1,It fails. can't reassociate floating ip,NSX plugin can't reassociate floating ip to differ...,"the NSX plugin is unable to associate a floating IP to a different internal address on the same port where it's currently associated.
How to reproduce:
- create a port with two IPs on the same subnet (just because the fip needs to go through the same router)
- associate a floating IP with IP #1
- associate the same floating IP with IP#2
- FAIL!
This happens with this new test being introduced in tempest: https://review.openstack.org/#/c/71251"
801,1278696,nova,f1b76971ba9f670b0ee916625318d65f18c3c4aa,1,1,wrong msg,'DescribeInstances' in ec2 shows wrong image-messa...,"'DescribeInstances' in ec2 shows the wrong image message('ami-00000002') to the instance booting from a volume:
n781dba539a84:~ # euca-describe-instances i-0000003a
RESERVATION     r-mdcwadip      c1c092e1c88f4027aeb203d50d63135b
INSTANCE        i-0000003a      ami-00000002            wjvm1   running None (c1c092e1c88f4027aeb203d50d63135b, n781dba57c996)  0               m1.small        2014-02-10T07:00:40.000Z        nova          20.20.16.12                      ebs
BLOCKDEVICE     /dev/vda        vol-0000000d            false
More info can be found here: http://paste.openstack.org/show/64087/
--------------
The codes in ec2utils.py:
def glance_id_to_id(context, glance_id):
    """"""Convert a glance id to an internal (db) id.""""""
    if glance_id is None:
        return
    try:
        return db.s3_image_get_by_uuid(context, glance_id)['id']
    except exception.NotFound:
        return db.s3_image_create(context, glance_id)['id']
------
The image_ref (as glance_id in this function) of the instance booting from a volume, will be """", not None.
The protected codes above can't take efforts."
802,1278991,neutron,d59d5a1c72c2324b857225680f5578d381a2ec73,0,0,Fix POSSIBLE deadlock,Bug #1278991 “NSX,The sync code that syncs the operational status to the db calls out to nsx within a db transaction which can cause deadlock if another request comes in and eventlet context switches it.
803,1279137,cinder,dfb1a090334d705425e739e8174a9c1c07009433,1,1,Bug in tests that didn’t show a bug in code,3par drivers can't delete a volume being copied,"When running the driver certification tests for Icehouse, we found a bug with the 3PAR FC/iSCSI drivers.
The tempest test calls cinder to create a clone of a volume, and then immediately calls delete on the clone.  The 3PAR won't allow you to do that unless you stop the copy first.   You get a very generic exception, that isn't helpful."
804,1279146,cinder,b5c4c5767fbfe9580e68343e722794326e6bb254,0,0,remove unused code,Removed unused context in _extend_snapshot method,"Input parameter 'context' of  _extend_snapshot method in cinder/api/extended_snapshot_attributes.py is not being used, so remove this 'context' parameter."
805,1279172,nova,53fe8696314fb73ca9943fce998d96fa6d0414b4,1,1,Bug encoding text,Unicode encoding error exists in extended Nova API...,"We have developed an extended Nova API, the API query disks at first, then add a disk to an instance.
After querying, if disk has non-english disk name, unicode will be converted to str in nova/api/openstack/wsgi.py line 451
""node = doc.createTextNode(str(data))"", then unicode encoding error exists."
806,1279189,nova,0b311bffc0f22f0b877d6bcad0c4cbcf1701dd54,1,1,Typo? Return instead of raise,Bug #1279189 “delete or query an not exist snapshot raise except... ,"In V2 API layer code, it would be better if we raise exception instead of return it
try:
            self.volume_api.delete_snapshot(context, id)
        except exception.NotFound:
            return exc.HTTPNotFound()  ----> use raise instead of return"
807,1279195,nova,f52a7f181ab8eb7b9e3e780183662fde8755cabd,0,0,Catch a new exception,get_spice_console should catch NotImplement except...,"in v2/v3 API layer
def get_spice_console(self, req, id, body):
didn't catch NotImplement exception and handle it"
808,1279300,nova,7fbb42296970ee30d21bbd66b144a2ab421aa17f,0,0,Give more info,detach boot device volume without warning,"If I use block_device_mapping_v2 api to set a backend volume as boot device for instance. The instance will be launched by this remote volume successfully.
But, in such case, we can also use detach api to force this boot device being detached. So, when we do it, the guestOS of this instance would be damaged, and the whole system would not work normally if do a I/O operation. It seems can not resume.
I think we should give a warning for user at least or even forbiden this operation."
809,1279317,nova,6def27b434a53aa8c1aca62640a6c072cc21209e,0,0,imrpove tests,Bug #1279317 “xen,"gkotton@ubuntu:~/nova$ tox nova.tests.virt.xenapi.test_xenapi
py26 create: /home/gkotton/nova/.tox/py26
ERROR: InterpreterNotFound: python2.6
py27 develop-inst-nodeps: /home/gkotton/nova
py27 runtests: commands[0] | python -m nova.openstack.common.lockutils python setup.py test --slowest --testr-args=nova.tests.virt.xenapi.test_xenapi
[pbr] Excluding argparse: Python 2.6 only dependency
running test
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests --list
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests  --load-list /tmp/tmp349WwU
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests  --load-list /tmp/tmpccaidD
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests  --load-list /tmp/tmpMvMcYb
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests  --load-list /tmp/tmpzJ2QHG
Ran 201 (+199) tests in 16.446s (+0.396s)
PASSED (id=350)
Slowest Tests
Test id                                                                                                                  Runtime (s)
-----------------------------------------------------------------------------------------------------------------------  -----------
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_agent_upgrade_fails_silently                              16.223
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_fails_agent_not_implemented                               16.222
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_fails_with_agent_bad_return                               16.034
nova.tests.virt.xenapi.test_xenapi.XenAPIAggregateTestCase.test_add_aggregate_host_raise_err                              1.116
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_maintenance_mode                                                 0.297
nova.tests.virt.xenapi.test_xenapi.XenAPIMigrateInstance.test_migrate_disk_and_power_off                                  0.277
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_vlanmanager                                                0.195
nova.tests.virt.xenapi.test_xenapi.XenAPIAggregateTestCase.test_remote_master_non_empty_pool                              0.179
nova.tests.virt.xenapi.test_xenapi.XenAPIMigrateInstance.test_migrate_disk_and_power_off_with_zero_gb_old_and_new_works   0.179
nova.tests.virt.xenapi.test_xenapi.XenAPIDom0IptablesFirewallTestCase.test_static_filters                                 0.171
py33 create: /home/gkotton/nova/.tox/py33
ERROR: InterpreterNotFound: python3.3"
811,1279551,neutron,464c307c68fca0353fb7d65acf650001c8de3f73,0,0,Evolution change (bug?),Bug #1279551 “NSX,NSX: newly created ports status should be DOWN
812,1279572,nova,30f73de399d9490f8f23d493c3cc20813ef71481,1,1,,DBError in nova-compute with baremetal driver afte...,"As of the following commit: https://github.com/openstack/nova/commit/8a7b95dccdbe449d5235868781b30edebd34bacd our nova-compute service on the seed node is throwing DBErrors.  If I reset my Nova tree to the previous commit and downgrade the database to 232 then I am able to use nova successfully again.  With that commit all boots fail with a No hosts found message, presumably related to the following messages in the nova-compute log:
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/eventlet/hubs/hub.py"", line 187, in switch
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/eventlet/greenthread.py"", line 194, in main
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/service.py"", line 480, in run_service
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     service.start()
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/service.py"", line 193, in start
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     self.manager.pre_start_hook()
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/manager.py"", line 835, in pre_start_hook
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     self.update_available_resource(nova.context.get_admin_context())
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/manager.py"", line 5121, in update_available_resource
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     rt.update_available_resource(context)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/lockutils.py"", line 249, in inner
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     return f(*args, **kwargs)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/resource_tracker.py"", line 353, in update_available_resource
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     self._sync_compute_node(context, resources)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/resource_tracker.py"", line 384, in _sync_compute_node
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     self._update(context, resources)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/resource_tracker.py"", line 456, in _update
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     context, self.compute_node, values)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/conductor/api.py"", line 241, in compute_node_update
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     return self._manager.compute_node_update(context, node, values)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/conductor/rpcapi.py"", line 364, in compute_node_update
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     prune_stats=prune_stats)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/rpc/client.py"", line 150, in call
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     wait_for_reply=True, timeout=timeout)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/transport.py"", line 87, in _send
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     timeout=timeout)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 390, in send
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     return self._send(target, ctxt, message, wait_for_reply, timeout)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 383, in _send
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     raise result
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup RemoteError: Remote error: DBError (ProgrammingError) (1064, 'You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near \': ""\'amd64\'"", u\'baremetal_driver\': ""\'nova.virt.baremetal.pxe.PXE\'""} WHERE compute\' at line 1') 'UPDATE compute_nodes SET updated_at=%s, vcpus_used=%s, memory_mb_used=%s, local_gb_used=%s, free_ram_mb=%s, free_disk_gb=%s, current_workload=%s, running_vms=%s, stats=%s WHERE compute_nodes.id = %s' (datetime.datetime(2014, 2, 12, 23, 8, 23, 932601), 0, 0, 0, 4096, 20, 0, 0, {u'cpu_arch': u'amd64', u'baremetal_driver': u'nova.virt.baremetal.pxe.PXE'}, 1L)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup [u'Traceback (most recent call last):\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/_executors/base.py"", line 36, in _dispatch\n    incoming.reply(self.callback(incoming.ctxt, incoming.message))\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in __call__\n    return self._dispatch(endpoint, method, ctxt, args)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 104, in _dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/conductor/manager.py"", line 458, in compute_node_update\n    result = self.db.compute_node_update(context, node[\'id\'], values)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/api.py"", line 228, in compute_node_update\n    return IMPL.compute_node_update(context, compute_id, values)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 110, in wrapper\n    return f(*args, **kwargs)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 166, in wrapped\n    return f(*args, **kwargs)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 614, in compute_node_update\n    compute_ref.update(values)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 447, in __exit__\n    self.rollback()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 58, in __exit__\n    compat.reraise(exc_type, exc_value, exc_tb)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 444, in __exit__\n    self.commit()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 354, in commit\n    self._prepare_impl()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalche
Feb 12 23:08:24 localhost nova-compute: my/orm/session.py"", line 334, in _prepare_impl\n    self.session.flush()\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/db/sqlalchemy/session.py"", line 616, in _wrap\n    raise exception.DBError(e)\n', u'DBError: (ProgrammingError) (1064, \'You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near \\\': ""\\\'amd64\\\'"", u\\\'baremetal_driver\\\': ""\\\'nova.virt.baremetal.pxe.PXE\\\'""} WHERE compute\\\' at line 1\') \'UPDATE compute_nodes SET updated_at=%s, vcpus_used=%s, memory_mb_used=%s, local_gb_used=%s, free_ram_mb=%s, free_disk_gb=%s, current_workload=%s, running_vms=%s, stats=%s WHERE compute_nodes.id = %s\' (datetime.datetime(2014, 2, 12, 23, 8, 23, 932601), 0, 0, 0, 4096, 20, 0, 0, {u\'cpu_arch\': u\'amd64\', u\'baremetal_driver\': u\'nova.virt.baremetal.pxe.PXE\'}, 1L)\n']."
813,1279611,neutron,a060b6c0b7d26b02ac2ca15ede49fa56a026efda,1,0,ulrparse not compatible with python3,"Bug #1279611 "" urlparse is incompatible for python 3” ","import urlparse
should be changed to :
import six.moves.urllib.parse as urlparse
for python3 compatible."
814,1279703,cinder,8bd9b9a63fd104ada41ae1e449e83da337068157,1,1,bugs in log messages…? I dont think it’s a bug.. but is typo in code,fixup ceph backup driver log messages,"This is to fix the log messges as per Jay's comment in https://review.openstack.org/#/c/51900/ - ""I think it is important that we be doing all LOG.* messages in a sentence format that ends with a '.' . This makes it easier to read through the messages when debugging."""
815,1279747,cinder,09b20b1b8493bef07cfc1caa8c345eba7ae33062,1,1,typo in code,Wrong example of “nova_endpoint_template,"Wrong example of ""nova_endpoint_template"" in cinder.conf
e.g. http://localhost:8774/v2/%(tenant_id)s
should be
e.g. http://localhost:8774/v2/%(project_id)s"
816,1279753,glance,31bcc97214358482e9ccc035189ef70ced1afc21,0,0,tests,The race condition caused by image created_at brea...,"For now there are two cases I got:
1. glance.tests.unit.v2.test_registry_api.TestRegistryRPC.test_get_index_sort_status_desc
http://logs.openstack.org/79/67079/3/gate/gate-glance-python27/2b0dea1/console.html#_2014-02-13_06_27_19_371
2. glance.tests.unit.v2.test_registry_api.TestRegistryRPC.test_get_index_sort_default_created_at_desc
http://logs.openstack.org/79/67079/3/gate/gate-glance-python27/348ad19/console.html#_2014-02-11_18_42_13_276
It is the same reason as https://bugs.launchpad.net/glance/+bug/1272136 fixed."
817,1279769,neutron,9af846caf7f4d00eddb2d839c032b909eb79d403,0,0,Not a bug now: ‘It may cause ‘,duplicated config-option registering,"Some config options(interface_driver, use_namespaces, periodic_interval) are defined in multiple sources in ad-hoc way.
It may cause DuplicateOptError exception when using those module at the same time.
Right now the exception is avoided in ad-hoc way by each executables.
Those definition/registering should be consolidated.
This is the blocker for BP of l3 agent consolidation.
https://blueprints.launchpad.net/neutron/+spec/l3-agent-consolidation"
818,1279813,neutron,3c0025abf4581dc3561637e802de7bba1434d2b3,0,0,'I don't see any issues due to this but it is better to be fixed.’,excutils.save_and_reraise_exception should be used...,"excutils.save_and_reraise_exception should be used when reraising an exception, as described in openstack.common.excutils.
I don't see any issues due to this but it is better to be fixed."
819,1279897,cinder,0c6c57d70826b36db30795fa993cf677a4c2a6e4,0,0,'may return ‘,HP LeftHand CLIQ proxy may return incorrect capaci...,"The HP Lefthand CLIQ proxy may return incorrect total_capacity_gb, and free_capacity_gb values if there are more than one cluster configured in the management group. The call to getCluster in _update_backend_status should include the cluster name."
820,1280033,cinder,24755ff21f0ee7643ba1cdeb339b260cde5683e9,0,0,‘we don't need this module now ‘,Remove dependent module py3kcompat,"Everything in module py3kcompat is ready in six > 1.4.0, we don't need this module now . It was removed from oslo-incubator recently, see  https://review.openstack.org/#/c/71591/.  This make us don't need maintain this module any more, use six directly."
821,1280035,neutron,fcad26e394cd6021aab2c94f1179533cc7866f8c,0,0,tests,Intermittent unit tests failures in NSX plugin,"Observed here: http://logs.openstack.org/92/71692/12/check/gate-neutron-python26/496b1b1/console.html
This behaviour might have been introduced by the patch for introducing a mapping between neutron and nsx uuids for networks."
822,1280055,nova,1a8dd2487d95327b97ebf8e8b4a27420d1432fd4,0,0,Remove duplicate code,Replace nova.db.sqlalchemy.utils with openstack.co...,"Most of the code in nova.db.sqlalchemy.utils is also in oslo-incubator.openstack.common.db.sqlalchemy.utils, except for the modify_indexes method which is not actually even used in the nova db migration code anymore now that it's been compacted in icehouse.
Also, the oslo.db code has been getting synced over to nova more frequently lately so rather than keep all of this duplicate code around we should move nova to using the oslo utils code and drop the internal nova one, with maybe moving the modify_indexes method to oslo first, then sync back to nova and then drop nova.db.sqlalchemy.utils from nova.
We will have to make sure that there are no behavior differences in the oslo code such that it would change the nova db schema, but we should be able to use Dan Prince's nova/tools/db/schema_diff.py script to validate that."
823,1280067,nova,d72d925fc1c3b8620b81531ec74524aceb89d4e8,1,1,Not catched exception,Missing catch InstanceNotFound that raise from Ins...,"As the comment in https://review.openstack.org/#/c/70901/6/nova/api/openstack/compute/plugins/v3/pause_server.py
After apply instance look helper, some compute api will invoke instance.save(), it will raise InstanceNotFound also. We should catch them."
825,1280132,nova,5856c3e585891103767eda87035dee8ecaee32ab,1,1,Option format not correctly set,--ephemeral option's format was not correctly set ...,"novaclient has following option
--ephemeral size=<size>[,format=<format>]
                        Create and attach a local ephemeral block device of
                        <size> GB and format it to <format>.
so
nova boot --flavor 21 --key_name mykey --image 43ca519b-979b-4803-95ad-b9f160f1a337 --security_group default  --ephemeral size=1 --ephemeral size=2,format=ext4 test12
should work
however, the eph disk created is ext3 ,ignore the option specified by format"
826,1280140,nova,d835163759527a651f3c4f2109ca0fdc3e968d37,1,1,Fix instance not found,cleanup_running_deleted_instances peroidic task fa...,"this is because the db  query is not including the deleted instance while
_delete_instance_files() in libvirt driver.
I can reproduce this bug both in master and stable havana.
reproduce steps:
1. create an instance
2. stop nova-compute
3. wait for nova-manage serivce list display xxx of nova-compute
4. modify the running_deleted_instance_poll_interval=60
running_deleted_instance_action = reap,
and start nova-compute and wait for this clean up peroidic task
5. a warnning will be given in the compute log:
2014-02-14 16:22:25.917 WARNING nova.compute.manager [-] [instance: c32db267-21a0-41e7-9d50-931d8396d8cb] Periodic cleanup failed to delete instance: Instance c32db267-21a0-41e7-9d50-931d8396d8cb could not be found.
the debug trace is:
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1006)cleanup()
   1005 block_device_mapping = driver.block_device_info_get_mapping(
-> 1006 block_device_info)
   1007 for vol in block_device_mapping:
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1007)cleanup()
   1006 block_device_info)
-> 1007 for vol in block_device_mapping:
   1008 connection_info = vol['connection_info']
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1041)cleanup()
   1040
-> 1041 if destroy_disks:
   1042 self._delete_instance_files(instance)
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1042)cleanup()
   1041 if destroy_disks:
-> 1042 self._delete_instance_files(instance)
   1043
ipdb> s
--Call--
> /opt/stack/nova/nova/virt/libvirt/driver.py(4950)_delete_instance_files()
   4949
-> 4950 def _delete_instance_files(self, instance):
   4951 # NOTE(mikal): a shim to handle this file not using instance objects
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(4953)_delete_instance_files()
   4952 # everywhere. Remove this when that conversion happens.
-> 4953 context = nova_context.get_admin_context()
   4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(4954)_delete_instance_files()
   4953 context = nova_context.get_admin_context()
-> 4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
   4955
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/virt/libvirt/driver.py(4954)_delete_instance_files()
   4953 context = nova_context.get_admin_context()
-> 4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
   4955
ipdb> n
--Return--
None
> /opt/stack/nova/nova/virt/libvirt/driver.py(4954)_delete_instance_files()
   4953 context = nova_context.get_admin_context()
-> 4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
   4955
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/virt/libvirt/driver.py(1042)cleanup()
   1041 if destroy_disks:
-> 1042 self._delete_instance_files(instance)
   1043
ipdb> n
--Return--
None
> /opt/stack/nova/nova/virt/libvirt/driver.py(1042)cleanup()
   1041 if destroy_disks:
-> 1042 self._delete_instance_files(instance)
   1043
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/virt/libvirt/driver.py(931)destroy()
    930 self.cleanup(context, instance, network_info, block_device_info,
--> 931 destroy_disks)
    932
ipdb> n
--Return--
None
> /opt/stack/nova/nova/virt/libvirt/driver.py(931)destroy()
    930 self.cleanup(context, instance, network_info, block_device_info,
--> 931 destroy_disks)
    932
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/compute/manager.py(1905)_shutdown_instance()
   1904 self.driver.destroy(context, instance, network_info,
-> 1905 block_device_info)
   1906 except exception.InstancePowerOffFailure:
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1906)_shutdown_instance()
   1905 block_device_info)
-> 1906 except exception.InstancePowerOffFailure:
   1907 # if the instance can't power off, don't release the ip
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1910)_shutdown_instance()
   1909 pass
-> 1910 except Exception:
   1911 with excutils.save_and_reraise_exception():
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1911)_shutdown_instance()
   1910 except Exception:
-> 1911 with excutils.save_and_reraise_exception():
   1912 # deallocate ip and fail without proceeding to
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1914)_shutdown_instance()
   1913 # volume api calls, preserving current behavior
-> 1914 self._try_deallocate_network(context, instance,
   1915 requested_networks)
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1915)_shutdown_instance()
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
ipdb> n
2014-02-14 16:22:02.701 DEBUG nova.compute.manager [-] [instance: c32db267-21a0-41e7-9d50-931d8396d8cb] Deallocating network for instance from (pid=19192) _deallocate_network /opt/stack/nova/nova/compute/manager.py:1531
2014-02-14 16:22:02.704 DEBUG oslo.messaging._drivers.amqpdriver [-] MSG_ID is e529a4edb22b480cb0641a62718e9b04 from (pid=19192) _send /opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py:358
2014-02-14 16:22:02.705 DEBUG oslo.messaging._drivers.amqp [-] UNIQUE_ID is aed682f94730441aaa14e43a37c86227. from (pid=19192) _add_unique_id /opt/stack/oslo.messaging/oslo/messaging/_drivers/amqp.py:333
2014-02-14 16:22:02.718 WARNING nova.openstack.common.loopingcall [-] task run outlasted interval by 11.632922 sec
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/compute/manager.py(1915)_shutdown_instance()
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
ipdb> n
--Return--
None
> /opt/stack/nova/nova/compute/manager.py(1915)_shutdown_instance()
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
ipdb> l
   1910 except Exception:
   1911 with excutils.save_and_reraise_exception():
   1912 # deallocate ip and fail without proceeding to
   1913 # volume api calls, preserving current behavior
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
   1917 self._try_deallocate_network(context, instance, requested_networks)
   1918
   1919 for bdm in vol_bdms:
   1920 try:
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/compute/manager.py(5225)_cleanup_running_deleted_instances()
   5224 self._shutdown_instance(context, instance, bdms,
-> 5225 notify=False)
   5226 self._cleanup_volumes(context, instance['uuid'], bdms)
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(5227)_cleanup_running_deleted_instances()
   5226 self._cleanup_volumes(context, instance['uuid'], bdms)
-> 5227 except Exception as e:
   5228 LOG.warning(_(""Periodic cleanup failed to delete ""
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(5228)_cleanup_running_deleted_instances()
   5227 except Exception as e:
-> 5228 LOG.warning(_(""Periodic cleanup failed to delete ""
   5229 ""instance: %s""),
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(5230)_cleanup_running_deleted_instances()
   5229 ""instance: %s""),
-> 5230 unicode(e), instance=instance)
   5231 else:
ipdb> n
2014-02-14 16:22:25.917 WARNING nova.compute.manager [-] [instance: c32db267-21a0-41e7-9d50-931d8396d8cb] Periodic cleanup failed to delete instance: Instance c32db267-21a0-41e7-9d50-931d8396d8cb could not be found."
827,1280363,nova,80ffc9b8b38293c05c9d48cb615b24fd7255a850,0,0,Replace exceptions (refactoring),Replace exception re-raises with excutils.save_and...,A few methods of the Hyper-V driver ops classes use a catch / raise pattern that replaces original exceptions with specific domain exceptions inherited from vmutils.HyperVUtilsException(). The original exception is thus masqueraded and trackable only by analysing the logs.
828,1280379,nova,c1a64c94ef848f825e6a36581814511a81945428,0,0,tests,VolumeOpsTestCase class is incorrectly inheriting ...,"The VolumeOpsTestCase class in test_hypervapi.py inherits from HyperVAPITestCase, resulting in all the tests from the base class being executed twice.
The patch that introduced the VolumeOpsTestCase is here:
https://github.com/openstack/nova/commit/d143540ad1b69ec93c2b7bfadd1f654c4d8c7a34"
829,1280409,cinder,9e858bebb89de05b1c9ecc27f5bd9fbff95a728e,1,1,,Cinder Eqlx driver fails to SSHInjectionThreat,"Cinder's Equallogic driver fails to create any volumes because /usr/lib/python2.6/site-packages/cinder/utils.py check_ssh_injection treats spaces as bad things. Tested with latest RDO Havana release.
2014-02-14 14:16:35.386 12786 ERROR cinder.openstack.common.rpc.common [req-b7476613-4e55-4407-be61-738081c20040 d9ac62582e7f4d4ab19a8df75bc8c06d bdf89879d78f4d278e8aad9f88cfb92e] ['Traceback (most recent call last):\n', '  File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data\n    **args)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch\n    return getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 819, in wrapper\n    return func(self, *args, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 624, in initialize_connection\n    conn_info = self.driver.initialize_connection(volume, connector)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 406, in initialize_connection\n    volume[\'name\'])\n', '  File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__\n    self.gen.next()\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 397, in initialize_connection\n    self._eql_execute(*cmd)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 219, in _eql_execute\n    args, attempts=self.configuration.eqlx_cli_max_retries)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 177, in _run_ssh\n    utils.check_ssh_injection(cmd_list)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 166, in check_ssh_injection\n    raise exception.SSHInjectionThreat(command=str(cmd_list))\n', ""SSHInjectionThreat: SSH command injection detected: ('volume', 'select', u'volume-3045438e-096a-4838-a769-ec39692fa41f', 'access', 'create', 'initiator', u'iqn.1994-05.com.redhat:249bde2d589', 'authmethod chap', 'username', 'cinder')\n""]
2014-02-14 14:16:38.308 12786 INFO cinder.volume.manager [req-2d51da59-6aba-4212-a9a2-61864fe18cf3 None None] Updating volume status
2014-02-14 14:16:39.470 12786 ERROR cinder.volume.drivers.eqlx [req-a73e9c7c-e8ed-4dd1-a9f4-f6897b52f996 d9ac62582e7f4d4ab19a8df75bc8c06d bdf89879d78f4d278e8aad9f88cfb92e] Failed to initialize connection to volume volume-3045438e-096a-4838-a769-ec39692fa41f
2014-02-14 14:16:39.470 12786 ERROR cinder.openstack.common.rpc.amqp [req-a73e9c7c-e8ed-4dd1-a9f4-f6897b52f996 d9ac62582e7f4d4ab19a8df75bc8c06d bdf89879d78f4d278e8aad9f88cfb92e] Exception during message handling
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 819, in wrapper
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 624, in initialize_connection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     conn_info = self.driver.initialize_connection(volume, connector)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 406, in initialize_connection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     volume['name'])
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 397, in initialize_connection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     self._eql_execute(*cmd)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 219, in _eql_execute
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     args, attempts=self.configuration.eqlx_cli_max_retries)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 177, in _run_ssh
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     utils.check_ssh_injection(cmd_list)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 166, in check_ssh_injection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     raise exception.SSHInjectionThreat(command=str(cmd_list))
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp SSHInjectionThreat: SSH command injection detected: ('volume', 'select', u'volume-3045438e-096a-4838-a769-ec39692fa41f', 'access', 'create', 'initiator', u'iqn.1994-05.com.redhat:249bde2d589', 'authmethod chap', 'username', 'cinder')
Reason and fix for this is simple:
Line 395 @ /usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py:
                cmd.extend(['authmethod chap', 'username',
When it should be (to pass the current requirements at /usr/lib/python2.6/site-packages/cinder/utils.py's check_ssh_injection):
                cmd.extend(['authmethod', 'chap', 'username',"
830,1280522,glance,8d4312a2c9b020cec1241fde0b9a1fb29af5e6f6,0,0,tests,"Replace assertEqual(None, *) with assertIsNone in ...","Replace assertEqual(None, *) with assertIsNone in tests to have
more clear messages in case of failure."
831,1280532,nova,fa72e49845179e29cf918e0742ef735ae868bb4c,1,1,,Detach volume fails with “Unexpected KeyError,"Detach volume fails with ""Unexpected KeyError"" in EC2 interface when I detach a ""attaching"" status volume.
The volume with ""attaching"" status don't contain a property""instance_uuid"", a KeyError will be raised at the following function.
    def _get_instance_from_volume(self, context, volume):
        if volume['instance_uuid']:
            ......
Attaching volume dict:
{
 'status': u'attaching',
 'volume_type_id': u'None',
 'display_name': None,
 'attach_time': '',
 'availability_zone': u'nova',
 'created_at': u'2014-02-13T16: 50: 53.620080',
 'attach_status': 'detached',
 'display_description': None,
 'volume_metadata': {
 },
 'snapshot_id': None,
 'mountpoint': '',
 'id': u'99d118ee-3666-4983-8825-f8c096bccbd1',
 'size': 1
}"
832,1280572,nova,e5cd9d90062d778b7725de0c76d44d6077e8e237,1,1,bad return status,[EC2] Attaching volume returns volume is 'detached...,"While attaching the volume, the API response is that the volume state is detached, while it should be 'attaching' as per EC2's API docs.
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-AttachVolume.html"
833,1280597,neutron,854a5ac0bec932130eac0ac1ca1e679cc6487c93,0,0,typo in commentary,Fix typo in service_drivers.ipsec,"neutron.services.vpn.service_drivers.ipsec has typo in its comment.
""""""Retuns the vpnservices on the host.""""""  - should be  """"""Returns the vpnservices on the host."""""""
834,1280826,nova,88b7380d0e7c398780d2bb20abd1936e7c879665,1,1,failure with msgs,config generator fails when project enables lazy m...,"When lazy message translation is enabled in Nova, the check_update.sh calls generate_sample.sh, which uses a copy of oslo's config/generator.py which produces the following message:
CRITICAL nova [-] TypeError: Message objects do not support addition.
The config/generator.py module installs i18n without lazy enabled (named parameter 'lazy' not specified):
gettextutils.install('nova')
To gather information about the projects options, it loads the project modules looking for entry points.   When these modules are loaded, they may contain code to enable lazy.   In the case of Nova this is the nova/cmds/__init__.py which calls:
gettextutils.enable_lazy()
This means that the messages returned with information for the entry points are lazy enabled.  Thus when config/generator.py tries to work with the help message for the option associated with the Nova modules:
opt_help += ' (' + OPT_TYPES[opt_type] + ')'
it fails because opt_help is a gettextutils.Message instance, which doesn't support addition."
835,1280827,neutron,3d24fe5710cbea6d7d1f88c3476f4a856347ab5e,0,0,No bug but regex parsing is tedious and error prone,Regex parsing in get_vif_port_by_id is tedious and...,"ovs-vsctl has a switch for returning json output, which is arguably better for machine processing, and therefore more suitable for neutron's OVS agent.
Indeed get_vif_port_set in neutron/agent/linux/ovs_lib.py already uses json output.
However, get_vif_port_by_id performs match on the text output from ovs_vsctl.
While it is true that regular expressions are extremely powerful, apparently trivial errors in the regular expression itself can lead to errors like [1].
In [1] the evaluation of a regular expression is failing because the name of a VIF was not wrapped in double quotes.
Even if the could be worth looking into ovs-vsctl output to understand in which cases VIF names are wrapped in quotes and in which not, it is probably better to just switch to JSON output as parsing JSON is easier and therefore less likely to cause errors.
It is also worth noting that the error in regex parsing [1] causes the VIF to not be wired triggering the same failure as bug 1253896.
[1]  http://logs.openstack.org/49/63449/16/experimental/check-tempest-dsvm-neutron-isolated/de067c3/logs/screen-q-agt.txt.gz#_2014-02-16_11_46_14_830"
836,1280964,cinder,6427b5506de72f0adf224d3643f6df9d4a465bdd,1,0,Incompatible with python3,cStringIO.StringIO is incompatible for python 3,"Import cStringIO
cStringIO.StringIO()
should be :
from six.moves import cStringIO
cStringIO.StringIO()=>cStringIO
For Python3 compatible."
837,1281083,neutron,abca726e405fec960b546319ea81295b0c6adb0c,1,1,bad validation code,Firewall policy update should validate rules as li...,"Firewall policy update should validate rules as list of uuids, otherwise malformed request will result in ""500 Internal server error""  returned to the client."
838,1281440,nova,eeb97eb16c1f56cecca83700ab28a594ed27ab1f,0,0,No bug. Should handle boolean string parameters,should handle boolean string parameters through cr...,"If specifying false string (""False"") as ""return_reservation_id"" parameter of create multiple servers API, Nova considers it as True.
On the other hand, nova can consider false string as false in the case of ""on_shared_storage"" parameter of evacuate API.
That behavior seems API inconsistency."
839,1281481,neutron,a98dc7680d4689f5ce5f602f9c5cb3bcc77de215,0,0,tests,test_openvswitch_plugin sometimes fail,"depending on the sequence of tests, test_openvswitch_plugin sometimes ends up to kick
unexpected code via impl_fake rpc backend.
an example of the failure:
http://logs.openstack.org/91/71791/5/check/gate-neutron-python27/2317e6a/"
840,1281574,neutron,a38d73d9449675374f45c44b35978735e53cdbaf,1,1,race condition,nec plugin should handle OFC port-deletion race co...,"There is a case where multiple OFC delete-port operations run in parallel. It is usually observed in tempest api tests:
ofc-delete-port triggered by delete-network API request and ofc-delete-port request from dhcp-agent (release_dhcp_port) triggered by delete-subnet.
There are several manifests I see,  however this kind of ""not found"" is a valid situation and should be ignored during deleting OFC port.
http://133.242.19.163:8000/neutron-ci-logs/Neutron_Gate/FAILURES/675/ (PortNotFound)
2014-01-28 08:12:37.449 32365 ERROR neutron.api.v2.resource [req-13a26437-c201-4fc6-8af4-dfbff6663f3b None] delete failed
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 356, in delete_network
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     port = self.deactivate_port(context, port)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 252, in deactivate_port
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     port_status)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 166, in _update_resource_status
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     obj_db = obj_getter(context, id)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 266, in _get_port
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     raise q_exc.PortNotFound(port_id=id)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource PortNotFound: Port b6d8480f-4c59-4095-bc72-bc7516fba1d7 could not be found
http://133.242.19.163:8000/neutron-ci-logs/Neutron_Gate/FAILURES/588/
2014-01-24 11:54:40.217 31910 DEBUG neutron.plugins.nec.common.ofc_client [-] OFC returns [202:] do_request /opt/stack/neutron/neutron/plugins/nec/common/ofc_client.py:85
2014-01-24 11:54:40.410 31910 DEBUG neutron.plugins.nec.common.ofc_client [-] OFC returns [404:] do_request /opt/stack/neutron/neutron/plugins/nec/common/ofc_client.py:85
2014-01-24 11:54:40.410 31910 WARNING neutron.plugins.nec.common.ofc_client [-] Operation on OFC failed: status=404, detail=
2014-01-24 11:54:40.411 31910 ERROR neutron.plugins.nec.nec_plugin [-] delete_ofc_port() failed due to An OFC exception has occurred: Operation on OFC failed
2014-01-24 11:54:40.523 31910 ERROR neutron.api.v2.resource [-] delete failed
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 362, in delete_network
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource     raise nexc.OFCException(reason=reason)
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource OFCException: An OFC exception has occurred: Failed to delete port(s)=f77286eb-8493-406c-8240-6e62e027c59d from OFC."
841,1281694,neutron,f50df8eb3c5bf38abbb0389b85c4b628cffb59f4,1,1,,Delete subnet fails if assoc port has IPs from ano...,"Perform the following:
- Create a network
- Create two subnets on the network
- Create a port on the network using a fixed IP from one of the subnets
- Delete the other subnet
= = = > FAILURE: Subnet delete fails because SUPPOSEDLY there
are port(s) still associated with that subnet.
Looking at delete_subnet() in neutron/db/db_base_plugin_v2.py,
the check for port(s) still being associated with that _subnet_
is actually checking for port(s) still being associated with
the _network_ (not the subnet), i.e. it's doing a:
    filter_by(network_id=subnet.network_id)
rather than a:
    filter_by(subnet_id=subnet['id'])"
842,1281772,neutron,084b7f62280241a9037b6f9e592c466135e5fec1,1,0,"It worked, but now it is a bug because of evolution",Bug #1281772 “NSX,"The NSX sync backend previously passed around a sqlalchemy model object
around which was nice because we did not need to query the database an
additional time to update the status of an object. Unfortinately, this
was add done within a db transaction which included a call to NSX which
could cause deadlock this needed to be removed."
843,1281789,neutron,d532864d832656adaca433792ed50c8ed71db40a,1,1,wrong id,Bug #1281789 “NSX,"2014-02-18 13:11:51.714 ERROR NeutronPlugin [-] Unable to update port id: 0bbbdf65-c654-4c30-b1b4-5d17b9b10ef7.
2014-02-18 13:11:51.714 TRACE NeutronPlugin Traceback (most recent call last):
2014-02-18 13:11:51.714 TRACE NeutronPlugin   File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 1316, in update_port
2014-02-18 13:11:51.714 TRACE NeutronPlugin     nvp_port_id)
2014-02-18 13:11:51.714 TRACE NeutronPlugin   File ""/opt/stack/neutron/neutron/plugins/nicira/nvplib.py"", line 1019, in get_port_status
2014-02-18 13:11:51.714 TRACE NeutronPlugin     port_id=port_id, net_id=lswitch_id)
2014-02-18 13:11:51.714 TRACE NeutronPlugin PortNotFoundOnNetwork: Port 0bbbdf65-c654-4c30-b1b4-5d17b9b10ef7 could not be found on network 0d192d2e-e64f-410e-8d3b-cb2eef5a2655
2014-02-18 13:11:51.714 TRACE NeutronPlugin"
844,1281904,nova,544745f9da3245a71d771cff26dc6d0255bb0470,1,1,Wrong exception type raised,Wrong exception type HTTPBadRequest is raised,"When user without admin permission wants to get a list of servers
which are in 'deleted' state, currently it raises HTTPBadRequest.
The code is:
 231         if search_opts.get(""vm_state"") == ['deleted']:
 232             if context.is_admin:
 233                 search_opts['deleted'] = True
 234             else:
 235                 msg = _(""Only administrators may list deleted instances"")
 236                 raise exc.HTTPBadRequest(explanation=msg)
This should be changed to HTTPForbidden exception."
845,1281916,neutron,b5d09ffe604a1f6d8272b773bb183e5f2bc3d0f3,1,1,Cannot distinguish IPv6,OpenStack cannot assign IPv6 address to instance v...,"Use dnsmasq as dhcp server, OpenStack deploy one instance, we found the deployed instance cannot get the targeted IPv6 address, but it can get the targeted IPv4 address.
At the earlier time, I found this issue when use Vmware vcenter driver, today I also found this issue is Linux env.
I use dnsmasq as dhcpv6 server, use 'tcpdump -i tapXXXX' to monitor the network data,  and in deployed instance, and run ""dhclient -6"",  the result of tcpdump as below:
  22:02:54.354287 IP6 fe80::f816:3eff:fe34:a80e.dhcpv6-client > ff02::1:2.dhcpv6-server: dhcp6 solicit
  22:02:54.354689 IP6 fe80::184d:82ff:fec4:d268.dhcpv6-server > fe80::f816:3eff:fe34:a80e.dhcpv6-client: dhcp6 advertise
  22:02:55.434954 IP6 fe80::f816:3eff:fe34:a80e.dhcpv6-client > ff02::1:2.dhcpv6-server: dhcp6 solicit
  22:02:55.435283 IP6 fe80::184d:82ff:fec4:d268.dhcpv6-server > fe80::f816:3eff:fe34:a80e.dhcpv6-client: dhcp6 advertise
  22:02:57.587164 IP6 fe80::f816:3eff:fe34:a80e.dhcpv6-client > ff02::1:2.dhcpv6-server: dhcp6 solicit
  22:02:57.587456 IP6 fe80::184d:82ff:fec4:d268.dhcpv6-server > fe80::f816:3eff:fe34:a80e.dhcpv6-client: dhcp6 advertise
  22:02:59.354082 IP6 fe80::184d:82ff:fec4:d268 > fe80::f816:3eff:fe34:a80e: ICMP6, neighbor solicitation, who has fe80::f816:3eff:fe34:a80e, length 32
  22:02:59.354922 IP6 fe80::f816:3eff:fe34:a80e > fe80::184d:82ff:fec4:d268: ICMP6, neighbor advertisement, tgt is fe80::f816:3eff:fe34:a80e, length 24
from dnsmasq log, I got ""no address available"" error.
The root cause is dnsmasq need to read host file, and distinguish MAC addresses from IPv6 addresses.
the current host file as below:
fa:16:3e:25:f4:31,host-2001-2011-0-f104--3.openstacklocal, 2001:2011:0:f104::3
We need to wrap the ipv6 address with '[]' to let dnsmasq can distinguish MAC addresses from IPv6 addresses."
846,1281921,neutron,47079b78b8d8414da9e876d31c6364d6c78389ab,1,1,typo in code,Fix class name typo in test_db_rpc_base,"There are typos in the following lines, as TestDhcpRpcCallbackMixin is misspelled.
test_db_rpc_base.py:24:class TestDhcpRpcCallackMixin(base.BaseTestCase):
test_db_rpc_base.py:27:        super(TestDhcpRpcCallackMixin, self).setUp()
test_db_rpc_base.py:39:        super(TestDhcpRpcCallackMixin, self).tearDown()"
847,1281936,nova,028217a0dc6cdc33a37cc303ab9ffb6633036cfe,1,1,Bug in a special case calling directly,Bug #1281936 “A v3 API  request of “GET /versions/,"When I request a v3 API request of ""GET /versions/:(id)"" occurs ""Unexpected API Error."".
--------------------
$ curl -i 'http://192.168.1.36:8774/v3/versions/123' -X GET -H ""X-Auth-Project-Id: demo"" -H ""User-Agent: python-novaclient"" -H ""Accept: application/json"" -H ""X-Auth-Token: <TOKEN>""
HTTP/1.1 500 Internal Server Error
Content-Length: 193
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-baf6f903-b3f1-4c80-9e59-93f1440609a4
Date: Wed, 19 Feb 2014 07:08:13 GMT
{""computeFault"": {""message"": ""Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.\n<type 'exceptions.TypeError'>"", ""code"": 500}}
--------------------
nova-api logged an error:
--------------------
2014-02-19 16:14:37.075 ERROR nova.api.openstack.extensions [req-282a97a5-8f42-4410-a8dd-ff97457d0241 admin demo] Unexpected exception in API method
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 472, in wrapped
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions TypeError: show() got an unexpected keyword argument 'id'
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions
--------------------"
848,1282084,cinder,38e192afecd0d08921d4fb4d7166fa156addb6a4,0,0,More pythonic,Use len instead of for loop to get the end index,"diff --git a/cinder/tests/test_backup_tsm.py b/cinder/tests/test_backup_tsm.py
index 37d528f..2548cbd 100644
--- a/cinder/tests/test_backup_tsm.py
+++ b/cinder/tests/test_backup_tsm.py
@@ -109,8 +109,7 @@ class TSMBackupSimulator:
             ret_msg += ('Total number of objects deleted:  1\n'
                         'Total number of objects failed:  0')
             retcode = 0
-            for idx, backup in enumerate(self._backup_list[path]):
-                index = idx
+            index = len(self._backup_list[path]) - 1
             del self._backup_list[path][index]
             if not len(self._backup_list[path]):
                 del self._backup_list[path]"
849,1282098,cinder,efd4ff9c2584de9acfbae432f889e222f5058ea1,0,0,unused function,Bug #1282098 “Remove unused function,"This function never used.
grep -r ""stub_out_rate_limiting"" *
cinder/tests/api/fakes.py:def stub_out_rate_limiting(stubs):"
850,1282303,neutron,326b85dd6154048b1671732d2d7df8deb8b99207,0,0,"'the only reason why I'm making this change is that I want to add
something to the __init__() class’",all plugins should call __init__ in db_base_plugin...,"Currently each plugin calls db.configure() within the plugin's __init__ class or defines an initialize() method that's sole job is to call this method. Instead we should just call the super method of the db_base_plugin so that it calls this for us automatically.
Note: the only reason why I'm making this change is that I want to add something to the __init__() class of the db_base_plugin that's needed for the nova-event-callback blueprint and adding it in the base class of init looks to be the best place."
851,1282352,neutron,2a0c679b00ecf8d047a4372bc38f1b2662348f48,1,1,"Bug, I think is not specific. It is a bug in the code",Neutron Tempest XML tests fail for Cisco N1KV plug...,"While running Neutron Tempest tests for Cisco N1KV plugin, XML tests fail with the following trace:
FAIL: tempest.api.network.test_networks.BulkNetworkOpsTestXML.test_bulk_create_delete_network[gate,smoke]
tags: worker-0
----------------------------------------------------------------------
Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
2014-02-13 11:08:50,454 Request: POST http://192.168.255.184:9696/v2.0/networks
2014-02-13 11:08:50,454 Request Headers: {'Content-Type': 'application/xml', 'Accept': 'application/xml', 'X-Auth-Token': '<Token omitted>'}
2014-02-13 11:08:50,454 Request Body: <?xml version=""1.0"" encoding=""UTF-8""?>
<networks xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""><network ><name >network--331846832</name></network><network ><name >network--859697792</name></network></networks>
2014-02-13 11:08:51,051 Response Status: 201
2014-02-13 11:08:51,051 Glance request id req-b2eb0771-614b-4edd-a4f8-df8c9f60e0dd
2014-02-13 11:08:51,051 Response Headers: {'content-length': '962', 'date': 'Thu, 13 Feb 2014 19:08:51 GMT', 'content-type': 'application/xml; charset=UTF-8', 'connection': 'close'}
2014-02-13 11:08:51,051 Response Body: <?xml version='1.0' encoding='UTF-8'?>
<networks xmlns=""http://openstack.org/quantum/api/v2.0"" xmlns:quantum=""http://openstack.org/quantum/api/v2.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""><network><status>ACTIVE</status><subnets quantum:type=""list"" /><name>network--331846832</name><admin_state_up quantum:type=""bool"">True</admin_state_up><tenant_id>770dbef985aa4f3583880717ea3ac943</tenant_id><n1kv:profile_id>1e82eb72-5514-4972-a76a-b1202e46c057</n1kv:profile_id><shared quantum:type=""bool"">False</shared><id>908124c9-f76c-4861-a875-21f9793d09f7</id></network><network><status>ACTIVE</status><subnets quantum:type=""list"" /><name>network--859697792</name><admin_state_up quantum:type=""bool"">True</admin_state_up><tenant_id>770dbef985aa4f3583880717ea3ac943</tenant_id><n1kv:profile_id>1e82eb72-5514-4972-a76a-b1202e46c057</n1kv:profile_id><shared quantum:type=""bool"">False</shared><id>50fff0cb-e592-4916-b7f2-1047c9174705</id></network></networks>
}}}
Traceback (most recent call last):
  File ""tempest/api/network/test_networks.py"", line 311, in test_bulk_create_delete_network
    resp, body = self.client.create_bulk_network(2, network_names)
  File ""tempest/services/network/network_client_base.py"", line 186, in create_bulk_network
    body = {'networks': self.deserialize_list(body)}
  File ""tempest/services/network/xml/network_client.py"", line 43, in deserialize_list
    return parse_array(etree.fromstring(body), self.PLURALS)
  File ""lxml.etree.pyx"", line 2754, in lxml.etree.fromstring (src/lxml/lxml.etree.c:54631)
  File ""parser.pxi"", line 1578, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:82748)
  File ""parser.pxi"", line 1457, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:81546)
  File ""parser.pxi"", line 965, in lxml.etree._BaseParser._parseDoc (src/lxml/lxml.etree.c:78216)
  File ""parser.pxi"", line 569, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:74472)
  File ""parser.pxi"", line 650, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:75363)
  File ""parser.pxi"", line 590, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:74696)
XMLSyntaxError: Namespace prefix n1kv on profile_id is not defined, line 2, column 211"
852,1282354,neutron,e8988fe72eaddb38282356fe92076fd9cd2897b1,0,0,No bug. ‘should’,Bug #1282354 “NSX,NSX: nicira_models should import model_base directly
853,1282366,neutron,2b16e91b3f0fe06153a3a890c159c52974a57b91,1,1,,NVP FWaaS occurs error when creating firewall with...,"VCNS related error should VcnsBadRequest other than BadRequest.
When creating a firewall without policy, it would report the following error:
Traceback (most recent call last):
  File ""neutron/api/v2/resource.py"", line 84, in resource
    result = method(request=request, **args)
  File ""neutron/api/v2/base.py"", line 411, in create
    obj = obj_creator(request.context, **kwargs)
  File ""neutron/plugins/nicira/NeutronServicePlugin.py"", line 893, in create_firewall
    self._vcns_update_firewall(context, fw, router_id)
  File ""neutron/plugins/nicira/NeutronServicePlugin.py"", line 842, in _vcns_update_firewall
    self.vcns_driver.update_firewall(context, edge_id, fw_with_rules)
  File ""neutron/plugins/nicira/vshield/edge_firewall_driver.py"", line 231, in update_firewall
    fw_req = self._convert_firewall(context, firewall)
  File ""neutron/plugins/nicira/vshield/edge_firewall_driver.py"", line 131, in _convert_firewall
    for rule in firewall['firewall_rule_list']:
TypeError: 'NoneType' object is not iterable
}}}"
854,1282394,neutron,0dd0b6b5ee75bd437b2a82f2e666a6c671b91514,0,0,No bug . ‘It is so annoying for debugging or log monitoring.’,Exceptions due to bad user requests should not be ...,"When a user requests non-existing resource or a bad parameter, neutron API module logs it as TRACE level (log.exception). It is so annoying for debugging or log monitoring. These kinds of events are usual behaviors and are not errors of neutron-server.
They should be logged as TRACE level. Errors mapped into 4xx HTTP response are categorized into this category."
855,1282400,glance,1c5797d53a53a990cef3d695f0752742fd213840,0,0,tests,The race condition caused by image created_at brea...,"FAIL: glance.tests.functional.db.test_registry.TestRegistryDriver.test_image_paginate
tags: worker-0
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""glance/tests/functional/db/base.py"", line 711, in test_image_paginate
    self.assertEqual(extra_uuids, [i['id'] for i in page])
  File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = ['0be9ce54-f56b-41d5-bef6-9ca75e846b59',
 'a75dc71b-df55-41e3-9218-f465b791b7da']
actual    = [u'a75dc71b-df55-41e3-9218-f465b791b7da',
 u'0be9ce54-f56b-41d5-bef6-9ca75e846b59']
http://logs.openstack.org/58/74858/1/check/gate-glance-python27/732468f/console.html#_2014-02-19_23_03_01_798"
856,1282401,neutron,2ea6eadfd574770f19c817f9451ab4e7ad6d77b9,0,0,tests,Bug #1282401 “API UT,"In neutron/tests/unit/test_api_v2_resource.py,
there are tests which expects HTTP success response but expect_errors=True is specified.
There are no direct negative impact now, but it is better to specify expect_errors=False (default value) to avoid a confusion for test authors."
857,1282423,nova,0192898e30035028b002f9aed3199be69a18efb3,1,1,,rebuild vm fail when using lvm type image,"I launch a vm with lvm-type image successfully, whose flavor includes ""OS-FLV-EXT-DATA:ephemeral"" flag, the size is 1G.
So, I can see two logical volumes have beed created by ""lvs"" command.
Then I call “nova rebuild $SERVERID $IMAGEID”, try to rebuild it with the same image. Error occur, the vm'state get to ERROR, the two volumes disapear sametime.
nova-compute's log is:
""""""
2014-02-18 19:54:39.290 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvs -o lv_size --noheadings --units b --nosuffix /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:39.626 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): dd bs=1048576 if=/dev/zero of=/dev/cinder-volumes/instance-0000007c_disk.local seek=0 count=1024 oflag=direct execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:47.622 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:48.061 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
2014-02-18 19:54:48.066 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] ['lvremove', '-f', '/dev/cinder-volumes/instance-0000007c_disk', '/dev/cinder-volumes/instance-0000007c_disk.local'] failed. Retrying. execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:184
2014-02-18 19:54:49.081 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:49.279 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
2014-02-18 19:54:49.286 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] ['lvremove', '-f', '/dev/cinder-volumes/instance-0000007c_disk', '/dev/cinder-volumes/instance-0000007c_disk.local'] failed. Retrying. execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:184
2014-02-18 19:54:50.222 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:50.359 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
2014-02-18 19:54:50.364 2368 ERROR nova.compute.manager [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Setting instance vm_state to ERROR
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Traceback (most recent call last):
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 4967, in _error_out_instance_on_exception
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     yield
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 2037, in rebuild_instance
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     block_device_info=block_device_info)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 835, in destroy
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     destroy_disks, context=context)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 935, in _cleanup
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     self._cleanup_lvm(instance)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 957, in _cleanup_lvm
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     libvirt_utils.remove_logical_volumes(*disks)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/utils.py"", line 392, in remove_logical_volumes
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     execute(*lvremove, attempts=3, run_as_root=True)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/utils.py"", line 50, in execute
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     return utils.execute(*args, **kwargs)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/utils.py"", line 177, in execute
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     return processutils.execute(*cmd, **kwargs)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py"", line 178, in execute
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     cmd=' '.join(cmd))
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] ProcessExecutionError: Unexpected error while running command.
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Command: lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Exit code: 5
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Stdout: ''
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Stderr: '  One or more specified logical volume(s) not found.\n'
""""""
In nova.virt.libvirt.utils.py:
""""""
def remove_logical_volumes(*paths):
    """"""Remove one or more logical volume.""""""
    for path in paths:
        clear_logical_volume(path)
    if paths:
        lvremove = ('lvremove', '-f') + paths
        execute(*lvremove, attempts=3, run_as_root=True)
""""""
It will try 3 times to remove two volumes (instance-0000007c_disk and instance-0000007c_disk.local).
Then I trace the problem in another example, when it happed again, pdb result show below.
The 1st try:
""""""
2014-02-18 22:33:48.579 24156 DEBUG nova.openstack.common.processutils [req-a97ce6d7-ee76-4349-9d89-7c2e7325f2d5 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
> /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py(173)execute()
-> if not ignore_exit_code and _returncode not in check_exit_code:
(Pdb) p cmd
['lvremove', '-f', '/dev/cinder-volumes/instance-0000007e_disk', '/dev/cinder-volumes/instance-0000007e_disk.local']
(Pdb) p result
('  Logical volume ""instance-0000007e_disk"" successfully removed\n', '  Can\'t remove open logical volume ""instance-0000007e_disk.local""\n')
(Pdb) c
...
""""""
It shows that remove ephemeral disk fail, but image disk success.
2nd try begin:
""""""
-> if not ignore_exit_code and _returncode not in check_exit_code:
(Pdb) p cmd
['lvremove', '-f', '/dev/cinder-volumes/instance-0000007e_disk', '/dev/cinder-volumes/instance-0000007e_disk.local']
(Pdb) p result
('  Logical volume ""instance-0000007e_disk.local"" successfully removed\n', '  One or more specified logical volume(s) not found.\n')
(Pdb) c
...
""""""
The ephemeral disk is remove in this time, but image disk have been removed in last step, so the whole command still fail.
3rd try:
""""""
-> if not ignore_exit_code and _returncode not in check_exit_code:
(Pdb) p cmd
['lvremove', '-f', '/dev/cinder-volumes/instance-0000007e_disk', '/dev/cinder-volumes/instance-0000007e_disk.local']
(Pdb) p result
('', '  One or more specified logical volume(s) not found.\n')
(Pdb)
""""""
Openstack think remove_logical_volumes fail, though two volumes have been removed in fact.
It seems that retry to remove open volume is useful, but if the situation that not all volumes were removed once time happen, the retry are useless, and remove_logical_volume must fail."
858,1282437,glance,eafa186b499eb8fd5d37a4c47e99156546e4cab8,0,0,"Refactor? Indices start in 0, not in 1 according to a rule.",Glance server PATCH operations should use 0-based ...,"PATCH indices on the Glance server currently use 1-based index for location entries. This goes against the JSON-pointer RFC (rfc6901) which requires array indices be 0-based.
The glance client should also be fixed to use 0-based indexing."
859,1282514,cinder,14903b4ca7d625b8e93fe431f854263bbe26e4bc,0,0,"No bug. In case of it, not BIC python3",python 3 only has  “__self__,"for code compatible with Python 3, we should use the ""__self__"" instead of ""im_self"".
for example :
cinder/volume/flows/common.py
def make_pretty_name(method):
    """"""Makes a pretty name for a function/method.""""""
    meth_pieces = [method.__name__]
    # If its an instance method attempt to tack on the class name
    if hasattr(method, 'im_self') and method.im_self is not None:
        try:
            meth_pieces.insert(0, method.im_self.__class__.__name__)
        except AttributeError:
            pass
    return ""."".join(meth_pieces)
For reference here(thanks Alex for adding this):
""Changed in version 2.6: For Python 3 forward-compatibility, im_func is also available as __func__, and im_self as __self__.""
http://docs.python.org/2/reference/datamodel.html"
860,1282662,neutron,e3e0401672c8745092d3f0d99686b2ca94c1eb58,1,1,,Bug #1282662 “l2-population with linuxbridge  ,"I run in multi node setup with ML2, L2-population and Linuxbridge MD, and vxlan TypeDriver.
I create a VM with ip 10.0.0.105 and mac 00:00:00:55:55:55
neighbouring table on network node :
# ip neigh show
10.0.0.105 dev vxlan-1001 lladdr 00:00:00:55:55:55 PERMANENT
I delete it.
neighbouring table on network node :
# ip neigh show
10.0.0.105 dev vxlan-1001  FAILED
I recreate a VM with the same ip/mac :
neighbouring table on network node :
# ip neigh show
10.0.0.105 dev vxlan-1001  FAILED
in q-agt.log, we can see that the ""ip neigh add"" command fails."
861,1282715,glance,18b4df178b7d708848cad9de7f9073f8fa7f20f9,0,0,No bug. ‘VMware datastore should use oslo.vmware common code ‘,VMware datastore should use oslo.vmware common cod...,The git repository oslo.vmware is now available: https://github.com/openstack/oslo.vmware/. It is time to consume it and get rid of the vmware folder in the glance folder.
862,1282754,neutron,f8ab9d4366c87724cba2e49e71dc72e2427d5a68,0,0,No bug. ‘Validate multicast ip range in Cisco N1kv’,Validate multicast ip range in Cisco N1kv Plugin,Updates network description and validates multicast ip range check
863,1282873,neutron,7ab7e70559733a97eb0c77442bfaae7e7e10b790,0,0,remove code,wsgi.run_server no longer used,wsgi.run_server no longer used
864,1282922,neutron,64acc3bd63846a6e7da8d1136f946372c698cb76,1,1,performance bug,Bug #1282922 “nec pluign,"(NEC Neutron third party CI blocking issue)
When multiple delete_port are run in parallel, nec plugin detects ResourceClosedError from sqlalchemy. The error itself can occur when a port row is deleted from ports table, but the problem is it occurs even after retrying db_plugin.delete_port by restarting a new transaction in plugin.delete_port().
At now it seems only nec plugin hits this problem, but potentially other plugins can hit this issue.
The case I observed is that delete-port from dhcp-agent (release_dhcp_port RPC call) and delete-port from delete-network API request are run in parallel. plugin.delete-port in nec plugin calls REST API call to an external controller in addition to operates on neutron database.
After my investigation and testing, db_plugin.delete_ports() calls plugin.delete_port() under a transaction.
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1367
This means the transaction continues over API calls to external controller and it leads to a long transaction.
When plugin.delete_ports() and plugin.delete_port() are run at the same time, even if plugin.delete_port() avoid long transaction, db operations in plugin.delete_port() is blocked and they can fail with timeout.
One example is : http://133.242.19.163:8000/neutron-ci-logs/Neutron_Master/917/logs/devstack/q-svc-filtered.txt.gz
2014-02-18 18:38:41.771 16025 ERROR neutron.api.v2.resource [-] delete failed
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/common/log.py"", line 43, in wrapper
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     data['exc'] = unicode(e)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/openstack/common/excutils.py"", line 68, in __exit__
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     six.reraise(self.type_, self.value, self.tb)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/common/log.py"", line 40, in wrapper
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     ret = method(*args, **kwargs)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 419, in delete_network
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     super(NECPluginV2, self).delete_network(context, id)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 968, in delete_network
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     for p in ports)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 967, in <genexpr>
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     only_auto_del = all(p['device_owner'] in AUTO_DELETE_PORT_OWNERS
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/loading.py"", line 65, in instances
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     fetch = cursor.fetchall()
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 752, in fetchall
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     self.cursor, self.context)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1027, in _handle_dbapi_exception
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     util.reraise(*exc_info)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 746, in fetchall
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     l = self.process_rows(self._fetchall_impl())
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 715, in _fetchall_impl
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     self._non_result()
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 720, in _non_result
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     ""This result object does not return rows. ""
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource ResourceClosedError: This result object does not return rows. It has been closed automatically.
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource
2014-02-18 18:38:41.772 16025 DEBUG neutron.wsgi [-] Data Request Failed: internal server error while processing your request. type is <type 'unicode'> _to_xml_node /opt/stack/neutron/neutron/wsgi.py:533
2014-02-18 18:38:41.772 16025 INFO neutron.wsgi [-] 10.56.45.201 - - [18/Feb/2014 18:38:41] ""DELETE /v2.0/networks/451384e4-a4f2-4558-8da0-40dc60f1a17f HTTP/1.1"" 500 517 103.278109"
865,1282925,neutron,5e4b0c6fc6670ea036d801ce53444272bc311929,0,0,no bug: ‘can lead to long transaction if ‘,Bug #1282925 “db_plugin.delete_ports() can lead to long transact... ,"db_plugin.delete_ports() can lead to long transaction if plugin.delete_port talks with external system.
it is observed first in nec plugin (bug 1282922), but it affects multiple plugins/drivers.
Note that it is about delete_ports and not about delete_port.
The detail is described in bug 1282922. Quoted from the original bug report.
----
The case I observed is that delete-port from dhcp-agent (release_dhcp_port RPC call) and delete-port from delete-network API request are run in parallel. plugin.delete-port in nec plugin calls REST API call to an external controller in addition to operates on neutron database.
After my investigation and testing, db_plugin.delete_ports() calls plugin.delete_port() under a transaction.
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1367
This means the transaction continues over API calls to external controller and it leads to a long transaction.
When plugin.delete_ports() and plugin.delete_port() are run at the same time, even if plugin.delete_port() avoid long transaction, db operations in plugin.delete_port() is blocked and they can fail with timeout.
----"
866,1282946,neutron,7ad82b95f611678f71fd500b6541e04ff8359d15,0,0,tests,Bug #1282946 “db_plugin.test_delete_port does not test port-dele... ,"test_delete_port in test_db_plugin does not test delete operation:
    def test_delete_port(self):
        with self.port() as port:
            req = self.new_show_request('port', self.fmt, port['port']['id'])
            res = req.get_response(self.api)
            self.assertEqual(res.status_int, webob.exc.HTTPNotFound.code)"
867,1283019,neutron,14cb886809e5cccbf799a0dc2e5b99f31b1ab3be,0,0,"NO bug. ‘No need to use ""is not"" when comparing values’",Bug #1283019 “nec plugin,"""is not"" operator compares two objects are identical.
When comparing values, ""!="" should be used."
868,1283080,glance,765b6c98e08577038d33c9a5d745bb75c6b9f06d,1,0,Evolution bug. ‘It seems this change is missing from Glance’,Bug #1283080 “KeyError,"Using all default setting for setting up devstack. Once devstack is done, g-reg and g-api console will show same error like this.
KeyError: 'user_identity'
18c2599c92e1db002 with project_id : a068a8a689554202999265989f44f448 and roles: admin  _build_user_headers /opt/stack/py
thon-keystoneclient/keystoneclient/middleware/auth_token.py:951
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 851, in emit
    msg = self.format(record)
  File ""/opt/stack/glance/glance/openstack/common/log.py"", line 684, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 724, in format
    return fmt.format(record)
  File ""/opt/stack/glance/glance/openstack/common/log.py"", line 648, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 467, in format
    s = self._fmt % record.__dict__
KeyError: 'user_identity'"
869,1283233,cinder,e31b9e78374bcaeae573cce2915897ceb870ffc3,1,1,,Bug #1283233 “3par,"Steps to reproduce:
1. Create a snapshot.
2. Manually delete it from 3PAR outside of cider
3. Delete the snapshot from cinder.
Result: Snapshot stuck in error deleting state
Expected Result: Snapshot removed from cinder."
870,1283250,neutron,ebf2763508535d9b3aaa6753139858b6fdd21f23,0,0,tests,remove pointless test TestN1kvNonDbTest,remove pointless test TestN1kvNonDbTest
871,1283338,cinder,379273925517e8b9bf971e018a1dc41f809c4814,1,1,unstable state,create snapshot success by error status volume,"I create a LVM volume with too large size, my cinder-volumes VG's free space is insufficient. so, the volume's status is ""error"".
But, when I try to create a snapshot by this error volume, it' ok, and the snapshot' status is ""available"", progress is ""100%"".
I find that  in cinder.volume.manager.py, create_shapshot, self.driver.create_snapshot return false, but only exception would be hanlde and set to error status.So, after that, the snapshot is ok in db.
some logs show that:
""""""
2014-02-22 10:55:07.439 29737 ERROR cinder.brick.local_dev.lvm [req-6d37df28-1cb6-444c-a1e9-26654905acb9 ca7015d9004a41528498c114e0a6ebf1 55a50a99a42441e585b7a5c34214ecef] Unable to find LV: volume-8cb6eb05-6bfa-4422-a683-2cceb4c4ff1f
2014-02-22 10:55:07.498 29737 INFO cinder.volume.manager [req-6d37df28-1cb6-444c-a1e9-26654905acb9 ca7015d9004a41528498c114e0a6ebf1 55a50a99a42441e585b7a5c34214ecef] snapshot 9e2e363b-a0ed-4be8-9874-ced1c8705fde: created successfully
"""""""
872,1283765,neutron,9bc29208bda6071a34bcc0da36a396eb8bab4f30,1,1,Bug. plugin changes port it shouldn't,ovs plugin changes port it shouldn't change,"(Using ML2 with ovs)
While playing with https://github.com/stackforge/cookbook-openstack-network/blob/master/files/default/neutron-ha-tool.py to migrate routers between L3 agents, I had the issue that the connectivity got lost.
After investigating, it turns out that interface for the port of the router that is connected to the external network (attached to br-public -- which is usually named br-ext) gets a tag. Manually removing the tag makes things work.
I'm attaching a bit of the log where the port_update message is received (it's received for the two interfaces of the router, so this needs some care when reading). We can see the following:
 Running command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--timeout=2', 'set', 'Port', 'qg-923f9b0e-aa', 'tag=2']
However, reading the code, it seems that this kind of actions should only be done for ports on br-int (because of ""vif_port = self.int_br.get_vif_port_by_id(port['id'])""). So this shouldn't be run for other ports."
873,1283872,cinder,330a476f8a82c672d636d7da78b81cc46db1e9dd,1,1,wrong msg,webob.exc.HTTPForbidden can't show correct message...,"In nova/api/ec2/__init__.py there are codes like:
154     def __call__(self, req):
155         access_key = str(req.params['AWSAccessKeyId'])
156         failures_key = ""authfailures-%s"" % access_key
157         failures = int(self.mc.get(failures_key) or 0)
158         if failures >= CONF.lockout_attempts:
159             detail = _(""Too many failed authentications."")
160             raise webob.exc.HTTPForbidden(detail=detail)
But webob.exc.HTTPForbidden should use the 'explanation' parameter to show the error message.
The source can be referred to
https://github.com/Pylons/webob/blob/master/webob/exc.py#L666"
874,1283876,nova,7eb42f18ce8574b28ecba16b478bcfc40dfa2005,1,1,Wrong message,webob.exc.HTTPUnprocessableEntity can't return cor...,"In nova/api/openstack/compute/contrib/services.py there are codes like:
190             if id == ""disable-log-reason"":
191                 reason = body['disabled_reason']
192                 if not self._is_valid_as_reason(reason):
193                     msg = _('The string containing the reason for disabling '
194                             'the service contains invalid characters or is '
195                             'too long.')
196                     raise webob.exc.HTTPUnprocessableEntity(detail=msg)
But HTTPUnprocessableEntity should use 'explanation' parameter to return the error message.
The source can be referred here:
https://github.com/Pylons/webob/blob/master/webob/exc.py#L885"
875,1283930,neutron,e95571a6e350338a1567234bc934f70f83e69d86,0,0,Improvement. Not a bug,"Bug #1283930 ""    Make metaplugin be used with a router service ... ","https://review.openstack.org/65034
commit c04785e0ced18ebab6bada1d3961c1394c541a69
Author: Itsuro Oda <email address hidden>
Date:   Mon Jan 6 15:03:14 2014 +0900
    Make metaplugin be used with a router service plugin
    ""l3_plugin_list"" configuration parameter of the metaplugin is permitted
    blank now.
    If ""l3_plugin_list"" is blank, router extension and extensions which extend
    the router extension don't be included in ""supported-extension-aliases"" of
    the metaplugin.
    This makes the metaplugin be able to be used with a router service plugin.
    Note that if ""l3_plugin_list"" is not blank, a router service plugin must
    not be specified, otherwise the error of the bug report still occurs.
    This patch removes some router extension related meaningless codes also.
    (e.g.  external-net extension belongs to L2 functionality and be handled
     by core plugins properly.)
    Closes-bug: 1266347
    DocImpact
    Change-Id: I0454bc0a4bd7eda5dad18b0538fb7baebe0b9f91"
876,1283987,nova,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,1,1,There is a deadlock. But also refactoring code.,Query Deadlock when creating >200 servers at once ...,"Query Deadlock when creating >200 servers at once in sqlalchemy.
--------
This bug occurred when I test this bug:
https://bugs.launchpad.net/nova/+bug/1270725
The original info is logged here:
http://paste.openstack.org/show/61534/
--------------
After checking the error-log, we can notice that the deadlock function is 'all()' in sqlalchemy framework.
Previously, we use '@retry_on_dead_lock' function to retry requests when deadlock occurs.
But it's only available for session deadlock(query/flush/execute). It doesn't cover some 'Query' actions in sqlalchemy.
So, we need to add the same protction for 'all()' in sqlalchemy."
877,1283990,neutron,3d9e183d596c806527008f6eb15edc9d249cb3c0,1,1,Small bug,Incorrect instantiation of MlnxException exception...,MlnxException exceptions expect a keyword argument with the err_msg key. This is not the case in the code: https://github.com/openstack/neutron/blob/8a70bfd97a6f27dcae41e0b895d84ce5c19238ad/neutron/plugins/mlnx/agent/eswitch_neutron_agent.py#L61
878,1284162,neutron,0049967a874a3dca7c6ab8e8e84ca7dea21dc0b2,1,1,,Cisco plugin fails test_port_list_filter_by_router...,"When the Cisco nexus plugin is configured on DevStack, the recently added tempest Neutron API test test_port_list_filter_by_router_id fails with the following error:
     ParseError: no element found: line 1, column 0
These failures occur for the following classes:
    NetworksIpV6TestXML
    NetworksTestXML"
879,1284227,cinder,115b6276ac3dd7b3e8c6a42e9d0d80709791b7a2,0,0,tests,race condition in test_delete_backup,"The test_delete_backup unit test fails sporadically as such:
2014-02-19 06:55:33.645 | Traceback (most recent call last):
2014-02-19 06:55:33.645 |   File ""cinder/tests/test_backup.py"", line 360, in test_delete_backup
2014-02-19 06:55:33.645 |     self.assertGreater(timeutils.utcnow(), backup.deleted_at)
2014-02-19 06:55:33.645 |   File ""cinder/test.py"", line 280, in assertGreater
2014-02-19 06:55:33.645 |     f(first, second, msg=msg)
2014-02-19 06:55:33.645 |   File ""/usr/lib/python2.7/unittest/case.py"", line 940, in assertGreater
2014-02-19 06:55:33.646 |     self.fail(self._formatMessage(msg, standardMsg))
2014-02-19 06:55:33.646 |   File ""/usr/lib/python2.7/unittest/case.py"", line 408, in fail
2014-02-19 06:55:33.646 |     raise self.failureException(msg)
2014-02-19 06:55:33.646 | AssertionError: datetime.datetime(2014, 2, 19, 6, 53, 14, 473836) not greater than datetime.datetime(2014, 2, 19, 6, 53, 14, 473836)"
880,1284265,neutron,aeea31b21245864888f4744c19a0fed2a9c6567e,0,0,tests,ovs-agent test_fdb_add_flows test invalid,"https://github.com/openstack/neutron/blob/14cb886809e5cccbf799a0dc2e5b99f31b1ab3be/neutron/tests/unit/openvswitch/test_ovs_neutron_agent.py#L577
has:
mock.patch.object(self.agent.tun_br, 'setup_tunnel_port')
but setup_tunnel_port is an attribute of self.agent and not self.agent.tun_br."
881,1284275,cinder,52389188372b38ef237898e90cc533cf27d3eda1,0,0,Add a feature,3PAR FC driver doesn't return initiator_target_map...,"In order for the Fibre Channel Zone Manager to automatically zone up the endpoints (initiator and target) for the Fibre Channel fabric, the 3PAR driver needs to return the initiator_target_map in initialize_connection and terminate_connection"
882,1284277,neutron,cb99c08e968685c6f59bab2e49f86d56a2ec509d,0,0,Add some logic to manage errors,Bug #1284277 “nsx,"If the NSX controller returns a 503, like below:
http://paste.openstack.org/show/69125/
(full log):
http://208.91.1.172/logs/neutron/69361/12/401769/logs/screen-q-svc.txt.gz?level=TRACE#_2014-02-23_01_09_35_875
Tempest failures may be observed, like:
http://208.91.1.172/logs/neutron/69361/12/401769"
883,1284284,cinder,5eac161aa81a68e7cc4adb92798f6c3e849da81c,1,1,It ignores a parameter. This implements the fix for the bug...,Bug #1284284 “VMware,"I believe this issue is part of the root cause for this nova bug: https://bugs.launchpad.net/nova/+bug/1255317
Basically, when creating a volume from a vmdk image, the adapter_type is ignored and it defaults to LSI logic.
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/vmware/volumeops.py#L368
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/vmware/volumeops.py#L327
So cinder creates a volume with LSI SCSI controller and IDE disk.  And eventually, when I try to boot from a volume in Nova, I get ""No operating system found"".  I don't think you can boot an IDE disk using SCSI adapter.
However, I am able to boot from an image with IDE adapter.  So seems the volume step is causing an issue.
My recreate steps:
1) glance image-create --name cirros-sparse --is-public=True --container-format=bare --disk-format=vmdk --property vmware_disktype=""sparse"" < cirros-0.3.0-i386-disk.vmdk
2) cinder create --name sparse-ide --image-id cd0a8dea-2e2f-43be-97ab-85e21375b757 1
3) Observe in vCenter that the newly created volume has adapter set as LSI logic.
My VMDK image metadata:
KDMV?9??
# Disk DescriptorFile
version=1
CID=5269c92d
parentCID=ffffffff
createType=""monolithicSparse""
# Extent description
RW 80325 SPARSE ""disk-vmdk.vmdk""
ericwb@ericwb-virtual-machine:~$ head -n 20disk-vmdk.vmdk
head: 20disk-vmdk.vmdk: invalid number of lines
ericwb@ericwb-virtual-machine:~$ head -n 20 disk-vmdk.vmdk
KDMV?9??
# Disk DescriptorFile
version=1
CID=5269c92d
parentCID=ffffffff
createType=""monolithicSparse""
# Extent description
RW 80325 SPARSE ""disk-vmdk.vmdk""
# The Disk Data Base
#DDB
ddb.virtualHWVersion = ""4""
ddb.geometry.cylinders = ""79""
ddb.geometry.heads = ""16""
ddb.geometry.sectors = ""63""
ddb.adapterType = ""ide"""
884,1284312,nova,a17d38b96c4b776de8938195f86953ff26f94c2f,1,1,race condition,vmware driver races to create instance images,"Change Ia0ebd674345734e7cfa31ccd400fdba93646c554 traded one race condition for another. By ignoring all mkdir() calls that would otherwise fail because an instance directory already exists, two nodes racing to create a single image will corrupt or lose data, or fail in a strange way. This call should fail in that case, but doesn't after the recent patch was merged:
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L350"
885,1284314,neutron,706a8b1ff30c011169846ed5e38f06aa9f15faf2,0,0,No bug. ‘stats table needs columns to be bigint’,poolstatisticss table should have columns of type ...,"2014-02-24 17:44:45.585 5282 TRACE neutron.openstack.common.db.sqlalchemy.session
2014-02-24 17:44:45.589 5282 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rp
c/amqp.py"", line 438, in _process_data
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     **args)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/common/rpc.py"", lin
e 44, in dispatch
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rp
c/dispatcher.py"", line 172, in dispatch
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/services/loadbalanc
er/drivers/stingray/plugin_driver.py"", line 155, in update_pool_stats
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self.plugin.update_pool_stats(context, pool_id, data=stats)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/db/loadbalancer/loa
dbalancer_db.py"", line 500, in update_pool_stats
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self.update_status(context, Member, member, stats_status)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 447, in __exit__
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self.rollback()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/util/langhelpers
.py"", line 58, in __exit__
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     compat.reraise(exc_type, exc_value, exc_tb)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 444, in __exit__
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self.commit()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 354, in commit
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self._prepare_impl()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 334, in _prepare_impl
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self.session.flush()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/db
/sqlalchemy/session.py"", line 545, in _wrap
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     raise exception.DBError(e)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp DBError: (DataError) integer out of range
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp  'UPDATE poolstatisticss SET bytes_in=%(bytes_in)s, bytes_out=%(byte
s_out)s, active_connections=%(active_connections)s, total_connections=%(total_connections)s WHERE poolstatisticss.pool_id = %(poolstatist
icss_pool_id)s' {'bytes_in': 290286902, 'poolstatisticss_pool_id': u'0df91b58-e04b-4a36-bed8-8fcb3f73ed6e', 'total_connections': 2362054,
 'active_connections': 1, 'bytes_out': 3152985859}"
886,1284338,cinder,85239cc81440d9e5a4aee3c0961c96a4197ad939,0,0,refactoring code,fibre channel zone manager config options aren't i...,"The new Fibre Channel Zone Manager settings in the cinder.conf aren't in their own group.  Also the zone fabric settings don't use a group.
The fabric settings use a dynamic name to group the values together instead of using the config groups
For example,
  the current fc fabric config options looked like
fc_fabric_address_BRCD_NAME1=some address
fc_fabric_user_BRCD_NAME1=some user
...
It should be
[BRCD_NAME1]
fc_fabric_address=some address
fc_fabric_user=some user"
887,1284345,nova,bf322d27a0eaec92a9ed6f42c60c2168456825f8,0,0,Make less calls,Some network API methods unnecessarily trigger mul...,"Network manager methods add_fixed_ip_to_instance() and remove_fixed_ip_from_instance() both return with updated nw_info models. The corresponding network API methods however returns nothing, which has the following effect:
Both API methods have the @refresh_cache decorator that tries to update instance info cache from the decorated method's return value. In absence of a return value, it will make a new rpc call to to get the missing nw_info model. By changing the two API methods so that they return the models that they in fact already get, these extra calls can be avoided altogether.
In addition, having the API methods return updated nw_info models make it possible to further improve as in compute manager, calls to these methods are immediately followed by calls to get updated nw_info."
888,1284362,cinder,65e1031c6aeaf4d88029e33694c0c32ef81e75a6,0,0,No bug. ‘Add versioning output’,zone manager doesn't report versioning information...,"When the zone manager starts, it doesn't report it's version number or the driver it's using."
889,1284368,cinder,e088c73b185073943d9b18b7ede0375c096ee8b3,1,1,Returning the wrong value,Extending volume is in GB yet 3PAR is expecting MB...,"Extending volume diff is in GB yet 3PAR is expecting MB
Extending Volume osv-iGH8frZFSDuht.TTE90Erw from 3 to 7, by 4 GB. from (pid=60905) extend_volume /opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py:246
Cinder is correct
cinder extend 7f688411-9618-460e-a0f6-eb5dc333bc1a 7
Results in incorrect virtual volume size for thin provisioned and extended volume"
892,1284709,nova,4c68f19cd8d1f045bde02be7eec50c34c1d87932,1,1,,nova evacuate fails with neutron,"When I deploy nova with a shared storage and with neutron/ML2/linuxbridge, I have an error when I want to to use ""nova evacuate""
command :
# nova evacuate 1fa486f3-259c-4e1e-ae82-8b52606f1efd devstack2 --on-shared-storage
here are the logs on the compute node (devstack2) which will host the VM after the evacuation:
2014-02-25 16:08:39.918 ERROR nova.compute.manager [req-8307b6e1-b6ee-423e-9baf-d05a0ac5e91d admin admin] [ins
tance:
1fa486f3-259c-4e1e-ae82-8b52606f1efd] Setting instance vm_state to ERROR
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] Traceback (most recent call last):
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/compute/manager.py"", line 5261, in _error_out_instance_on_exception
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     yield
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/compute/manager.py"", line 2267, in rebuild_instance
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     extra_usage_info=extra_usage_info)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/conductor/api.py"", line 271, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     system_metadata, extra_usage_info)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 428, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     extra_usage_info=extra_usage_info_p)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/client.py"", line 150, in call
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     wait_for_reply=True, timeout=timeout)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/transport.py"", line 87, in _send
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     timeout=timeout)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 393, in send
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 386, in _send
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     raise result
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] TypeError: 'NoneType' object is not iterable
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] Traceback (most recent call last):
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     incoming.message))
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     return self._do_dispatch(endpoint, method, ctxt, args)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     result = getattr(endpoint, method)(ctxt, **new_args)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/conductor/manager.py"", line 502, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     system_metadata, extra_usage_info)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/compute/utils.py"", line 276, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     ignore_missing_network_data)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/notifications.py"", line 288, in bandwidth_usage
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     macs = [vif['address'] for vif in nw_info]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] TypeError: 'NoneType' object is not iterable
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]"
893,1284733,nova,aaf5762be5d37cac022dc321b6400b9743a25303,1,1,Bug. But they only deletes 1 line,metadata injected at nova boot does not arrive in ...,"The command:
   nova boot --flavor $FLAV --key_name $KEY --image $IMG --meta foo=bar meta1
should inject a file into `/meta.js` with content `{""foo"":""bar""}`. Currently in devstack this doesn't work.
It looks as if the data is arriving to n-cpu as:
    metadata={u'foo': u'bar'}
But n-cpu is expecting:
    metadata = [{""key"": ""foo"", ""value"": ""bar""}]"
894,1284881,neutron,50edccfad29d3c602eba628fc80e46a0a67de090,0,0,Delete unnecessary code,BigSwitch plugin unnecessarily uses external locks...,"The BigSwitch servermanager uses the synchronized decorators on rest backend calls. It currently sets the external flag to True, which isn't necessary since there aren't multiple independent processes running the plugin on the same host.
This results in the unnecessary creation of a lock file when it can just be handled by the default in-memory locks."
895,1284930,nova,635c5b611ae765bca3a888b1fcabe6bf0027c948,0,0,Just add an except. No bug,Bug #1284930 “deallocate_fixed_ip should handle exception to rol... ,"in nova-network, deallocate_fixed_ip function reserve quota first then do deallocate operations
if any operation failed, the quota reserve operation need to be rollback"
896,1284979,cinder,7ca4bea7e3d8688c847bdbcfe3e60900c61c297c,1,1,Non authenticated error.,Bug #1284979 “vmware,"1)  cinder create   --name vol1 1
2)  cinder create --source-volid 749c9cfd-e969-4199-a090-87daff1a9d54 --display-name vo2 1 [here volume created but status : error]
+-------------------+--------------------------------------+
|      Property     |                Value                 |
+-------------------+--------------------------------------+
|    attachments    |                  []                  |
| availability_zone |                 nova                 |
|      bootable     |                false                 |
|     created_at    |      2014-02-25T12:35:43.000000      |
|    description    |                 None                 |
|         id        | d55009de-0454-48a8-b73d-01f65c071b6c |
|      metadata     |                  {}                  |
|        name       |            fastvol_clone1            |
|        size       |                  1                   |
|    snapshot_id    |                 None                 |
|    source_volid   | 749c9cfd-e969-4199-a090-87daff1a9d54 |
|       status      |                error                 |
|      user_id      |   bb6690d82af84ae4b7152498088b517f   |
|    volume_type    |              fast_clone              |
+-------------------+--------------------------------------+
3) if you try to delete above created volume  - volume not getting deleted and volume status continuously say ""Error in deleting""
c-vol log :  at step2
2014-02-26 10:34:09.395 ERROR cinder.volume.drivers.vmware.api [-] Not authenticated error occurred. Will create session and try API call again: Error(s): NotAuthenticated occurred in the cal
l to RetrievePropertiesEx..
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api Traceback (most recent call last):
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api   File ""/opt/stack/cinder/cinder/volume/drivers/vmware/api.py"", line 194, in _invoke_api
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api     return api_method(*args, **kwargs)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api   File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vim_util.py"", line 212, in get_objects
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api     options=options)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api   File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vim.py"", line 174, in vim_request_handler
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api     retrieve_properties_ex_fault_checker(response)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api   File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vim.py"", line 153, in retrieve_properties_ex_fault_checker
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api     exc_msg_list)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api VimFaultException: Error(s): NotAuthenticated occurred in the call to RetrievePropertiesEx.
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api
2014-02-26 10:34:09.611 ERROR suds.client [-] <?xml version=""1.0"" encoding=""UTF-8""?>
<SOAP-ENV:Envelope xmlns:ns0=""urn:vim25"" xmlns:ns1=""http://schemas.xmlsoap.org/soap/envelope/"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:SOAP-ENV=""http://schemas.xmlsoap.org
/soap/envelope/"">
   <ns1:Body>
      <ns0:Login>
         <ns0:_this type=""SessionManager"">SessionManager</ns0:_this>
         <ns0:userName>root</ns0:userName>
         <ns0:password>vmware</ns0:password>
      </ns0:Login>
   </ns1:Body>
</SOAP-ENV:Envelope>
2014-02-26 10:34:09.613 ERROR cinder.openstack.common.loopingcall [-] in dynamic looping call
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall Traceback (most recent call last):
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall   File ""/opt/stack/cinder/cinder/openstack/common/loopingcall.py"", line 123, in _inner
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall     idle = self.f(*self.args, **self.kw)
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall   File ""/opt/stack/cinder/cinder/volume/drivers/vmware/api.py"", line 83, in _func
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall     raise excep
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall VimFaultException: Server raised fault: 'Cannot complete login due to an incorrect user name or password.'
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall"
897,1284996,nova,478f447070e2aa11ac14d419ae283e6bb9edeeb3,0,0,No bug. Feature. reduce VM network down time,reduce VM network down time during live (block) mi...,"It appears that during live migration, for certain duration network is unavailable and there is a scope to reduce the network downtime.
Please refer http://paste.openstack.org/show/69718/ to check the packet loss during live migration."
898,1285029,cinder,6d9bb4bdc915b1ed6ac9d49012eb185a369cc624,0,0,tests,Fibre Channel Zone Manager hacking violations,"Some hacking violations have come up after the FCZM landed and need to be addressed.  John commented on several of them in the post merge review for this patch.
https://review.openstack.org/#/c/76011"
899,1285035,nova,026583e60e9ec2b83049e8ceee16f182560d2562,1,1,bug for 1.0,Bug #1285035 “compute FakeDriver ,"by setting nova.conf:
compute_driver = fake.FakeDriver
I got  this error on conductor when nova-compute  update_available_resource :
2014-02-26 08:48:04.631 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: (DataError) invalid input syntax for integer: ""1.0""
LINE 1: ...6T07:48:04.627618'::timestamp, hypervisor_version='1.0' WHER...
                                                             ^
 'UPDATE compute_nodes SET updated_at=%(updated_at)s, hypervisor_version=%(hypervisor_version)s WHERE compute_nodes.id = %(compute_nodes_id)s' {'hypervisor_version': u'1.0', 'updated_at': datetime.datetime(2014, 2, 26, 7, 48, 4, 627618), 'compute_nodes_id': 1}
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/conductor/manager.py"", line 466, in compute_node_update
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     result = self.db.compute_node_update(context, node['id'], values)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/db/api.py"", line 228, in compute_node_update
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     return IMPL.compute_node_update(context, compute_id, values)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/db/sqlalchemy/api.py"", line 110, in wrapper
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     return f(*args, **kwargs)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/db/sqlalchemy/api.py"", line 166, in wrapped
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     return f(*args, **kwargs)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/db/sqlalchemy/api.py"", line 614, in compute_node_update
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     compute_ref.update(values)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     self.commit()
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     self._prepare_impl()
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     self.session.flush()
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/openstack/common/db/sqlalchemy/session.py"", line 616, in _wrap
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     raise exception.DBError(e)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher DBError: (DataError) invalid input syntax for integer: ""1.0""
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher LINE 1: ...6T07:48:04.627618'::timestamp, hypervisor_version='1.0' WHER...
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher                                                              ^
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher  'UPDATE compute_nodes SET updated_at=%(updated_at)s, hypervisor_version=%(hypervisor_version)s WHERE compute_nodes.id = %(compute_nodes_id)s' {'hypervisor_version': u'1.0', 'updated_at': datetime.datetime(2014,
2, 26, 7, 48, 4, 627618), 'compute_nodes_id': 1}"
901,1285060,cinder,b868ae707f9ecbe254101e21d9d7ffa0b05b17d1,1,1,,create_export and remove_export broken in driver.p...,"There are some issues with create_export and remove_export in driver.py:
1) There is a call to a create_export RPC, but the volume manager does not implement create_export
2) remove_export is not called in _detach_volume in driver.py, which means we have exports left over from several calls"
902,1285209,nova,dc716bd0ce77b56f4aabe54d6633b7f3bf9b0a5d,1,1,They dont umount…. ,[libvirt] nfs and glusterfs volume drivers don't u...,"When attaching volumes from NFS or GlusterFS backends, nova mounts the share in a temporary directory, this is reused when attaching another volume from the same share so there is no need to mount it several times.
On the other hand, when disconnecting those volumes nova doesn't even try to unmount the share which may remain mounted and unused there. To clean up after detach, nova could at least try to umount the shares."
903,1285259,nova,8cd2b890710ba6e53884f43b5d6ce095672732a4,0,0,Change in requirements,After evacuate origin host still report a runing v...,"After evacuate a host with one instance to a target host it still report there is an instance in that hypervisor
Pre evacuate report:
$ nova hypervisor-stats
+----------------------+-------+
| Property             | Value |
+----------------------+-------+
| count                | 2     |
| current_workload     | 0     |
| disk_available_least | 22    |
| free_disk_gb         | 50    |
| free_ram_mb          | 3860  |
| local_gb             | 50    |
| local_gb_used        | 0     |
| memory_mb            | 4948  |
| memory_mb_used       | 1088  |
| running_vms          | 1     |
| vcpus                | 3     |
| vcpus_used           | 1     |
+----------------------+-------+
$ nova hypervisor-list
+----+---------------------+
| ID | Hypervisor hostname |
+----+---------------------+
| 1  | jmolle-Controller   |
| 2  | jmolle-Node1        |
+----+---------------------+
$ nova hypervisor-show jmolle-Controller
+---------------------------+-------------------------------------------------------+
| Property                  | Value                                                 |
+---------------------------+-------------------------------------------------------+
| cpu_info_arch             | x86_64                                                |
| cpu_info_features         | [""rdtscp"", ""hypervisor"", ""x2apic"", ""ss"", ""ds"", ""vme""] |
| cpu_info_model            | Westmere                                              |
| cpu_info_topology_cores   | 1                                                     |
| cpu_info_topology_sockets | 2                                                     |
| cpu_info_topology_threads | 1                                                     |
| cpu_info_vendor           | Intel                                                 |
| current_workload          | 0                                                     |
| disk_available_least      | 10                                                    |
| free_disk_gb              | 25                                                    |
| free_ram_mb               | 3378                                                  |
| host_ip                   | 192.168.41.101                                        |
| hypervisor_hostname       | jmolle-Controller                                     |
| hypervisor_type           | QEMU                                                  |
| hypervisor_version        | 1000000                                               |
| id                        | 1                                                     |
| local_gb                  | 25                                                    |
| local_gb_used             | 0                                                     |
| memory_mb                 | 3954                                                  |
| memory_mb_used            | 576                                                   |
| running_vms               | 1                                                     |
| service_host              | jmolle-Controller                                     |
| service_id                | 4                                                     |
| vcpus                     | 2                                                     |
| vcpus_used                | 1                                                     |
+---------------------------+-------------------------------------------------------+
$ nova hypervisor-servers jmolle-Controller
+--------------------------------------+-------------------+---------------+---------------------+
| ID                                   | Name              | Hypervisor ID | Hypervisor Hostname |
+--------------------------------------+-------------------+---------------+---------------------+
| a3c291e5-05b0-43fc-b16b-121bf36f30c0 | instance-00000001 | 1             | jmolle-Controller   |
+--------------------------------------+-------------------+---------------+---------------------+
But after evacuate the instanse we get:
$ nova hypervisor-stats
+----------------------+-------+
| Property             | Value |
+----------------------+-------+
| count                | 2     |
| current_workload     | 1     |
| disk_available_least | 22    |
| free_disk_gb         | 50    |
| free_ram_mb          | 3796  |
| local_gb             | 50    |
| local_gb_used        | 0     |
| memory_mb            | 4948  |
| memory_mb_used       | 1152  |
| running_vms          | 2     |
| vcpus                | 3     |
| vcpus_used           | 2     |
+----------------------+-------+
here we see that now there are 2 running instances instead of one
and if we use show command we get:
$ nova hypervisor-show jmolle-Controller
+---------------------------+-------------------------------------------------------+
| Property                  | Value                                                 |
+---------------------------+-------------------------------------------------------+
| cpu_info_arch             | x86_64                                                |
| cpu_info_features         | [""rdtscp"", ""hypervisor"", ""x2apic"", ""ss"", ""ds"", ""vme""] |
| cpu_info_model            | Westmere                                              |
| cpu_info_topology_cores   | 1                                                     |
| cpu_info_topology_sockets | 2                                                     |
| cpu_info_topology_threads | 1                                                     |
| cpu_info_vendor           | Intel                                                 |
| current_workload          | 0                                                     |
| disk_available_least      | 10                                                    |
| free_disk_gb              | 25                                                    |
| free_ram_mb               | 3378                                                  |
| host_ip                   | 192.168.41.101                                        |
| hypervisor_hostname       | jmolle-Controller                                     |
| hypervisor_type           | QEMU                                                  |
| hypervisor_version        | 1000000                                               |
| id                        | 1                                                     |
| local_gb                  | 25                                                    |
| local_gb_used             | 0                                                     |
| memory_mb                 | 3954                                                  |
| memory_mb_used            | 576                                                   |
| running_vms               | 1                                                     |
| service_host              | jmolle-Controller                                     |
| service_id                | 4                                                     |
| vcpus                     | 2                                                     |
| vcpus_used                | 1                                                     |
+---------------------------+-------------------------------------------------------+
we also see 1 running instance
but if we list servers we get 0
$ nova hypervisor-servers jmolle-Controller
+----+------+---------------+---------------------+
| ID | Name | Hypervisor ID | Hypervisor Hostname |
+----+------+---------------+---------------------+
+----+------+---------------+---------------------+
and the instance was evacuate to the other host correctly
$ nova hypervisor-servers jmolle-Node1
+--------------------------------------+-------------------+---------------+---------------------+
| ID                                   | Name              | Hypervisor ID | Hypervisor Hostname |
+--------------------------------------+-------------------+---------------+---------------------+
| a3c291e5-05b0-43fc-b16b-121bf36f30c0 | instance-00000001 | 2             | jmolle-Node1        |
+--------------------------------------+-------------------+---------------+---------------------+
I think this is a nova bug due to the inconcistency of hypervisor-show and hypervisor-servers nova commands"
904,1285289,neutron,038a56c13dafb1993ea2c6cba5a1a06b573cfa03,1,0,Bug because a field has been removed,get_<resource> functions in N1kv plugin,"The N1kv plugin includes the 'fields' argument in the call to the super class function. Hence, only the specified fields are returned by the get_<..> function in the super class.
The problem is that a field that has been removed may be needed in the subsequent processing in the N1kv's get_<...> function.
The (trivial) solution is to set the fields argument to None in the call to the super class function.
A patch with the solution will be submitted momentarily."
905,1285335,neutron,03500414cf463905a28ea30d069630dcfda1d90b,0,0,Evolution,Openvswitch agent should use ovs patch ports inste...,"https://bugs.launchpad.net/neutron/+bug/1045613 is no more valid because ""patch port support has been added to the upstream Linux kernel OVS implementation"" (comment #4). That's why we can use openvswitch patch ports to interconnect br-int to physnets in order to use the same interconnection technology with br-tun and physnets and increase performance (according to http://www.opencloudblog.com/?p=96)."
906,1285383,neutron,42c882b9e98755b9e119f6c566fc00a62a420af7,0,0, make replication mode configurable ,Bug #1285383 “NSX,"The replication mode on switches and routers should have been configurable
to use use source replication if one did not want to deploy service node(s)."
907,1285437,nova,16e360d3a9183e9fd2b3dc163fe3e1ce4ec18e8b,0,0,Improve messages,HTTPNotFound response does not contain enough expl...,"In each RESTful API, there are useful explanation messages for HTTPNotFound response in the implementation.
However, some messages are not output to the response.
For example, the implementation of the pause action API contains ""Server not found"" message for nonexistent server error, but now the message is not output into the response like the following:
$ curl -i 'http://localhost:8774/v2/<project-id>/servers/<nonexistent-server-id>/action' -X POST [..]"" -d '{""pause"": null}'
HTTP/1.1 404 Not Found
Content-Length: 78
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-a5282f5e-7e59-48cf-a86b-fe2e34347a2f
Date: Thu, 27 Feb 2014 10:21:52 GMT
{""itemNotFound"": {""message"": ""The resource could not be found."", ""code"": 404}}
$
When receiving the above message, client may consider ""what is not found? project, server, or action?""
so it is better to output right message into a response."
908,1285473,neutron,c75aada35b13da33d08f51d14fd36640d8b735fa,0,0,Use x to avoid inconsistencies,Use database session from the context in Cisco N1k...,Use database session from the context wherever possible during database transactions to avoid inconsistencies in the Cisco N1kv plugin.
909,1285482,nova,ccb68ab1cd50f00546794c4bda9acb2580d8b3a4,0,0,add a sample file,There is not any API sample file for “unshelve a s...,"Now there is not any API sample file of ""unshelve a server"" API,
and OpenStack API documentation[1] also does not describe the API.
[1]: http://api.openstack.org/api-ref-compute-ext.html"
910,1285641,neutron,273b169029e6693cb862be38df1c70e540808645,1,0,Evolution bug,Bug #1285641 “different fully qualified class name for VPNaaS mi... ,"In migrations 52ff27f7567a_support_for_vpnaas.py and  338d7508968c_vpnaas_peer_address_.py different class names are set:  neutron.services.vpn.plugin.VPNDriverPlugin and neutron.services.vpn.plugin.VPNPlugin.
This cause the following exception:
neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head
No handlers could be found for logger ""neutron.common.legacy""
INFO  [alembic.migration] Context impl MySQLImpl.
INFO  [alembic.migration] Will assume non-transactional DDL.
INFO  [alembic.migration] Running upgrade None -> folsom, folsom initial database
INFO  [alembic.migration] Running upgrade folsom -> 2c4af419145b, l3_support
INFO  [alembic.migration] Running upgrade 2c4af419145b -> 5a875d0e5c, ryu
INFO  [alembic.migration] Running upgrade 5a875d0e5c -> 48b6f43f7471, DB support for service types
INFO  [alembic.migration] Running upgrade 48b6f43f7471 -> 3cb5d900c5de, security_groups
INFO  [alembic.migration] Running upgrade 3cb5d900c5de -> 1d76643bcec4, nvp_netbinding
INFO  [alembic.migration] Running upgrade 1d76643bcec4 -> 2a6d0b51f4bb, cisco plugin cleanup
INFO  [alembic.migration] Running upgrade 2a6d0b51f4bb -> 1b693c095aa3, Quota ext support added in Grizzly
INFO  [alembic.migration] Running upgrade 1b693c095aa3 -> 1149d7de0cfa, initial port security
INFO  [alembic.migration] Running upgrade 1149d7de0cfa -> 49332180ca96, ryu plugin update
INFO  [alembic.migration] Running upgrade 49332180ca96 -> 38335592a0dc, nvp_portmap
INFO  [alembic.migration] Running upgrade 38335592a0dc -> 54c2c487e913, 'DB support for load balancing service
INFO  [alembic.migration] Running upgrade 54c2c487e913 -> 45680af419f9, nvp_qos
INFO  [alembic.migration] Running upgrade 45680af419f9 -> 1c33fa3cd1a1, Support routing table configuration on Router
INFO  [alembic.migration] Running upgrade 1c33fa3cd1a1 -> 363468ac592c, nvp_network_gw
INFO  [alembic.migration] Running upgrade 363468ac592c -> 511471cc46b, Add agent management extension model support
INFO  [alembic.migration] Running upgrade 511471cc46b -> 3b54bf9e29f7, NEC plugin sharednet
INFO  [alembic.migration] Running upgrade 3b54bf9e29f7 -> 4692d074d587, agent scheduler
INFO  [alembic.migration] Running upgrade 4692d074d587 -> 1341ed32cc1e, nvp_net_binding
INFO  [alembic.migration] Running upgrade 1341ed32cc1e -> grizzly, grizzly
INFO  [alembic.migration] Running upgrade grizzly -> f489cf14a79c, DB support for load balancing service (havana)
INFO  [alembic.migration] Running upgrade f489cf14a79c -> 176a85fc7d79, Add portbindings db
INFO  [alembic.migration] Running upgrade 176a85fc7d79 -> 32b517556ec9, remove TunnelIP model
INFO  [alembic.migration] Running upgrade 32b517556ec9 -> 128e042a2b68, ext_gw_mode
INFO  [alembic.migration] Running upgrade 128e042a2b68 -> 5ac71e65402c, ml2_initial
INFO  [alembic.migration] Running upgrade 5ac71e65402c -> 3cbf70257c28, nvp_mac_learning
INFO  [alembic.migration] Running upgrade 3cbf70257c28 -> 5918cbddab04, add tables for router rules support
INFO  [alembic.migration] Running upgrade 5918cbddab04 -> 3cabb850f4a5, Table to track port to host associations
INFO  [alembic.migration] Running upgrade 3cabb850f4a5 -> b7a8863760e, Remove cisco_vlan_bindings table
INFO  [alembic.migration] Running upgrade b7a8863760e -> 13de305df56e, nec_add_pf_name
INFO  [alembic.migration] Running upgrade 13de305df56e -> 20ae61555e95, DB Migration for ML2 GRE Type Driver
INFO  [alembic.migration] Running upgrade 20ae61555e95 -> 477a4488d3f4, DB Migration for ML2 VXLAN Type Driver
INFO  [alembic.migration] Running upgrade 477a4488d3f4 -> 2032abe8edac, LBaaS add status description
INFO  [alembic.migration] Running upgrade 2032abe8edac -> 52c5e4a18807, LBaaS Pool scheduler
INFO  [alembic.migration] Running upgrade 52c5e4a18807 -> 557edfc53098, New service types framework (service providers)
INFO  [alembic.migration] Running upgrade 557edfc53098 -> e6b16a30d97, Add cisco_provider_networks table
INFO  [alembic.migration] Running upgrade e6b16a30d97 -> 39cf3f799352, FWaaS Havana-2 model
INFO  [alembic.migration] Running upgrade 39cf3f799352 -> 52ff27f7567a, Support for VPNaaS
INFO  [alembic.migration] Running upgrade 52ff27f7567a -> 11c6e18605c8, Pool Monitor status field
INFO  [alembic.migration] Running upgrade 11c6e18605c8 -> 35c7c198ddea, remove status from HealthMonitor
INFO  [alembic.migration] Running upgrade 35c7c198ddea -> 263772d65691, Cisco plugin db cleanup part II
INFO  [alembic.migration] Running upgrade 263772d65691 -> c88b6b5fea3, Cisco N1KV tables
INFO  [alembic.migration] Running upgrade c88b6b5fea3 -> f9263d6df56, remove_dhcp_lease
INFO  [alembic.migration] Running upgrade f9263d6df56 -> 569e98a8132b, metering
INFO  [alembic.migration] Running upgrade 569e98a8132b -> 86cf4d88bd3, remove bigswitch port tracking table
INFO  [alembic.migration] Running upgrade 86cf4d88bd3 -> 3c6e57a23db4, add multiprovider
INFO  [alembic.migration] Running upgrade 3c6e57a23db4 -> 63afba73813, Add unique constraint for id column of TunnelEndpoint
INFO  [alembic.migration] Running upgrade 63afba73813 -> 40dffbf4b549, nvp_dist_router
INFO  [alembic.migration] Running upgrade 40dffbf4b549 -> 53bbd27ec841, Extra dhcp opts support
INFO  [alembic.migration] Running upgrade 53bbd27ec841 -> 46a0efbd8f0, cisco_n1kv_multisegment_trunk
INFO  [alembic.migration] Running upgrade 46a0efbd8f0 -> 2a3bae1ceb8, NEC Port Binding
INFO  [alembic.migration] Running upgrade 2a3bae1ceb8 -> 14f24494ca31, DB Migration for Arista ml2 mechanism driver
INFO  [alembic.migration] Running upgrade 14f24494ca31 -> 32a65f71af51, ml2 portbinding
INFO  [alembic.migration] Running upgrade 32a65f71af51 -> 66a59a7f516, NEC OpenFlow Router
INFO  [alembic.migration] Running upgrade 66a59a7f516 -> 51b4de912379, Cisco Nexus ML2 mechanism driver
INFO  [alembic.migration] Running upgrade 51b4de912379 -> 1efb85914233, allowedaddresspairs
INFO  [alembic.migration] Running upgrade 1efb85914233 -> 38fc1f6789f8, Cisco N1KV overlay support
INFO  [alembic.migration] Running upgrade 38fc1f6789f8 -> 4a666eb208c2, service router
INFO  [alembic.migration] Running upgrade 4a666eb208c2 -> 338d7508968c, vpnaas peer_address size increase
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 145, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 80, in do_upgrade_downgrade
    do_alembic_command(config, cmd, revision, sql=CONF.command.sql)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 59, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 124, in upgrade
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 199, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 205, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 105, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 89, in run_migrations_online
    options=build_options())
  File ""<string>"", line 7, in run_migrations
  File ""/usr/local/lib/python2.7/dist-packages/alembic/environment.py"", line 681, in run_migrations
    self.get_context().run_migrations(**kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/migration.py"", line 225, in run_migrations
    change(**kw)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/338d7508968c_vpnaas_peer_address_.py"", line 48, in upgrade
    type_=sa.String(255), existing_type=sa.String(64))
  File ""<string>"", line 7, in alter_column
  File ""<string>"", line 1, in <lambda>
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 322, in go
    return fn(*arg, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/operations.py"", line 300, in alter_column
    existing_autoincrement=existing_autoincrement
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/mysql.py"", line 42, in alter_column
    else existing_autoincrement
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/impl.py"", line 76, in _exec
    conn.execute(construct, *multiparams, **params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
    compiled
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
    cursor.execute(statement, parameters)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 174, in execute
    self.errorhandler(self, exc, value)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler
    raise errorclass, errorvalue
sqlalchemy.exc.ProgrammingError: (ProgrammingError) (1146, ""Table 'neutron_ml2.ipsec_site_connections' doesn't exist"") 'ALTER TABLE ipsec_site_connections CHANGE peer_address peer_address VARCHAR(255) NULL' ()"
912,1285735,nova,d9a5a80bc06f7a25c259ff763a59dffd9514371e,0,0,Evolution change. NO bug. Change in the decisions made,libvirt lvm volumes based on instance['name'] not ...,"because libvirt lvm volumes are based on instance['name'], it means that the actual names used in lvm storage are based on an operator configuration variable: instance_name_template
the default is 'instance-%08x'
however this is site changable, and changable at any time. This creates 2 failure modes.
#1) operator changes this, the result is all volumes created before the change are no longer able to be cleaned up by nova
#2) operator has changed this to something that includes end user input, like %(display_name), which would allow one user to impact another (use A has display name ""bob"", user B has displayname ""bob_joe"") because of https://github.com/openstack/nova/blob/master/nova/virt/libvirt/driver.py#L1068
specifically:
            pattern = '%s_' % instance['name']
            def belongs_to_instance(disk):
                return disk.startswith(pattern)
#2 is a non default situation, and requires specific config by an adminstrator and specific naming by users, but it should be protected against.
A much better approach would be to use instance['uuid'] which has no operator or user impact on naming."
913,1285886,nova,621fc02f70fa4fa50b2f05167eefa39b71f72024,0,0,"Before was ‘’, now is None",update_port passes device_id=None but neutron expe...,"2014-02-27 14:08:23.013 ERROR nova.network.neutronv2.api [req-598b0d2f-e4e9-40eb-a9d4-027975d08b39 demo demo] Failed to delete neutron port 153f472b-f662-497b-bc7c-3cc362157ab1
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api Traceback (most recent call last):
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 420, in deallocate_for_instance
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     neutron.update_port(port, port_req_body)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 111, in with_params
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     ret = self.function(instance, *args, **kwargs)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 321, in update_port
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     return self.put(self.port_path % (port), body=body)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1245, in put
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     headers=headers, params=params)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1221, in retry_request
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     headers=headers, params=params)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1164, in do_request
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     self._handle_fault_response(status_code, replybody)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1134, in _handle_fault_response
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     exception_handler_v20(status_code, des_error_body)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 84, in exception_handler_v20
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     message=error_dict)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api NeutronClientException: Invalid input for device_id. Reason: 'None' is not a valid string.
2014-02-27 14:08:23.011 ERROR neutron.api.v2.resource [req-3f133c17-198f-412d-b57e-66bbf0fcfbcb neutron abd2b56aa998417ba5af609a680a138d] update failed
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 466, in update
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource     allow_bulk=self._allow_bulk)
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 600, in prepare_request_body
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource     raise webob.exc.HTTPBadRequest(msg)
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource HTTPBadRequest: Invalid input for device_id. Reason: 'None' is not a valid string.
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource"
914,1285906,cinder,fb25917fe17fc80c1ae704759c8f6487ac2e9a22,1,1,,Bug #1285906 “3PAR,"1. Create a volume
2. Create a snapshot of the volume
3. Create a volume based on the snapshot
4. Extend the volume.
5. Horizon shows the volume with status = Error Extending
2014-02-27 15:27:55.028 ERROR cinder.volume.manager [req-bb0716d0-5d3c-430a-83fc-df0e2d85444c dd968af1de0d457b8d43217a821edf1a 63be499ec75e48d597b83679112d32bd] volume 7f440946-8b67-4b0a-85de-9b94add2d258: Error trying to extend volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager Traceback (most recent call last):
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1114, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     volume_id)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     retval = f(*args, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_iscsi.py"", line 437, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     try:
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 251, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     self.client.growVolume(volume_name, growth_size)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     six.reraise(self.type_, self.value, self.tb)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 248, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     LOG.debug(""Extending Volume %s from %s to %s, by %s GB."" %
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/client.py"", line 452, in growVolume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     response, body = self.http.put('/volumes/%s' % name, body=info)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 306, in put
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     return self._cs_request(url, 'PUT', **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 239, in _cs_request
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     resp, body = self._do_reauth(url, method, ex, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 216, in _do_reauth
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     resp, body = self._time_request(self.api_url + url, method, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 205, in _time_request
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     resp, body = self.request(url, method, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 199, in request
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     raise exceptions.from_response(resp, body)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager HTTPForbidden: Forbidden (HTTP 403) 150 - invalid operation:  Cannot grow this type of volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager"
915,1285993,neutron,71deb11f6fd5186cabcc42431316deb5dbfa3dfe,1,1,Crashes,neutron-server crashes when running on an empty da...,"operation:
$ mysql
> create database neutron_ml2 character set utf8;
> exit
$ neutron-server --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini
error:
2014-02-28 15:02:46.550 TRACE neutron ProgrammingError: (ProgrammingError) (1146, ""Table 'neutron_ml2.ml2_vlan_allocations' doesn't exist"") 'SELECT ml2_vlan_allocations.physical_network AS ml2_vlan_allocations_physical_network, ml2_vlan_allocations.vlan_id AS ml2_vlan_allocations_vlan_id, ml2_vlan_allocations.allocated AS ml2_vlan_allocations_allocated \nFROM ml2_vlan_allocations FOR UPDATE' ()
investigation:
This problem introduced by https://review.openstack.org/#/c/74896/ .
Not that this problem does not occur if nuetron-db-manage is run before running neutron-server since ml2_vlan_allocations table is created by neutron-db-manage.
I did skip running neutron-db-manage usually and it was no problem. Is it prohibited now ?"
916,1286285,cinder,b493dcce93418ed09f0e2b7e1dccc20270deb75c,0,0,'It should be calling’ (Just in case),3PAR driver common uses time.sleep,"hp_3par_common calls time.sleep()
It should be calling eventlet.greenthread.sleep() to make sure we don't block the volume manager."
917,1286297,nova,daedfffd95b375f00deacb43b6de5e02ddde6df4,1,1,There is a bug. The decision is to delete some code,Adding current project as flavor access is throwin...,"Steps to reproduce
1. Create a new flavor, setting and add the current project as flavor access
2. The flavor is created but an error is displayed saying ""Unable to set flavor access for project.....""
The error is thrown because Horizon creates the flavor and add the access after that. The problem is that once a private flavor is created, nova adds the current project within the flavor accesses so when Horizon tries to add the access, nova throws an ""Access already exist for this flavor"" exception"
918,1286375,glance,965e2c359477b7445b36f84ad1186f541cb831ca,0,0,Feature: Add OVA container format ,Add OVA container format,An OVA package is a tar archive usually containing an OVF directory inside it. Nova needs to be able to differentiate OVF and OVA based on the container format in order to extract the relevant information from it.
919,1286412,neutron,9869cc1d100ced5f50cf03b557639835c364fbb5,0,0,'Add support...’,Add support for router and network scheduling in C...,Added functionality to schedule routers and networks.
920,1286528,nova,a72a3f4c956338b05db2c6fa958112cf5df8dcad,1,0,Windows dependency?,guru-meditation fails on Windows due to non portab...,"The guru-meditation report fails on Hyper-V due to missing signal handling.
This is a blocking issue on Windows.
http://64.119.130.115/74060/3/Hyper-V_logs/hv-compute1/nova-console.log.gz
Traceback (most recent call last):
  File ""c:\OpenStack\virtualenv\Scripts\nova-compute-script.py"", line 9, in <module>
    load_entry_point('nova==2014.1.dev954.g3a611cc', 'console_scripts', 'nova-compute')()
  File ""c:\OpenStack\virtualenv\lib\site-packages\pkg_resources.py"", line 353, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File ""c:\OpenStack\virtualenv\lib\site-packages\pkg_resources.py"", line 2321, in load_entry_point
    return ep.load()
  File ""c:\OpenStack\virtualenv\lib\site-packages\pkg_resources.py"", line 2048, in load
    entry = __import__(self.module_name, globals(),globals(), ['__name__'])
  File ""c:\OpenStack\virtualenv\lib\site-packages\nova\cmd\compute.py"", line 32, in <module>
    from nova.openstack.common.report import guru_meditation_report as gmr
  File ""c:\OpenStack\virtualenv\lib\site-packages\nova\openstack\common\report\guru_meditation_report.py"", line 63, in <module>
    class GuruMeditation(object):
  File ""c:\OpenStack\virtualenv\lib\site-packages\nova\openstack\common\report\guru_meditation_report.py"", line 100, in GuruMeditation
    def setup_autorun(cls, version, signum=signal.SIGUSR1):
AttributeError: 'module' object has no attribute 'SIGUSR1'
Nova patch that introduced the issue:
https://github.com/openstack/nova/commit/cec532848f569afb4832029bce4969578472a57a
Review link:
https://review.openstack.org/#/c/69058/"
921,1286565,neutron,c36ddaf930ba309be3c127dc1836f28a8ab97c5d,1,1,"I think cisco is not a specific platform, is 1 of their plugins",Bug #1286565 “Cisco Nexus,"If DevStack is configured for the Cisco Nexus plugin with the latest DevStack, the following
infinite recursion error is observed:
Exception RuntimeError: 'maximum recursion depth exceeded' in <bound method
ConnectionContext.__del__ of <neutron.openstack.common.rpc.amqp.ConnectionContext
object at 0x403a3d0>> ignored
An investigation shows that this failure triggered when the DB base plugin's
_is_native_pagination_supported method is called. The infinite recursion begins
when this exception is raised:
             raise AttributeError(
                 _(""'%(model)s' object has no attribute '%(name)s'"") %
                {'model': self._model, 'name': name})
in the PluginV2.__getattr__ method in
neutron/plugins/cisco/network_plugin.py. The problem is that self._model
object is being % mod'd as a string in the unicode message, and this
causes many levels of recursion into deepcopy (a deepcopy for all objects
embedded in this object)."
922,1286694,cinder,f7d99b07f72037b87d5e9f63b48a7c49f7116fe5,0,0,Feature. Add initiator target map ,Add initiator target map which is required by the ...,Add initiator target map in initialize_connection and terminate_connection of the EMC SMI-S FC driver as it is required by the FC zone manager.
923,1286712,cinder,5c1d0a6246cbd3f05f6c41e7cfc7046fa3431726,0,0,cleanup,Remove useless funciton stub_out_key_pair_funcs,"stub_out_key_pair_funcs was useless, remove it.
-def stub_out_key_pair_funcs(stubs, have_key_pair=True):
-    def key_pair(context, user_id):
-        return [dict(name='key', public_key='public_key')]
-
-    def one_key_pair(context, user_id, name):
-        if name == 'key':
-            return dict(name='key', public_key='public_key')
-        else:
-            raise exc.KeypairNotFound(user_id=user_id, name=name)
-
-    def no_key_pair(context, user_id):
-        return []"
924,1286714,cinder,5c1d0a6246cbd3f05f6c41e7cfc7046fa3431726,0,0,cleanup,Remove unused enumerate,"diff --git a/cinder/tests/api/contrib/test_extended_snapshot_attributes.py b/cinder/tests/api/contrib/test_extended_snapshot_attributes.py
index e90c291..7df8ebb 100644
--- a/cinder/tests/api/contrib/test_extended_snapshot_attributes.py
+++ b/cinder/tests/api/contrib/test_extended_snapshot_attributes.py
@@ -91,7 +91,7 @@ class ExtendedSnapshotAttributesTest(test.TestCase):
         res = self._make_request(url)
         self.assertEqual(res.status_int, 200)
-        for i, snapshot in enumerate(self._get_snapshots(res.body)):
+        for snapshot in self._get_snapshots(res.body):
             self.assertSnapshotAttributes(snapshot,
                                           project_id='fake',
                                           progress='0%')"
925,1286733,neutron,142c55e82a4d36af887439a94054617a5a0ede9c,0,0,Change in requirements,Bug #1286733 “nec plugin,"Before Grizzly release, data format of OFC ID mapping tables was changed  and there are two types of ID mapping tables for old and new format.  The old mapping tables are only used for resources (networks, ports, tenants, filters) in pre-Grizzly system. pre-Grizzly system is no longer supported and we no longer need to consider old data format. Migrating data from the old mapping tables to the new tables reduces the code complexity."
926,1286742,cinder,5c1d0a6246cbd3f05f6c41e7cfc7046fa3431726,0,0,cleanup,Remove unused function fake_execute2,"diff --git a/cinder/tests/brick/test_brick_linuxscsi.py b/cinder/tests/brick/test_brick_linuxscsi.py
index 47b73dc..e0ec010 100644
--- a/cinder/tests/brick/test_brick_linuxscsi.py
+++ b/cinder/tests/brick/test_brick_linuxscsi.py
@@ -101,15 +101,6 @@ class LinuxSCSITestCase(test.TestCase):
                    )
             return out, None
-        def fake_execute2(*cmd, **kwargs):
-            out = (""350002ac20398383d dm-3 3PARdata,VV\n""
-                   ""size=2.0G features='0' hwhandler='0' wp=rw\n""
-                   ""`-+- policy='round-robin 0' prio=-1 status=active\n""
-                   ""  |- 0:0:0:1  sde 8:64 active undef running\n""
-                   ""  `- 2:0:0:1  sdf 8:80 active undef running\n""
-                   )
-            return out, None
-
         self.stubs.Set(self.linuxscsi, '_execute', fake_execute)
         info = self.linuxscsi.find_multipath_device('/dev/sde')"
928,1287031,neutron,aa5ace6b7555dd9ed59dd07ea022585f44767f27,0,0,unused code,Unused code 'as e' in exception blocks,"unused code  'as e' in exception blocks with flowing files:
neutron/api/v2/resource.py
neutron/plugins/vmware/nsxlib/router.py"
929,1287176,cinder,5eac161aa81a68e7cc4adb92798f6c3e849da81c,1,1,Bug. Make something optional,Bug #1287176 “VMware,"Currently a sparse disk based image is copied as a flat file which is one of the causes of the following bug:
https://bugs.launchpad.net/nova/+bug/1255317
Also, 'vmdk_type' extra spec property of the cinder volume is ignored.
The image should be copied, and then converted to appropriate type based on 'vmdk_type'."
930,1287185,cinder,5eac161aa81a68e7cc4adb92798f6c3e849da81c,0,0,Add support for an argument,Bug #1287185 “VMware,The volume's vmdk_type extra_spec property is ignored while copying image with vmware_disktype=thin/preallocated to a volume. The image should be downloaded and converted to appropriate type based on vmdk_type.
931,1287292,nova,4640542cc502a442d4807822d75d3fa3eff0a33b,1,1,incorrect use,Bug #1287292 “VMware,"The vim.get_soap_url function incorrectly builds an IPv6 address using hostname/IP and port.
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vim.py#L151
The result of this line would create an address as follows:
https://[2001:db8:85a3:8d3:1319:8a2e:370:7348:443]/sdk
Ports should be outside the square brackets, not inside, as follows:
https://[2001:db8:85a3:8d3:1319:8a2e:370:7348]:443/sdk
For reference see: http://en.wikipedia.org/wiki/IPv6_address section Literal IPv6 addresses in network resource identifiers"
932,1287367,nova,f3f46b532c528c31fbcdc0d18b18105c42a0d74c,1,1,Raising the improper code,The rescue API should handle NotImplementedError,"There are several nova virt drivers that don't implement the rescue API, but the os compute API doesn't handle NotImplementedError, it returns a 400 instead of a 501.
https://wiki.openstack.org/wiki/HypervisorSupportMatrix
The API could be tightened up a bit to return a 501 instead like how the pause admin action is handling NotImplementedError."
933,1287407,neutron,17624e21f4812be80ca535a1f119127c021db54e,0,0,cleanup,remove unused method update_fixed_ip_lease_expirat...,should have been removed long ago here: Ifcb4f093c92904ceb896438987d53e692eb7fb26
934,1287419,neutron,6c91bdfe24e275ede65df369d714ca119d5ce5f2,1,1,wrong parameter passed,Bug #1287419 “NSX,"2014-03-03 15:06:33.913 ERROR neutron.plugins.vmware.api_client.client [req-ffc2204c-e957-48b3-9bd8-cbbd66fa9645 demo 3cb63bd6fa2d4ea397df6de7770c2e8a] Server Error Message: Entity '50d064c4-0591-43ee-8dfd-ac2409c13a5e' not registered.
2014-03-03 15:06:33.913 ERROR neutron.plugins.vmware.nsxlib.switch [req-ffc2204c-e957-48b3-9bd8-cbbd66fa9645 demo 3cb63bd6fa2d4ea397df6de7770c2e8a] Port or Network not found, Error: An unknown exception occurred.
2014-03-03 15:06:33.914 ERROR NeutronPlugin [-] Unable to update port id: 6b7c9502-8e58-4930-b440-7022f47fff89.
2014-03-03 15:06:33.914 TRACE NeutronPlugin Traceback (most recent call last):
2014-03-03 15:06:33.914 TRACE NeutronPlugin   File ""/opt/stack/neutron/neutron/plugins/vmware/plugins/base.py"", line 1307, in update_port
2014-03-03 15:06:33.914 TRACE NeutronPlugin     ret_port.get(addr_pair.ADDRESS_PAIRS))
2014-03-03 15:06:33.914 TRACE NeutronPlugin   File ""/opt/stack/neutron/neutron/plugins/vmware/nsxlib/switch.py"", line 324, in update_port
2014-03-03 15:06:33.914 TRACE NeutronPlugin     port_id=lport_uuid, net_id=lswitch_uuid)
2014-03-03 15:06:33.914 TRACE NeutronPlugin PortNotFoundOnNetwork: Port 6b7c9502-8e58-4930-b440-7022f47fff89 could not be found on network b460b092-9482-48ee-9c5f-04ec384fbc8c"
935,1287432,neutron,dba196702b43f90928a644bdafaf42c97b71267f,0,0,No bug. ‘ should be renamed’,Bug #1287432 “nec plugin,"ID mapping tables in NEC plugin have columns named ""quantum_id"". Following renaming to Neutron, they should be renamed."
937,1287476,cinder,0e44ba4c273f41195d0361d3c48df76e2e5add76,0,0,Add initiator_target_map for IBM Storwize/SVC,IBM Storwize/SVC driver does not support initiator...,Fibre Channel zoning code requires that the storage drivers return an initiator_target_map from initialize_connection and terminate_connection.
938,1287495,glance,e7c1bdf146c0492a7f122399b099f0fee8b6d125,0,0,No bug. should check,VMware parse_uri method should check for the schem...,"Need to add a check in parse_uri(...) [1] to make sure that a URI with the vsphere scheme is provided. Throw a BadStoreUri otherwise.
[1] https://github.com/openstack/glance/blob/master/glance/store/vmware_datastore.py#L151"
939,1287524,neutron,40390598c5a440d1bbfa4f229130eeedf5cd4dba,0,0,'This code could be simplified’,ip_lib netns.execute should work with or without n...,"There are a number of places in the neutron code that run an ip command like this:
        if self.network.namespace:
            ip_wrapper = ip_lib.IPWrapper(self.root_helper,
                                          self.network.namespace)
            ip_wrapper.netns.execute(cmd)
        else:
            utils.execute(cmd, self.root_helper)
This code could be simplified if netns.execute simply checked if there was a namespace defined or not."
940,1287542,nova,5eafe1ccb30462d9b78b13323135fb9f90d1fd54,1,0,Bug because of evolution,Error importing module nova.openstack.common.sslut...,"Error importing module nova.openstack.common.sslutils: duplicate option: ca_file
This is seen in the nova gate - for unrelated patches - it might be a bad slave I guess, or it might be happening to all  subsequent patches, or it might be a WTF.
http://logstash.openstack.org/#eyJzZWFyY2giOiJcIkVycm9yIGltcG9ydGluZyBtb2R1bGUgbm92YS5vcGVuc3RhY2suY29tbW9uLnNzbHV0aWxzOiBkdXBsaWNhdGUgb3B0aW9uOiBjYV9maWxlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjkwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzOTM5MTUyNTE4ODl9 suggest it has only happened once so far.
commit 5188052937219badaa692f67d9f98623c15d1de2
Merge: af626d0 88b7380
Author: Jenkins <email address hidden>
Date:   Tue Mar 4 02:47:02 2014 +0000
    Merge ""Sync latest config file generator from oslo-incubator""
Was the latest merge prior to this, but it may be coincidental."
941,1287643,cinder,e8fac5eda3309a19ee2e1b36883a4e30476644a9,1,1,Fix conversion methods,NaElement translate_struct does not work for non u...,Method translate_struct of NaElement does not work properly in case of unique tags.
942,1287760,glance,412e793780665b43c134ddd59907766549ecadd8,0,0,No bug: Task create should return 'Location' header ,Task create should return 'Location' header,"When creating a new task, the api correctly returns a 201. However per spec it should also return a 'Location' header with the URI to the newly created resource. Currently it does not."
943,1287844,nova,f5bc15687bcab1e0e4a2163a194c04bd8c5d14d9,0,0,We need to raise a exception,Bug #1287844 “VMware,We need to raise an exception for an disk_format that is not supported
944,1287944,cinder,48955e56b886c0da51c1555aca62e099761ad99b,1,1,Many bugs in 1 commit,EMC VNX Direct Driver needs to be cleaned up,"There are quite a few review comments on the EMC VNX Direct Driver:
https://review.openstack.org/#/c/73672
These issues need to be addressed right after the code is merged."
945,1287945,nova,bb83f1a743150779e190705d2875a2319720c60c,1,1,,Instance doesn't have a task state,"I am getting with the latest version of the trunk:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/service.py"", line 480, in run_service
    service.start()
  File ""/opt/stack/nova/nova/service.py"", line 180, in start
    self.manager.init_host()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 824, in init_host
    self._init_instance(context, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 684, in _init_instance
    if instance.task_state == task_states.DELETING:
AttributeError: 'dict' object has no attribute 'task_state'
Removing descriptor: 6
2014-03-04 13:59:14.304 ERROR nova.openstack.common.threadgroup [-] 'dict' object has no attribute 'task_state'
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 117, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     x.wait()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 49, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     return self.thread.wait()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     return self._exit_event.wait()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     return hubs.get_hub().switch()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 480, in run_service
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     service.start()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/service.py"", line 180, in start
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     self.manager.init_host()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 824, in init_host
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     self._init_instance(context, instance)
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 684, in _init_instance
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     if instance.task_state == task_states.DELETING:
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup AttributeError: 'dict' object has no attribute 'task_state'
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup
I get this exception when creating a snapshot, put a breakpoint in the virt layer (in this case vmware_images/snapshot), stop nova-cpu, restart nova-cpu, delete the snapshot.
nova-cpu crashes."
946,1288178,nova,a88d9d5936aabc52046ed3ae566741422bfaed78,0,0,Rewrite nova policy,Sync new policy from oslo,"The oslo has changed the common policy for a long time, using a Enforer class to replace the old check function .In order to sync the common policy to nova, we have to rewrite the nova policy and the related unittests."
947,1288188,neutron,6206d555c3cd129a9ea83174ee786e1e15a4c48c,1,1,Fix usage of save_and_reraise_exception,unwanted lbaas related error logs in q-svc screen,"2014-03-03 09:25:31.621 5910 ERROR root [-] Original exception being dropped: ['Traceback (most recent call last):\n', '  File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 206, in _get_resource\n    r = self._get_by_id(context, model, id)\n', '  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 144, in _get_by_id\n    return query.filter(model.id == id).one()\n', '  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2323, in one\n    raise orm_exc.NoResultFound(""No row was found for one()"")\n', 'NoResultFound: No row was found for one()\n']
2014-03-03 09:25:31.622 5910 WARNING neutron.services.loadbalancer.drivers.common.agent_driver_base [req-6949f3b4-f991-48b7-8424-b3bfbca7c822 None] Cannot update status: member d439c879-55f7-400f-b6a8-32753f057b05 not found in the DB, it was probably deleted concurrently
There is no need for error log about original exception being dropped as warning log is enough.
This happens due to using save_and_reraise_exception() in loadbalancer_db code:
    def _get_resource(self, context, model, id):
        try:
            r = self._get_by_id(context, model, id)
        except exc.NoResultFound:
            with excutils.save_and_reraise_exception():
                if issubclass(model, Vip):
                    raise loadbalancer.VipNotFound(vip_id=id)
                elif issubclass(model, Pool):
                    raise loadbalancer.PoolNotFound(pool_id=id)
                elif issubclass(model, Member):
                    raise loadbalancer.MemberNotFound(member_id=id)
                elif issubclass(model, HealthMonitor):
                    raise loadbalancer.HealthMonitorNotFound(monitor_id=id)
        return r
where the whole purpose of exception handler is to reraise proper type of exception.
I think save_and_reraise_exception() was designed for cases when new exceptions raised inside exception handler are not expected.
In this particular case I don't see the reason for using save_and_reraise_exception().
As an option I think a parameter can be added to save_and_reraise_exception() constructor to disable logging."
948,1288281,nova,b69584a523f41f8171805e22ac6a0b29172e832e,0,0,This can mislead…,Change parameters of add_timestamp in ComputeDrive...,'cls' may mislead developpers that this Decorator return a classmethod.  Change parameter 'cls' to 'self' in wrapper.
949,1288283,cinder,b2447503b0ba98e644439d31435d47435c46cd69,1,1,Fix the incorrect behavior,NetApp QOS extra spec is not implemented properly,"The NetApp NFS and iSCSI QOS extra spec for volume types is not implemented correctly.  It currently requires a QOS policy to be applied at the flexVol level.  The scheduler then assigns a new cinder volume to the flexVol which has the QOS policy applied to it.  This results in a situation where multiple cinder volumes are all fighting for the same QOS limits, rather than each getting the implied limit.  For example:
QOS policy of 100 MB/s is applied to a flexVol by the NetApp admin.
5 cinder volumes are created with the 100 MB/s QOS policy applied via volume-type. They are all placed into the flexVol created in the first step.
These 5 cinder volumes are now fighting each other for the 100 MB/s that the flexVol has allocated to it.
The expected behavior is that each cinder volume would independently have their own 100 MB/s limit, not a combined limit.
In order to do this, the QOS policy should be applied at the LUN level for the iSCSI driver.  The NFS driver is another can of worms, as I'm not aware of a way to apply a QOS policy to a file."
950,1288296,nova,2b2decfd1b5b0d68813bc7e5b5e4349a1ab592aa,1,1,Conflict. Not allow duplicates names.,Update aggregate allows duplicate names,"The behaviour to manage naming conflicts is different between aggregate creation and aggregate update.
Aggregate create doesn't let you create 2 aggregates with the same name.
Aggregate update lets you update an aggregate to a name that already exists.
It seems to me it should be consistent, and probably both check for conflict.
Here's an example, using a recent devstack:
$ nova aggregate-create test
+----+------+-------------------+-------+----------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+------+-------------------+-------+----------+
| 14 | test | -                 |       |          |
+----+------+-------------------+-------+----------+
$ nova aggregate-create test
ERROR: There was a conflict when trying to complete your request. (HTTP 409) (Request-ID: req-6711e05e-4efc-4a2d-9117-52d034c74a4f)
$ nova aggregate-create test2
+----+-------+-------------------+-------+----------+
| Id | Name  | Availability Zone | Hosts | Metadata |
+----+-------+-------------------+-------+----------+
| 15 | test2 | -                 |       |          |
+----+-------+-------------------+-------+----------+
$ nova aggregate-update 15 test
Aggregate 15 has been successfully updated.
+----+------+-------------------+-------+----------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+------+-------------------+-------+----------+
| 15 | test | -                 |       |          |
+----+------+-------------------+-------+----------+
$ nova aggregate-list
+----+--------------------+-------------------+
| Id | Name               | Availability Zone |
+----+--------------------+-------------------+
| 14 | test               | -                 |
| 15 | test               | -                 |
+----+--------------------+-------------------+
Nova api logs from when the aggregate creation fails as expected:
2014-04-05 14:45:34.865 INFO nova.api.openstack.compute.contrib.aggregates [req-aeb307d4-c4c9-4684-8b13-1d81f0b8c692 admin demo] Aggregate test already exists.
2014-04-05 14:45:34.865 INFO nova.api.openstack.wsgi [req-aeb307d4-c4c9-4684-8b13-1d81f0b8c692 admin demo] HTTP exception thrown: There was a conflict when trying to complete your request.
2014-04-05 14:45:34.865 DEBUG nova.api.openstack.wsgi [req-aeb307d4-c4c9-4684-8b13-1d81f0b8c692 admin demo] Returning 409 to user: There was a conflict when trying to complete your request. from (pid=1517) __call__ /opt/stack/nova/nova/api/openstack/wsgi.py:1223"
951,1288337,glance,4da02aeabc9bb072bd33b21a1c8fb6e10cc4e262,0,0,remove code,Remove task-specific validation from tasks resourc...,"Currently there is import-specific validation of the input field hard-coded into the create method of the tasks resource. Since this field is provider specific, validation should be left up to the actual import process. The task resource should only validate the fields common to all tasks (type and input - presence only)."
952,1288358,neutron,466e89970f11918a809aafe8a048d138d4664299,1,0,Architecture/db specific,mysql engine InnoDB not being used on centos or re...,"when using centos or redhat 6.5, it has mysql 5.1, with default engine of myISAM.  Openstack projects require InnoDB.  I see some code, neutron/db/model_base.py#L25,  that looks to try to set the engine type for create tables.  But after an install, tables are instead created with default engine myISAM.
Where seeing this on neutron and ceilometer."
953,1288379,neutron,2648aa3561d23e1215e0cc6f446253e5df56c8f6,1,1,deadlock,db Deadlock detected when running 'delete_port,"I'm seeing Deadlocks when deleting large numbers of VMs in a multinode system.
The port delete fails, and then ports are left behind after the VMs are deleted.
VMs cannot be created as the IP are not released.
The ports have to be manually deleted.
2014-02-24 15:18:12.606 1819 ERROR neutron.api.v2.resource [-] delete failed
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/api/v2/resource.py"", line 84, in resource
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/api/v2/base.py"", line 432, in delete
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/openvswitch/ovs_neutron_plugin.py"", line 634, in delete_port
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     super(OVSNeutronPluginV2, self).delete_port(context, id)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 1403, in delete_port
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     self._delete_port(context, id)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 1425, in _delete_port
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     a['ip_address'])
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 415, in _recycle_ip
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     ip_address)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 449, in _delete_ip_allocation
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     subnet_id=subnet_id).delete()
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2581, in delete
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     delete_op.exec_()
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/persistence.py"", line 816, in exec_
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     self._do_exec()
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/persistence.py"", line 942, in _do_exec
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     params=self.query._params)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 531, in _wrap
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     _raise_if_deadlock_error(e, get_engine().name)"
954,1288392,nova,c47900064f2f3b4384cfa3fd6fe21f452514fa36,1,1,Bug. Remove some callls,instances get stuck in ERROR/deleting when Neutron...,"We had a temporary outage of Neutron, and many instances got stuck in this state. 'nova delete' on them does not work until nova-compute is forcibly restarted.
+--------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Property                             | Value                                                                                                                                                                                                                      |
+--------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                                                                                                                                                                     |
| OS-EXT-AZ:availability_zone          | nova                                                                                                                                                                                                                       |
| OS-EXT-SRV-ATTR:host                 | ci-overcloud-novacompute1-4q2dbhdklrkq                                                                                                                                                                                     |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | ci-overcloud-novacompute1-4q2dbhdklrkq.novalocal                                                                                                                                                                           |
| OS-EXT-SRV-ATTR:instance_name        | instance-00003f80                                                                                                                                                                                                          |
| OS-EXT-STS:power_state               | 1                                                                                                                                                                                                                          |
| OS-EXT-STS:task_state                | deleting                                                                                                                                                                                                                   |
| OS-EXT-STS:vm_state                  | error                                                                                                                                                                                                                      |
| OS-SRV-USG:launched_at               | 2014-03-05T03:54:49.000000                                                                                                                                                                                                 |
| OS-SRV-USG:terminated_at             | -                                                                                                                                                                                                                          |
| accessIPv4                           |                                                                                                                                                                                                                            |
| accessIPv6                           |                                                                                                                                                                                                                            |
| config_drive                         |                                                                                                                                                                                                                            |
| created                              | 2014-03-05T03:46:25Z                                                                                                                                                                                                       |
| default-net network                  | 10.0.58.225                                                                                                                                                                                                                |
| fault                                | {""message"": ""Connection to neutron failed: Maximum attempts reached"", ""code"": 500, ""details"": ""  File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 253, in decorated_function |
|                                      |     return function(self, context, *args, **kwargs)                                                                                                                                                                        |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2038, in terminate_instance                                                                                               |
|                                      |     do_terminate_instance(instance, bdms)                                                                                                                                                                                  |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/lockutils.py\"", line 249, in inner                                                                                                  |
|                                      |     return f(*args, **kwargs)                                                                                                                                                                                              |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2036, in do_terminate_instance                                                                                            |
|                                      |     self._set_instance_error_state(context, instance['uuid'])                                                                                                                                                              |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/excutils.py\"", line 68, in __exit__                                                                                                 |
|                                      |     six.reraise(self.type_, self.value, self.tb)                                                                                                                                                                           |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2026, in do_terminate_instance                                                                                            |
|                                      |     reservations=reservations)                                                                                                                                                                                             |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/hooks.py\"", line 103, in inner                                                                                                                       |
|                                      |     rv = f(*args, **kwargs)                                                                                                                                                                                                |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2005, in _delete_instance                                                                                                 |
|                                      |     user_id=user_id)                                                                                                                                                                                                       |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/excutils.py\"", line 68, in __exit__                                                                                                 |
|                                      |     six.reraise(self.type_, self.value, self.tb)                                                                                                                                                                           |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 1975, in _delete_instance                                                                                                 |
|                                      |     self._shutdown_instance(context, db_inst, bdms)                                                                                                                                                                        |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 1884, in _shutdown_instance                                                                                               |
|                                      |     network_info = self._get_instance_nw_info(context, instance)                                                                                                                                                           |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 902, in _get_instance_nw_info                                                                                             |
|                                      |     instance)                                                                                                                                                                                                              |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/api.py\"", line 48, in wrapper                                                                                                                |
|                                      |     res = f(self, context, *args, **kwargs)                                                                                                                                                                                |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/neutronv2/api.py\"", line 445, in get_instance_nw_info                                                                                        |
|                                      |     result = self._get_instance_nw_info(context, instance, networks)                                                                                                                                                       |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/neutronv2/api.py\"", line 452, in _get_instance_nw_info                                                                                       |
|                                      |     nw_info = self._build_network_info_model(context, instance, networks)                                                                                                                                                  |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/neutronv2/api.py\"", line 1010, in _build_network_info_model                                                                                  |
|                                      |     data = client.list_ports(**search_opts)                                                                                                                                                                                |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 112, in with_params                                                                                                  |
|                                      |     ret = self.function(instance, *args, **kwargs)                                                                                                                                                                         |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 307, in list_ports                                                                                                   |
|                                      |     **_params)                                                                                                                                                                                                             |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1251, in list                                                                                                        |
|                                      |     for r in self._pagination(collection, path, **params):                                                                                                                                                                 |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1264, in _pagination                                                                                                 |
|                                      |     res = self.get(path, params=params)                                                                                                                                                                                    |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1237, in get                                                                                                         |
|                                      |     headers=headers, params=params)                                                                                                                                                                                        |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1229, in retry_request                                                                                               |
|                                      |     raise exceptions.ConnectionFailed(reason=_(\""Maximum attempts reached\""))                                                                                                                                              |
|                                      | "", ""created"": ""2014-03-05T04:22:04Z""}                                                                                                                                                                                      |
| flavor                               | h1.large (872d8f61-c45a-45c3-87da-466d9f0f241b)                                                                                                                                                                            |
| hostId                               | 0bab209cc6f26a8d5c4bc76e3da39d4fa68e5fefc6e5c0eada7a90d2                                                                                                                                                                   |
| id                                   | ae9c75e3-51d2-43a3-8b20-34375b4c72d3                                                                                                                                                                                       |
| image                                | tripleo-precise-1393812840.template.openstack.org (114e4b92-567e-4348-9ed8-e88281104208)                                                                                                                                   |
| key_name                             | -                                                                                                                                                                                                                          |
| metadata                             | {}                                                                                                                                                                                                                         |
| name                                 | tripleo-precise-tripleo-test-cloud-2125650.slave.openstack.org                                                                                                                                                             |
| os-extended-volumes:volumes_attached | []                                                                                                                                                                                                                         |
| security_groups                      | default, default                                                                                                                                                                                                           |
| status                               | ERROR                                                                                                                                                                                                                      |
| tenant_id                            | 64d2d3bc07084ef1accd4e3502909c77                                                                                                                                                                                           |
| tripleo-bm-test network              | 192.168.1.78                                                                                                                                                                                                               |
| updated                              | 2014-03-05T04:22:04Z                                                                                                                                                                                                       |
| user_id                              | 35ef3ce265cb4a25b5303f3daa143f4e                                                                                                                                                                                           |
+--------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
955,1288407,neutron,f83b2ef4fc042d8559b1f75dbe949460f2843c7e,1,1,,Fix segment allocation tables in Cisco N1kv plugin...,"The segment ranges for VLAN and VXLAN are being populated using an in memory dictionary. The segment allocation table is emptied on deleting any network profile.
This change allows the use of segment range from the network profile table.
By using the network profile UUID as a foreign key in the segment allocations table, tables are cleaned up only for the segments associated with
the deleted network profile via CASCADE, leaving no inconsistencies.
Add more UT along with this."
956,1288420,neutron,c9226a858201a6c3691390259b46a643e8dc8b2f,0,0, IBM Plugin need db migration,IBM Plugin need db migration for ext gw mode,The plugin requires one more Alembic migration script; needs to be added to the list of plugins in the script for ext_gw_mode.
957,1288441,neutron,1959092bca5646306cfec5303f43627e78f6e2d4,0,0,tests,FWaaS tests don't delete firewalls,"When the tests for FWaaS create a firewall and then delete it, the firewall object is still in the database in a PENDING_DELETE state waiting for the agent to remove it. Parent objects then can't be deleted because the firewall object is dependent on them.
Currently, all of the tests just leave the parent objects by using no_delete[1], which leaves things in the database and could lead to potential conflicts later.
1. https://github.com/openstack/neutron/blob/ac8c0c645de001a0d074cdfd9448f9680a5d5e34/neutron/tests/unit/db/firewall/test_db_firewall.py#L732"
958,1288463,nova,fa73a5d037471dbec11893ad83bd93276968b9af,1,1,Security bug,neutron_metadata_proxy_shared_secret should not be...,neutron_metadata_proxy_shared_secret should not be written to log file
959,1288492,neutron,7f6a6cdfc95610cea0a6e4bb01e7de330e1f76d4,1,1,typo in code,Typo in floating ip migration script,"Missing comma here:
https://github.com/openstack/neutron/blob/30f6f2fa4e6e73cf8045febbd1e8d26360714ac5/neutron/db/migration/alembic_migrations/versions/2eeaf963a447_floatingip_status.py#L38"
960,1288574,nova,3824051b1e5618388a17c88867a3037397bc96b7,0,0,no bug yet. it will be an issue if too many backup failed,backup operation should delete image if snapshot f...,"when we snapshot an instance, we will use @delete_image_on_error to delete any failed snapshot
however, the image will not be removed by backup code flow, it will be an issue if too many backup failed
at last ,all useful image will be removed and we have only 'error' image left in host"
961,1288609,nova,e251535eaa7c6a564eddb9218de177c9bea656bb,1,1,wrong,nova-manage creates network with wrong vlanid,"I faced a bug in latest nova network:
[root@host awasilyev]# /usr/bin/nova-manage network create novanetwork 172.26.0.0/24  --vlan 500
[root@host awasilyev]# /usr/bin/nova-manage network create novanetwork 172.26.1.0/24  --vlan 501
[root@host awasilyev]# /usr/bin/nova-manage network create novanetwork 172.26.2.0/24  --vlan 502
[root@host awasilyev]# nova-manage network list
id   	IPv4              	IPv6           	start address  	DNS1           	DNS2           	VlanID         	project        	uuid
64   	172.26.0.0/24     	None           	172.26.0.3     	8.8.4.4        	None           	500            	None           	225c8cbf-89bb-4171-b405-0047012a7803
65   	172.26.1.0/24     	None           	172.26.1.3     	8.8.4.4        	None           	502            	None           	d461b285-d9c6-4a8c-ae39-5a657bb5926a
66   	172.26.2.0/24     	None           	172.26.2.3     	8.8.4.4        	None           	504            	None           	4c5a5d5b-24c8-4833-8bd0-6dcca11acb68
I try to create 3 networks, specifying exact vlan number for each network. But nova-manage creates networks using wrong vlan id's.
My previous openstack install (it was 3-4 monthes ago) does not have this bug."
962,1288645,cinder,df03a9b9b7025a0e13d9387a74c304875dec0918,1,1,keyerror,"Bug #1288645 "" KeyError","when using iSCSI protocol to conenct IBM v7000, starting cinder-volume service maybe occur KeyError.
log:
2014-03-06 01:48:26.409 25160 ERROR cinder.volume.manager [req-ac4f3744-1826-4456-a726-1d07ba5cb37e None] CN-18C9F33 Error encountered during initialization of driver: StorwizeSVCDriver
2014-03-06 01:48:26.409 25160 ERROR cinder.volume.manager [req-ac4f3744-1826-4456-a726-1d07ba5cb37e None] 'license_compression_enclosures'
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager Traceback (most recent call last):
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 207, in init_host
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager     self.driver.do_setup(ctxt)
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 153, in do_setup
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager     self._helpers.compression_enabled()
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 54, in compression_enabled
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager     if resp[key] != '0':
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager KeyError: 'license_compression_enclosures'
it should lack of  license_compression_enclosures when compression is not enabled for the system. so need add checking in the code."
963,1288661,nova,1284c4d2e471809635a070289ace0049abf38f18,1,1,typo in code,log messages typos in rebuild_instance function,"one simple log typos in compute/manager.py rebuild_instance function.
             else:
                    image_ref = orig_image_ref = instance.image_ref
                    LOG.info(_(""disk not on shared storagerebuilding from:""
                               "" '%s'"") % str(image_ref))
should change to
                    LOG.info(_(""disk not on shared storage, rebuilding from:""
                               "" '%s'"") % str(image_ref))"
964,1288809,nova,fa2f139e22ed1317f4afe01faaf7ee3943444715,1,1,incorrect allocation,Bug #1288809 “pci passthrough,"The PCI filter for scheduling runs on the basis of pci stats pools available in a compute node. If the PCI requests match one or more pools, the number of devices will be subtracted from the total number of devices available in those pools.
On the compute node, PCI device allocation is performed based on the list of free devices that are available on the node. The PCI requests are used to match against the device itself, rather than the pci stats pools.
The unsymmetrical handling of scheduling versus allocating could cause incorrect pci stats, and thus incorrect scheduling, and nova instances failed to boot."
965,1288816,cinder,5119d323650f97f12e12fdb6a2a80e46739629fc,1,0,Adds xiv_chap to xiv/ds8k driver configuration,xiv_ds8k driver fails chap support due to missing ...,"CHAP encryption is supported in the xiv_ds8k cinder driver, but the value specified in the cinder conf file is not passed and the feature does not work.
The fix is minimal (patch attached)."
966,1288915,neutron,13fd601415418407bb2d2f6bafffb470e5879df6,0,0,Improve message,Unclear message when try show quotas of different ...,"When a quota-show for a different tenant is requested without admin permissions, you get the following message that is confusing and could be improved.
""Non-admin is not authorised to access quotas for another tenant"""
967,1288923,neutron,d5c0a37999f9e3a611a322baacabebc06b13283b,1,1,There is a bug. (Maybe make a feature(,Failover of a network from one dhcp agent to anoth...,"Failing over a network from one dhcp agent to another results in a new IP address for the dhcp port.  This breaks dns for all vms on that network.    This can be reproduced by simply doing a ""neutron dhcp-agent-network-remove"" and then a ""neutron dhcp-agent-network-add"" and observing that the dhcp port ip address will change."
968,1288926,nova,00f5125745dc72afbc9aeade8b780d7a3be49a30,1,1,bug rebooting,incorrect error code when rebooting a rebooting_ha...,"This is using the latest nova from trunk. In our deployment, we had a hypervisor go down and the tenant issued a hard reboot prior. When attempting a reboot on a guest with the state HARD_REBOOT, nova controller throws this error in it's logs and returns 'ERROR: The server has either erred or is incapable of performing the requested operation. (HTTP 500)' to the user:
2014-03-06 18:21:00,535 (routes.middleware): DEBUG middleware __call__ Matched POST /tenant1/servers/778032b2-469d-445e-abde-7b9b0b673324/action
2014-03-06 18:21:00,536 (routes.middleware): DEBUG middleware __call__ Route path: '/{project_id}/servers/:(id)/action', defaults: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x5242c90>}
2014-03-06 18:21:00,536 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x5242c90>, 'project_id': u'tenant1', 'id': u'778032b2-469d-445e-abde-7b9b0b673324'}
2014-03-06 18:21:00,537 (nova.api.openstack.wsgi): DEBUG wsgi _process_stack Action: 'action', body: {""reboot"": {""type"": ""SOFT""}}
2014-03-06 18:21:00,538 (nova.api.openstack.wsgi): DEBUG wsgi _process_stack Calling method <bound method Controller._action_reboot of <nova.api.openstack.compute.contrib.keypairs.Controller object at 0x4c35a50>>
2014-03-06 18:21:00,747 (nova.api.openstack): ERROR __init__ _error Caught error: Unexpected task state: expecting [None, 'rebooting'] but the actual state is rebooting_hard
Traceback (most recent call last):
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/__init__.py"", line 125, in __call__
    return req.get_response(self.application)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/request.py"", line 1320, in send
    application, catch_exc_info=False)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/request.py"", line 1284, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/keystoneclient/middleware/auth_token.py"", line 598, in __call__
    return self.app(env, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/routes/middleware.py"", line 131, in __call__
    response = self.app(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/wsgi.py"", line 925, in __call__
    content_type, body, accept)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/wsgi.py"", line 987, in _process_stack
    action_result = self.dispatch(meth, request, action_args)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/wsgi.py"", line 1074, in dispatch
    return method(req=request, **action_args)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/compute/servers.py"", line 1145, in _action_reboot
    self.compute_api.reboot(context, instance, reboot_type)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 199, in wrapped
    return func(self, context, target, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 189, in inner
    return function(self, context, instance, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 170, in inner
    return f(self, context, instance, *args, **kw)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 2073, in reboot
    instance.save(expected_task_state=[None, task_states.REBOOTING])
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/objects/base.py"", line 151, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/objects/instance.py"", line 472, in save
    columns_to_join=_expected_cols(expected_attrs))
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/api.py"", line 739, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py"", line 128, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py"", line 2164, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py"", line 2215, in _instance_update
    actual=actual_state, expected=expected)
UnexpectedTaskStateError: Unexpected task state: expecting [None, 'rebooting'] but the actual state is rebooting_hard
2014-03-06 18:21:00,750 (nova.api.openstack): INFO __init__ _error http://nova-controller.isg.apple.com:8774/v2/tenant1/servers/778032b2-469d-445e-abde-7b9b0b673324/action returned with HTTP 500
The actual error message back to the user must be something along the lines of 'Unexpected task state: expecting [None, 'rebooting'] but the actual state is rebooting_hard' with a 4xx HTTP code."
969,1288962,cinder,aa4a89eda8941bce22ead9db2dd5f7bc6ca90e04,0,0,No bug. doesn't enforce 32 block-ranges limit,NetApp iSCSI driver doesn't enforce 32 block-range...,"The NetApp zapi for clone create has an undocumented limit of 32 block ranges (of a max 2^24 blocks each).
The driver should check to make sure the number of block range segments does not exceed 32.  In the case of an excessively large request, the xml may be rejected by the filer silently and the volume will become stuck in an extending state, with 2 flexVols (the original and the new sized one) present on the filer."
970,1289007,neutron,4559a5f381082dba30bb535b69deeb09135da680,1,1,This leads to erroneous values ,Hyper-V agent does not count metrics for individua...,"The Hyper-V agent is currently obtaining aggregated metrics instead of values for each individual port.
This leads to erroneous values in case instances have multiple ports."
971,1289027,neutron,f64eacfd27220c180f6afc979087b35aa1385550,0,0,improve unit test coverage,Bug #1289027 “BigSwitch,The server manager component of the BigSwitch plugin needs more unit tests to exercise some functions that are currently mocked out as part of the existing unit tests.
972,1289039,neutron,fe2ef9f0dd1f33b61fe11eebf2ae3d577a73ef53,1,0,Bug due to evolution,Cisco Neutron plugin fails in DB migration,"For Cisco Neutron plugin:
Alembic migration from revision f44ab9871cd6 to 2eeaf963a447 fails because the floatingips table doesn't exist.
The fix is to add the plugin to the DB migration path for L3."
973,1289066,neutron,fe2ca9a75878a445a54ecfe4a97c79b696abf503,1,1,,L3 Agent cannot process RPC messages until _sync_r...,"When L3 agent starts or restarts, it almost immediately goes in to a _sync_routers_task run.  This task is synchronized with _rpc_loop so that only one can happen at a time.
The problem with this is that -- at least at scale -- the _sync_routers_task can take a VERY LONG time to run.  I've observed it take 1-2 hours!  This is WAY too long to wait before I can do something with my router like add a floating ip.
The thing is, _sync_routers_task is important to do periodically but it is mostly just checking that things are still in the right state.  It should never take precedence over responding to RPC messages.  The RPC messages represent work that the system has just been asked to perform.  It is silly to make it wait a long time for a maintenance task to complete."
974,1289079,nova,adeeabf313a15162c45d4e6ba6cf9596f318f3ed,1,1,There is a bug. Add exception,using “nova diagnostics,"Reproduce:
1. nova stop cirros
2.nova diagnostics cirros
[root@control-compute00 ~(keystone_admin)]# nova diagnostics cirros
ERROR: 'NoneType' object has no attribute 'iteritems'"
975,1289100,neutron,5d782b33b772987b472ae5aeb5ec13e1bd30335f,0,0,"For conformity wit Neutron agents, this needs to be added",SDN-VE plugin agent does not report its state,"The agent for SDN-VE plugin does not show its state periodically and the plugin does not support the ""agent"" extension. For conformity wit Neutron agents, this needs to be added to the plugin and the agent."
976,1289130,neutron,fb9886b903434321e62373cb4c11ba014921e4df,1,1,Fix bad error msg in some cases,ERROR log “No DHCP agents are associated with netw...,"dhcp-agent notifier outputs ERROR log ""No DHCP agents are associated with network""
but it is not an error and valid cases in most cases. It is so annoying for debugging and monitoring.
No dhcp-agent association for a network was logged as ERROR level. However it is completely incorrect for network_create_end, and usually wrong for subnet_create_end (because a subnet is created just after a network is created in most cases). We should not log it for these cases.
For other cases, a dhcp-agent is usually associated with a network, and no dhcp-agent assocation might be a symptom of some error. On the other hand there are valid cases where no dhcp-agent is associated (e.g., delete dhcp-agent association intentionally, network/subnet_update_end before a port is created). Considering these and the fact that error will be logged in
dhcp-agent scheduler, it would be better to be logged as INFO level now.
2014-03-07 02:29:08.533 5415 ERROR neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-0babab8c-d638-4f1e-aef3-d58ad6af8b86 None] No DHCP agents are associated with network '6c3c60c7-2fce-4513-971c-b49b98d153c8'. Unable to send notification for 'network_create_end' with payload: {'network': {'status': 'ACTIVE', 'subnets': [], 'name': u'private', 'provider:physical_network': None, 'admin_state_up': True, 'tenant_id': u'33f39be0287d4316a29a4247dcd6db5c', 'provider:network_type': u'local', 'shared': False, 'id': '6c3c60c7-2fce-4513-971c-b49b98d153c8', 'provider:segmentation_id': None}}
http://logs.openstack.org/35/78835/1/check/check-tempest-dsvm-neutron/cd96f2f/logs/screen-q-svc.txt.gz?level=ERROR"
977,1289132,neutron,97c0723cfe7ffa1dbc6c278110029df755df8045,1,1,,Bug #1289132 “BigSwitch,"If the server port is misconfigured, the plugin dies with an unfriendly error when starting the BigSwitch plugin.
e.g. ValueError: invalid literal for int() with base 10: 'a'"
978,1289134,neutron,84368554d8a13a421297298de40693956af25fcd,1,1,bad formating error,Bug #1289134 “BigSwitch,"In an exception handling case a config error is raised; however, the call is currently formatted incorrectly. It passes the details as a second parameter rather than performing the string formatting in place.
It also refers to strerror, which may not be present on all exception types that could be raised during the certificate retrieval.
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L358"
979,1289138,neutron,caf7ecaef75f4d9eba0d85db8917be5a9118792d,1,1,bad call,Bug #1289138 “BigSwitch,"The consistency watchdog for the BigSwitch plugin server manager currently does not work. It incorrectly calls rest_call on a servers list rather than calling it on the server pool object.
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L554"
980,1289164,nova,27b405e46c62e7084ac8db038f4b213a30dc3d23,1,1,No exception handled.. Error raised,FloatingIpNotFoundForHost isn't handled when showi...,"When showing the floating ips by host, I got this error.
$ nova floating-ip-bulk-list --host xxx
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-9a17c8cf-b1cd-4092-b853-7e24126db7e8)
and in the nova-api log I got this message:
014-03-05 04:01:40.270 ERROR nova.api.openstack [req-9a17c8cf-b1cd-4092-b853-7e24126db7e8 admin demo] Caught error: Floating ip not found for host xxx.
2014-03-05 04:01:40.270 TRACE nova.api.openstack Traceback (most recent call last):
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/__init__.py"", line 125, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return req.get_response(self.application)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2014-03-05 04:01:40.270 TRACE nova.api.openstack     application, catch_exc_info=False)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2014-03-05 04:01:40.270 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 598, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return self.app(env, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     response = self.app(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 929, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     content_type, body, accept)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 991, in _process_stack
2014-03-05 04:01:40.270 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 1078, in dispatch
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return method(req=request, **action_args)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips_bulk.py"", line 48, in show
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return self._get_floating_ip_info(context, id)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips_bulk.py"", line 57, in _get_floating_ip_info
2014-03-05 04:01:40.270 TRACE nova.api.openstack     floating_ips = db.floating_ip_get_all_by_host(context, host)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/api.py"", line 356, in floating_ip_get_all_by_host
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return IMPL.floating_ip_get_all_by_host(context, host)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 110, in wrapper
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return f(*args, **kwargs)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 918, in floating_ip_get_all_by_host
2014-03-05 04:01:40.270 TRACE nova.api.openstack     raise exception.FloatingIpNotFoundForHost(host=host)
2014-03-05 04:01:40.270 TRACE nova.api.openstack FloatingIpNotFoundForHost: Floating ip not found for host xxx.
2014-03-05 04:01:40.270 TRACE nova.api.openstack
The FloatingIpNotFoundForHost exception should be handled."
981,1289192,neutron,288e3127440158f177beaae1972236def4916251,1,1,Fix certificate file helper functions ,Bug #1289192 “BigSwitch,"The BigSwitch plugin has helper methods for writing certificates to the file system that are incorrectly defined.
They are missing the self argument that will be passed in.
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L368
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L319
The unit tests were also incorrect in this case since they were refactored at right before the merge to avoid any file-system writes during unit tests."
982,1289230,cinder,7f643ca464194067c70d86efdc262daa9d2d3a92,1,1,typo in code,Conversion types is missing in some strings,"Ex.:
             except nexenta.NexentaException as exc:
-                LOG.warning(_('Cannot delete snapshot %(origin): %(exc)s'),
+                LOG.warning(_('Cannot delete snapshot %(origin)s: %(exc)s'),
                             {'origin': origin, 'exc': exc})"
983,1289236,neutron,815dd8c4ea026b4064a853c7ad5bab47afec06b6,0,0,Change the except. Exception very narrow,Bug #1289236 “BigSwitch,"The BigSwitch plugin currently determines if it needs to reconnect by checking for an httplib ImproperConnectionState exception. However, this exception is too narrow and does not cover the other httplib exceptions that indicate a reconnection is necessary (e.g. NotConnected).
It should just catch httplib.HTTPExceptions."
984,1289256,neutron,f5601393e97baf87daa5b2065cb8783e3fd34214,1,1,Bad usage,Incorrect usage of sqlalchemy type Integer,"In migration folsom_initial in cisco_upgrade function table nexusport_bindings create column vlan_id with incorrect type Integer(255). It causes the following exception:
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 87, in upgrade
    upgrade_cisco()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 455, in upgrade_cisco
    sa.Column('vlan_id', sa.Integer(255)),
TypeError: object() takes no parameters"
985,1289361,nova,527d9e532d1c2692fdd367314d98c37237ecc834,1,1,,Bug #1289361 “xenapi,The resize ephemeral disk blueprint has regressed the ability to spawn instances with ephemeral disks.
986,1289382,cinder,aca0fa8354670d2d7b5ae0a65a539db4cf455995,1,1,Bad log raise exception,GPFS driver Log statement creates exception if hit...,"String formatting template Issues
Name collision on the _ variable"
987,1289397,nova,a516ae71d57eda013f7cb9428f945dfea08ead3e,1,1,,nova  instance delete fails if dhcp_release fails,"ssatya@devstack:~$ nova boot --image 1e95fe6b-cec6-4420-97d1-1e7bc8c81c49 --flavor 1  testdummay
+--------------------------------------+-----------------------------------------------------------+
| Property                             | Value                                                     |
+--------------------------------------+-----------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                    |
| OS-EXT-AZ:availability_zone          | nova                                                      |
| OS-EXT-STS:power_state               | 0                                                         |
| OS-EXT-STS:task_state                | networking                                                |
| OS-EXT-STS:vm_state                  | building                                                  |
| OS-SRV-USG:launched_at               | -                                                         |
| OS-SRV-USG:terminated_at             | -                                                         |
| accessIPv4                           |                                                           |
| accessIPv6                           |                                                           |
| adminPass                            | fK8SPGtHLUds                                              |
| config_drive                         |                                                           |
| created                              | 2014-03-07T14:33:49Z                                      |
| flavor                               | m1.tiny (1)                                               |
| hostId                               | 2c1ae30aa2a235d9c0c8b04aae3f4199cd98356e44a03b5c8f878adb  |
| id                                   | eae503d9-c6f7-4e3e-9adc-0b8b6803c90e                      |
| image                                | debian-2.6.32-i686 (1e95fe6b-cec6-4420-97d1-1e7bc8c81c49) |
| key_name                             | -                                                         |
| metadata                             | {}                                                        |
| name                                 | testdummay                                                |
| os-extended-volumes:volumes_attached | []                                                        |
| progress                             | 0                                                         |
| security_groups                      | default                                                   |
| status                               | BUILD                                                     |
| tenant_id                            | 209ab7e4f3744675924212805db3ad74                          |
| updated                              | 2014-03-07T14:33:50Z                                      |
| user_id                              | f3756a4910054883b84ee15acc15fbd1                          |
+--------------------------------------+-----------------------------------------------------------+
ssatya@devstack:~$ nova list
+--------------------------------------+------------+--------+------------+-------------+------------------+
| ID                                   | Name       | Status | Task State | Power State | Networks         |
+--------------------------------------+------------+--------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | BUILD  | spawning   | NOSTATE     |                  |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1    | ERROR  | deleting   | Shutdown    | private=10.0.0.2 |
+--------------------------------------+------------+--------+------------+-------------+------------------+
ssatya@devstack:~$ nova list
+--------------------------------------+------------+--------+------------+-------------+------------------+
| ID                                   | Name       | Status | Task State | Power State | Networks         |
+--------------------------------------+------------+--------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | ACTIVE | -          | Running     | private=10.0.0.3 |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1    | ERROR  | deleting   | Shutdown    | private=10.0.0.2 |
+--------------------------------------+------------+--------+------------+-------------+------------------+
ssatya@devstack:~$ nova stop testdummay
ssatya@devstack:~$ nova list
+--------------------------------------+------------+---------+------------+-------------+------------------+
| ID                                   | Name       | Status  | Task State | Power State | Networks         |
+--------------------------------------+------------+---------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | SHUTOFF | -          | Shutdown    | private=10.0.0.3 |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1    | ERROR   | deleting   | Shutdown    | private=10.0.0.2 |
+--------------------------------------+------------+---------+------------+-------------+------------------+
ssatya@devstack:~$ nova delete testdummay
ssatya@devstack:~$ nova list
+--------------------------------------+------------+--------+------------+-------------+------------------+
| ID                                   | Name       | Status | Task State | Power State | Networks         |
+--------------------------------------+------------+--------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | ERROR  | deleting   | Shutdown    | private=10.0.0.3 |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1    | ERROR  | deleting   | Shutdown    | private=10.0.0.2 |
+--------------------------------------+------------+--------+------------+-------------+------------------+"
988,1289627,nova,62cb0dc6257daac5ec9fd1a90ee5721e6543dd76,0,0,Add more infor to the error,VMware NoPermission faults do not log what permiss...,"NoPermission object has a privilegeId that tells us which permission the user did not have. Presently the VMware nova driver does not log this data. This is very useful for debugging user permissions problems on vCenter or ESX.
http://pubs.vmware.com/vsphere-55/index.jsp#com.vmware.wssdk.apiref.doc/vim.fault.NoPermission.html"
989,1289696,cinder,b5f7cf989c6f233fb2b0dd759cc33afb94761ec2,0,0,can cause problems when several requests …,request_id middleware uses wrong request ID value,"The request_id middleware is designed to generate a request ID during process_request() and attach this value to the as an HTTP header during process_response(). Unfortunately, it stores the request ID value in a variable within the RequestIdMiddleware class. This violates the ""shared nothing"" rule, and can cause problems when several requests are run concurrently. For example, if requests A and B come in back-to-back, and A completes first, A will have B's request ID value in the HTTP response.
This problem was discovered when running nova's api.compute.servers.test_instance_actions test in parallel while working on  https://review.openstack.org/#/c/66903/"
990,1289993,nova,eeaf046c18d08403a0fefd778d4eb8401db0d357,0,0,remove code,dhcp_options_enabled used in comment,"https://review.openstack.org/#/c/58089/ removes check CONF.dhcp_options_enabled from nova but it is still used in the help string in nova/virt/baremetal/pxe.py:
    cfg.StrOpt('pxe_bootfile_name',
               help='This gets passed to Neutron as the bootfile dhcp '
               'parameter when the dhcp_options_enabled is set.',
               default='pxelinux.0'),"
991,1290036,nova,100c42ed15be5fc3ca52fb7cba0987e768a18a3f,0,0,typo in commentary and add test,Test refresh_instance_security_rules in nova.tests...,"Change https://review.openstack.org/#/c/69600/ adds a new method refresh_instance_security_rules to the nova ComputeDriver interface but it missed adding a test to nova.tests.virt.test_virt_driver to make sure all virt drivers are explicitly handling it.
There is also a typo in the method's docstring."
992,1290234,glance,d2a81315316a95dbb6888f778e4f207582c626c3,0,0,do not use xx in python3,do not use __builtin__ in Python3,"__builtin__ does not exist in Python 3, use six.moves.builtins instead."
993,1290261,nova,b4ef81d6a018a3e6781bbfd5c9a2c73727645cc5,1,1, Missing translation support,Missing translation support,There are a number of places that do not contain translation support in the virt drivers
994,1290294,nova,b4964eb6a570e290545f95d45411dc8441985cd5,1,1,terrible bug,Instance's XXX_resize dir never be deleted if we r...,"reproduce steps:
1. create an instance under Folsom
2. update nova to Havana
3. resize the instance to another host
4. confirm the resize
5. examine the instance dir on source host
you will find the instance-0000xxxx_resize dir exists there which was not deleted while confirming resize.
the reason is that:
in the _cleanup_resize in libvirt driver:
def _cleanup_resize(self, instance, network_info):
        target = libvirt_utils.get_instance_path(instance) + ""_resize""
we get the instance path by using get_instance_path method in libvirt utils,
but we check the original instance dir of pre-grizzly instances' before we return it,
if this instance is a resized one which original instance dir exists on another host(the dest host),
the wrong instance path with uuid will be returned, and then the `target` existing check will be failed,
then the instance-xxxx_resize dir will never be deleted.
def get_instance_path(instance, forceold=False, relative=False):
    """"""Determine the correct path for instance storage.
    This method determines the directory name for instance storage, while
    handling the fact that we changed the naming style to something more
    unique in the grizzly release.
    :param instance: the instance we want a path for
    :param forceold: force the use of the pre-grizzly format
    :param relative: if True, just the relative path is returned
    :returns: a path to store information about that instance
    """"""
    pre_grizzly_name = os.path.join(CONF.instances_path, instance['name'])
    if forceold or os.path.exists(pre_grizzly_name):                  ############### here we check the original instance dir, but if we have resized the instance to another host, this check will be failed, and a wrong dir with instance uuid will be returned.
        if relative:
            return instance['name']
        return pre_grizzly_name
    if relative:
        return instance['uuid']
    return os.path.join(CONF.instances_path, instance['uuid'])"
995,1290317,cinder,7521e6dce39a9ec99d35892780a39bf8049e38db,1,1,message format error,Bug #1290317 “UnboundLocalError,"Hi , i was trying migrate a volume from one backend to another
while when operation is in copy_volume_data (cinder/volume/driver.py)
error occured while copy data from src_volume to dst_volume
root@DVT-Ubuntu-srv2:~# cinder --version
1.0.6
Below is trace back ==================================================
2014-03-06 17:38:36.768 12090 ERROR cinder.openstack.common.rpc.amqp [req-beec71a7-1eec-43af-bb83-a66c58376b42 bfe8d3624c044e5db34bc8abd12cd752 42d88e9851cc4af4afa6a0896834b915] Exception during message handling
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/utils.py"", line 808, in wrapper
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 798, in migrate_volume
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     self.db.volume_update(ctxt, volume_ref['id'], updates)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 791, in migrate_volume
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     self._migrate_volume_generic(ctxt, volume_ref, host)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 729, in _migrate_volume_generic
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     new_volume['migration_status'] = None
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 709, in _migrate_volume_generic
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     remote='dest')
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/volume/driver.py"", line 326, in copy_volume_data
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     force=copy_error)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp UnboundLocalError: local variable 'copy_error' referenced before assignment
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp
2014-03-06 17:39:17.404 12096 DEBUG cinder.openstack.common.periodic_task [-] Running periodic task VolumeManager._publish_service_capabilities run_periodic_tasks /usr/lib/python2.7/dist-packages/cinder/openstack/common/periodic_task.py:176
2014-03-06 17:39:17.404 12096 DEBUG cinder.manager [-] Notifying Schedulers of capabilities ... _publish_service_capabilities /usr/lib/python2.7/dist-packages/cinder/manager.py:135
2014-03-06 17:39:17.405 12096 DEBUG cinder.openstack.common.rpc.amqp [-] Making asynchronous fanout cast... fanout_cast /usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py:640
2014-03-06 17:39:17.405 12096 DEBUG cinder.openstack.common.rpc.amqp [-] UNIQUE_ID is b0e5e286d0094ff9a5e908fb49b5106d. _add_unique_id /usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py:345
2014-03-06 17:39:17.407 12096 DEBUG amqp [-] Closed channel #1 _do_close /usr/lib/python2.7/dist-packages/amqp/channel.py:88
2014-03-06 17:39:17.407 12096 DEBUG amqp [-] using channel_id: 1 __init__ /usr/lib/python2.7/dist-packages/amqp/channel.py:70
2014-03-06 17:39:17.408 12096 DEBUG amqp [-] Channel open _open_ok /usr/lib/python2.7/dist-packages/amqp/channel.py:420
2014-03-06 17:39:17.408 12096 DEBUG cinder.openstack.common.periodic_task [-] Running periodic task VolumeManager._report_driver_status run_p
from the source code of Havana, following exception handling may somehow contain defect:
 312         try:
 313             size_in_mb = int(src_vol['size']) * 1024    # vol size is in GB
 314             volume_utils.copy_volume(src_attach_info['device']['path'],
 315                                      dest_attach_info['device']['path'],
 316                                      size_in_mb)
 317             copy_error = False
 318         except Exception:
 319             with excutils.save_and_reraise_exception():
 320                 msg = _(""Failed to copy volume %(src)s to %(dest)d"")
 321                 LOG.error(msg % {'src': src_vol['id'], 'dest': dest_vol['id']})
 322                 copy_error = True
 323         finally:
 324             self._copy_volume_data_cleanup(context, dest_vol, properties,
 325                                            dest_attach_info, dest_remote,
 326                                            force=copy_error)
 327             self._copy_volume_data_cleanup(context, src_vol, properties,
 328                                            src_attach_info, src_remote,
 329                                            force=copy_error)"
996,1290362,nova,3da0d898f487fbc4ca668e57133a2b3f102f73be,1,0,x86 specific,"Bug #1290362 “HPET timer not supported on non-x86 targets "" ","High Precision Event Timer is x86 specific hardware design to replace older PIT and RTC.
Also, '-no-hpet' option makes qemu to fail on non x86 targets.
he libvirt's xml generated has the following:
<timer name=""hpet"" present=""no""/>
The error produced...
libvirtError: internal error: process exited while connecting to monitor: Option no-hpet not supported for this target
For non x86 arch, this bug is affecting test_server_basicops test in Tempest:
tempest.scenario.test_server_basic_ops.TestServerBasicOps.test_server_basicops
No valid host was found."
997,1290403,nova,a41ef0ede2cdace1db4d932e0931509a84ba837e,1,1, erroneous values collected,Hyper-V agent does not enable disk metrics for ind...,"The Hyper-V agent is currently enabling metrics collection per vm, instead of per disk.
This leads to erroneous values collected for disk metrics."
998,1290478,neutron,665352f169a7d1e7eca590d466241cc313adb7c3,0,0, could cause a timeout ,Query for port before calling l3plugin.disassociat...,"The call to l3plugin.disassociate_floatingips() trigggers several events
that could cause a timeout to occur trying to query the db for the port
therefore this patch changes the code to query first for the port."
999,1290486,neutron,8e9f00a19dab98e5cfc7ca32beb9f17ebb5bc1bb,1,1,,neutron-openvswitch-agent does not recreate flows ...,"The DHCP requests were not being responded to after they were seen on the undercloud network interface.  The neutron services were restarted in an attempt to ensure they had the newest configuration and knew they were supposed to respond to the requests.
Rather than using the heat stack create (called in devtest_overcloud.sh) to test, it was simple to use the following to directly boot a baremetal node.
    nova boot --flavor $(nova flavor-list | grep ""|[[:space:]]*baremetal[[:space:]]*|"" | awk '{print $2}) \
          --image $(nova image-list | grep ""|[[:space:]]*overcloud-control[[:space:]]*|"" | awk '{print $2}') \
          bm-test1
Whilst the baremetal node was attempting to pxe boot a restart of the neutron services was performed.  This allowed the baremetal node to boot.
It has been observed that a neutron restart was needed for each subsequent reboot of the baremetal nodes to succeed."
1000,1290487,nova,dba898226ec26ab49cecd24e9d6c255cf6153cd1,0,0,potentially causing threading issues,Libvirt native thread used for “forbidden,"In the nova.virt.libvirt.driver.LibvirtDriver. _get_new_connection method  two different libvirt event handlers are registered, one for lifecycle events (_event_lifecycle_callback) and one for connection events (_close_callback).  These callbacks are called by a native thread that is continually calling libvirt.virEventRunDefaultImpl() in the _native_thread method; the latter method's Docstring contains the following note:
        This is a native thread which runs the default
        libvirt event loop implementation. This processes
        any incoming async events from libvirtd and queues
        them for later dispatch. This thread is only
        permitted to use libvirt python APIs, and the
        driver.queue_event method. In particular any use
        of logging is forbidden, since it will confuse
        eventlet's greenthread integration
while this rule is adhered to by the _event_lifecycle_callback method, it is violated by _close_callback (the other callback) because it calls the _set_host_enabled method which, among other things, writes to the log.
The _close_callback method needs to be altered so that it does not execute any logic that may interfere with eventlet's greenthread integration."
1001,1290503,cinder,bb5228cf79ff5bea543abc321b2dfad9fea5e9a5,1,1,key_error. Add default value,Bug #1290503 “KeyError,"Saw this while looking at logs for bug 1290468:
http://logs.openstack.org/85/78385/3/gate/gate-grenade-dsvm/2b66f90/logs/new/screen-c-api.txt.gz
You'll see a ton of errors like this:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/new/cinder/cinder/openstack/common/log.py"", line 705, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/new/cinder/cinder/openstack/common/log.py"", line 669, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 467, in format
    s = self._fmt % record.__dict__
KeyError: 'user_identity'
Logged from file middleware.py, line 100
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiS2V5RXJyb3I6IFxcJ3VzZXJfaWRlbnRpdHlcXCdcIiBBTkQgZmlsZW5hbWU6bG9ncypzY3JlZW4tYy1hcGkudHh0IiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzk0NDc2MTE4MTQ2fQ==
Thinking it's related to this:
https://github.com/openstack/cinder/blob/master/cinder/openstack/common/log.py#L370
Does that need to be popped off kwargs?
From the review https://review.openstack.org/#/c/55938/ it looks like Ben and Doug were questioning it.
Also wondering if that's somehow impacted by the logging format condition used here:
https://github.com/openstack/cinder/blob/master/cinder/openstack/common/log.py#L657
Because logging_context_format_string is the format that uses user_identity.
This started showing up on 3/7 which is when the lazy translation code was enabled in Cinder so it looks like that is exposing the bug."
1002,1290540,nova,6a36d367b54538ef6347c948b3e549f21993771a,1,1,"Revert deprecation warning.. Bug, Ithink",neutron_admin_tenant_name deprecation warning is w...,ARNING nova.network.neutronv2 [req-eb54925d-c466-4069-be4e-691e155ea85d None None] Using neutron_admin_tenant_name for authentication is deprecated and will be removed in the next release.  Use neutron_admin_tenant_id instead.
1003,1290549,neutron,31f051a0349a40fe629a7bb5ce0ea683aa28a659,0,0,Refactoring code,update floatingip status tracebacks,"Logs are showing plenty of tracebacks like the following: http://logs.openstack.org/96/66796/15/check/check-tempest-dsvm-neutron-full/d940e56/logs/screen-q-svc.txt.gz?level=DEBUG#_2014-03-06_10_14_46_352
An operation (either update_floatingip_status or get_floating_ips) in update_floatingip_statuses (l3_rpc_base.py) triggers this traceback. Apparently this happens because if a floatingIP is removed, the transaction is aborted because of the exception.
Optimizing as suggested here: https://github.com/openstack/neutron/blob/master/neutron/db/l3_db.py#L680 might solve the issue."
1004,1290550,neutron,1dfd65f4cff1133cff45e103083fc3ae3130877b,0,0,tests,Base test case should stop mock patches,"Currently if a unit test creates a patch and does not stop it, the patch will hang around and could potentially affect other tests that rely on the mocked class/method. This can make it difficult for developers creating new tests as unrelated tests could be causing new ones to sporadically fail or vice versa depending on concurrency and test order."
1005,1290561,neutron,793224b2668f6522c8197bd3c5e3a7612dc4c9f0,1,1,,Failure in Cisco N1KV controller due to lack of L3...,"A recent API change in Cisco N1KV controller requires L3 params for port creation.
N1KV Neutron plugin does not pass these params in the REST call to the controller.
The fix is to add the missing params in the REST body."
1006,1290627,cinder,657b5106c35e2834d75e77e4702b9fd99bde817f,1,1,Fix exception msg,Exception message of CoraidESMConfigureError is di...,"Expectation:
cinder.exception.CoraidESMConfigureError: ESM configure request failed: Reply is empty.
Actual:
cinder.exception.CoraidESMConfigureError: Reply is empty."
1008,1290975,nova,dd3f7cddd380205c26e5e2b9e2002d773eab9047,0,0,New style,cells AttributeError with compute api methods usin...,"The nova-cells service looks up instances locally before passing them to the local compute api, and only converts them to objects if the compute api method is explicitly listed in the run_compute_api method.  There is in fact a FIXME around this process, but it appears to not have been addressed yet :)
2014-03-10 17:27:59.881 30193 ERROR nova.cells.messaging [req-3e27c8c0-6b3c-482d-bb9b-d638933ec949 10226892 5915610] Error processing message locally: 'dict' object has no attribute 'metadata'
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging Traceback (most recent call last):
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 211, in _process_locally
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     resp_value = self.msg_runner._process_message_locally(self)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 1290, in _process_message_locally
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     return fn(message, **message.method_kwargs)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 706, in run_compute_api_method
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     return fn(message.ctxt, *args, **method_info['method_kwargs'])
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 199, in wrapped
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     return func(self, context, target, *args, **kwargs)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 189, in inner
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     return function(self, context, instance, *args, **kwargs)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 170, in inner
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     return f(self, context, instance, *args, **kw)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 2988, in update_instance_metadata
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     orig = dict(instance.metadata)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging AttributeError: 'dict' object has no attribute 'metadata'
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging"
1009,1291007,nova,d19c75c19d2de8b20e82e6de9413ba53671ad7fb,1,1,,Bug #1291007 “device_path not available at detach time for boot ... ,"When you do a normal volume attach to an existing VM and then detach it, the connection_info contains the following
connection_info['data']['device_path'] at libvirt volume driver disconnect_volume(self, connection_info, mount_device) time.
When you boot a VM from a volume, not an image, and then terminate the VM, the libvirt volume driver disconnect_volume's
connection_info['data'] doesn't contain the 'device_path' key.   The libvirt volume driver's need this information to correctly disconnect the LUN from the kernel."
1010,1291014,nova,8babd6a99014ccaf51d955769eaec085e037cc76,1,1,Attribute error,Bug #1291014 “Nova boot fails,"[req-d4c97a98-2b0e-419e-83d6-0e88332a699a account1 account1] [instance: 036a26b1-7fe2-4d56-b7a2-4781e8cad696] Error: is_public
Traceback (most recent call last):
  File ""/opt/nova/nova/compute/manager.py"", line 1254, in _build_instance
    set_access_ip=set_access_ip)
  File ""/opt/nova/nova/compute/manager.py"", line 394, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/nova/nova/compute/manager.py"", line 1655, in _spawn
    LOG.exception(_('Instance failed to spawn'), instance=instance)
  File ""/opt/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/nova/nova/compute/manager.py"", line 1652, in _spawn
    block_device_info)
  File ""/opt/nova/nova/virt/libvirt/driver.py"", line2230, in spawn
    admin_pass=admin_password)
  File ""/opt/nova/nova/virt/libvirt/driver.py"", line2538, in _create_image
    imagehandler_args=imagehandler_args)
  File ""/opt/nova/nova/virt/libvirt/imagebackend.py"", line 184, in cache
    *args, **kwargs)
  File ""/opt/nova/nova/virt/libvirt/imagebackend.py"", line 310, in create_image
    prepare_template(target=base, max_size=size, *args, **kwargs)
  File ""/opt/nova/nova/openstack/common/lockutils.py"", line 249, in inner
    return f(*args, **kwargs)
  File ""/opt/nova/nova/virt/libvirt/imagebackend.py"", line 174, in fetch_func_sync
    fetch_func(target=target, *args, **kwargs)
  File ""/opt/nova/nova/virt/libvirt/utils.py"", line 654, in fetch_image
    max_size=max_size, imagehandler_args=imagehandler_args)
  File ""/opt/nova/nova/virt/images.py"", line 108, in fetch_to_raw
    imagehandler_args=imagehandler_args)
  File ""/opt/nova/nova/virt/images.py"", line 98, in fetch
    fetched_to_local = handler.is_local()
  File ""/usr/lib/python2.7/contextlib.py"", line 35, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/opt/nova/nova/openstack/common/fileutils.py"", line 98, in remove_path_on_error
    remove(path)
  File ""/opt/nova/nova/virt/images.py"", line 71, in _remove_image_on_exec
    image_path):
  File ""/opt/nova/nova/virt/imagehandler/__init__.py"", line 154, in handle_image
    img_locs = image_service.get_locations(context, image_id)
  File ""/opt/nova/nova/image/glance.py"", line 307, in get_locations
    if not self._is_image_available(context, image_meta):
  File ""/opt/nova/nova/image/glance.py"", line 446, in _is_image_available
    if image.is_public or context.is_admin:
  File ""/usr/local/lib/python2.7/dist-packages/warlock/model.py"", line 72, in __getattr__
    raise AttributeError(key)
AttributeError: is_public"
1011,1291032,neutron,ea1aa7794585c7cca3118ece282e08cfb760218b,0,0,More pythonic,Correct H301 and H302 violations,There are many violations of this rule.  Originally it was intended to only fix H302 but flake8 will not check for H302 unless H301 is enabled as well.  So must fix them both at the same time.
1012,1291103,neutron,7360e67d32dbd8c98ef59c46bf0c7fff16f48d0c,1,1,broken,admin_state_up check broken for update network in ...,"in update_network API call, the network dictionary has the contents: {u'network': {u'admin_state_up': True}}
_network_admin_state functions expects network name in the dictionary and it raises error. This has to be fixed. Network name should not be expected.
2014-03-11 16:20:41.920 29794 ERROR neutron.api.v2.resource [-] update failed
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 486, in update
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource     obj = obj_updater(request.context, id, **kwargs)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/lockutils.py"", line 233, in inner
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource     retval = f(*args, **kwargs)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py"", line 133, in update_network
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource     self._network_admin_state(network)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py"", line 597, in _network_admin_state
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource     raise plum_excep.PLUMgridException(err_msg=err_message)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource PLUMgridException: An unexpected error occurred in the PLUMgrid Plugin: Network Admin State Validation Falied:"
1013,1291108,cinder,71853a32111ec9a36f8a2082b41b52519b66be85,0,0,tests,test_force_delete_snapshot fails with no volume de...,"2014-03-11 18:50:26.524 | FAIL: cinder.tests.api.contrib.test_admin_actions.AdminActionsTest.test_force_delete_snapshot
2014-03-11 18:50:26.524 | tags: worker-0
2014-03-11 18:50:26.524 | ----------------------------------------------------------------------
2014-03-11 18:50:26.524 | Empty attachments:
2014-03-11 18:50:26.524 |   pythonlogging:''-1
2014-03-11 18:50:26.524 |   stderr
2014-03-11 18:50:26.524 |   stdout
2014-03-11 18:50:26.524 |
2014-03-11 18:50:26.525 | pythonlogging:'': {{{
2014-03-11 18:50:26.525 | Starting cinder-volume node (version 2014.1)
2014-03-11 18:50:26.525 | Starting volume driver FakeISCSIDriver (2.0.0)
2014-03-11 18:50:26.525 | volume cedd21ea-cd89-45ca-82d8-5a7cf1e0e6b5: skipping export
2014-03-11 18:50:26.525 | Updating volume status
2014-03-11 18:50:26.525 | Initializing extension manager.
2014-03-11 18:50:26.525 | Loaded extension: os-vol-tenant-attr
2014-03-11 18:50:26.525 | Loaded extension: os-types-extra-specs
2014-03-11 18:50:26.525 | Loaded extension: os-vol-host-attr
2014-03-11 18:50:26.525 | Loaded extension: os-volume-encryption-metadata
2014-03-11 18:50:26.525 | Loaded extension: OS-SCH-HNT
2014-03-11 18:50:26.525 | Loaded extension: os-availability-zone
2014-03-11 18:50:26.526 | Loaded extension: os-vol-image-meta
2014-03-11 18:50:26.526 | Loaded extension: os-snapshot-actions
2014-03-11 18:50:26.526 | Loaded extension: os-quota-sets
2014-03-11 18:50:26.526 | Loaded extension: os-volume-actions
2014-03-11 18:50:26.526 | Loaded extension: os-volume-manage
2014-03-11 18:50:26.526 | Loaded extension: os-image-create
2014-03-11 18:50:26.526 | Loaded extension: qos-specs
2014-03-11 18:50:26.526 | Loaded extension: backups
2014-03-11 18:50:26.526 | Loaded extension: encryption
2014-03-11 18:50:26.526 | Loaded extension: os-used-limits
2014-03-11 18:50:26.526 | Loaded extension: os-types-manage
2014-03-11 18:50:26.526 | Loaded extension: os-vol-mig-status-attr
2014-03-11 18:50:26.527 | Loaded extension: os-extended-services
2014-03-11 18:50:26.527 | Loaded extension: os-quota-class-sets
2014-03-11 18:50:26.527 | Loaded extension: os-volume-transfer
2014-03-11 18:50:26.527 | Loaded extension: os-volume-unmanage
2014-03-11 18:50:26.527 | Loaded extension: os-hosts
2014-03-11 18:50:26.527 | Loaded extension: os-extended-snapshot-attributes
2014-03-11 18:50:26.527 | Loaded extension: os-services
2014-03-11 18:50:26.527 | Loaded extension: os-admin-actions
2014-03-11 18:50:26.527 | POST http://localhost/v2/fake/snapshots/5ccb04ea-6a62-4e57-9299-e8ae34ecc34b/action
2014-03-11 18:50:26.527 | http://localhost/v2/fake/snapshots/5ccb04ea-6a62-4e57-9299-e8ae34ecc34b/action returned with HTTP 202
2014-03-11 18:50:26.527 | Arguments dropped when creating context: {'user': 'admin', 'tenant': 'fake'}
2014-03-11 18:50:26.527 | snapshot 5ccb04ea-6a62-4e57-9299-e8ae34ecc34b: deleting
2014-03-11 18:50:26.528 | Volume device file path /tmp/tmp.udMSkMDFJg/tmpKYdE0X/tmpSeiWSd/tmpzCdDA-cow does not exist.
2014-03-11 18:50:26.528 | Exception during message handling: Bad or unexpected response from the storage volume backend API: Volume device file path /tmp/tmp.udMSkMDFJg/tmpKYdE0X/tmpSeiWSd/tmpzCdDA-cow does not exist.
2014-03-11 18:50:26.528 | Traceback (most recent call last):
2014-03-11 18:50:26.528 |   File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-03-11 18:50:26.528 |     incoming.message))
2014-03-11 18:50:26.528 |   File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-03-11 18:50:26.528 |     return self._do_dispatch(endpoint, method, ctxt, args)
2014-03-11 18:50:26.528 |   File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-03-11 18:50:26.528 |     result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-11 18:50:26.528 |   File ""cinder/volume/manager.py"", line 166, in lso_inner1
2014-03-11 18:50:26.528 |     return lso_inner2(inst, context, snapshot_id, **kwargs)
2014-03-11 18:50:26.529 |   File ""cinder/openstack/common/lockutils.py"", line 247, in inner
2014-03-11 18:50:26.529 |     retval = f(*args, **kwargs)
2014-03-11 18:50:26.529 |   File ""cinder/volume/manager.py"", line 165, in lso_inner2
2014-03-11 18:50:26.529 |     return f(*_args, **_kwargs)
2014-03-11 18:50:26.529 |   File ""cinder/volume/manager.py"", line 542, in delete_snapshot
2014-03-11 18:50:26.529 |     {'status': 'error_deleting'})
2014-03-11 18:50:26.529 |   File ""cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-03-11 18:50:26.529 |     six.reraise(self.type_, self.value, self.tb)
2014-03-11 18:50:26.529 |   File ""cinder/volume/manager.py"", line 530, in delete_snapshot
2014-03-11 18:50:26.529 |     self.driver.delete_snapshot(snapshot_ref)
2014-03-11 18:50:26.529 |   File ""cinder/volume/drivers/lvm.py"", line 252, in delete_snapshot
2014-03-11 18:50:26.529 |     self._delete_volume(snapshot, is_snapshot=True)
2014-03-11 18:50:26.530 |   File ""cinder/volume/drivers/lvm.py"", line 128, in _delete_volume
2014-03-11 18:50:26.530 |     self._clear_volume(volume, is_snapshot)
2014-03-11 18:50:26.530 |   File ""cinder/volume/drivers/lvm.py"", line 155, in _clear_volume
2014-03-11 18:50:26.530 |     raise exception.VolumeBackendAPIException(data=msg)
2014-03-11 18:50:26.530 | VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Volume device file path /tmp/tmp.udMSkMDFJg/tmpKYdE0X/tmpSeiWSd/tmpzCdDA-cow does not exist.
2014-03-11 18:50:26.530 | }}}
2014-03-11 18:50:26.530 |
2014-03-11 18:53:19.295 | Traceback (most recent call last):
2014-03-11 18:53:19.295 |   File ""cinder/tests/api/contrib/test_admin_actions.py"", line 312, in test_force_delete_snapshot
2014-03-11 18:53:19.296 |     snapshot['id'])
2014-03-11 18:53:19.296 |   File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 393, in assertRaises
2014-03-11 18:53:19.296 |     self.assertThat(our_callable, matcher)
2014-03-11 18:53:19.296 |   File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
2014-03-11 18:53:19.296 |     raise mismatch_error
2014-03-11 18:53:19.296 | MismatchError: <function snapshot_get at 0x2348848> returned <cinder.db.sqlalchemy.models.Snapshot object at 0x6d02050>
http://logs.openstack.org/68/79568/1/gate/gate-cinder-python27/b823a23/console.html"
1015,1291144,neutron,3d8ee7eb633e251c3e343239b0e0ee6234df9393,0,0,tests,Calling cfg.CONF.reset not necessary in individual...,oslo.config.cfg.CONF.reset is added to cleanup in BaseTestCase.setUp(). No need for individual test classes to do it.
1017,1291238,nova,878a82bf547464f934b1b49896886e3abd8f6387,0,0,It’s not a bug,Bug #1291238 “VMware,"When using the VC driver the following message is received:
2014-03-12 01:42:32.889 INFO nova.openstack.common.periodic_task [-] Skipping periodic task _periodic_update_dns because its interval is negative
2014-03-12 01:42:32.940 INFO nova.virt.driver [-] Loading compute driver 'vmwareapi.VMwareVCDriver'
2014-03-12 01:42:32.979 WARNING nova.virt.vmwareapi.driver [-] The VMware ESX driver is now deprecated and will be removed in the Juno release. The VC driver will remain and continue to be supported."
1018,1291291,cinder,36b5782dea6d0e627c65c73dbfee39d55dbee318,0,0,it should ignore datastores,Bug #1291291 “vmware,"The vmdk driver ignores the maintenanceMode property of a datastore and considers it as a potential candidate for volume creation. Rather, it should ignore datastores in maintenance mode for volume creation."
1019,1291346,cinder,36b5782dea6d0e627c65c73dbfee39d55dbee318,1,1,There is a bug because they chose a bad critery,Bug #1291346 “vmware,"The vmdk driver uses the number of hosts connected to a datastore as one of the criteria for selecting a datastore for volume creation. It might incorrectly consider a host to which the datastore is inaccessible while computing the number of connected hosts.
if hasattr(mount_info, ""accessible""):
            accessible = mount_info.accessible
else:
           # If accessible attribute is not set, we look at summary
            summary = self.get_summary(datastore)
            accessible = summary.accessible
If a datastore is accessible through a subset of hosts, then the value of summary.accessible will be true."
1020,1291364,nova,73da55e4ef626283ae58a97c7ad89854ec77daa3,1,1,,_destroy_evacuated_instances fails randomly with h...,"In our production environment (2013.2.1), we're facing a random error thrown while starting nova-compute in Hyper-V nodes.
The following exception is thrown while calling '_destroy_evacuated_instances':
16:30:58.802 7248 ERROR nova.openstack.common.threadgroup [-] 'NoneType' object is not iterable
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
(...)
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup   File ""C:\Python27\lib\site-packages\nova\compute\manager.py"", line 532, in _get_instances_on_driver
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup     name_map = dict((instance['name'], instance) for instance in instances)
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup TypeError: 'NoneType' object is not iterable
Full trace: http://paste.openstack.org/show/73243/
Our first guess is that this problem is related with number of instances in our deployment (~3000), they're all fetched in order to check evacuated instances (as Hyper-V is not implementing ""list_instance_uuids"").
In the case of KVM, this error is not happening as it's using a smarter method to get this list based on the UUID of the instances.
Although this is being reported using Hyper-V, it's a problem that could occur in other drivers not implementing ""list_instance_uuids"""
1021,1291489,nova,c32986966dfd034c3d706b2e9ab2820a2c3cfc3e,1,1,,list-secgroup fail if no secgroups defined for ser...,"No issues if there are atleast 1 secgroup defined for the server.
If no secgroups are defined for the server, it fails with 400 error.
$ nova --debug list-secgroup vp25q00cs-osfe11b124f4.isg.apple.com
.
.
.
RESP: [400] CaseInsensitiveDict({'date': 'Wed, 12 Mar 2014 17:08:11 GMT', 'content-length': '141', 'content-type': 'application/json; charset=UTF-8', 'x-compute-request-id': 'req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61'})
RESP BODY: {""badRequest"": {""message"": ""The server could not comply with the request since it is either malformed or otherwise incorrect."", ""code"": 400}}
DEBUG (shell:740) The server could not comply with the request since it is either malformed or otherwise incorrect. (HTTP 400) (Request-ID: req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61)
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/novaclient/shell.py"", line 737, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/Library/Python/2.7/site-packages/novaclient/shell.py"", line 673, in main
    args.func(self.cs, args)
  File ""/Library/Python/2.7/site-packages/novaclient/v1_1/shell.py"", line 1904, in do_list_secgroup
    groups = server.list_security_group()
  File ""/Library/Python/2.7/site-packages/novaclient/v1_1/servers.py"", line 328, in list_security_group
    return self.manager.list_security_group(self)
  File ""/Library/Python/2.7/site-packages/novaclient/v1_1/servers.py"", line 883, in list_security_group
    base.getid(server), 'security_groups', SecurityGroup)
  File ""/Library/Python/2.7/site-packages/novaclient/base.py"", line 61, in _list
    _resp, body = self.api.client.get(url)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 229, in get
    return self._cs_request(url, 'GET', **kwargs)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 213, in _cs_request
    **kwargs)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 195, in _time_request
    resp, body = self.request(url, method, **kwargs)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 189, in request
    raise exceptions.from_response(resp, body, url, method)
BadRequest: The server could not comply with the request since it is either malformed or otherwise incorrect. (HTTP 400) (Request-ID: req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61)
ERROR: The server could not comply with the request since it is either malformed or otherwise incorrect. (HTTP 400) (Request-ID: req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61)"
1022,1291515,nova,09dd7bf800bcfc18a2a57b9e1ce0c3c24653f5ac,1,1,Bug after a change.,Recent Change to default state_path can silently  ...,"the change to the default value of state_path introduced by
I94502bcfac8b372271acd0dbc1710c0e3009b8e1 for the reasons set out
in my -1 review of the same that seems to have been skipped when the
change was accepted.
As implemented the change will break any existing systems that are using
the default value of state_path with no warning period, which goes beyond
the scope of change for UpgradeImpact"
1023,1291535,neutron,b2f65d9d447ddf2caf3b9c754bd00a5148bdf12c,1,1,  Correct OVS VXLAN version check,'Unable to retrieve OVS kernel module version' whe...,"If we are using openvswitch in a system with a newer kernel (3.13/trusty) it should have the features required for neutron and not require an openvswitch dkms package. Therefore we should be able to use the native module.
In neutron/agent/linux/ovs_lib.py:
def get_installed_ovs_klm_version():
    args = [""modinfo"", ""openvswitch""]
    try:
        cmd = utils.execute(args)
        for line in cmd.split('\n'):
            if 'version: ' in line and not 'srcversion' in line:
                ver = re.findall(""\d+\.\d+"", line)
                return ver[0]
    except Exception:
        LOG.exception(_(""Unable to retrieve OVS kernel module version.""))
So if we run modinfo on a system without a DKMS package we get:
$ modinfo openvswitch
filename:       /lib/modules/3.13.0-16-generic/kernel/net/openvswitch/openvswitch.ko
license:        GPL
description:    Open vSwitch switching datapath
srcversion:     1CEE031973F0E4024ACC848
depends:        libcrc32c,vxlan,gre
intree:         Y
vermagic:       3.13.0-16-generic SMP mod_unload modversions
signer:         Magrathea: Glacier signing key
sig_key:        1A:EE:D8:17:C4:D5:29:55:C4:FA:C3:3A:02:37:FE:0A:93:44:6D:69
sig_hashalgo:   sha512
Because 'version' isn't provided we need an alternative way of checking if the openvswitch module has the required features."
1024,1291565,nova,6e5f6041a38ddb0b2643dff66d264f3594f1a875,0,0,remove unnecessary querying,validate_networks does unnecessary querying to neu...,"This patch optimizes validate_networks so that it only queries neutron
when needed. Previously, this method would perform an additional net_list,
list_ports, and show_quota regardless if a request contains only
port_ids. If a request only contains port ids we do not need to check neutron
for quota as these ports are already allocated."
1025,1291646,glance,551cad4c7bf7043c8942319fc46e58d851051b2c,0,0,Refactoring code,Make the VMware datastore backend more robust,"Several issues to address:
- need better error handling for the add,get,get_size,delete operations: need to catch exception when httplib call fails, also need to log when the response is not expected.
- need to handle cases where the store_image_dir contains non expected characters. It should support the following use cases:
/openstack_glance
openstack_glance
openstack_glance/
openstack glance  -> this one should fail with logging
openstack+glance
etc.
- need to quote special characters"
1026,1291690,neutron,c57f11321220a21af10d884c87ab989dc7dc2e69,1,1,,Bug #1291690 “delete router interface fail if neutron and nvp ou... ,"It's similar to https://bugs.launchpad.net/neutron/+bug/1251422, but wrt routers.
If we delete a router from neutron that is already deleted in nvp, it throw 404 error. The correct behavior should be to delete it from neutron, if it's already deleted in nvp.
rainbow:~ bhuvan$ neutron router-interface-delete tempest-router 67056b2d-a924-4456-9050-ed0baa0eaf1a
404-{u'NeutronError': {u'message': u'Router d6f3c0c6-6884-467f-9a84-5a64b88f8936 has no interface on subnet 67056b2d-a924-4456-9050-ed0baa0eaf1a', u'type': u'RouterInterfaceNotFoundForS
ubnet', u'detail': u''}}
neutron server log. Note: 404 error from nvp  is logged at INFO level. It should be a WARNING.
2014-03-12 22:42:26,149 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: tempest-admin with project_id : csi-tenant-tempest and ro
les: csi-tenant-admin,csi-role-admin
2014-03-12 22:42:26,151 (routes.middleware): DEBUG middleware __call__ Matched PUT /routers/d6f3c0c6-6884-467f-9a84-5a64b88f8936/remove_router_interface.json
2014-03-12 22:42:26,151 (routes.middleware): DEBUG middleware __call__ Route path: '/routers/:(id)/remove_router_interface.:(format)', defaults: {'action': u'remove_router_interface', 'c
ontroller': <wsgify at 68316752 wrapping <function resource at 0x410fb18>>}
2014-03-12 22:42:26,151 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'remove_router_interface', 'controller': <wsgify at 68316752 wrapping <function resource at
 0x410fb18>>, 'id': u'd6f3c0c6-6884-467f-9a84-5a64b88f8936', 'format': u'json'}
2014-03-12 22:42:26,208 (neutron.api.v2.resource): INFO resource resource remove_router_interface failed (client error): Router d6f3c0c6-6884-467f-9a84-5a64b88f8936 has no interface on s
ubnet 67056b2d-a924-4456-9050-ed0baa0eaf1a
2014-03-12 22:42:26,210 (neutron.wsgi): INFO log write 17.199.81.86 - - [12/Mar/2014 22:42:26] ""PUT /v2.0/routers/d6f3c0c6-6884-467f-9a84-5a64b88f8936/remove_router_interface.json HTTP/1
.1"" 404 329 0.066915"
1027,1291695,neutron,402175a787c2064404eaf7c260a16ea05e9a99a3,1,1, should call eventlet sleep in watchdog ,Bug #1291695 “BigSwitch,"The consistency watchdog in eventlet currently calls time.sleep which will block other greenthreads who are members of the same pool.
https://github.com/openstack/neutron/blob/288e3127440158f177beaae1972236def4916251/neutron/plugins/bigswitch/servermanager.py#L554
It should use eventlet.sleep so it yields to other members of the same pool."
1028,1291741,nova,7ae506a4b1829fbd8cbecc0a6b267f76230face7,1,1,Bug. Didnt resize,Bug #1291741 “VMWare,"In ""nova/virt/vmwareapi/vmops.py""
def finish_migration(self, context, migration, instance, disk_info,
                         network_info, image_meta, resize_instance=False,
                         block_device_info=None, power_on=True):
        """"""Completes a resize, turning on the migrated instance.""""""
        if resize_instance:
            client_factory = self._session._get_vim().client.factory
            vm_ref = vm_util.get_vm_ref(self._session, instance)
            vm_resize_spec = vm_util.get_vm_resize_spec(client_factory,
                                                        instance)
            reconfig_task = self._session._call_method(
                                            self._session._get_vim(),
                                            ""ReconfigVM_Task"", vm_ref,
                                            spec=vm_resize_spec)
.....................
finish_migration uses vm_util.get_vm_resize_spec() to get resize parameters.
But in ""nova/virt/vmwareapi/vm_util.py""
def get_vm_resize_spec(client_factory, instance):
    """"""Provides updates for a VM spec.""""""
    resize_spec = client_factory.create('ns0:VirtualMachineConfigSpec')
    resize_spec.numCPUs = int(instance['vcpus'])
    resize_spec.memoryMB = int(instance['memory_mb'])
    return resize_spec
the get_vm_resize_spec action does not set up disk size to resize."
1029,1291804,cinder,1e4d070a5b638de58c87363b18a76171dcd0ce38,1,1,Profile attributes not reflecting. Change the way to do it,Bug #1291804 “vmware ,"1) create profile say 'default'  having content tagged to nfs data store.
2) Now 'default' profile in /etc/cinder/cinder.conf for ""pbm_default_policy=default""
3) now restart cinder api service.
4) Now create volume with out any volume type ""cinder create --name testvol 1""
5) Now attach to instance.
6) As per default profile define respective volume is present to nfs data store.
how ever default profile  details not appearing in  "" VM storage policies pane"" . See attached snapshot for reference
ssatya@devstack:/etc/cinder$ cat cinder.conf  | grep pbm
pbm_default_policy=default
ssatya@devstack:/etc/cinder$ cinder list | grep in-use
| 4db6af03-7c60-469d-803e-2ba21893fc0d |   in-use  |          testvol2         |  1   |         None         |  false   | 26ffc3d5-af49-4c04-926f-9b94563bfd65 |
| 862219d5-941a-4f96-85a8-9b73a83d4507 |   in-use  |          testvol3         |  1   |         None         |  false   | 26ffc3d5-af49-4c04-926f-9b94563bfd65 |
| d33d102f-f863-441d-bf87-29465cef3760 |   in-use  |      ThickSliver_vol1     |  1   |  ThickSliver_volume  |  false   | 26ffc3d5-af49-4c04-926f-9b94563bfd65 |"
1030,1291805,nova,8fa190c0d4ede0acc8ed4a462a5ebb751380143c,1,1,Bug after change to list,Don't change list to tuple when get info from libv...,"In the libvirt.driver, we now use the code like this:
(state, _max_mem, _mem, _cpus, _t) = virt_dom.info()
if the libvirt add new variables in the domain info, the code will be failed.
the error will like this :
 File ""/opt/stack/nova/nova/service.py"", line 180, in start
    self.manager.init_host()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 974, in init_host
    self._init_instance(context, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 882, in _init_instance
    drv_state = self._get_power_state(context, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 990, in _get_power_state
    return self.driver.get_info(instance)[""state""]
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3462, in get_info
    (state, max_mem, mem, num_cpu, cpu_time) = virt_dom.info()
ValueError: too many values to unpack"
1032,1291915,neutron,907bf41afbdb9f565c45a535f637c8928d0be52a,1,1,,neutron-netns-cleanup script doesn't work in iceho...,"1st) Some configuration options are not registered on the tool, but they're used in neutron.agent.linux.dhcp  during execution
$ neutron-netns-cleanup --debug --force --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/dhcp_agent.ini --config-file  /etc/neutron/plugins/ml2/ml2_conf.ini
2014-03-12 14:55:44.791 INFO neutron.common.config [-] Logging enabled!
2014-03-12 14:55:44.792 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'list'] from (pid=1785) create_process /opt/stack/neutron/neutron/agent/linux/utils.py:48
2014-03-12 14:55:45.001 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'list']
Exit code: 0
Stdout: 'qdhcp-65cb66de-82d0-407c-aa23-2c544528f0d2\nqrouter-acc5f724-a169-4ffc-9e81-f00d43954509\nqrouter-5ed23337-9538-4994-823f-c64720506e54\n'
Stderr: '' from (pid=1785) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-03-12 14:55:47.006 ERROR neutron.agent.linux.dhcp [-] Error importing interface driver 'neutron.agent.linux.interface.OVSInterfaceDriver': no such option: ovs_use_veth
Error importing interface driver 'neutron.agent.linux.interface.OVSInterfaceDriver': no such option: ovs_use_veth
2nd) When we try to destroy a network, there's a dependency on the .namespace attribute of the network, that wasn't before.
Stderr: '' from (pid=1969) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-03-12 15:08:53.048 ERROR neutron.agent.netns_cleanup_util [-] Error unable to destroy namespace: qdhcp-65cb66de-82d0-407c-aa23-2c544528f0d2
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util Traceback (most recent call last):
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 131, in destroy_namespace
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util     kill_dhcp(conf, namespace)
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 86, in kill_dhcp
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util     dhcp_driver.disable()
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 181, in disable
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util     self.device_manager.destroy(self.network, self.interface_name)
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 814, in destroy
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util     self.driver.unplug(device_name, namespace=network.namespace)
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util AttributeError: 'FakeNetwork' object has no attribute 'namespace'
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util
3rd) This error will happen because no plugin rpc connection is provided,
and that's used in /opt/stack/neutron/neutron/agent/linux/dhcp.py as self.plugin.release_dhcp_port
2014-03-13 12:00:07.880 ERROR neutron.agent.netns_cleanup_util [-] Error unable to destroy namespace: qdhcp-388a37af-556d-4f4c-98b4-0ba41f944e32
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util Traceback (most recent call last):
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 132, in destroy_namespace
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util     kill_dhcp(conf, namespace)
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 87, in kill_dhcp
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util     dhcp_driver.disable()
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 181, in disable
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util     self.device_manager.destroy(self.network, self.interface_name)
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 816, in destroy
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util     self.plugin.release_dhcp_port(network.id,
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util AttributeError: 'NoneType' object has no attribute 'release_dhcp_port'
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util"
1033,1292105,neutron,3445715f98e3aaa967f1661e403e45ad85392909,1,1,Bug after a change.,ovs tunnel state not syncing (failure pinging over...,"I saw this in a recent CI overcloud run: http://logs.openstack.org/66/74866/8/check-tripleo/check-tripleo-overcloud-precise/aa490f1/console.html
2014-03-12 20:01:46.509 | Timing out after 300 seconds:
2014-03-12 20:01:46.509 | COMMAND=ping -c 1 192.0.2.46
2014-03-12 20:01:46.509 | OUTPUT=PING 192.0.2.46 (192.0.2.46) 56(84) bytes of data.
2014-03-12 20:01:46.509 | From 192.0.2.46 icmp_seq=1 Destination Host Unreachable
It appears as though everything ran fine up until it tried to ping the booted overcloud instance.  I'm fairly certain it has nothing to do with my change, so I wanted to open a bug to track it in case anyone else runs into a similar problem."
1034,1292114,neutron,f23d081b26071e1b309f49e5a5ab0cdc1a739e9d,1,1,Mixing cisco plugin when it is working,cisco plugin missing from few migration files caus...,"cisco plugin is missing from a few migration files causing missing DB tables. The following files in particular:
versions/folsom_initial.py
versions/176a85fc7d79_add_portbindings_db.py"
1035,1292119,cinder,371a58fd1d3fb1c2db7e1234f9c51f22b1b8787e,1,1,performance bug,Performance issue with brcd_fc_zone_client_cli.py,"Performance issue with brcd_fc_zone_client_cli.py
Every ssh command creates a new switch connection login and teardown which needs to be optimized
The more fiber channel zones the longer it takes, we added logging to capture this.
Add logging which tracks zoning on attach cinder/volume/manager.py
           vol_type = conn_info.get('driver_volume_type', None)
           if (vol_type == 'fibre_channel'):
               import uuid
               myid = uuid.uuid4()
               LOG.warn(""START ZONING!!!! %s"" % myid)
               self._add_or_delete_fc_connection(conn_info)
               LOG.warn(""END ZONING!!!! %s"" % myid)
           return conn_info
21 seconds to zone Brocade switch with no zones on the switch
2014-02-07 15:24:08.334 WARNING cinder.volume.manager [req-db7b03f8-be1e-4778-a319-c964fe8fbed8 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] START ZONING!!!!
<mdenny> 2014-02-07 15:24:29.391 WARNING cinder.volume.manager [req-db7b03f8-be1e-4778-a319-c964fe8fbed8 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] END ZONING!!!!
48 seconds to zone Brocade switch with 230 zones on the switch
<mdenny> 2014-02-08 15:22:05.219 WARNING cinder.volume.manager [req-8d8dfdae-e4e7-400c-973a-e461e5e3e5c6 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] START ZONING !!!!
<mdenny> 2014-02-08 15:22:53.258 WARNING cinder.volume.manager [req-8d8dfdae-e4e7-400c-973a-e461e5e3e5c6 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] END ZONING!!!!
This occurred on FOS v6.4.3 and v7.0.2
Re-factor connection by creating one login connection with locking and performing all or a group of ssh commands might be a solution."
1036,1292173,neutron,e0f69d69293f0ffba22a6540f483f05baa48cd6f,0,0,Remove list events,Remove list events API from Cisco N1kv neutron,"Earlier Cisco was using the list events api to poll policies from VSM.
It was inefficient and caused delay in processing.
So, now Cisco switched to list profiles to poll policies from VSM."
1037,1292181,nova,c5e17f4dc2277ed422b060f860443d71a47fc440,1,1,,Bug #1292181 “Cells,"Prior to a rebuild, nova issues a compute.instance.exists notification to capture usage for the period between the beginning of the day and the rebuild.
It should have the info for the instance prior to the rebuild, so if you are rebuilding from Centos 6.0 to Ubuntu 10.04, it should list a os_distribution of centos and an os_version of 6.0 Instead it's listing a distribution of Ubuntu and an os_version of 10.04 (i.e. the info for after the rebuild)."
1038,1292185,nova,c5e17f4dc2277ed422b060f860443d71a47fc440,1,1,,Bug #1292185 “Cells,"When updating the metadata for an instance, the values should end up in xenstore for a guest to be able to query.  This is related to https://bugs.launchpad.net/nova/+bug/1292181 but with instance objects metadata is now synced down to cells earlier than it used to be.  This causes an issue with the metadata diff detection at the cell level so new keys are not pushed to the virt driver.  This causes them to not be set in xenstore in the xenserver driver."
1039,1292243,nova,1d44bc5d7ac4adb1ad8a3aaabf0c27c593ec07e1,0,0,No bug. should delete all neutron ports on error,Bug #1292243 “deallocate_for_instance should delete all neutron ... ,"When deleting an instance if an instance has multiple ports and one
of the deletes fail nova should LOG an error and continue trying
to delete the other ports. Previously, nova would stop deleting ports
are the first non 404 error."
1040,1292285,nova,8ff06df88b1f979f5a67f9d55fa0e828a6e73d04,1,0,version sql or related to a special db,equal_any() DB API helper produces incorrect SQL q...,"Given an attribute name and a list of values equal_any() is meant to produce a WHERE clause which returns rows for which the column (denoted by an attribute of an SQLAlchemy model) is equal to ANY of passed values that involves using of SQL OR operator. In fact, AND operator is used to combine equality expressions.
E.g. for a model:
class Instance(BaseModel):
    __tablename__ = 'instances'
   id = sa.Column('id', sa.Integer, primary_key=True)
   ...
   task_state = sa.Column('task_state', sa.String(30))
using of equal_any():
  q = model_query(context, Instance).
  constraint = Constraint({'task_state': equal_any('error', 'deleting')})
  q = constraint.apply(Instance, q)
will produce:
SELECT * from instances
WHERE task_state = 'error' AND task_state = 'deleting'
instead of expected:
SELECT * from instances
WHERE task_state = 'error' OR task_state = 'deleting'"
1041,1292309,nova,e0844f257cd170d01255aa3f8bfd48ff50731f16,0,0,Catch a new exception,Exception handle in be better when create a flavor...,"If I create a flavor but failed to operate on db layer , a InstanceTypeCreateFailed exception will be raised but
nova-api didn't handle it well, it will report
[root@controller ~]# nova flavor-create test1 111 512 1 1
ERROR: The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-74050da0-6f78-408e-8a1b-97c2a03e597d)
give more accurate reason will be better"
1042,1292339,nova,1c0c1eb01d45eb016ba6b2ad63b9e3e652365b34,0,0,a trace log can be ignored. No bug,instance not found trace log can be ignored while ...,"during we cold-migrating or resizing an instance to another host(or during deleting it), the get_console_output
method may raise an InstanceNotFound excetpion if the instance is not on the hypervisor, this is an expected error,
so we should add the InstanceNotFound excetpion to expected exceptions list in the compute manager.
2014-03-14 11:59:58.884 AUDIT nova.compute.manager [req-6414594a-7fcd-427e-9ed6-1d78f12d40e0 demo demo] [instance: c68b2e95-8299-415a-a837-ff9f7303e6db] Get console output
2014-03-14 11:59:58.901 ERROR oslo.messaging._executors.base [-] Exception during message handling
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base Traceback (most recent call last):
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/oslo.messaging/oslo/messaging/_executors/base.py"", line 36, in _dispatch
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     incoming.reply(self.callback(incoming.ctxt, incoming.message))
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 134, in __call__
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     return self._dispatch(endpoint, method, ctxt, args)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 104, in _dispatch
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/server.py"", line 153, in inner
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     return func(*args, **kwargs)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     payload)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     six.reraise(self.type_, self.value, self.tb)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     return f(self, context, *args, **kw)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/compute/manager.py"", line 293, in decorated_function
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     return function(self, context, *args, **kwargs)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/compute/manager.py"", line 3932, in get_console_output
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     output = self.driver.get_console_output(context, instance)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2268, in get_console_output
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     virt_dom = self._lookup_by_name(instance.name)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3437, in _lookup_by_name
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     raise exception.InstanceNotFound(instance_id=instance_name)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base InstanceNotFound: Instance instance-0000000c could not be found.
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base"
1043,1292380,cinder,3f67de92cfcfc61ca26156961e1f2d4d2ebded66,1,1,The info of the first exception is lost,In some case lose information of the first excepti...,"When an exception occurs during exception handling, it lose the
information of the first exception.
In SwiftBackupDriver.backup(), in some cases, before re-transmission
of the exception, the exception is rewritten."
1044,1292455,glance,5b0ec5a3be0d92687c552f197f5474d5066a5a5c,1,1,"The info of the first exception is lost, again",In some case lose information of the first excepti...,"When an exception occurs during exception handling, it lose the
information of the first exception.
In glance.store.rbd.Store.add(), in some cases, before re-transmission
of the exception, the exception is rewritten."
1045,1292494,nova,362e998e89a1f1d9dce3379c4610b205ebeec7c0,1,1,Strange values when updating quota,Invalid validation for unlimited values when updat...,"There is a possible scenario that is not correctly handled when validating quota limit. If the remaining quota is unlimited (-1) and new quota value is unlimited (-1) then both values are summed resulting in a -2 value that then causes the following error: ""Quota limit must be greater than 0""."
1046,1292633,nova,d39442007157c3074c199aba64424b2f4b08f461,1,1,,Bug #1292633 “VMware,"When using VC Driver, booting an ISO will fail when a flavor with a 0 GB root disk size is specified.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1703, in _spawn
    block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 598, in spawn
    admin_password, network_info, block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 562, in spawn
    _create_virtual_disk(dest_vmdk_path, root_gb_in_kb)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 370, in _create_virtual_disk
    self._session._wait_for_task(vmdk_create_task)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 906, in _wait_for_task
    ret_val = done.wait()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
    return hubs.get_hub().switch()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
    return self.greenlet.switch()
VMwareDriverException: A specified parameter was not correct.
The error message seen in vSphere client is ""The virtual disk size is too small"".
Log available here: http://paste.openstack.org/show/73516/"
1047,1292644,nova,6fcb34b825b6fe4ae750f6d38d9cf62fa0254a30,0,0, nova.compute.api should return Objects ,nova.compute.api should return Objects,"nova.compute.api should return Aggregate Objects, and they should be converted into the REST API expected results in the aggregates API extensions."
1048,1292733,nova,d3acac0f5bffca59441d9a4a12c89db1d45ec4cf,1,1,Ironic bug,Bug #1292733 “Ironic,"During instance spawn, Ironic attempts to unplug any plugged VIFs from ports associated with an instance.  If there are no associated VIFs to unplug, instance spawn fails with a nova-compute errror:
2014-03-14 21:15:35.907 16640 TRACE nova.openstack.common.loopingcall HTTPBadRequest: Couldn't apply patch '[{'path': '/extra/vif_port_id', 'op': 'remove'}]'. Reason: u'vif_port_id'
The driver should be only attempt to unplug VIFs from ports that actually have them associated."
1049,1292742,neutron,773352e39f371cda9c2f1be4c55accf4b7dc017b,0,0,It’s a bug related to the tests,supported_extension_aliases in cisco network_plugi...,"In Cisco network_plugin unit tests, supported_extension_aliases grows while running unit tests.
It is because cisco network plugin extends supported_extension_aliases.
supported_exntension_aliases is a class attribute, so it will not reset to the original even after each unit test finished.
Message like this is annoying when debugging unit tests.
Note that in a real environment the plugin is initialized only once and there is no negative impact.
2014-03-15 08:35:39,867     INFO [neutron.manager] Loading core plugin: neutron.plugins.cisco.network_plugin.PluginV2
2014-03-15 08:35:39,928     INFO [neutron.plugins.openvswitch.ovs_neutron_plugin] Network VLAN ranges: {'physnet1': [(1000, 1100)]}
2014-03-15 08:35:39,957     INFO [neutron.manager] supported_extension_aliases=['credential', 'Cisco qos', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'ag
ent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', '
agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute',
 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute
', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_schedule
r', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_sched
uler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_sch
eduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_
opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-addres
s-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs']
2014-03-15 08:35:39,957     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,957     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.api.extensions] Initializing extension manager.
2014-03-15 08:35:39,959    ERROR [neutron.api.extensions] Extension path 'unit/extensions' doesn't exist!
2014-03-15 08:35:39,959     INFO [neutron.api.extensions] Loading extension file: __init__.py
2014-03-15 08:35:39,959     INFO [neutron.api.extensions] Loading extension file: __init__.pyc
2014-03-15 08:35:39,959     INFO [neutron.api.extensions] Loading extension file: _credential_view.py
2014-03-15 08:35:39,959     INFO [neutron.api.extensions] Loading extension file: _qos_view.py"
1050,1292764,glance,3bbdc70d4e1d0a8c960d7d0a7634f10b4b3163fe,1,1,,Bug #1292764 “Tempest failure,"This failure happens with the vmware backend when the size provided to the store is zero but the actual data size is larger than zero.
2014-03-14 17:22:53 | [10.36.11.112] out: Traceback (most recent call last):
2014-03-14 17:22:53 | [10.36.11.112] out:   File ""tempest/api/image/v1/test_images.py"", line 50, in test_register_then_upload
2014-03-14 17:22:53 | [10.36.11.112] out:     self.assertEqual(1024, body.get('size'))
2014-03-14 17:22:53 | [10.36.11.112] out:   File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 321, in assertEqual
2014-03-14 17:22:53 | [10.36.11.112] out:     self.assertThat(observed, matcher, message)
2014-03-14 17:22:53 | [10.36.11.112] out:   File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 406, in assertThat
2014-03-14 17:22:53 | [10.36.11.112] out:     raise mismatch_error
2014-03-14 17:22:53 | [10.36.11.112] out: MismatchError: 1024 != 0"
1051,1292782,glance,6b8a91940b4fd410098e366689f2753010fc1ec1,1,1,It’s like a typo in msg,test_create_backup 500 on image get,"Log stash:
message: ""Object GET failed"" AND filename:""logs/screen-g-api.txt""
53 failure in the past 7 days.
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/console.html#_2014-03-14_18_26_47_575
Test runner worker pid: 1541
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/logs/tempest.txt.gz#_2014-03-14_17_55_47_916
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/logs/screen-g-api.txt.gz#_2014-03-14_17_55_47_912
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/logs/screen-g-api.txt.gz#_2014-03-14_17_55_47_912"
1053,1292984,nova,8d2b250105564bc3288bf01881dfdc7f48012708,0,0,remove log msg,Log message “fetching image %s from glance,"In nova.image.glance:
def get_remote_image_service(context, image_href):
    """"""Create an image_service and parse the id from the given image_href.
    The image_href param can be an href of the form
    'http://example.com:9292/v1/images/b8b2c6f7-7345-4e2f-afa2-eedaba9cbbe3',
    or just an id such as 'b8b2c6f7-7345-4e2f-afa2-eedaba9cbbe3'. If the
    image_href is a standalone id, then the default image service is returned.
    :param image_href: href that describes the location of an image
    :returns: a tuple of the form (image_service, image_id)
    """"""
    # Calling out to another service may take a while, so lets log this
    LOG.debug(_(""fetching image %s from glance"") % image_href)
    #NOTE(bcwaldon): If image_href doesn't look like a URI, assume its a
    # standalone image ID
    if '/' not in str(image_href):
        image_service = get_default_image_service()
        return image_service, image_href
    try:
        (image_id, glance_host, glance_port, use_ssl) = \
            _parse_image_ref(image_href)
        glance_client = GlanceClientWrapper(context=context,
                host=glance_host, port=glance_port, use_ssl=use_ssl)
    except ValueError:
        raise exception.InvalidImageRef(image_href=image_href)
    image_service = GlanceImageService(client=glance_client)
    return image_service, image_id
Clearly the LOG.debug() message above is incorrect. The method does not fetch an image at all. It just returns an ImageService object."
1054,1292997,nova,048cd541c7066185b3802bded7b2f009859443c3,1,1,Correct inheritance of nova.volume.cinder.API,nova.volume.cinder.API incorrectly derives from no...,"For some reason, nova.volume.cinder.API derives from nova.db.base.Base, which looks like this (in its entirety):
class Base(object):
    """"""DB driver is injected in the init method.""""""
    def __init__(self, db_driver=None):
        super(Base, self).__init__()
        if not db_driver:
            db_driver = CONF.db_driver
        self.db = importutils.import_module(db_driver)  # pylint: disable=C0103
I checked and nova.volume.cinder.API makes no reference at all to self.db, therefore unless I am mistaken, there's no reason for this inheritance."
1055,1293083,neutron,e13d19cab384a9f5f8a00436ad39118f342af32c,1,1,report interval leading to overload,report_interval too frequent; Causing load on serv...,"report_interval is how often an agent sends out a heartbeat to the service. The Neutron service responds to these 'report_state' RPC messages by updating the agent's heartbeat DB record. The last heartbeat is then compared to the configured agent_down_time to determine if the agent is up or down. The agent's status is used when scheduling networks on DHCP and L3 agents.
The defaults are 4 seconds for report_interval and 9 for agent_down_time.
On a setup with 18 agents (15 layer 2, L3, DHCP, metadata) sitting on 16 nodes, and a Neutron service sitting on a dedicated powerful machine, the service was idle with 20% CPU usage. Changing the report_interval to 28 seconds and agent_down_time to 60 seconds changed the CPU usage to 1%, and allowed bulk operations on a larger scale. (In this case: Creating 30 instances at the same time with 60 ports). With the original values the operation failed (The instances did not get IP addresses), and with the new values we were able to boot 60 instances successfully. Side note: This flow will work better once the Nova-Neutron race is resolved, but that's orthogonal to this proposal."
1056,1293184,neutron,7a2053c7a27bdec02e97b46736c5fcce5f2d053a,1,1,Bug,Can't clear shared flag of unused network,"A network marked as external can be used as a gateway for tenant routers, even though it's not necessarily marked as shared.
If the 'shared' attribute is changed from True to False for such a network you get an error:
Unable to reconfigure sharing settings for network sharetest. Multiple tenants are using it
This is clearly not the intention of the 'shared' field, so if there are only service ports on the network there is no reason to block changing it from shared to not shared."
1057,1293298,nova,4ecc1c52dbe02d5c4211d3936e2eec57dcc22de3,0,0,remove import,Don't import guestfs  directly,"Library guestfs is not in requirements file, but is imported directly
in nova/virt/disk/vfs/guestfs.py. That conflicts with global variable
guestfs, leads out one more time import and shadow issue, so remove it
from import group."
1058,1293435,nova,cfdd9441b1545b5d89be6e8bca036ba185099f12,1,1,Fix location,Bug #1293435 “VMware,"A rescue of an image that is not linked clone will leave the rescue image disk in the original VM folder. The disk will not be cleaned up properly.
This leads to a number of problems:
1. usage of unnecessary disk space
2. additional rescues for the same VM may fail"
1059,1293508,neutron,13c9f3b813f5bb368e311ba0d428fa759d68289a,1,1,Wrong code 500 instead of 400 and add error msg,Bug #1293508 “NSX plugin,"When a gateway device is created, the client certificate is not stored anywhere on the neutron server, but then passed directly to the backend, which validates the certificate.
Currently the NSX backend raises an exception when the certificate is not valid.
This exception is treated by the NSX plugin as a backend failure and a 500 is then returned.
However, the correct error would a 400 with an appropriate error message."
1060,1293587,neutron,b5917e35acb6189079f33ebb5562b8d2288dcd4f,0,0,add support for https,https support for nova metadata requests,Current metadata agent supports only http connection to nova metadata service. Implement https support.
1061,1293641,nova,dba898226ec26ab49cecd24e9d6c255cf6153cd1,1,1,deadlock,Libvirt's close callback causes deadlocks,"Libvirt's close callback causes deadlocks
Unlike libvirt's lifecycle event callback which is triggered every time an event occurs, the close callback is only triggered when an attempt is made to use a connection that has been closed.  In that case, the sequence of events is usually as follows:
LibvirtDriver._get_connection acquires _wrapped_conn_lock
LibvirtDriver._get_connection calls _test_connection
LibvirtDriver._test_connection calls libvirt.getLibVersion
libvirt.getLibVersion triggers LibvirtDriver._close_callback (because the connection is closed and this method is the registered handler)
LibvirtDriver._close_callback attempts to acquire _wrapped_conn_lock
_get_connection cannot release the lock because it is waiting for _close_callback to return and the latter cannot complete until it has acquired the lock.
Making the handling of the close callback asynchronous (like the lifecycle event handling) won't work, because by the time the lock is released, the connection object that was passed into the callback will no longer be equal to LibvirtDriver._wrapped_conn.  Even if the connection object is ignored, the instance will have already been disabled via the _get_new_connection method's existing error-handling logic.
The best solution would appear to be to simply not register a close callback.  The only case where it might provide some benefit is when a connection is closed after _get_connection has returned a reference to it.  The benefit of disabling the instance a little earlier in such marginal cases is arguably outweighed by the complexity of a thread-safe implementation, especially when the difficulty of testing such an implementation (to ensure it is indeed thread safe) is taken into consideration.
Note that having _close_callback use _wrapped_conn_lock.acquire(False) instead of ""with _wrapped_conn_lock"" by itself is not a viable solution, because due to connections being opened via tpool.proxy_call, the close callback is called by a native thread, which means it should not be used to perform the various operations (including logging) involved in disabling the instance."
1062,1293750,nova,8e3dc81837ddb76dff5267ff5fbbd750b20fc31f,1,1,Bug. Object doesnt have attribute,Bug #1293750 “Cells,"When a delete is issued for an instance that doesn't have a cell_name in the db, a delete is broadcast to all cells. As that message passes through the cells rpc layer the instance is converted from an object to a dict. This causes a problem when it gets to the cell since the delete methods expect to receive an object.
2014-03-13 15:21:49.717 31063 ERROR nova.cells.messaging [req-d2d5f4ea-4010-405a-b52a-8f19d3991498 10110789 5877036] Error processing message locally: 'dict' object has no attribute 'disable_terminate'
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging Traceback (most recent call last):
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 211, in _process_locally
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 1291, in _process_message_locally
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 1101, in instance_delete_everywhere
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging self.compute_api.delete(message.ctxt, instance)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 199, in wrapped
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 189, in inner
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return function(self, context, instance, *args, **kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 216, in _wrapped
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return fn(self, context, instance, *args, **kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 170, in inner
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return f(self, context, instance, *args, **kw)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 1710, in delete
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging self._delete_instance(context, instance)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 1700, in _delete_instance
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging task_state=task_states.DELETING)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 1395, in _delete
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging if instance.disable_terminate:
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging AttributeError: 'dict' object has no attribute 'disable_terminate'
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging"
1063,1293792,cinder,4cc2366623743393f89a198581fcb69dc04d31cd,0,0,tests,test_volume_get_all_filters_limit incorrectly asse...,"cinder.tests.test_db_api.DBAPIVolumeTestCase.test_volume_get_all_filters_limit
This test indirectly asserts that the metadata on a volume is returned in a certain order. This is incorrect because python dictionaries to not maintain ordering. This causes spuratic unit test failures with this error:
http://paste.openstack.org/show/73692/"
1064,1293799,neutron,dafe6598c70e74781053acf68a8d5a7cea064a0e,1,1,Bug. Change order,BigSwitch consistency watchdog launched too soon,"The BigSwitch consistency watchdog is launched too soon during initialization.
It's launched before the server pool has added the servers so by the time the greenthread initializes, the pool is not fully populated."
1065,1293818,neutron,d37b6c9ee8ddf2cc90197e5a442ba759eca5a0c0,1,1,Bug using root,Agents don't need root to list namespaces,"Given the expense of sudo (at scale) and rootwrap calls, agents should not be using root for commands that don't need it.  Listing namespaces is one of those.
(I could have sworn I already fixed this which is why I didn't fix it until today)"
1066,1293938,nova,8a50755b9df445a07140f385f1ff32db20bf683b,0,0,Changing in the code for the tests,nova.image.glance unit tests should not use fake g...,"The unit tests in nova.tests.image.test_glance unfortunately make use of a faked glance image service in nova.tests.image.fakes. What this does is actually mask a number of problems and makes it harder to assert for specific behavior in the real glanceclient client classes.
The unit tests should be rewritten to simply mock out the very specific code boundaries where tested calls interact with the glanceclient client classes, and that's it. Unit tests should just test one little unit of code, not giant swathes of dependent code."
1067,1294069,nova,1b2570877846598d3545a9c9f31680f2a995491f,1,1,,Bug #1294069 “XenAPI,"https://review.openstack.org/#/c/78194/ changed tempest to clear image_ref for some BFV tests - in particular the test_volume_boot_pattern
This now results in a ""KeyError: 'disk_format'"" exception from Nova when using the XenAPI driver.
http://paste.openstack.org/show/73733/ is a nicer format of the below - but might disappear!
2014-03-18 11:20:07.475 ERROR nova.compute.manager [req-82096fe0-921a-4bc1-9c41-d0aafad4c923 TestVolumeBootPattern-581093620 TestVolumeBootPattern-1800543246] [instance: 2b047f24-675c-4921-8cf3-85584097f106] Error: 'disk_format'
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] Traceback (most recent call last):
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1306, in _build_instance
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     set_access_ip=set_access_ip)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/compute/manager.py"", line 394, in decorated_function
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     return function(self, context, *args, **kwargs)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1708, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     LOG.exception(_('Instance failed to spawn'), instance=instance)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     six.reraise(self.type_, self.value, self.tb)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1705, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     block_device_info)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/driver.py"", line 236, in spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     admin_password, network_info, block_device_info)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 357, in spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     network_info, block_device_info, name_label, rescue)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 526, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     undo_mgr.rollback_and_reraise(msg=msg, instance=instance)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/utils.py"", line 812, in rollback_and_reraise
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     self._rollback()
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     six.reraise(self.type_, self.value, self.tb)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 501, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     disk_image_type = determine_disk_image_type_step(undo_mgr)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 146, in inner
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     rv = f(*args, **kwargs)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 414, in determine_disk_image_type_step
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     return vm_utils.determine_disk_image_type(image_meta)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vm_utils.py"", line 1647, in determine_disk_image_type
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     disk_format = image_meta['disk_format']
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] KeyError: 'disk_format'
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]
I've confirmed that running without https://review.openstack.org/#/c/78194/ passes the tests (although there is a race condition, which is why test_volume_boot_pattern is disabled in the XenServer CI system) but will always fail with https://review.openstack.org/#/c/78194/ applied."
1068,1294096,neutron,acf0e109f3b01e4918d49305a6d95b3732a10b4e,0,0,some cleanup,Bug #1294096 “LBaaS,"Need to remove unnecessary 'context' parameter from a couple of methods in
neutron.services.loadbalancer.agent.agent_device_driver.AgentDeviceDriver"
1069,1294166,neutron,e10f406a653f3037109e2d7f59bed65bd54300a8,1,1,,nec plugin fails with XML test_list_ports_binding_...,"commit 1ec6e18007691c92fc27235c677d11b0fe1c1f6b in tempest adds a validation for neutron port binding extension, but the check is too strict. binding:profile attribute in port binding extension is designed to allow plugin specific field but the check assumes binding:profile attribute implemented in ML2 plugin. This leads to test failures with neutron plugins which has different keys in binding:profile like NEC plugin or Mellanox plugin (though mellanox plugin does not test the related test now).
The failed test is test_list_ports_binding_ext_attr. It only affects XML.
  tempest.api.network.test_ports.PortsAdminExtendedAttrsIpV6TestXML test_list_ports_binding_ext_attr
  tempest.api.network.test_ports.PortsAdminExtendedAttrsTestXML test_list_ports_binding_ext_attr
Tempest test should allow plugin-specifc binding:profile attribute. If not, it should provide an options to disable these tests."
1070,1294308,neutron,9014f66fce619af6d72823b3fdb0ecf148582649,1,1,,ML2 bigswitch driver modifies PortContext.current,"The ML2 bigswitch mechanism driver's create_port_postcommit() and update_port_postcommit() methods pass the PortContext to a _prepare_port_for_controller() method that modifies the port dictionary accessed as PortContext.current. This modified port dictionary is currently returned by ML2 as the result of the port create or update operation, but the changes do not get persisted in the DB. The fix for bug 1276391 is likely to result in these changes no longer being returned to the client. Mechanism drivers are not supposed to modify this port dictionary, except by calling PortContext.set_binding() from within bind_port(). It does not appear that this mechanism driver actually binds ports. If the port dictionary needs to be modified before being passed to the bigswitch controller, it should be copied first.
Also, the TestBigSwitchMechDriverPortsV2.test_port_vif_details() unit test asserts that the returned binding:vif_type is the modified value, so this test will also need to be changed or eliminated as part of this fix."
1071,1294346,nova,3ad414597043979b8d73c17c05e666cff33a9b88,1,1,keyerror,When creating Neutron Security Group Rules with a ...,"With the following set in /etc/nova/nova.conf:
security_group_api=neutron
You can view security groups and rules that have been created in Neutron with nova secgroup-* commands.
If you create a Neutron Security Group rule with a different protocol though, nova secgroup-* calls fail with a 500 and a lot of stack trace in /var/log/nova/nova-api-os-compute.log:
<snip>
014-03-18 20:23:46.599 25278 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/contrib/security_groups.py"", line 215, in _format_security_group_rule
2014-03-18 20:23:46.599 25278 TRACE nova.api.openstack     sg_rule['from_port'] = rule['from_port']
2014-03-18 20:23:46.599 25278 TRACE nova.api.openstack KeyError: 'from_port'
2014-03-18 20:23:46.599 25278 TRACE nova.api.openstack
2014-03-18 20:23:46.600 25278 INFO nova.api.openstack [req-507402d7-788e-413a-a005-b852e1b7efa2 3d0524290859416f886f49a2973ab616 1be2c0f9589d4822856a9ac2e16f0406] http://10.240.0.100:8774/v2/1be2c0f9589d4822856a9ac2e16f0406/os-security-groups returned with HTTP 500
2014-03-18 20:23:46.601 25278 INFO nova.osapi_compute.wsgi.server [req-507402d7-788e-413a-a005-b852e1b7efa2 3d0524290859416f886f49a2973ab616 1be2c0f9589d4822856a9ac2e16f0406] 10.240.0.100 ""GET /v2/1be2c0f9589d4822856a9ac2e16f0406/os-security-groups HTTP/1.1"" status: 500 len: 335 time: 0.0474379
To recreate:
# Test nova secgroup-list works
nova secgroup-list
+--------------------------------------+-------------+-------------+
| Id                                   | Name        | Description |
+--------------------------------------+-------------+-------------+
| ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9 | default     | default     |
+--------------------------------------+-------------+-------------+
# Add rule with a different protocol
neutron security-group-rule-create --direction ingress --protocol 50 --remote-ip-prefix 0.0.0.0/0 ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9
Created a new security_group_rule:
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | d98e83cf-2aab-4eec-89ed-f9aa4d00d57b |
| port_range_max    |                                      |
| port_range_min    |                                      |
| protocol          | 50                                   |
| remote_group_id   |                                      |
| remote_ip_prefix  | 0.0.0.0/0                            |
| security_group_id | ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9 |
| tenant_id         | 1be2c0f9589d4822856a9ac2e16f0406     |
+-------------------+--------------------------------------+
# Test
neutron security-group-list # works
nova secgroup-list # now errors
# Delete rule
neutron security-group-rule-delete d98e83cf-2aab-4eec-89ed-f9aa4d00d57b
Deleted security_group_rule: d98e83cf-2aab-4eec-89ed-f9aa4d00d57b
# Test nova again
nova secgroup-list
+--------------------------------------+-------------+-------------+
| Id                                   | Name        | Description |
+--------------------------------------+-------------+-------------+
| ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9 | default     | default     |
+--------------------------------------+-------------+-------------+"
1072,1294368,neutron,7026d96ce44522c95c785376178347d2b0a9d750,1,1,,Hyper-V agent security groups rules are not applie...,"The Hyper-V agent in Windows Server 2012 R2 is currently applying the security group rules wrong due to two reasons:
First, when assigning a weight for a rule, the agent takes into account the weight of the default reject rules, which causes certain reject rules to have a higher priority than allow rules.
Secondly, if a rule with no port_range_min and no port_range_max defined is applied first, it is replaced by the rules with the port range defined. This can cause faulty connectivity for the virtual machine, including for the default security group."
1073,1294434,neutron,ccfe47d4b9c2abaa25b3b2a2dc655c32beb0b377,0,0,"Implementation, ",Add missing ondelete foreignkey constraint to Cisc...,"Current DB migration script to populate N1kv tables is missing the ondelete=""CASCADE"" option in the ForeignKey constraint for network and port binding tables. This causes the network and port delete calls to fail with Integrity error. This occurs when the database is populated with db migration scripts.
super(N1kvNeutronPluginV2, self).delete_network(context, id)
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 1013, in delete_network
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource     context.session.delete(network)
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource     self.commit()
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource     self._prepare_impl()
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource     self.session.flush()
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 616, in _wrap
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource     raise exception.DBError(e)
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource DBError: (IntegrityError) (1451, 'Cannot delete or update a parent row: a foreign key constraint fails (`cisco_neutron`.`cisco_n1kv_network_bindings`, CONSTRAINT `cisco_n1kv_network_bindings_ibfk_1` FOREIGN KEY (`network_id`) REFERENCES `networks` (`id`))') 'DELETE FROM networks WHERE networks.id = %s' ('12fb857a-cb23-44f0-81a4-9cc8fc669de0',)"
1074,1294445,neutron,77ddd463bd2e96041c0b6ecea1c46c4ac4d55851,0,0,I think is implementation of keeping a record of the problems,Record and log reason for dhcp agent resync,"A dhcp resync can be triggered at a number of points, but the actual resync is done asynchronously by a helper thread. This means by the time the resync happens, it's hard to establish what actually caused it.
I've seen a number of problems in production systems that cause excessive resyncs. One is a ipv6/dnsmasq issue (rhbz#1077487) and another is db corruption with duplicate entries [1]. The resync triggers a whole lot of logs itself, so it becomes very unclear how to establish any causality.
What I propose is to keep track of what triggered the resync with some helpful information.
[1] The logs will contain output like ""DBDuplicateEntry
(IntegrityError) (1062, ""Duplicate entry
'6d799c6a-7a09-4c1e-bb63-7d30fd052c8a-d3e3ac5b-9962-428a-a9f8-6b2' for
key 'PRIMARY'"") ..."" in this case"
1075,1294527,neutron,a79aa79b2ae1222bd8776cde10bebeefc6bc8791,0,0, To avoid unnecessary user-visible errors ,Bug #1294527 “nec plugin,OpenFlow controller which nec plugin talks to sometimes returns retry-after when it is busy. It is better to honor retry-after header to avoid unnecessary user-visible errors due to temporary busy condition.
1076,1294537,neutron,98fc828801f370c0561f22d97ffdd1055d4e0663,1,1,,Sync excutils from oslo,In order to fix undesired error logs in Neutron (bug 1288188) fixed save_and_reraise_exception() should be synced from oslo.
1077,1294568,neutron,5d6cb0c62e0245735a9d511d2f871776055ff224,1,1,Bug. Change the name to avoid conflicts,Unable to create the Neutron network net_local bec...,"CREATE TABLE subnets (tenant_id VARCHAR(255),id VARCHAR(36) NOT NULL, name VARCHAR(255),network_id VARCHAR(36), ip_version INT NOT NULL, cidr VARCHAR(64) NOT NULL,gateway_ip VARCHAR(64),enable_dhcp SMALLINT,shared SMALLINT,ipv6_ra_mode VARCHAR(16), ipv6_address_mode VARCHAR(16),PRIMARY KEY (id), FOREIGN KEY(network_id) REFERENCES networks (id),CHECK (enable_dhcp IN (0, 1)), CHECK (shared IN (0, 1)), CONSTRAINT ipv6_modes CHECK (ipv6_ra_mode IN ('slaac', 'dhcpv6-stateful', 'dhcpv6-stateless')),CONSTRAINT ipv6_modes CHECK (ipv6_address_mode IN ('slaac', 'dhcpv6-stateful', 'dhcpv6-stateless')))
for db2, this fails because the name ipv6_modes is used twice for a contraint name.  In db2, A constraint-name must not identify a constraint that was already specified within the same CREATE TABLE statement. (SQLSTATE 42710).
=============
Checked neutron server.log and found
2014-03-18 18:37:45.799 19954 TRACE neutron Traceback (most recent call last):
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/bin/neutron-server"", line 10, in <module>
2014-03-18 18:37:45.799 19954 TRACE neutron     sys.exit(main())
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/server/__init__.py"", line 54, in main
2014-03-18 18:37:45.799 19954 TRACE neutron     neutron_api = service.serve_wsgi(service.NeutronApiService)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 113, in serve_wsgi
2014-03-18 18:37:45.799 19954 TRACE neutron     LOG.exception(_('Unrecoverable error: please check log '
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/excutils.py"", line 68, in __exit_
_
2014-03-18 18:37:45.799 19954 TRACE neutron     six.reraise(self.type_, self.value, self.tb)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 106, in serve_wsgi
2014-03-18 18:37:45.799 19954 TRACE neutron     service.start()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 75, in start
2014-03-18 18:37:45.799 19954 TRACE neutron     self.wsgi_app = _run_wsgi(self.app_name)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 175, in _run_wsgi
2014-03-18 18:37:45.799 19954 TRACE neutron     app = config.load_paste_app(app_name)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/common/config.py"", line 170, in load_paste_app
2014-03-18 18:37:45.799 19954 TRACE neutron     app = deploy.loadapp(""config:%s"" % config_path, name=app_name)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 247, in loadapp
2014-03-18 18:37:45.799 19954 TRACE neutron     return loadobj(APP, uri, name=name, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 272, in loadobj
2014-03-18 18:37:45.799 19954 TRACE neutron     return context.create()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 710, in create
2014-03-18 18:37:45.799 19954 TRACE neutron     return self.object_type.invoke(self)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-03-18 18:37:45.799 19954 TRACE neutron     **context.local_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/util.py"", line 56, in fix_call
2014-03-18 18:37:45.799 19954 TRACE neutron     val = callable(*args, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/urlmap.py"", line 25, in urlmap_factory
2014-03-18 18:37:45.799 19954 TRACE neutron     app = loader.get_app(app_name, global_conf=global_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-03-18 18:37:45.799 19954 TRACE neutron     name=name, global_conf=global_conf).create()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 710, in create
2014-03-18 18:37:45.799 19954 TRACE neutron     return self.object_type.invoke(self)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-03-18 18:37:45.799 19954 TRACE neutron     **context.local_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/util.py"", line 56, in fix_call
2014-03-18 18:37:45.799 19954 TRACE neutron     val = callable(*args, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/auth.py"", line 69, in pipeline_factory
2014-03-18 18:37:45.799 19954 TRACE neutron     app = loader.get_app(pipeline[-1])
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-03-18 18:37:45.799 19954 TRACE neutron     name=name, global_conf=global_conf).create()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 710, in create
2014-03-18 18:37:45.799 19954 TRACE neutron     return self.object_type.invoke(self)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 146, in invoke
2014-03-18 18:37:45.799 19954 TRACE neutron     return fix_call(context.object, context.global_conf, **context.local_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/util.py"", line 56, in fix_call
2014-03-18 18:37:45.799 19954 TRACE neutron     val = callable(*args, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 71, in factory
2014-03-18 18:37:45.799 19954 TRACE neutron     return cls(**local_config)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 75, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron     plugin = manager.NeutronManager.get_plugin()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 211, in get_plugin
2014-03-18 18:37:45.799 19954 TRACE neutron     return cls.get_instance().plugin
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 206, in get_instance
2014-03-18 18:37:45.799 19954 TRACE neutron     cls._create_instance()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-03-18 18:37:45.799 19954 TRACE neutron     return f(*args, **kwargs)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 200, in _create_instance
2014-03-18 18:37:45.799 19954 TRACE neutron     cls._instance = cls()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 112, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron     plugin_provider)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 140, in _get_plugin_instance
2014-03-18 18:37:45.799 19954 TRACE neutron     return plugin_class()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 105, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron     super(Ml2Plugin, self).__init__()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/db/db_base_plugin_v2.py"", line 225, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron     db.configure_db()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/db/api.py"", line 34, in configure_db
2014-03-18 18:37:45.799 19954 TRACE neutron     register_models()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/db/api.py"", line 53, in register_models
2014-03-18 18:37:45.799 19954 TRACE neutron     base.metadata.create_all(engine)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/schema.py"", line 2571, in create_all
2014-03-18 18:37:45.799 19954 TRACE neutron     tables=tables)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 2302, in _run_visitor
2014-03-18 18:37:45.799 19954 TRACE neutron     conn._run_visitor(visitorcallable, element, **kwargs)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1972, in _run_visitor
2014-03-18 18:37:45.799 19954 TRACE neutron     **kwargs).traverse_single(element)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/sql/visitors.py"", line 106, in traverse_singl
e
2014-03-18 18:37:45.799 19954 TRACE neutron     return meth(obj, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/ddl.py"", line 67, in visit_metadata
2014-03-18 18:37:45.799 19954 TRACE neutron     self.traverse_single(table, create_ok=True)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/sql/visitors.py"", line 106, in traverse_singl
e
2014-03-18 18:37:45.799 19954 TRACE neutron     return meth(obj, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/ddl.py"", line 86, in visit_table
2014-03-18 18:37:45.799 19954 TRACE neutron     self.connection.execute(schema.CreateTable(table))
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1449, in execute
2014-03-18 18:37:45.799 19954 TRACE neutron     params)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
2014-03-18 18:37:45.799 19954 TRACE neutron     compiled
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_conte
xt
2014-03-18 18:37:45.799 19954 TRACE neutron     context)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_conte
xt
2014-03-18 18:37:45.799 19954 TRACE neutron     context)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/ibm_db_sa/ibm_db.py"", line 104, in do_execute
2014-03-18 18:37:45.799 19954 TRACE neutron     cursor.execute(statement, parameters)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/ibm_db_dbi.py"", line 1335, in execute
2014-03-18 18:37:45.799 19954 TRACE neutron     self._execute_helper(parameters)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/ibm_db_dbi.py"", line 1247, in _execute_helper
2014-03-18 18:37:45.799 19954 TRACE neutron     raise self.messages[len(self.messages) - 1]
2014-03-18 18:37:45.799 19954 TRACE neutron ProgrammingError: (ProgrammingError) ibm_db_dbi::ProgrammingError: Statement Execute Failed: [IBM][
CLI Driver][DB2/LINUXX8664] SQL0601N  The name of the object to be created is identical to the existing name ""IPV6_MODES"" of type ""CHECK CONST""
.  SQLSTATE=42710 SQLCODE=-601 ""\nCREATE TABLE subnets (\n\ttenant_id VARCHAR(255), \n\tid VARCHAR(36) NOT NULL, \n\tname VARCHAR(255), \n\tnet
work_id VARCHAR(36), \n\tip_version INT NOT NULL, \n\tcidr VARCHAR(64) NOT NULL, \n\tgateway_ip VARCHAR(64), \n\tenable_dhcp SMALLINT, \n\tshar
ed SMALLINT, \n\tipv6_ra_mode VARCHAR(16), \n\tipv6_address_mode VARCHAR(16), \n\tPRIMARY KEY (id), \n\tFOREIGN KEY(network_id) REFERENCES netw
orks (id), \n\tCHECK (enable_dhcp IN (0, 1)), \n\tCHECK (shared IN (0, 1)), \n\tCONSTRAINT ipv6_modes CHECK (ipv6_ra_mode IN ('slaac', 'dhcpv6-
stateful', 'dhcpv6-stateless')), \n\tCONSTRAINT ipv6_modes CHECK (ipv6_address_mode IN ('slaac', 'dhcpv6-stateful', 'dhcpv6-stateless'))\n)\n\n
"" ()
2014-03-18 18:37:45.799 19954 TRACE neutron"
1078,1294598,cinder,bc115eaa477d30826ac17986a6e45844b456e206,1,1,,Bug #1294598 “vmware,The vmdk cinder drivers retry the VIM APIs including login API even during errors which are unrelated to connection or session overload problems. The VIM APIs need to retried only during session overload or a stale session scenario.
1079,1294603,neutron,14b6611efea4c9658c611e8cfcfb42fc7dd82f55,1,1,,scenario test_load_balancer_basic fails,"http://logs.openstack.org/98/81098/2/check/check-tempest-dsvm-neutron-pg/df24b97/console.html
2014-03-19 09:58:15.379 | Traceback (most recent call last):
2014-03-19 09:58:15.379 |   File ""tempest/test.py"", line 121, in wrapper
2014-03-19 09:58:15.379 |     return f(self, *func_args, **func_kwargs)
2014-03-19 09:58:15.379 |   File ""tempest/scenario/test_load_balancer_basic.py"", line 225, in test_load_balancer_basic
2014-03-19 09:58:15.379 |     self._check_load_balancing()
2014-03-19 09:58:15.379 |   File ""tempest/scenario/test_load_balancer_basic.py"", line 213, in _check_load_balancing
2014-03-19 09:58:15.379 |     ""http://{0}/"".format(self.vip_ip)).read())
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/urllib.py"", line 86, in urlopen
2014-03-19 09:58:15.380 |     return opener.open(url)
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/urllib.py"", line 207, in open
2014-03-19 09:58:15.380 |     return getattr(self, name)(url)
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/urllib.py"", line 345, in open_http
2014-03-19 09:58:15.380 |     errcode, errmsg, headers = h.getreply()
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/httplib.py"", line 1102, in getreply
2014-03-19 09:58:15.380 |     response = self._conn.getresponse()
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/httplib.py"", line 1030, in getresponse
2014-03-19 09:58:15.380 |     response.begin()
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/httplib.py"", line 407, in begin
2014-03-19 09:58:15.381 |     version, status, reason = self._read_status()
2014-03-19 09:58:15.381 |   File ""/usr/lib/python2.7/httplib.py"", line 365, in _read_status
2014-03-19 09:58:15.381 |     line = self.fp.readline()
2014-03-19 09:58:15.381 |   File ""/usr/lib/python2.7/socket.py"", line 430, in readline
2014-03-19 09:58:15.381 |     data = recv(1)
2014-03-19 09:58:15.381 | IOError: [Errno socket error] [Errno 104] Connection reset by peer"
1080,1294724,cinder,e145ef798d2c80ea48be2826f4c648e72958f5d1,1,1,RPC mechanism was forgoten.,c-api and c-vol emmit tons of oslo.messaging ERROR...,"On a successful test run cinder vol and api services are emmitting tons of oslo.messaging errors in their consoles of the form:
2014-03-18 23:16:34.316 27093 ERROR oslo.messaging.notify._impl_messaging [req-ecce1edc-8383-4fe9-ad11-704ef9deb1e8 fee2f4dc1ff648f0a37e2ef527d9fc20 39971f7616a447e5ad26dbe3ac202a32 - - -] Could not send notification to notifications. ... <payload>
This can be seen in a live log at: http://logs.openstack.org/95/80895/3/check/check-tempest-dsvm-full/bf79bbb/console.html#_2014-03-18_23_20_29_508
This ends up being hugely scary for deployers, and it's lots of ERRORs in logs, even when things are working perfectly normally."
1081,1294812,cinder,bdfc450f59a6b73c1548734e0a5cca5a9e6256ab,0,0,Implement validate_connector in FibreChannelDriver,FibreChannelDriver doesn't implement validate_conn...,"The base FibreChannelDriver doesn't implement the validate_connector() method.  It falls back to the parent class which simply does a pass.   The FC Driver should do a check on the connector and ensure that is has wwnns, wwpns to ensure that the child classes  can actually do a volume attach.   Without wwnns, wwpns, no FC driver can successfully do initialize_connection."
1082,1294892,neutron,3666dcf76dd8cf7a22743e407343b528331130f7,0,0,add debugs,Debug logging for DHCP agent config files,"We've been seeing things that appear to be races between the hosts files being written out for dnsmasq and dhcp requests coming in. We will get occasional errors from dnsmasq saying ""no address available"", ""duplicate IP address"" but by the time you look, the corresponding host file has long since been replaced.
Outputting the dnsmsaq config file in the debug logs would help in establishing what the DHCP server state was at the time of the problem"
1083,1295187,neutron,ef7e17e66591bb93eeeff8d58a8e5c96bf2eca89,1,1,typo in code,Fix typo in lbaas agent exception message,Fix typo in lbaas agent exception message
1084,1295214,neutron,98b3f4a95104234e5106bc1ee58efd427da96c00,0,0,refactor the error message,LbaaS agent scheduling error exposes implementatio...,"Right now when there is no active lbaas agent for HAProxy driver (or other agent-based driver), the following error is returned to the client:
""No eligible loadbalancer agent found for pool <pool_id>""
We need to return some more generic error to the user, skipping the notion of agent.
Also, pool will remain in PENDING_CREATE state, while it probably should move to an ERROR state."
1085,1295239,cinder,cf18199571b7965a8fe77db3d910b0e798f12946,1,1, Upload volume to image fails,Bug #1295239 “VMware,"Upload volume to image fails with the following error:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/io_util.py"", line 108, in _inner
    data=self.input_file)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 311, in update
    _reraise_translated_image_exception(image_id)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 309, in update
    **image_meta)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 177, in call
    raise exception.GlanceConnectionFailed(reason=e)
GlanceConnectionFailed: Connection to glance failed: Error communicating with http://10.20.72.32:9292 timed out
When this happens, the volume gets stuck in “Uploading” state and never completes."
1086,1295381,nova,91ddf85abb8a516cfa2da346b393aa7234660f6c,1,1,Fix the insstance resize bug,Bug #1295381 “VMware,"The resize operation when using the VCenter driver ends up resizing the original VM and not the newly cloned VM.
To recreate:
1) create a new VM from horizon using default debian image.  I use a flavor of nano.
2) wait for it to complete and go active
3) click on resize and choose a flavor larger than what you used originally.  i then usually choose a flavor of small.
4) wait for horizon to prompt you to confirm or revert the migration.
5) Switch over to vSphere Web Client.  Notice two VMs for your newly created instance.  One with a UUID name and the other with a UUID-orig name.  ""-orig"" indicating the original.
6) Notice the original has be resized (cpu and mem are increased, disk is not, but that's a separate bug) and not the new clone.  This is problem #1.
7) Now hit confirm in horizon.  It works, but the logs contain a warning: ""The attempted operation cannot be performed in the current state (Powered on)."".  I suspect its attempting to destroy the orig VM, but the orig was the VM resized and powered on, so it fails.  This is problem #2.
Results in a leaked VM."
1087,1295438,neutron,083324df06828e1b8fb97220b7415c2a28ee16c4,0,0,"Refactoring the code, no bug",Bug #1295438 “BigSwitch plugin,The Big Switch plugin unnecessarily deletes individual ports from the controller when a network is being deleted. This isn't needed because the controller will automatically clean up the ports when their parent network is deleted.
1088,1295448,neutron,913f45bc16dd2c35f7be772d5867f9208064498a,0,0,tests,Big Switch Restproxy unit test unnecessarily dupli...,"The VIF type tests currently have separate classes that all extend the ports test class. This means in addition to testing the VIF changing logic, it's unnecessarily exercising a lot of code that is not impacted by the VIF type."
1089,1295491,neutron,af0c9ea3b8e8f5cc4fd523ff336887ae94bb6ef0,0,0,add more info to the error msg,expected active pool error raised in lbaas agent,"2014-03-20 14:51:23.246 29247 ERROR neutron.openstack.common.rpc.amqp [req-7805af6e-ec03-4f79-92c0-396144d6fc58 None] Exception during message handling
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 462, in _process_data
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp     **args)
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/common/agent_driver_base.py"", line 99, in get_logical_device
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp     raise n_exc.Invalid(_('Expected active pool'))
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp Invalid: Expected active pool
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp
2014-03-20 14:51:23.247 29247 ERROR neutron.openstack.common.rpc.common [req-7805af6e-ec03-4f79-92c0-396144d6fc58 None] Returning exception Expected active pool to caller"
1090,1295558,nova,2318f9ba64b4c793eb913330b862d608d86c03b2,0,0,Future bug,make unshelving and rescuing as io state which wil...,"IOopsFilter will take instance in following state
task_states.RESIZE_MIGRATING, task_states.REBUILDING, task_states.RESIZE_PREP, task_states.IMAGE_SNAPSHOT,               task_states.IMAGE_BACKUP
into consideration ,if the instance on this host exceed the maximum number of instance allowed to do I/O
the scheduler will fail
we need to take UNSHELVING and RESCUING into consideration"
1091,1295703,neutron,252a83890abfb825497f308147aeafc6ee1e0731,1,1,There is a bug. They decided to delete the priority,ci-overcloud job failing “Error while processing V...,"ci overcloud jobs started failing between 5 and 8 AM GMT
Error from http://logs.openstack.org/73/79873/5/check-tripleo/check-tripleo-overcloud-precise/859d4d4/
var/log/upstart/neutron-openvswitch-agent.log ( on contoller and 1 compute)
[-] Error while processing VIF ports
Traceback (most recent call last):
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1230, in rpc_loop
    sync = self.process_network_ports(port_info)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1084, in process_network_ports
    devices_added_updated)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 984, in treat_devices_added_or_updated
    details['admin_state_up'])
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 893, in treat_vif_port
    physical_network, segmentation_id)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 593, in port_bound
    physical_network, segmentation_id)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 459, in provision_local_vlan
    (segmentation_id, ofports))
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 190, in mod_flow
    flow_str = _build_flow_expr_str(kwargs, 'mod')
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 546, in _build_flow_expr_str
    raise exceptions.InvalidInput(error_message=msg)
InvalidInput: Invalid input for operation: Cannot match priority on flow deletion or modification.
2014-03-21 05:20:56.329 7601 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent
merge times and traceback details seem to match up with
https://review.openstack.org/#/c/58533/19
currently I'm testing a revert to see if it fixes things"
1092,1295754,neutron,f45c1c52423dcbb2ea7690d56b3edc232d16636e,1,1,"Bug , cannot delete",Bug #1295754 “nec plugin,"In NEC Plugin, if a resource is in ERROR status and there is no corresponding resource on OpenFlow controller, the resource     cannot be deleted by an API request. Unless critical reasons that resources cannot be deleted, resources should be able to be deleted from API."
1093,1295790,neutron,66f22e754785638e059c0eba4f48bb5953386838,0,0,"rename a variable, evolution",rename ACTIVE_PENDING to ACTIVE_PENDING_STATUSES,Looking at the lbaas code it's not very obvious that constants.ACTIVE_PENDING is actually a list of statuses that contain states that are ACTIVE or PENDING. This patch renames ACTIVE_PENDING to ACTIVE_PENDING_STATUSES so it's obvious that this is a list and not just a string called ACTIVE_PENDING.
1094,1295802,neutron,3d2f3cbde7bb99ed1371bca835a8e63ddc6323d9,0,0,No bug. Allow to add prefix to,Bug #1295802 “nec plugin,"At now NEC plugin assumes REST API of OpenFlow controller starts with /, but NEC OpenFlow controller supports a prefix for REST API. NEC plugin allows to use URI prefix when talking with OpenFlow controller."
1095,1295887,neutron,9730c185847e1cc3fe0ced10f98a14a112e23a07,0,0,test,Bug #1295887 “unit test contextmanager,"In unit tests, resource contextmanagers such as network(), subnet() try to delete themselves after returning from yield even if an   exception occurs. However when an exception occurs, there is a case  where deletion fails. In this case original exception will be hidden and it makes difficult to debug test failures.
Before each time starts, resources like database entries will be recreated, so there is no need to try to delete resources even when an exception occurs.
For example, there is a test with programming error below:
    def test_create_dummy(self):
        with self.network() as network:
            port_res = self._create_port(self.fmt, network['network']['id'])
            port = self.deserialize(self.fmt, port_res)
            # --> Some programming error!!!!
            self.assertEqual(20, hoge)
            self._delete('ports', port['port']['id'])
When running this unit tests, we will get the following error. It is hard to understand.
Traceback (most recent call last):
  File ""neutron/tests/unit/test_db_plugin.py"", line 816, in test_create_dummy
    self._delete('ports', port['port']['id'])
  File ""/usr/lib/python2.7/contextlib.py"", line 35, in __exit__
    self.gen.throw(type, value, traceback)
  File ""neutron/tests/unit/test_db_plugin.py"", line 534, in network
    self._delete('networks', network['network']['id'])
  File ""neutron/tests/unit/test_db_plugin.py"", line 450, in _delete
    self.assertEqual(res.status_int, expected_code)
  File ""/home/ubuntu/neutron/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/ubuntu/neutron/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: 409 != 204
It is better we have the original exception:
Traceback (most recent call last):
  File ""neutron/tests/unit/test_db_plugin.py"", line 809, in test_create_dummy
    self.assertEqual(20, hoge)
NameError: global name 'hoge' is not defined"
1096,1295906,cinder,7320422e5a496350906dcd2a1cd5487088cf63e5,1,1,There's a problem in the attach volume logic ,EMC SMI-S driver needs to check if a volume is att...,"There's a problem in the attach volume logic of the EMC SMI-S driver.  The existing logic checks if a volume is already attached to a host, but it doesn’t check whether it is attached to the host inidicated by the connector info.
So we need to add a check to see if a volume is already attached to a specific host.  If not, it will do the attach."
1097,1296164,nova,46d8dcfa5807ef84d6b96b056957a1b9aa0daff7,0,0, Missing implementation,"Missing implementation to get, delete, and update ...","There are four methods in nova/nova/volume/cinder.py which are NotImplemented.  They are as follows:
1.  get_volume_metadata
2.  delete_volume_metadata
3.  update_volume_metadata
4.  get_volume_metadata_value
These methods are required in cases where nova needs to modify a cinder volume's metadata, e.g. attach and detach time.
The latest code in nova's master branch shows these methods as NotImplemented."
1098,1296478,nova,a5405fa3532d9dd3d33e848f36cc6530e74e6bb7,1,1,,The Hyper-V driver's list_instances() returns an e...,This issue is related to different values that MSVM_ComputerSystem's Caption property can have on different locales.
1099,1296519,nova,e686131fc4b8724328f0922067569120c90eb261,0,0,implementation: should handle ,finish migration should handle exception to revert...,"when instance resize, it will call finish_migration at last to create new instance and destroy old instance
if driver layer has problem in create new instance, the instance will be set to 'ERROR' state
we are able to use  reset-state --active  to reset the instance and use it
but the instance information is set to new instance and not reverted to old one"
1100,1296590,nova,501ceec8b96ba70df4bce0f5aff33a2bb1aabbac,1,1,I think is a bug. They left a snapshot in a state that shouldnt be,[libvirt] snapshots in progress are not cleaned wh...,"When creating an instance snapshot, if such instance is deleted while in the middle of the process, the snapshot may be left in the SAVING state because the instance disappears in the middle of the process or moves to the deleting task_state.
Steps to reproduce:
$ nova boot --image <image_id> --flavor <flavor> test
$ nova image-create test test-snap
$ nova delete test
The image 'test-snap' will be left in the SAVING state although it should be deleted when we detect the situation."
1101,1296662,nova,f0817d3ac11f98c3d518303a50711a6a9c677851,1,0,"Bug because evolution, wrong exception applied","Bug #1296662 “NotFound should be insteaded of InstanceNotFound "" ","we can see the exception is InstanceNotFound  from: https://github.com/openstack/nova/blob/master/nova/compute/api.py#L1758,
but, the exception is NotFound in https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/contrib/server_diagnostics.py#L47.
so, the NotFound should be insteaded of InstanceNotFound"
1102,1296839,nova,115ed3e1cf2cf63bba3a37e61f2a34a186b9f3ad,1,1,"Attemp to resize, when it doesnt have to",xen boot from volume attempts resize,When attempting a boot-from-volume with a volume size that doesn't match the disk size the compute manager will attempt to resize the volume (which fails). It's fine to press on with the given size.
1103,1296913,nova,2bca0c9011d9868aff3e80f4f6f432d29f07adf6,0,0,Evolution. Add implementation,GroupAntiAffinityFilter scheduler hint no longer w...,"Passing a scheduler hint for the GroupAntiAffinityFilter no longer works:
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=909e7fa9-b3af-4601-84c2-01145b1dea72 --hint group=foo server-foo
ERROR (NotFound): The resource could not be found. (HTTP 404) (Request-ID: req-21430f41-e6ca-46db-ab5c-890a1d1dbd01)
screen-n-api.log contains message:
Caught error: Instance group foo could not be found."
1104,1296940,nova,32d948124adeb4615ba141164a1c191eed11598d,1,1,"Bug!  Missing exception handling. I think this is not en evolutionary error.. in my opinion, the original author did not consider that a variable could be empty (0722a8fb230697026c54086fd81ae82690530cb7)",Potential AttributeError in _get_servers if Flavor...,"In the Nova servers API (v2 and v3), this line could fail with an AtributeError if there is a FlavorNotFound exception above it:
https://github.com/openstack/nova/blob/2014.1.b3/nova/api/openstack/compute/servers.py#L611
That code should either check if instance_list is empty first or set instance_list to an empty InstanceList object if FlavorNotFound is hit."
1105,1296957,neutron,5c6ff449bbd7386f0f3e41efc524024434f325df,1,1,,Security_Group FirewallDriver default=None cause L...,"Default value for FirewallDriver set to None in security_group_rpc.py.
L2Agent fails when using default value with following error:
/opt/stack/neutron/neutron/agent/securitygroups_rpc.py:129
2014-03-07 08:15:09.120 31995 CRITICAL neutron [req-63f8e61b-9b71-4178-95b9-ab070a4e3b26 None] 'NoneType' object has no attribute 'rpartition'
2014-03-07 08:15:09.120 31995 TRACE neutron Traceback (most recent call last):
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/usr/local/bin/neutron-linuxbridge-agent"", line 10, in <module>
2014-03-07 08:15:09.120 31995 TRACE neutron     sys.exit(main())
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/opt/stack/neutron/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 987, in main
2014-03-07 08:15:09.120 31995 TRACE neutron     root_helper)
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/opt/stack/neutron/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 787, in __init__
2014-03-07 08:15:09.120 31995 TRACE neutron     self.init_firewall()
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/securitygroups_rpc.py"", line 130, in init_firewall
2014-03-07 08:15:09.120 31995 TRACE neutron     self.firewall = importutils.import_object(firewall_driver)
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/opt/stack/neutron/neutron/openstack/common/importutils.py"", line 38, in import_object
2014-03-07 08:15:09.120 31995 TRACE neutron     return import_class(import_str)(*args, **kwargs)
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/opt/stack/neutron/neutron/openstack/common/importutils.py"", line 26, in import_class
2014-03-07 08:15:09.120 31995 TRACE neutron     mod_str, _sep, class_str = import_str.rpartition('.')
2014-03-07 08:15:09.120 31995 TRACE neutron AttributeError: 'NoneType' object has no attribute 'rpartition'
2014-03-07 08:15:09.120 31995 TRACE neutron
This can be fixed by setting default  firewall_driver = neutron.agent.firewall.NoopFirewallDriver or verification on L2 Agent start-up for firewall_driver is not being None."
1106,1297052,nova,d62fb490b10d9372bd52189d4c688a7c1b495d8b,1,1,wrong behaviour,resize fail didn't show a correct info when --poll...,"[root@controller ~]# nova resize --poll a9dd1fd6-27fb-4128-92e6-93bcab085a98 100
Instance resizing... 100% complete
Finished
but the instance is not finished yet and error logs in nova log"
1107,1297145,neutron,f85f240cabb0a7cc1adb3fa1c3139984e8e30d4c,1,0,"deprecated, no BIC",L3 agent uses deprecated root_helper option,"L3 agent uses deprecated root_helper option through self.conf.root_helper in neutron.agent.l3_agent.L3NATAgent._update_routing_table instead of using self.root_helper (result of neutron.agent.common.config.get_root_helper.
It implies if AGENT/root_helper option is configured and root_helper is not configured, extra routes are not pushed in routers."
1108,1297358,glance,8eb31484547b3cfc6c42e06e5d0124501b6339ef,0,0,Change in requirements,Bug #1297358 “Glance v1,"Attempting a HEAD request on the /images/detail resource results in a 500 response. This should ideally be an HTTP 405 response.
This issue does not occur when doing a HEAD on /images for some reason
curl -i -X HEAD -H ""X-Auth-Token: $AUTH_TOKEN"" -H 'Content-Type: application/json' -H 'User-Agent: python-glanceclient' http://localhost:9292/v1/images/detail
HTTP/1.1 500 Internal Server Error
Content-Type: text/plain
Content-Length: 0
Date: Tue, 25 Mar 2014 15:18:34 GMT
Connection: close
The traceback for the error can be found here: http://paste.openstack.org/show/74261/"
1109,1297362,glance,853b5c9b24a169ec609253637adf093862db35b6,1,1,wrong behaviour in http codes,Bug #1297362 “Glance v2,"Requests for many resources in Glance v2 will return a 404 if the request is using an unsupported HTTP verb for that resource. For example, the /v2/images resource does exist but a 404 is returned when attempting a DELETE on that resource. Instead, this should return an HTTP 405 MethodNotAllowed response."
1110,1297594,neutron,14b6611efea4c9658c611e8cfcfb42fc7dd82f55,1,1,Fixing commit introduces a bug,"List namespaces properly, fix regression introduce...","fix to 1293818 introduced a regression for the code path that is used in loadbalancer namespace driver to list namespaces.
The following tracebacks are seen during tempest runs:
http://logs.openstack.org/14/82514/2/check/check-tempest-dsvm-neutron-pg/73ff42d/logs/screen-q-lbaas.txt.gz?level=TRACE#_2014-03-25_09_49_57_261
As a consequence, this causes agent to resync balancer state and to respawn haproxy process sometimes leading to unavailable service. That in turn makes loadbalancer basic scenario test fail.
Related bugs: 1294603, 1295165"
1111,1297685,nova,8f299f5545e19b36533742458de43649d41080ed,1,1,Problem introduced by commit 114109dbf4094ae6b6333d41c84bebf6f85c4e48,Spaces in SSH public key comment breaks cloud-init...,"A Nova generated public SSH key contains the comment 'Generated by Nova'. Earlier versions of cloud-init (prior to 0.7.2) can't properly handle spaces in key comments and as a result, fail to disable the root account (if configured to do so).
Earlier Nova versions (Essex and older) didn't have spaces in the comment.
Problem introduced by commit: 114109dbf4094ae6b6333d41c84bebf6f85c4e48
cloud-init bug report: https://bugs.launchpad.net/ubuntu/+source/cloud-init/+bug/1220273"
1112,1297875,neutron,c6c4a20777921dc1b21e80edb96ccd957a054c68,0,0,tests,some tests call “called_once_with_args,"A few tests use ""called_once_with_args""  instead of mock's ""assert_called_once_with_args""
without checking the result.
That means that we're not asserting for that to happen.
Those tests need to be fixed.
[majopela@f20-devstack neutron]$ grep "".called_once_with"" * -R | grep -v assert
neutron/tests/unit/test_dhcp_agent.py:            disable.called_once_with_args(network.id)
neutron/tests/unit/test_dhcp_agent.py:                uuid5.called_once_with(uuid.NAMESPACE_DNS, 'localhost')
neutron/tests/unit/test_post_mortem_debug.py:        mock_print_exception.called_once_with(*exc_info)
neutron/tests/unit/test_db_migration.py:                mock_open.write.called_once_with('a')
neutron/tests/unit/test_agent_netns_cleanup.py:                ovs_br_cls.called_once_with('br-int', conf.AGENT.root_helper)
neutron/tests/unit/test_metadata_agent.py:            self.eventlet.wsgi.server.called_once_with("
1113,1298042,cinder,31569827c749dca6e0affd4646360b839f321758,1,1,Bug. Wrong state,Image stuck in “Queued,"When uploading a volume to an image with an unsupported disk type (anything other than vmdk), an orphan image stuck in ""Queued"" state gets left behind.
Steps to reproduce:
1. cinder create 1
2. cinder upload-to-image --disk raw <volume id> <name>
The result is an orphan image in Glance that gets left behind. This image should not exist."
1114,1298131,nova,af44b50b6b8187c559c56b9d3f7dc047fc5be407,1,1,Wrong hhtp code,improper usage of HTTP 413 status code,"HTTP 413 is supposed to mean (per RFC2616) that the request entity was too large. E.g., if you send an enormous body with the request. That is not at all how it is being used in the server resize request example below. The nova/api/openstack/compute/servers.py is coded to return 413 for QuotaError and PortLimitExceeded on create as well as for QuotaError on resize, and there may be other places 413 is being returned inappropriately.
POST /v2/6ce8fae0510349dcbf9b3965f7a20061/servers/8ebaabfc-9018-4ac1-afc6-630aee8a8ae3/action
Request body: {  ""resize"": {
            ""flavor"": {
              ""vcpus"": 1,
              ""ram"": 9999999999,
              ""disk"": 20
          }}}
Response: HTTP 413 (Request Entity Too Large)
Response body:
{
overLimit: {
message: ""Quota exceeded for ram: Requested 1410063359, but already used 6144 of 8000000 ram""
code: 413
retryAfter: ""0""
}
-
}"
1115,1298201,cinder,24eb8234ff4b907b8c88f26fb0c427cb7a452140,1,1,Bug. Add a exception,"If volume copy not finished, user add a copy of th...","2014-03-27 14:22:13.716 ERROR cinder.volume.drivers.san.san [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b879ba6 c9c4178
0fac445b3b6e9a1ff2c805d8d] Error running SSH command: svctask addvdiskcopy -rsize 2% -autoexpand -warning 20% -grainsize 256 -easytier on -mdi
skgrp Anna_Test volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef
2014-03-27 14:22:13.717 ERROR cinder.volume.drivers.ibm.storwize_svc.ssh [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b8
79ba6 c9c41780fac445b3b6e9a1ff2c805d8d] CLI Exception output:
 command: ['svctask', 'addvdiskcopy', '-rsize', '2%', '-autoexpand', '-warning', '20%', '-grainsize', '256', '-easytier', 'on', '-mdiskgrp', u
'Anna_Test', u'volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef']
 stdout:
 stderr: CMMVC6352E The command failed because the number of copies of this virtual disk (VDisk) would exceed the limit.
2014-03-27 14:22:13.718 ERROR cinder.volume.manager [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b879ba6 c9c41780fac445b3b6e9a1ff2c805d8d] Volume a9b15c37-f70f-4a47-8f29-101d7e3970ef: driver error when trying to retype, falling back to generic mechanism.
2014-03-27 14:22:13.719 ERROR cinder.volume.manager [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b879ba6 c9c41780fac445b3b6e9a1ff2c805d8d] Bad or unexpected response from the storage volume backend API: CLI Exception output:
 command: ['svctask', 'addvdiskcopy', '-rsize', '2%', '-autoexpand', '-warning', '20%', '-grainsize', '256', '-easytier', 'on', '-mdiskgrp', u'Anna_Test', u'volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef']
 stdout:
 stderr: CMMVC6352E The command failed because the number of copies of this virtual disk (VDisk) would exceed the limit.
2014-03-27 14:22:13.719 TRACE cinder.volume.manager Traceback (most recent call last):
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1232, in retype
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     diff, host)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 734, in retype
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     self.configuration)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 703, in add_vdisk_copy
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     new_copy_id = self.ssh.addvdiskcopy(vdisk, dest_pool, params)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/ssh.py"", line 304, in addvdiskcopy
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     return self.run_ssh_check_created(ssh_cmd)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/ssh.py"", line 60, in run_ssh_check_created
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     out, err = self._run_ssh(ssh_cmd)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/ssh.py"", line 41, in _run_ssh
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     raise exception.VolumeBackendAPIException(data=msg)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: CLI Exception output:
2014-03-27 14:22:13.719 TRACE cinder.volume.manager  command: ['svctask', 'addvdiskcopy', '-rsize', '2%', '-autoexpand', '-warning', '20%', '-grainsize', '256', '-easytier', 'on', '-mdiskgrp', u'Anna_Test', u'volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef']"
1116,1298420,nova,04fa99095cd82f7b6030694996c75d2fd95e7c61,1,1,,Libvirt's image caching fetches images multiple ti...,"When launching several VMs in rapid succession, it is possible that libvirt's image caching will fetch the same image several times.  This can occur when all of the VMs in question are using the same base image and this base image has not been previously fetched. The inline fetch_func_sync method prevents multiple threads from fetching the same image at the same time, but it does not prevent a thread that is waiting to acquire the lock from fetching the image that was being fetched while the lock was still in use. This is because the presence of the image is checked only before the lock has been acquired, not after."
1117,1298459,neutron,d3be7b040eaa61a4d0ac617026cf5c9132d3831e,0,0,add missing migrations,Bug #1298459 “db migration,"For bigswitch plugin, networkdhcpagentbindings and subnetroutes tables are not created by db migration.
http://openstack-ci-gw.bigswitch.com/logs/refs-changes-96-40296-13/BSN_PLUGIN/logs/screen/screen-q-svc.log.gz
FOr bigswitch ML2 mech driver, neutron_ml2.consistencyhashes is not created by db migration.
http://openstack-ci-gw.bigswitch.com/logs/refs-changes-96-40296-13/BSN_ML2/logs/screen/screen-q-svc.log.gz"
1118,1298608,cinder,57e8cdb9dd8e24919bc2243438a6bded53491b3e,1,0,"Bug , HP LeftHand driver fails with Paramiko 1.13.0",HP LeftHand driver fails with Paramiko 1.13.0,"When the HP LeftHand driver is configured in legacy mode it will fail with the following exception, if paramiko 1.13.0 is installed:
2014-03-27 13:33:22.189 DEBUG cinder.openstack.common.lockutils [req-c2080e55-ec3e-40e3-a7a6-329e48d22295 None None] Released file lock ""lefthand"" at /opt/stack
/data/cinder/cinder-lefthand for method ""get_volume_stats""... from (pid=35801) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:239
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/semaphore.py"", line 121, in _do_acquire
    waiter.switch()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/openstack/common/service.py"", line 483, in run_service
    service.start()
  File ""/opt/stack/cinder/cinder/service.py"", line 103, in start
    self.manager.init_host()
  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 308, in init_host
    self.publish_service_capabilities(ctxt)
  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1105, in publish_service_capabilities
    self._report_driver_status(context)
  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1094, in _report_driver_status
    volume_stats = self.driver.get_volume_stats(refresh=True)
  File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
    retval = f(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_iscsi.py"", line 121, in get_volume_stats
    data = self.proxy.get_volume_stats(refresh)
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py"", line 421, in get_volume_stats
    self._update_backend_status()
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py"", line 436, in _update_backend_status
    'clusterName': self.configuration.san_clustername})
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py"", line 109, in _cliq_run_xml
    result_xml = etree.fromstring(out)
  File ""lxml.etree.pyx"", line 3003, in lxml.etree.fromstring (src/lxml/lxml.etree.c:67314)
  File ""parser.pxi"", line 1724, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:101147)
ValueError: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.
This bug also exists with the Havana version of the driver. cinder.volume.drivers.san.hp_lefthand.HpSanISCSIDriver
Workaround: install paramiko 1.10.0"
1119,1298658,neutron,80eb1faf1b9466176a0fbf1f8a8b18cb500c6811,1,1,,Stale external gateway devices left behind,"This is a follow on to https://bugs.launchpad.net/neutron/+bug/1244853.
I found today that the same problem can happen with external gateway devices.  Those should be identified and removed in a manner similar to the fix before the other bug."
1120,1298699,neutron,979ec03f0be170b7b83fb8c9b6d76c678d8fca66,1,1,Wrong order of lines :/,Bug #1298699 “BigSwitch,"The server manager module references an attribute of a variable that could either be an HTTPSConnection object or None. If it's None, the attribute reference will fail."
1121,1298865,neutron,25103df197c1f366eac8dd3069fabc01d3bd18e9,1,1,Wrong order of actions,NVP advanced service plugin should check router st...,"With NVP advanced service plugin, router creation is asynchronous while all service call is synchronous, so it is possible that advanced service request is called before edge deployment completed.
One solution is to check the router status before deploying an advanced service. If the router is not in ACTIVE status, the service deployment request would return ""Router not ready"" error."
1122,1298975,nova,642e6acc5bf469dcf419da3bc5dce9d83aa48d6a,0,0,tests and some refactoring ,libvirt.finish_migration is too large and not test...,"This method needs to be split in several small methods then each methods has to be tested.
A possible solution could be:
  * determines the disk size from instance properties
  * methods to convert disk from qcow2 to raw and raw to qcow2
  * method to resize the disk"
1123,1298981,nova,ddd92b229daa31f6d8f6683986e135ebd36829c5,0,0,Add an option. Evolution,Skip resizing disk if the parameter resize_instanc...,"On the libvirt driver the driver.finish_migration method is called with an extra
parameter 'resize_instance'. It should be used to know if it is
necessary or not to resize the disks."
1124,1299124,cinder,601d54c9ef926fa2d8abfe0121019f7ad94eefd5,1,1,Windows dependency?,Cinder fails to delete volumes on Windows,"This commit https://github.com/openstack/cinder/commit/beecd769af02ba5915be827d28a7b46d970e41b0 has changed the flow of volume creation, so Cinder does not create an iSCSI target anymore when a volume is created. Even so, the Windows Cinder driver tries to remove the according iSCSI target every time a volume is deleted, resulting into an error when the target has not been created. The fix consists in checking if the target exists before attempting to delete it.
Trace: http://paste.openstack.org/show/73912/"
1125,1299131,cinder,23e820abb8685573f97f5e3764225a97dd695127,1,1,Windows dependency?,Cinder fails to associate targets to initiators on...,"Due to a small nit, the initiator name and the target name are inverted when passing the arguments to the method which associates the iSCSI target to an initiator. For this reason, this operation will fail.
The connection to the iSCSI target cannot be initialized properly as the method which gets portal information is missing the return value.
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/windows/windows.py#L71-72
Trace: http://paste.openstack.org/show/74581/"
1126,1299145,neutron,408ef55d4ef1a9d246571511203ab337ba5346c6,1,1, Documentation Bug,"Documentation Bug, BigSwitch should be “Big Switch...","BigSwitch references should be changed to ""Big Switch""."
1127,1299150,cinder,1f6972f3fdfb87b09c03f3c2c7ab1870d90e0dc2,1,1,Windows dependency?,Cinder fails to create volumes on Windows server 2...,"Windows Server 2012 R2 does not support vhd images as iSCSI disks, requiring VHDX images. For this reason, the cinder driver fails to create volumes. For the moment, the default format is vhd. On WSS 2012 R2, we should use vhdx images as default."
1128,1299156,neutron,6d49834976bc08d2b8f7939bc2a217f5cc81db47,1,0,Bug because of evolution,Hyper-V agent does not disable security group rule...,"A new config option was introduced recently, enable_security_group. This config option specifies if the agent should have the security group enabled or not.
The Hyper-V agent does not take this config option into account, which means the security groups rules are always applied."
1129,1299159,neutron,f56972193be903c67cd54c57bcc2becf6222aced,1,1,Fix the rules and add new ones,Hyper-V agent cannot add ICMP security group rules...,"The Hyper-V agent throws exceptions each time it tries to add a security group rule which has ICMP as protocol.
This is caused by the fact that Hyper-V Msvm_EthernetSwitchPortExtendedAclSettingData can accept only TCP and UDP as text as protocols. Any other protocol must be a string of the protocol number.
Source: http://technet.microsoft.com/en-us/library/dn464289.aspx
Agent log:
http://pastebin.com/HwrczsfX"
1130,1299331,nova,a868fcedf8e46070cae6aa8e59e61934fa23db1c,1,1,,There isn't effect when attach/detach interface fo...,"$ nova boot --flavor 1 --image 76ae1239-0973-44cf-9051-0e1bc8f41cdd --nic net-id=a15cfbed-86d8-4660-9593-46447cb9464e vm1
$ nova list
+--------------------------------------+------+--------+------------+-------------+-------------------+
| ID                                   | Name | Status | Task State | Power State | Networks          |
+--------------------------------------+------+--------+------------+-------------+-------------------+
| f7e2877d-c7f5-4493-89d4-c68e9839a7ff | vm1  | ACTIVE | -          | Running     | private=10.0.0.22 |
+--------------------------------------+------+--------+------------+-------------+-------------------+
$ brctl show
bridge name	bridge id		STP enabled	interfaces
br-eth0		0000.fe989d8bd148	no
br-ex		0000.8a1d06d8854e	no
br-ex2		0000.4a98bdebe544	no
br-int		0000.229ad5053a41	no
br-tun		0000.2e58a2f0e047	no
docker0		8000.000000000000	no
lxcbr0		8000.000000000000	no
qbr0ad6a86e-d9		8000.9e5491dd719a	no		qvb0ad6a86e-d9
       tap0ad6a86e-d9
$ neutron port-list
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                          |
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
| 0ad6a86e-d967-424e-9bf5-e6821cc0cd0d |      | fa:16:3e:3a:3e:5a | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.22""}   |
| 1e6bed8d-aece-4d3e-abcc-3ad7957d6d72 |      | fa:16:3e:9e:dc:83 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.12""} |
| 5f522a9a-2856-4a95-8bd8-c354c00abf0f |      | fa:16:3e:01:47:43 | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.1""}    |
| 6226f6d3-3814-469c-bf50-8c99dfec481e |      | fa:16:3e:46:0e:35 | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.2""}    |
| a3f2ab1c-a634-446d-8885-d7d8e5978fa1 |      | fa:16:3e:cf:02:d6 | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.20""}   |
| c10390a9-6f84-44f5-8a17-91cb330a9e12 |      | fa:16:3e:41:7c:34 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.15""} |
| c814425c-be1a-4c06-a54b-1788c7c6fb31 |      | fa:16:3e:f5:fc:d3 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.2""}  |
| ebd874b7-43e6-4d18-b0ed-f86bb349d8b9 |      | fa:16:3e:e6:b5:09 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.19""} |
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
$ nova pause vm1
$ nova interface-detach vm1 0ad6a86e-d967-424e-9bf5-e6821cc0cd0d
$ nova list
+--------------------------------------+------+--------+------------+-------------+----------+
| ID                                   | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+--------+------------+-------------+----------+
| f7e2877d-c7f5-4493-89d4-c68e9839a7ff | vm1  | PAUSED | -          | Paused      |          |
+--------------------------------------+------+--------+------------+-------------+----------+
$ brctl show
bridge name	bridge id		STP enabled	interfaces
br-eth0		0000.fe989d8bd148	no
br-ex		0000.8a1d06d8854e	no
br-ex2		0000.4a98bdebe544	no
br-int		0000.229ad5053a41	no
br-tun		0000.2e58a2f0e047	no
docker0		8000.000000000000	no
lxcbr0		8000.000000000000	no
But tap still alive
$ ifconfig|grep tap0ad6a86e-d9
tap0ad6a86e-d9 Link encap:Ethernet  HWaddr fe:16:3e:3a:3e:5a
And login into instance, exec 'ifconfig', it will found the interface still attach to the instance"
1131,1299946,neutron,d9eea51187ddd9b951e9943030724489f1c21a26,1,1,"""Added the missing plugin session.”",OneConvergence plugin create_network fails for ext...,create_network for networks with --router:external=True call doesn't add the network to the externalnetworks table. This is resulting in failures due to the missing entry in externalnetworks table. The reason is _process_l3_create() (called from create_network) expects to be invoked with a plugin session. Until recently process_l3_create was adding the entry to externalnetworks table even without the plugin session. Plugin third-party tests have been failing the last few days due to this.
1132,1300136,cinder,115b8447618073d3383bbd612ebd2fc68c8c5d28,1,1,"""Fix wrong exception reference”",Failed to delete a never attached volume with lioa...,"/etc/cinder/cinder.conf:
[DEFAULT]
iscsi_helper = lioadm
As admin user, I am unable to delete a volume if it was not attached before.
the volume status became: error_deleting
$ cinder create 1
$ cinder delete  daf0a29a-542b-4571-9d4d-5b9b59a7baf3
$ cinder list
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
|                  ID                  |     Status     | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
| daf0a29a-542b-4571-9d4d-5b9b59a7baf3 | error_deleting |     None     |  1   |     None    |  false   |             |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
../screen-logs/screen-c-vol.log:
2014-03-31 10:16:43.863 787 ERROR oslo.messaging.rpc.dispatcher [req-8f294457-08a3-4039-9626-ce63a2435bed 46a5bd04d46f49aabcf9edf8214df7a1 d587e4e2b14a41e4b38f62b07a130b3e - - -] Exception during message handling: No target id found for volume daf0a29a-542b-4571-9d4d-5b9b59a7baf3.
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 144, in lvo_inner1
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     return lvo_inner2(inst, context, volume_id, **kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     retval = f(*args, **kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 143, in lvo_inner2
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     return f(*_args, **_kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 416, in delete_volume
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     {'status': 'error_deleting'})
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 400, in delete_volume
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     self.driver.remove_export(context, volume_ref)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 540, in remove_export
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     self.target_helper.remove_export(context, volume)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/iscsi.py"", line 232, in remove_export
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     volume['id'])
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/db/api.py"", line 234, in volume_get_iscsi_target_num
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     return IMPL.volume_get_iscsi_target_num(context, volume_id)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 119, in wrapper
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     return f(*args, **kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1344, in volume_get_iscsi_target_num
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     raise exception.ISCSITargetNotFoundForVolume(volume_id=volume_id)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher ISCSITargetNotFoundForVolume: No target id found for volume daf0a29a-542b-4571-9d4d-5b9b59a7baf3.
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher
When the lioadm is the iscsi helper the target just exists when the volume attached.
I should be able to delete a volume what was never attached.
The volumes which was attached to a vm before the delete request are deletable."
1133,1300148,cinder,77b6a9f129977799bc5c6ccec6caaf707fa3047c,1,1,,cannot attach volume as regular user with lioadm i...,"In usual devstack setup but with the following config changes:
/etc/cinder/cinder.conf:
[DEFAULT]
iscsi_helper = lioadm
As admin user I was able to attach the volume to vm, but as demo user it fails with require_admin_context.
 ../screen-logs/screen-c-vol.log:
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 797, in initialize_connection
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     self.driver.remove_export(context, volume)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 540, in remove_export
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     self.target_helper.remove_export(context, volume)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/iscsi.py"", line 232, in remove_export
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     volume['id'])
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/db/api.py"", line 234, in volume_get_iscsi_target_num
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     return IMPL.volume_get_iscsi_target_num(context, volume_id)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 118, in wrapper
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     raise exception.AdminRequired()
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher AdminRequired: User does not have admin privileges
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher
 ../screen-logs/screen-n-cpu.log:
2014-03-31 10:04:25.586 ERROR nova.compute.manager [req-99dc297a-db00-4409-a634-8a17a8313a84 demo demo] [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] Failed to attach a6fd9536-151d-48ac-b084-7e5419efad78 at /dev/vdc
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] Traceback (most recent call last):
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 4140, in _attach_volume
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     do_check_attach=False, do_driver_attach=True)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/nova/nova/virt/block_device.py"", line 44, in wrapped
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     ret_val = method(obj, context, *args, **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/nova/nova/virt/block_device.py"", line 225, in attach
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     connector)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/nova/nova/volume/cinder.py"", line 173, in wrapper
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     res = method(self, ctx, volume_id, *args, **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/nova/nova/volume/cinder.py"", line 271, in initialize_connection
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     connector)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/python-cinderclient/cinderclient/v1/volumes.py"", line 321, in initialize_connection
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     {'connector': connector})[1]['connection_info']
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/python-cinderclient/cinderclient/v1/volumes.py"", line 250, in _action
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     return self.api.client.post(url, body=body)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/python-cinderclient/cinderclient/client.py"", line 210, in post
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     return self._cs_request(url, 'POST', **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/python-cinderclient/cinderclient/client.py"", line 174, in _cs_request
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/python-cinderclient/cinderclient/client.py"", line 157, in request
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     raise exceptions.from_response(resp, body)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] Forbidden: User does not have admin privileges (HTTP 403) (Request-ID: req-7a872133-72f4-4765-b055-4791629d73f8)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]"
1134,1300303,cinder,5f00cad02da1093d71f636add0810a538cbd444f,1,1,,Bug #1300303 “glusterfs,"When deleting a volume that was snapshot, the volume is not properly deleted.
Steps to recreate bug:
1.  Create a volume
2.  Attach volume to a running instance.
3.  Take an online snapshot of the volume.
Note that the active volume used by the instance is now switched to volume-<uuid>.<snapshot-uuid>.
4.  Delete the snapshot.
The snapshot volume will be rebased, and the ""active"" volume as seen in volume-<uuid>.info is now set to
volume-<uuid>.<snapshot-uuid>.
Under the glusterfs mount point, there are now only 2 files related to the volume:
  - volume-<uuid>.info
  - volume-<uuid>.<snapshot-uuid>
The original volume base volume-<uuid> is deleted after the rebase.
5.  Detach the volume from the running instance.
6.  Delete the volume.
Under the glusterfs mount point, the  volume-<uuid>.<snapshot-uuid> file is not deleted as expected."
1135,1300325,nova,721e7f939859fbfe6b0c79ef3a6d5e43c916da65,1,1,broken by version 46922068ac167f492dd303efb359d0c649d69118. “I was able to reproduce this. Good find! My fault :)”,nic port ordering is not honored,"This bug was fixed by https://bugs.launchpad.net/nova/+bug/1064524 previously but broken by version 46922068ac167f492dd303efb359d0c649d69118.
Instead of iterating the already ordered port list, the new code iterates the list from neutron and the result is random ordering."
1136,1300380,nova,72cd430e971589f7872a9d4e8f173c68b4955375,1,1,,errors_out_migration decorator does not work with ...,"The errors_out_migration decorator in nova/compute/manager.py is attempting to use positional arguments to get the migration parameter.  However, at run time, the decorated methods are called via RPC calls which specify all of their arguments as keyword arguments.  The decorator needs to be fixed so that it works with RPC calls."
1137,1300546,glance,819f28a0b8863bd18f8a14491b5966c8b2723432,0,0,,config.generator could not handle split configs,"Current config.generator could not handle split configs for the different services within a project, all configurations be collected and save to a single large template file. But for most project, the code repo contains more then one service, like nova repo/project contains nova-api, nova-compute. So the single template file is hard to be used/maintained for separated service. IMO, it will be cool if config.generator could generate separated configs based on service instead of project.
Example of ""split configs"" is glance-api.conf versus glance-registry.conf"
1138,1300570,neutron,8fb023ed53aff31f37ebebb94aca4f8e2a188342,1,1,"""This problem is brought by the patch:  https://review.openstack.org/#/c/72565/""",Bug #1300570 “dhcp_agent fails in RPC communication with neutron... ,"This problem occurs when ml2 plugin runs under Metaplugin.
error log of dhcp_agent is as follows:
---
2014-03-28 18:57:17.062 ERROR neutron.agent.dhcp_agent [req-9c53d7a6-d850-42de-896f-184827b33bfd None None] Failed reporting state!
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent Traceback (most recent call last):
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/dhcp_agent.py"", line 564, in _report_state
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent     self.state_rpc.report_state(ctx, self.agent_state, self.use_call)
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/rpc.py"", line 72, in report_state
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent     return self.call(context, msg, topic=self.topic)
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/openstack/common/rpc/proxy.py"", line 129, in call
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent     exc.info, real_topic, msg.get('method'))
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent Timeout: Timeout while waiting on RPC response - topic: ""q-plugin"", RPC method: ""report_state"" info: ""<unknown>""
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent
---
This problem is brought by the patch:
 https://review.openstack.org/#/c/72565/
because ml2 plguin does not become to open RPC connection at plugin initialization."
1139,1300628,neutron,c70c2719d700902854ef0381cb725722ac2da05e,0,0,Refactoring “This needs to be removed because it's resulting in the entire portbinding_db schema for one small function.”,BigSwitch ML2 driver uses portbindingsports table,The Big Switch ML2 driver references the deprecated portbindings_db in the port location tracking code. This needs to be removed because it's resulting in the entire portbinding_db schema for one small function.
1140,1300808,neutron,d4fdac7ae596888b8e83d9559019eafb728cd3fe,1,1,"“This is due to bug #1291535 and this fix: https://git.openstack.org/cgit/openstack/neutron/commit/?id=b2f65d9d447ddf2caf3b9c754bd00a5148bdf12c""",Invalid version number '['3.13.0']' with Ubuntu tr...,"Running latest devstack on Trusty Ubuntu, the Neutron agent fails to start with the following backtrace:
2014-04-01 13:49:54.227 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] Running command: ['uname', '-r'] from (pid=14900) create_process /opt/stack/neutron/neutron/agent/linux/utils.py:48
2014-04-01 13:49:54.232 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None]
Command: ['uname', '-r']
Exit code: 0
Stdout: '3.13.0-20-generic\n'
Stderr: '' from (pid=14900) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-04-01 13:49:54.233 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--version'] from (pid=14900) create_process /opt/stack/neutron/neutron/agent/linux/utils.py:48
2014-04-01 13:49:54.348 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--version']
Exit code: 0
Stdout: 'ovs-vsctl (Open vSwitch) 2.0.1\nCompiled Feb 23 2014 14:42:32\n'
Stderr: '' from (pid=14900) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-04-01 13:49:54.349 DEBUG neutron.agent.linux.ovs_lib [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] Checking OVS version for VXLAN support installed klm version is None, installed Linux version is ['3.13.0'], installed user version is 2.0  from (pid=14900) check_ovs_vxlan_version /opt/stack/neutron/neutron/agent/linux/ovs_lib.py:541
2014-04-01 13:49:54.350 CRITICAL neutron [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] invalid version number '['3.13.0']'
2014-04-01 13:49:54.350 TRACE neutron Traceback (most recent call last):
2014-04-01 13:49:54.350 TRACE neutron   File ""/usr/local/bin/neutron-openvswitch-agent"", line 10, in <module>
2014-04-01 13:49:54.350 TRACE neutron     sys.exit(main())
2014-04-01 13:49:54.350 TRACE neutron   File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1360, in main
2014-04-01 13:49:54.350 TRACE neutron     agent = OVSNeutronAgent(**agent_config)
2014-04-01 13:49:54.350 TRACE neutron   File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 214, in __init__
2014-04-01 13:49:54.350 TRACE neutron     self._check_ovs_version()
2014-04-01 13:49:54.350 TRACE neutron   File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 232, in _check_ovs_version
2014-04-01 13:49:54.350 TRACE neutron     ovs_lib.check_ovs_vxlan_version(self.root_helper)
2014-04-01 13:49:54.350 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/linux/ovs_lib.py"", line 550, in check_ovs_vxlan_version
2014-04-01 13:49:54.350 TRACE neutron     'kernel', 'VXLAN')
2014-04-01 13:49:54.350 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/linux/ovs_lib.py"", line 507, in _compare_installed_and_required_version
2014-04-01 13:49:54.350 TRACE neutron     installed_kernel_version) >= dist_version.StrictVersion(
2014-04-01 13:49:54.350 TRACE neutron   File ""/usr/lib/python2.7/distutils/version.py"", line 40, in __init__
2014-04-01 13:49:54.350 TRACE neutron     self.parse(vstring)
2014-04-01 13:49:54.350 TRACE neutron   File ""/usr/lib/python2.7/distutils/version.py"", line 107, in parse
2014-04-01 13:49:54.350 TRACE neutron     raise ValueError, ""invalid version number '%s'"" % vstring
2014-04-01 13:49:54.350 TRACE neutron ValueError: invalid version number '['3.13.0']'
2014-04-01 13:49:54.350 TRACE neutron
q-agt failed to start
This is due to bug #1291535 and this fix: https://git.openstack.org/cgit/openstack/neutron/commit/?id=b2f65d9d447ddf2caf3b9c754bd00a5148bdf12c"
1141,1300906,cinder,e6c03e15e8caa957716eabfee047240622ac3bf5,1,0,"""Windows does not support using dynamic vhds as iSCSI”",Cinder fails to create volumes from images on Wind...,"After the volume is created, qemu-img is supposed to copy the downloaded image to the volume. This fails as the disk is enabled and is not accessible. To fix this, the flow of creating a volume from an image must be changed when using Windows. So instead of creating the volume before copying the image to it, it would be better to skip the volume creation and let the driver create the disk based on the downloaded image and then import it as an iSCSI disk.
Windows does not support using dynamic vhds as iSCSI disks and as qemu-img cannot create fixed vhd images, there must be an intermediate conversion before importing the disk.
Trace: http://paste.openstack.org/show/74765/"
1142,1301035,neutron,3be1d7a75c5ec754825e99e0a8d95b4e1521ae4b,1,1,,Nova notifier thread does not run in rpc_worker su...,"This reported to me today by Maru.  When an rpc worker is spawned as a sub-process, that happens after the nova notifier thread has already started.
eventlet.hubs.use_hub() is the call in neutron/openstack/common/service.py that causes all thread execution to stop.
From the event let documentation:  ""Make sure to do this before the application starts doing any I/O! Calling use_hub completely eliminates the old hub, and any file descriptors or timers that it had been managing will be forgotten.""
Maru's observation is that this means that thread should not spawn before forking the process if they need to run in the child process.  I agree.
The reason that threads spawn is that the plugin gets loaded prior to forking and the thread for the nova notifier is started in the __init__ method of a sub-class of the plugin."
1143,1301042,neutron,8ab6fd6d7e521a3692f57542e5c5c5d513d57ccc,1,1,,Routers may never be torn down if router_delete_na...,"The code recently added https://review.openstack.org/#/c/30988/ very nicely cleans out stale routers assuming that self.conf.router_delete_namespaces is true.
The problem is that automatic namespace deletion is still a bit unstable because of problems with the kernel and the iproute utility.  So, many users may not have self.conf.router_delete_namespaces set to True.  In this case, all of the advantages added by the above mentioned patch don't help us.
The problem arises if a router gets deleted or moved to another agent while the L3 agent is down.  When the L3 agent comes back up, it will not touch the router and the router will continue to function as if it were never deleted."
1144,1301101,neutron,f86d244e45310dbef7f22c6f320fac6897fdce1f,0,0,Bug in test,Add functional tests to verify VXLAN support,Functional tests to verify that a host can support VXLAN are needed. These can also test the logic in ovs_lib and compare the results of the functional test to the ovs_lib result.
1145,1301105,neutron,9df867c672bfb3f80511086c889b744113c56604,1,1,,Second firewall creation returns 500,"Second firewall creation returns 500.
It is an expected behavior of firewall reference implementation and an internal server error should not be returned.
It is some kind of quota error and 409 looks appropriate."
1146,1301172,cinder,d75a90ec1daff1444f20f6b68255890391bdb4e5,1,0,"“Keys like display_name and display_description were deprecated for other actions in v2 like creating volumes, so for consistency they should work with updating.”","Can not update volume  by display_name, display_de...","Can not update volume  by display_name, display_description in  cinder Block api v2.
reason:
when updating volume , v2 API allows name instead of display_name,description Instead of display_description. but hasn't get
display_name,display_description,so cann't  update volume by the two prameters ,should get the two prameters value  first.
filepath:
cinder/api/v2/volumes.py
function :
@wsgi.serializers(xml=VolumeTemplate)
    def update(self, req, id, body):
add code:
        # NOTE(zhuozhe) fix bug : v2 API allows name instead of display_name
        # description Instead of display_description, so should get old parameter first
        valid_update_keys_v1 = (
            'display_name',
            'display_description',
        )
        for key in valid_update_keys_v1:
            if key in volume:
                update_dict[key] = volume[key]"
1147,1301337,neutron,58f33fd3ef7b66bb570ef757b9f70a023e0c8c59,0,0,Bug in test,We have three duplicated tests for check_ovs_vxlan...,"test_ovs_lib, test_ovs_neutron_agent and test_ofa_neutron_agent have duplicated same unit tests for check_ovs_vxlan_version. The only difference is SystemError (from ovs_lib) and SystemExit (from agents).
The tested logic is 99% same, and unit tests in ovs/ofa agent looks unnecessary."
1148,1301340,nova,878f767b9e1cdc2b57858c41d973dd087f37bbaa,0,0,"feature “We should create helper to remove this duplicate code and help to implement new filters based on aggregates""",Remove duplicate code with aggregate filters,"Some filters are using the same logic to handle per-aggregate options. We should create helper to remove this duplicate code and help to implement new filters based on aggregates
Filters that needs to be addressed:
 * AggregateRamFilter
 * AggregateCoreFilter,
 * AggregateTypeAffinityFilter"
1149,1301348,cinder,4be8913520f5e9fe4109ade101da9509e4a83360,1,1,,Bug #1301348 “vmware,"Create a volume with volume type mapped to gold profile. [gold profile mapped to datastore1]
Now attach to instance and observe that volume is placed in datastore1.
Now detach above volume .
Now change volume type of above volume to Silver profile   [Silver profile mapped to datastore2]
Now re attach volume to instance and observe that volume is still  placed in datastore1 instead of datastore2."
1150,1301363,neutron,83a11d406eb421285a7fffd3ce853394211ac397,0,0,Feature “It makes sense to refactor the functional tests to provide a base class which all these classes can inherit from”,Refactor functional tests to create base class for...,I'm writing additional functional tests which require sudo. There is also additional shared code from some existing agent tests which would be nice to use. It makes sense to refactor the functional tests to provide a base class which all these classes can inherit from.
1151,1301384,nova,5d39189df6ecb559c3f8b7e2fa3beff25da9f452,1,1,Bug in logs,"Note that XML support *may* be removed, not *will*...","In Icehouse we marked the v2 API XML support as deprecated.  The log message says it *will* be removed, but should be updated to be more accurate and say *may* be removed, pending finalizing the discussion around it."
1152,1301396,neutron,73705174c9cdd847501e36385ffb942c17c8ef61,1,1,,Enum type is changed incorrectly in migration 1341...,"In migration 1341ed32cc1e_nvp_netbinding_update Enum type had been changed incorrectly from ('flat', 'vlan', 'stt', 'gre') to ('flat', 'vlan', 'stt', 'gre', 'l3_ext')  for PostgeSQL so in database this type was not changed.
This could be checked as it shown there http://paste.openstack.org/show/74835/.
The same problem is taken place for vlan_type in migrations 38fc1f6789f8_cisco_n1kv_overlay from ('vlan', 'vxlan', 'trunk',
'multi-segment') to ('vlan', 'overlay', 'trunk', 'multi-segment') and in 46a0efbd8f0_cisco_n1kv_multisegm from ('vlan', 'vxlan') to
('vlan', 'vxlan', 'trunk', 'multi-segment').
This could be checked as it shown there http://paste.openstack.org/show/75387/"
1153,1301432,neutron,2da351af7d2a10a3055020d572d357c56ba2689b,1,1,,ODL ML2 driver doesn't warn if the url parameter i...,"If you don't define the ""url"" parameter in the [ml2_odl] section of the ML2 configuration file, the ODL driver will run happily but no request will be made to the ODL service. See http://git.openstack.org/cgit/openstack/neutron/tree/neutron/plugins/ml2/drivers/mechanism_odl.py?id=a57dc2c30ab78ba74cfc51b8fdb457d3374cc87d#n313
The parameter should have a sane default value (eg http://127.0.0.1:8080/controller/nb/v2/neutron) and/or a message should be logged to warn the deployer."
1154,1301515,nova,93f29574c416aebcd1bd7527d3007664aa073766,0,0,"refactoring “Reduce logging in scheduler""",reduce logging in the scheduler to improve perform...,"The current debug logs in the scheduler are at critical points in the code, and are causing performance issues.
After the DB, the scheduler is spending more time doing logging, than anything else.
This was discovered using the test_performance_check_select_destination unit test, and modifying it to look at when there are around 200 hosts, which is still quite a modest size."
1155,1301602,nova,570a84e92c88b1c7d9fd48d107bc3fad75e32efe,0,0,Bug in test,nova unit tests fail test_get_domain_info_with_mor...,"Regression caused by https://github.com/openstack/nova/commit/23158ad8b340ed5c53fe6ad0fe582f47467c9127
Traceback (most recent call last):
  File ""nova/tests/virt/libvirt/test_libvirt.py"", line 6447, in test_get_domain_info_with_more_return
    mock_domain = libvirt.virDomain('qemu:///system', None)
  File ""nova/tests/virt/libvirt/fakelibvirt.py"", line 213, in __init__
    self._def = self._parse_definition(xml)
  File ""nova/tests/virt/libvirt/fakelibvirt.py"", line 220, in _parse_definition
    tree = etree.fromstring(xml)
  File ""lxml.etree.pyx"", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68106)
  File ""parser.pxi"", line 1784, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102444)
ValueError: can only parse strings
I think the domain should be mocked and shouldn't be created directly.
I'm facing this issue with py26 on OEL 6.3, if it matter."
1156,1301696,nova,0b2cf281be48e76a80e2fc73f00529f22e8d9126,1,1,,automatic confirm resize should not set migration ...,"function in compute/manager.py _poll_unconfirmed_resizes will translate the migrate status from finished to error
whenever it find a problem , consider following case
1) _poll_unconfirmed_resizes running, it found several migrations to be confirmed
2) user want to delete an instance ,its task state will be changed to DELETING by
3) _poll_unconfirmed_resized will found the task state is not None, it will make the migration error status
4) following code in _delete
if instance.vm_state == vm_states.RESIZED:
                self._confirm_resize_on_deleting(context, instance)
will fail because migration status already updated
so we should not set the migration status to 'error' , let it be and report warning message is enough"
1158,1301982,nova,1680cf8cef9bd670306a0ef74803409f60f21f28,1,1,,anti-affinity server boot failure shows stack trac...,"If anti-affinity policy is set for two different servers, and only one host is available, a stack trace is shown instead of a error message.  Failure is expected, but a message about why it failed would be helpful.
On a single node devstack setup:
nova server-group-create --policy anti-affinity aagroup
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=ea784f2b-b262-451a-821a-5ee7f69b3d63 --hint group=0f2b71ea-3dc3-423d-a9f4-41ac5d0fff07 server1
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=ea784f2b-b262-451a-821a-5ee7f69b3d63 --hint group=0f2b71ea-3dc3-423d-a9f4-41ac5d0fff07 server2
server1 boots fine
server2 has Status Error and Power State No State
Horizon shows:
Fault
Message
unsupported operand type(s) for |: 'list' and 'set'
Code
500
Details
File ""/opt/stack/nova/nova/scheduler/manager.py"", line 140, in run_instance legacy_bdm_in_spec) File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 86, in schedule_run_instance filter_properties, instance_uuids) File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 289, in _schedule filter_properties) File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 275, in _setup_instance_group filter_properties['group_hosts'] = user_hosts | group_hosts
Created
April 3, 2014, 1:58 p.m."
1159,1302007,neutron,ceee7b4ac9d027499dfd2568868f1192bfb77879,1,1,,l3 agent uses method concatenating two constants,"In l3 agent is method ns_name() that concatenates constant NS_PREFIX with router.id that doesn't change during router's lifecycle. It's ineffective, namespace name can be an instance attribute."
1160,1302091,neutron,c49ec8b3ba10bb414a0f135d29ff3685e26d58b3,0,0,Bug in test,Redundant SG rule create calls in unit tests,"The following test cases in TestSecurityGroups of test_extension_security_group.py file
are making multiple calls to create a SG rule.
test_create_security_group_rule_min_port_greater_max()
test_create_security_group_rule_ports_but_no_protocol()
test_create_security_group_rule_port_range_min_only()
test_create_security_group_rule_port_range_max_only()
test_create_security_group_rule_icmp_type_too_big()
test_create_security_group_rule_icmp_code_too_big()
The redundant calls can be removed."
1161,1302211,nova,2634616e3d94b57600090f3262c96e2233a521f8,0,0,Bug in test,RbdTestCase.test_cache_base_dir_exists is duplicat...,"RbdTestCase in nova/tests/virt/libvirt/test_imagebackend.py now has two slightly different versions of the same test case:
https://github.com/openstack/nova/blob/dc8de426066969a3f0624fdc2a7b29371a2d55bf/nova/tests/virt/libvirt/test_imagebackend.py#L759
https://github.com/openstack/nova/blob/dc8de426066969a3f0624fdc2a7b29371a2d55bf/nova/tests/virt/libvirt/test_imagebackend.py#L806
The redundant version was added in:
https://review.openstack.org/82840
I think it should be removed, it doesn't do anything the original test case doesn't already have."
1162,1302238,nova,18917d9e0d6fe250cc3b4f7301d37a6c5f5faffb,1,0,"""After https://blueprints.launchpad.net/nova/+spec/instance-group-api-extension,""",throw exception if no configured affinity filter,"After https://blueprints.launchpad.net/nova/+spec/instance-group-api-extension, nova has the feature of creating instance groups with affinity or anti-affinity policy and creating vm instance with affinity/anti-affinity  group.
If did not enable ServerGroupAffinityFilter and ServerGroupAntiAffinityFilter, then the instance group will not able to leverage affinity/anti-affinity.
Take the following case:
1) Create a group with affinity
2) Create two vms with this group
3) The result is that those two vms was not created on the same host.
We should  throw exception if using server group with no configured affinity filter"
1163,1302272,neutron,0c202ab3e453e38c09f04978e4fce30d6ee6350c,1,1,,neutron iptables manager is slow modifying a large...,"Sudhakar Gariganti has noticed that with a very large number of iptables rules that _modify_rules() was taking so long to complete (140 seconds) that VMs couldn't be reliably booted because the rules weren't getting put in place before the initial DHCP requests had timed out.  With a small change the update can be done much quicker, and also allow each node to support a larger set of iptables rules.
I've included a snippet from the related bug for reference, https://bugs.launchpad.net/neutron/+bug/1253993
""We have done significant testing with this patch and want to share few results from our experiments.
We were basically trying to see how many VMs we can scale with the OVS agent in use. With default security groups(which has remote security group), beyond 250-300 VMs, VMs were not able to get DHCP IPs. We were having 16 CNs, with VMs uniformly distributed across them. The VM image had a wait period of 120 secs to receive the DHCP response.
By the time we have around 18-19 VMs on each CN(there were around 6k Iptable rules), each RPC loop was taking close to 140 seconds(if there is any update). And the reason VMs were not getting IPs was that the Iptable rules required for the VM to send out the DHCP request were not in place before the 120 secs wait period. Upon further investigations we discovered that the ""for loop searching iptable rules"" in _modify_rules method of iptables_manger.py is eating a big chunk of the overall time spent.
After this patch, we were able to see close to 680 VMs were able to get IPs. The number of Iptable rules at this point was close to 20K, with around 40 VMs per CN.
To summarize, we were able to increase the processing capability of compute node from 6K Iptable rules to 20K Iptable rules, which helped more VMs get DHCP IP within the 120 sec wait period. You can imagine the situation when the wait time is less than 120 secs."""
1164,1302282,neutron,5f042a64308cb00698b454da09b4a87f2ac3fafd,0,0,Bug in test,test_iptables_firewall uses invalid MAC addresses,"The unit test test_iptables_firewall.py has invalid MAC addresses in the code - 'ff:ff:ff:ff', an actual MAC has 6 octets and should be 'ff:ff:ff:ff:ff:ff'.  The test still seems to run fine, but it should be cleaned-up."
1165,1302283,neutron,0f2de5a3353c6951a88542f35be80b9ba9a55c0f,1,1,,LBaaS Haproxy occurs error if no member is added,"if VIP (only with session_persistence=HTTP_COOKIE)&Pool without one member added, the Haproxy agent would report config file error like the following:
2014-04-02 10:14:10.632 17283 DEBUG neutron.openstack.common.lockutils [req-18b873c4-abd7-4483-824f-f553c29ae549 None] Semaphore / lock released ""deploy_instance"" inner /opt/stack/neutron/neutron/openstack/common/lockutils.py:252
2014-04-02 10:14:10.633 17283 ERROR neutron.services.loadbalancer.agent.agent_manager [req-18b873c4-abd7-4483-824f-f553c29ae549 None] Unable to deploy instance for pool: a61dd6b6-8dbe-4576-b213-f8632892a58c
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Traceback (most recent call last):
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/agent/agent_manager.py"", line 180, in _reload_pool
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     self.device_drivers[driver_name].deploy_instance(logical_config)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     return f(*args, **kwargs)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 280, in deploy_instance
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     self.update(logical_config)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 95, in update
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     self._spawn(logical_config, extra_args)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 110, in _spawn
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     ns.netns.execute(cmd)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 466, in execute
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     check_exit_code=check_exit_code)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/agent/linux/utils.py"", line 76, in execute
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     raise RuntimeError(m)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager RuntimeError:
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qlbaas-a61dd6b6-8dbe-4576-b213-f8632892a58c', 'haproxy', '-f', '/opt/stack/data/neutron/lbaas/a61dd6b6-8dbe-4576-b213-f8632892a58c/conf', '-p', '/opt/stack/data/neutron/lbaas/a61dd6b6-8dbe-4576-b213-f8632892a58c/pid', '-sf', '4524']
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Exit code: 1
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Stdout: ''
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Stderr: '[ALERT] 091/101410 (20752) : config : HTTP proxy a61dd6b6-8dbe-4576-b213-f8632892a58c has a cookie but no server list !\n[ALERT] 091/101410 (20752) : Fatal errors found in configuration.\n'
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager
I think one solution: check the member list, if no member exists, Haproxy agent would not deploy the instance or undeploy existed haproxy process."
1166,1302312,neutron,4e9d4824050eedfd1495469a69e28864b6fec757,1,1,,Bug #1302312 “dhcp agent sometimes fail to add gateway to interf... ,"2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 217, in _as_root
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent     kwargs.get('use_root_namespace', False))
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 70, in _as_root
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent     namespace)
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 81, in _execute
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent     root_helper=root_helper)
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/linux/utils.py"", line 75, in execute
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent     raise RuntimeError(m)
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent RuntimeError:
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qdhcp-e1eaed6e-ef91-4741-acb2-62daba3ffede', 'ip', 'route', 'replace', 'default', 'via', '10.2.0.1', 'dev', 'tapb1e4235c-4e']
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Exit code: 2
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Stdout: ''
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Stderr: 'RTNETLINK answers: Network is unreachable\n'
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent"
1167,1302334,nova,b5280671768bdf861d9f72f014a83bb07af09ab6,1,1,“Change I140bfec2a52bf659a725a7dbe78ba5c527ed26de converted the_post_live_migration() call to assume it was passed an object”,"Bug #1302334 “live migration failed "" ","When I try to live migration a VM, I got the following exception from source host.
2014-04-04 12:50:20.330 8862 INFO nova.compute.manager [-] [instance: 160fb719-7f84-466a-a19d-9284dd6d56fa] NV-FA2EA85 _post_live_migration() is started..
2014-04-04 12:50:20.371 8862 ERROR nova.openstack.common.loopingcall [-] in fixed duration looping call
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall Traceback (most recent call last):
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/loopingcall.py"", line 78, in _inner
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     self.f(*self.args, **self.kw)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 4495, in wait_for_live_migration
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     migrate_data)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 88, in wrapped
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     payload)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     six.reraise(self.type_, self.value, self.tb)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 71, in wrapped
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     return f(self, context, *args, **kw)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 315, in decorated_function
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     e, sys.exc_info())
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     six.reraise(self.type_, self.value, self.tb)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 302, in decorated_function
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     return function(self, context, *args, **kwargs)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 4600, in _post_live_migration
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     ctxt, instance.uuid)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall AttributeError: 'dict' object has no attribute 'uuid'
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall"
1168,1302463,neutron,49502246ef319cf8981b8a867ce2a3b6a163fc9c,0,0,"Feature, “LinuxBridgeManager is adding a new method for checking”",LinuxBridgeManager is not using device_exist from ...,LinuxBridgeManager is adding a new method for checking the existence of a device instead of using the one in ip_lib
1169,1302482,nova,529e7d14a716fcc1264b2e8053be3c842afd2153,1,1,,Bug #1302482 “VMware driver,"when there is a Datacenter in the vCenter with no datastore associated with it, nova boot fails  even though there are data-centers configured properly.
The log error trace
Error from last host: devstack (node domain-c162(Demo-1)): [u'Traceback (most recent call last):\n', u'  File ""/opt/stack/nova/nova/compute/manager.py"", line 1322, in _build_instance\n    set_access_ip=set_access_ip)\n', u'  File ""/opt/stack/nova/nova/compute/manager.py"", line 399, in decorated_function\n    return function(self, context, *args, **kwargs)\n', u'  File ""/opt/stack/nova/nova/compute/manager.py"", line 1734, in _spawn\n    LOG.exception(_(\'Instance failed to spawn\'), instance=instance)\n', u'  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/opt/stack/nova/nova/compute/manager.py"", line 1731, in _spawn\n    block_device_info)\n', u'  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 619, in spawn\n    admin_password, network_info, block_device_info)\n', u'  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 211, in spawn\n    dc_info = self.get_datacenter_ref_and_name(data_store_ref)\n', u'  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1715, in get_datacenter_ref_and_name\n    self._update_datacenter_cache_from_objects(dcs)\n', u'  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1693, in _update_datacenter_cache_from_objects\n    datastore_refs = p.val.ManagedObjectReference\n', u""AttributeError: 'Text' object has no attribute 'ManagedObjectReference'\n""]
2014-04-04 03:05:41.629 WARNING nova.scheduler.driver [req-cc690e5a-2bf3-4566-a697-30ca882df815 nova service] [instance: f0abb23a-943a-475d-ac63-69d2563362cb] Setting instance to ERROR state."
1170,1302514,cinder,bc115eaa477d30826ac17986a6e45844b456e206,1,1,,Bug #1302514 “vmware,"Observed Error message in logs : "" 'The session is not authenticated.' to caller""
After above message I am un-able to create volumes , all volumes created are in error state.
Please see attached logs for reference"
1171,1302609,cinder,eacab523be3d8f2f97809c785dba98e5e2e75c7b,1,1,,"Create empty volume when snapshot id is """" or srv ...","When call create volume api with image id """", it will create an empty volume.
But when create volume with snapshot id """" or srv volume """", it will return an 404 error.
So we should create an empty volume when call create volume api with snapshot id """" or srv volume """"."
1172,1302621,cinder,0756cf8ec3f2c73b4141b4b6dc597e82e47de669,1,1,,Only create volume with an acitve image,"When call create volume api with image id """", it will create an empty
volume.
But when create volume with snapshot id """" or srv volume """", it will
return an 404 error.
So it should create volume fail when call create volume api with image id
""""."
1173,1302661,glance,164d1ab46f06141e2c12b944e15ae10f20dace38,1,1,“Backwards compatibility? (5fa3d2ef956aa463caf45405a911ef5733085058),Notifier fails to start when old config params are...,"The current implementation always tries to get a transport from oslo.messaging assuming the transport_url option has been set. This is done to keep backwards compatibility. However, since the default `rpc_backend` is rabbit, it'll always try to load such driver. The problem raises when `kombu` is not installed and the `notifier_strategy` is set to qpid. This will make glance-api fail because it'll try to load the rabbit driver *before* loading the qpid one."
1174,1302670,cinder,0edfa2eaab08a0756bf3ec7818cdc9a8a379ba87,0,0,Bug in test,StorwizeSVCDriverTestCase.test_storwize_vdisk_copy...,"StorwizeSVCDriverTestCase.test_storwize_vdisk_copy_ops test case requires that there will be no context switches between green threads while it's being run.
This is achieved by monkey patching greenthread.sleep() function in setUp():
     self.sleeppatch = mock.patch('eventlet.greenthread.sleep')
     self.sleeppatch.start()
This seems to be a workaround in order to make the test pass, while it's hiding the actual reason of the problem. Any code causing the thread context switch in some other way will break the test (e.g. syncing the latest oslo.db code from oslo.incubator breaks this test, because it uses time.sleep() instead of eventlet.greenthread.sleep(), and the former is not monkey patched in the test).
Monkey patching of greenthread.sleep() in order to prevent thread context switches doesn't seem to be a good solution anyway as  it is an assumption which is not true when Cinder is run in production."
1175,1303179,neutron,acae91475775a8c85598b1bfdc4910e5fe81ced9,1,1,,Missing comma in nsx router mappings migration,"Found during review https://review.openstack.org/40296
There is a comma missing in the migration_for_plugins list
https://github.com/openstack/neutron/blob/master/neutron/db/migration/alembic_migrations/versions/4ca36cfc898c_nsx_router_mappings.py#L30"
1176,1303312,neutron,7f9c6259923747f0629166dc85561b0ab231ff70,0,0,"feature ""Allow combined certificate/key files for SSL”",SSL mode with combined cert/keys not supported,"When Neutron WSGI is running in SSL mode, it requires a separate cert file and key file. However, there are cases where these may be combined into one file and neutron currently does not support this mode of operation even though the underlying SSL library does[1].
1. https://docs.python.org/2/library/ssl.html#ssl.wrap_socket"
1177,1303360,nova,a2059b67b207d908b1c65c78dd806b1f5e79582d,1,1,,GroupAntiAffinityFilter scheduler hint still doesn...,"After seeing that the following bug was fixed
https://bugs.launchpad.net/nova/+bug/1296913
executed the cmd:
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=909e7fa9-b3af-4601-84c2-01145b1dea72 --hint group=foo server-foo
now the server-foo is stuck in scheduling state forever.
see attached logs."
1178,1303536,nova,60c899b9da8d64b4a5979b69c43c77ed9d5bf248,1,1,,Bug #1303536 “Live migration fails. XML error,"Description of problem
---------------------------
Live migration fails.
libvirt says ""XML error: CPU feature `wdt' specified more than once""
Version
---------
ii  libvirt-bin                                         1.2.2-0ubuntu2                        amd64        programs for the libvirt library
ii  python-libvirt                                      1.2.2-0ubuntu1                        amd64        libvirt Python bindings
ii  nova-compute                                        1:2014.1~b3-0ubuntu2                  all          OpenStack Compute - compute node base
ii  nova-compute-kvm                                    1:2014.1~b3-0ubuntu2                  all          OpenStack Compute - compute node (KVM)
ii  nova-cert                                           1:2014.1~b3-0ubuntu2                  all          OpenStack Compute - certificate management
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=14.04
DISTRIB_CODENAME=trusty
DISTRIB_DESCRIPTION=""Ubuntu Trusty Tahr (development branch)""
NAME=""Ubuntu""
VERSION=""14.04, Trusty Tahr""
Test env
----------
A two node openstack havana on ubuntu 14.04. Migrating a instance to other node.
Steps to Reproduce
------------------
 - Migrate the instance
And observe /var/log/nova/compute.log and /var/log/libvirt.log
Actual results
--------------
/var/log/nova-conductor.log
2014-04-04 13:42:17.128 3294 ERROR oslo.messaging._drivers.common [-] ['Traceback (most recent call last):\n', '  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply\n    incoming.message))\n', '  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', '  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', '  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner\n    return func(*args, **kwargs)\n', '  File ""/usr/lib/python2.7/dist-packages/nova/conductor/manager.py"", line 668, in migrate_server\n    block_migration, disk_over_commit)\n', '  File ""/usr/lib/python2.7/dist-packages/nova/conductor/manager.py"", line 769, in _live_migrate\n    raise exception.MigrationError(reason=ex)\n', 'MigrationError: Migration error: Remote error: libvirtError XML error: CPU feature `wdt\' specified more than once\n[u\'Traceback (most recent call last):\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply\\n    incoming.message))\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch\\n    return self._do_dispatch(endpoint, method, ctxt, args)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch\\n    result = getattr(endpoint, method)(ctxt, **new_args)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 88, in wrapped\\n    payload)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\\n    six.reraise(self.type_, self.value, self.tb)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 71, in wrapped\\n    return f(self, context, *args, **kw)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 272, in decorated_function\\n    e, sys.exc_info())\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\\n    six.reraise(self.type_, self.value, self.tb)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 259, in decorated_function\\n    return function(self, context, *args, **kwargs)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 4159, in check_can_live_migrate_destination\\n    block_migration, disk_over_commit)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4094, in check_can_live_migrate_destination\\n    self._compare_cpu(source_cpu_info)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4236, in _compare_cpu\\n    LOG.error(m, {\\\'ret\\\': ret, \\\'u\\\': u})\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\\n    six.reraise(self.type_, self.value, self.tb)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4232, in _compare_cpu\\n    ret = self._conn.compareCPU(cpu.to_xml(), 0)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 179, in doit\\n    result = proxy_call(self._autowrap, f, *args, **kwargs)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 139, in proxy_call\\n    rv = execute(f,*args,**kwargs)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77, in tworker\\n    rv = meth(*args,**kwargs)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 3191, in compareCPU\\n    if ret == -1: raise libvirtError (\\\'virConnectCompareCPU() failed\\\', conn=self)\\n\', u""libvirtError: XML error: CPU feature `wdt\' specified more than once\\n""].\n']
2014-04-04 13:52:18.161 3295 ERROR nova.conductor.manager [req-471d2933-354a-4417-af50-c48399e19663 42fab7a8b7434bfc8473767c01e8378d b1cf6337c229491c96ad6e0a96e82979] Migration of instance 47d1fe7d-b812-4588-85eb-aa813267fc82 to host c2 unexpectedly failed.
/var/log/libvirtd.log
2014-03-27 18:23:17.141+0000: 2659: info : libvirt version: 1.2.2
2014-03-27 18:23:17.141+0000: 2659: error : virCPUDefParseXML:413 : XML error: CPU feature `wdt' specified more than once
Expected results
----------------
Successful migration
Additional info
----------------
Related with: https://bugs.launchpad.net/nova/+bug/1267191
On the file /usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py, the list info['features'] have the duplicate feature."
1179,1303591,nova,86d1007c2078a0ad80e49088b37660466b1a5cb3,1,1,,InvalidAggregateAction exception is not handled,"When an aggregate with 'host' attribute not empty is deleted, InvalidAggregateAction exception will be raised, but this exception
is not handled.
$ nova --os-compute-api-version 3 aggregate-delete agg5
ERROR (ClientException): Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.InvalidAggregateAction'> (HTTP 500) (Request-ID: req-9a8500ae-379a-4121-b217-7e7ea6188ad0)
2014-04-07 23:56:16.347 ERROR nova.api.openstack.extensions [req-f7c09203-a681-496c-a84e-18fb3d2e3659 admin demo] Unexpected exception in API method
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 472, in wrapped
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/aggregates.py"", line 155, in delete
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     self.api.delete_aggregate(context, id)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     payload)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     six.reraise(self.type_, self.value, self.tb)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     return f(self, context, *args, **kw)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 3363, in delete_aggregate
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     reason='not empty')
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions InvalidAggregateAction: Cannot perform action 'delete' on aggregate 3. Reason: not empty."
1180,1303605,neutron,b12e38327af83f44cc8e38a23d16bb51a2346f57,0,0,Bug in test,"Bug #1303605 “test_rollback_on_router_delete fails "" ","gate-neutron-python26 failis for test_rollback_on_router_delete with following error:
2014-04-07 03:53:51,643    ERROR [neutron.plugins.bigswitch.servermanager] ServerProxy: POST failure for servers: ('localhost', 9000) Response: {'status': 'This server is broken, please try another'}
2014-04-07 03:53:51,643    ERROR [neutron.plugins.bigswitch.servermanager] ServerProxy: Error details: status=500, reason='Internal Server Error', ret={'status': 'This server is broken, please try another'}, data=""{'status': 'This server is broken, please try another'}""
}}}
Traceback (most recent call last):
  File ""neutron/tests/unit/bigswitch/test_router_db.py"", line 536, in test_rollback_on_router_delete
    expected_code=exc.HTTPInternalServerError.code)
  File ""neutron/tests/unit/test_db_plugin.py"", line 450, in _delete
    self.assertEqual(res.status_int, expected_code)
  File ""/home/jenkins/workspace/gate-neutron-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/jenkins/workspace/gate-neutron-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: 204 != 500
full log is here:
http://logs.openstack.org/29/82729/3/check/gate-neutron-python26/a1065eb/testr_results.html.gz"
1181,1303890,neutron,9a830b370551019a4bd3a0c7504f48961e755bd4,1,1,,Uncaught qpid error can break a consumer,"The following exception was originally observed against the old rpc code, but the same problem exists in oslo.messaging.
 Traceback (most recent call last):
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 78, in inner_func
     return infunc(*args, **kwargs)
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 698, in _consumer_thread
     self.consume()
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 689, in consume
     it.next()
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 606, in iterconsume
     yield self.ensure(_error_callback, _consume)
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 540, in ensure
     return method(*args, **kwargs)
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 597, in _consume
     nxt_receiver = self.session.next_receiver(timeout=timeout)
   File ""<string>"", line 6, in next_receiver
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 665, in next_receiver
     if self._ecwait(lambda: self.incoming, timeout):
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 50, in _ecwait
     result = self._ewait(lambda: self.closed or predicate(), timeout)
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 571, in _ewait
     result = self.connection._ewait(lambda: self.error or predicate(), timeout)
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 214, in _ewait
     self.check_error()
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 207, in check_error
     raise self.error
 InternalError: Traceback (most recent call last):
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 667, in write
     self._op_dec.write(*self._seg_dec.read())
   File ""/usr/lib/python2.6/site-packages/qpid/framing.py"", line 269, in write
     if self.op.headers is None:
 AttributeError: 'NoneType' object has no attribute 'headers'
It's possible for something to put the qpid client into a bad state.  In particular, I have observed a case that will cause session.next_receiver() to immediately raise an InternalError.  This exception makes it all the way out.  If the eventlet executor is used, the forever_retry_uncaught_exceptions() decorator will get hit.  It will go back into this code and get the same error, stuck in an infinite loop of retrying.
The connection needs to be reset in this case to recover."
1182,1303983,nova,7b8402ed3ba734836119441bdf1a6d6c661c8df2,0,0, Feature “Enable ServerGroup scheduler filters by default”,Enable ServerGroup scheduler filters by default,"The Icehouse release includes a server group REST API.  For these groups to actually function properly, the server group scheduler filters must be enabled.  So, these filters should be enabled by default since the API is also enabled by default.  If the API is not used, the scheduler filters will be a no-op.
http://lists.openstack.org/pipermail/openstack-dev/2014-April/032068.html"
1183,1304115,cinder,0c4a94eac94399a524ff758fa7046e98b07951ae,1,1,,Storwize/SVC driver crashes when check volume copy...,"Loopingcall will failed if user delete the volume before it finish copy, .
2014-03-31 16:01:04.086 ERROR cinder.openstack.common.loopingcall [-] in fixed duration looping call
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall Traceback (most recent call last):
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/openstack/common/loopingcall.py"", line 76, in
_inner
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall self.f(*self.args, **self.kw)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/_init_.py"",
line 634, in _check_volume_copy_ops
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall volume = self.db.volume_get(ctxt, vol_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/api.py"", line 205, in volume_get
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return IMPL.volume_get(context, volume_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 135, in wrapper
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return f(*args, **kwargs)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 1152, in volume_ge
t
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return _volume_get(context, volume_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 135, in wrapper
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return f(*args, **kwargs)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 1145, in _volume_g
et
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall raise exception.VolumeNotFound(volume_id=volume_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall VolumeNotFound: Volume 8ff1a61e-21c2-48a7-890e-e7d958172721 could not be found.
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall"
1184,1304127,neutron,4cb3eb2a98246f28f8016cfe32946d365203fbae,1,1,"""https://review.openstack.org/#/c/69465/""",Bug #1304127 “NSX,"The DHCP agent used to have a leg on the metadata network, this is a regression caused by:
https://review.openstack.org/#/c/69465/"
1185,1304181,neutron,0e44b7b5418c400af7358a5391f350f2f737929e,1,1,“Git commit c3706fa2 introduced the force_gateway_on_subnet”,neutron should validate gateway_ip is in subnet,"I don't believe this is actually a valid network configuration:
arosen@arosen-MacBookPro:~/devstack$ neutron subnet-show  be0a602b-ea52-4b13-8003-207be20187da
+------------------+------------------------------------------------+
| Field            | Value                                          |
+------------------+------------------------------------------------+
| allocation_pools | {""start"": ""10.11.12.1"", ""end"": ""10.11.12.254""} |
| cidr             | 10.11.12.0/24                                  |
| dns_nameservers  |                                                |
| enable_dhcp      | True                                           |
| gateway_ip       | 10.0.0.1                                       |
| host_routes      |                                                |
| id               | be0a602b-ea52-4b13-8003-207be20187da           |
| ip_version       | 4                                              |
| name             | private-subnet                                 |
| network_id       | 53ec3eac-9404-41d4-a899-da4f32045abd           |
| tenant_id        | f2d9c1726aa940d3bd5a8ee529ea2480               |
+------------------+------------------------------------------------+"
1186,1304184,nova,2bc0877d3cbe5e6ed9bad0907f61011014b75a2d,1,1,,instance stuck into rebuild state when nova-comput...,"when rebuild an instance, and nova-compute died unexpectedly, then the instance will be in REBUILD state forever unless admin take actions
[root@controller ~]# nova list
+--------------------------------------+--------+---------+------------+-------------+--------------------------------------------+
| ID                                   | Name   | Status  | Task State | Power State | Networks                                   |
+--------------------------------------+--------+---------+------------+-------------+--------------------------------------------+
| a9dd1fd6-27fb-4128-92e6-93bcab085a98 | test11 | REBUILD | rebuilding | Running"
1187,1304326,neutron,de403598f608172b1cf41ca4ec0d880642e215d5,1,1,,neutron-db-manage offline mode requires connection...,"When using offline migration mode with neturon-db-manage, we need to pass config file containing connection string to database. For offline migration is sufficient to know database engine. This and used plugins could be passed directly from command line."
1188,1304516,cinder,131777688562b1ec020327067d89da1535b99640,1,1,,Cinder fails to delete error state volumes on Wind...,"If the volume creation fails, we won't be able to delete the volume. It will go from error state to error-deleting. The reason is that the delete_volume method fails when the iSCSI disk does not exist. In fact, it should skip deleting the disk if it does not exist.
Trace: http://paste.openstack.org/show/75340/"
1189,1304558,neutron,77a58ecd0a65b688d9645a3c11a6dfb87e180449,0,0,"Refactoring “This should be removed once the root cause of the patching issue is found.
“",Bug #1304558 “BigSwitch,"httplib.HTTPConnection is being assigned to a module-level variable in the Big Switch server manager module. This was a work-around to mock not correctly stopping patches targeting the normal library path.
This should be removed once the root cause of the patching issue is found.
https://github.com/openstack/neutron/blob/f64eacfd27220c180f6afc979087b35aa1385550/neutron/plugins/bigswitch/servermanager.py#L74"
1190,1304647,neutron,d90d71cfdae1d9c9c3b54cc33adfabce683c633c,1,1,,KeyError in nsx sync nsx_router_id mapping not fou...,"2014-03-26 16:07:51,111 (neutron.plugins.vmware.common.nsx_utils): WARNING nsx_utils get_nsx_router_id Unable to find NSX router for Neutron router e90010ab-f630-4f81-9f6b-ac3da4b29aef
2014-03-26 16:07:51,111 (neutron.plugins.vmware.api_client.base): DEBUG base acquire_connection [0] Acquired connection https://17.176.14.50:443. 8 connection(s) available.
2014-03-26 16:07:51,112 (neutron.plugins.vmware.api_client.request): DEBUG request _issue_request [0] Issuing - request GET https://17.176.14.50:443//ws.v1/lrouter?relations=LogicalRouterStatus
2014-03-26 16:07:51,320 (neutron.plugins.vmware.api_client.request): DEBUG request _issue_request [0] Completed request 'GET https://17.176.14.50:443//ws.v1/lrouter?relations=LogicalRouterStatus': 200 (0.207953929901 seconds)
2014-03-26 16:07:51,321 (neutron.plugins.vmware.api_client.base): DEBUG base release_connection [0] Released connection https://17.176.14.50:443. 9 connection(s) available.
2014-03-26 16:07:51,321 (neutron.plugins.vmware.api_client.eventlet_request): DEBUG eventlet_request _handle_request [0] Completed request 'GET /ws.v1/lrouter?relations=LogicalRouterStatus': 200
2014-03-26 16:07:51,322 (neutron.plugins.vmware.api_client.client): DEBUG client request Request returns ""<httplib.HTTPResponse instance at 0x3d97b00>""
2014-03-26 16:07:51,324 (neutron.openstack.common.loopingcall): ERROR loopingcall _inner in dynamic looping call
Traceback (most recent call last):
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/openstack/common/loopingcall.py"", line 123, in _inner
    idle = self.f(*self.args, **self.kw)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 606, in _synchronize_state
    scan_missing=scan_missing)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 380, in _synchronize_lrouters
    ctx, router, lrouter and lrouter.get('data'))
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 339, in synchronize_router
    self._nsx_cache.update_lrouter(lrouter)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 134, in update_lrouter
    self._update_resources(self._lrouters, [lrouter])
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 86, in _update_resources
    item_id = item['uuid']
KeyError: 'uuid'"
1191,1304721,nova,3012eab2f7e70796ddc06d211dbc76bcf62c3736,1,1,,virNodeListDevices may not be supported,"If libvirt wasn't compiled with the appropriate flags, listDevices may return NOT_SUPPORTED.
This is currently not gracefully handled, which prevents the compute node from starting up.
http://paste.openstack.org/show/75375/
A better fix is to handle the error, and issue a warning when we detect this case."
1192,1304724,nova,275a165cf0e74112d7ec9addacb1f84d703977c8,1,1,,DBNotAllowed raised if trying to create network wi...,"Steps to reproduce:
- Setup a devstack from scratch using nova-network
- delete the default network
  # nova-manage network delete 10.0.0.0/24
- change nova.conf to use VlanManager:
network_manager = nova.network.manager.VlanManager
- restart nova-network
- create a new network with a vlan id:
nova-manage network create --label=network --fixed_range_v4 10.0.1.0/24 --vlan 42
- boot a vm on the cirros image:
nova --debug boot --flavor 1 --image 0b969819-2d85-4f7f-af76-125c5bb5789f test
Expected behavior: The new VM goes to Active state
Actual behavior: The new VM goes to Error state, also nova-network log has this exception:
a7-abaf-78db50a4b62c] network allocations from (pid=13676) allocate_for_instance /opt/stack/nova/nova/network/manager.py:494
2014-04-07 15:32:02.137 ERROR nova.network [req-87a65a9e-9196-4203-9de2-f6911d2aef4b admin demo] No db access allowed in nova-network: File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 128, in <lambda>
    yield lambda: self._dispatch_and_reply(incoming)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/network/floating_ips.py"", line 119, in allocate_for_instance
    **kwargs)
  File ""/opt/stack/nova/nova/network/manager.py"", line 497, in allocate_for_instance
    requested_networks=requested_networks)
  File ""/opt/stack/nova/nova/network/manager.py"", line 1837, in _get_networks_for_instance
    networks = self.db.project_get_networks(context, project_id)
  File ""/opt/stack/nova/nova/db/api.py"", line 1370, in project_get_networks
    return IMPL.project_get_networks(context, project_id, associate)
  File ""/opt/stack/nova/nova/cmd/network.py"", line 47, in __call__
    stacktrace = """".join(traceback.format_stack())
I think the exception was introduced by this patch that disables direct database access from nova-network: https://review.openstack.org/#/c/79716/
However, VlanManager still relies on database access for the given scenario, and there are 3 other places in manager.py that rely on direct db access:
devuser@ubuntu:/opt/stack/nova$ grep self.db nova/network/manager.py -n
1389: vifs = self.db.virtual_interface_get_by_instance(context,
1446: vif = self.db.virtual_interface_get_by_address(context,
1837: networks = self.db.project_get_networks(context, project_id)
1914: not self.db.network_in_use_on_host(context, network['id'],
Therefore, I cannot currently use conductor with nova-network VlanManager, which is a regression from Havana.
===
devstack defaults the network_manager to FlatDHCPManager so we don't test VlanManager in the gate."
1193,1304755,swift,41d851387cec122f4795d447458fd81e48e256b0,1,1,,Account-reaper never reaps account when account de...,"Account-reaper works at account-server with the first account replica, and reaps accounts with ""deleted"" status.
When swift fails to delete some account replicas, account-replicator doesn't replicate ""deleted"" status, but only *_timestamp.
So when swift fails to delete the first account replica, account-reaper will never reap it because its first replica will never have ""deleted"" status.
(If replica count is set as 3, swift returns 204, success return code to delete request in this situation)"
1194,1304790,nova,98f5496cc2b236f54848eb069ea1bc8c25f43b13,1,1,,flavor is not limit by osapi_max_limit when pagena...,"when flavor pagenation, the conf of osapi_max_limit does not work.so we must modify."
1195,1304917,nova,5aa4331cc8d9c0e4615f9859395a9ae8c6d40f09,0,0,Bug in test,Bug #1304917 “VMware,The file nova/virt/vmwareapi/fake.py should be moved to the tests directory. This is solely used in the unit tests. There is no need for this to reside in the virt directory.
1196,1304968,nova,53ad788af6ced83bd9d6e58a25196a325d60fc4e,1,1,,Nova cpu full of instance_info_cache stack traces ...,"The bulk of the stack traces in n-cpu is because emit_event is getting triggered on a VM delete, however by the time we get to emit_event the instance is deleted (we see this exception 183 times in this log - which means it's happening on *every* compute terminate) so when we try to look up the instance we hit the exception found here:
    @base.remotable_classmethod
    def get_by_instance_uuid(cls, context, instance_uuid):
        db_obj = db.instance_info_cache_get(context, instance_uuid)
        if not db_obj:
            raise exception.InstanceInfoCacheNotFound(
                    instance_uuid=instance_uuid)
        return InstanceInfoCache._from_db_object(context, cls(), db_obj)
A log trace of this interaction looks like this:
2014-04-08 11:14:25.475 DEBUG nova.openstack.common.lockutils [req-fe9db989-416e-4da0-986c-e68336e3c602 TenantUsagesTestJSON-153098759 TenantUsagesTestJSON-953946497] Semaphore / lock released ""do_terminate_instance"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:252
2014-04-08 11:14:25.907 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore ""75da98d7-bbd5-42a2-ad6f-7a66e38977fa"" lock /opt/stack/new/nova/nova/openstack/common/lockutils.py:168
2014-04-08 11:14:25.907 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore / lock ""do_terminate_instance"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:248
2014-04-08 11:14:25.907 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore ""<function _lock_name at 0x41635f0>"" lock /opt/stack/new/nova/nova/openstack/common/lockutils.py:168
2014-04-08 11:14:25.908 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore / lock ""_clear_events"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:248
2014-04-08 11:14:25.908 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Semaphore / lock released ""_clear_events"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:252
2014-04-08 11:14:25.928 AUDIT nova.compute.manager [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] [instance: 75da98d7-bbd5-42a2-ad6f-7a66e38977fa] Terminating instance
2014-04-08 11:14:25.989 DEBUG nova.objects.instance [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Lazy-loading `system_metadata' on Instance uuid 75da98d7-bbd5-42a2-ad6f-7a66e38977fa obj_load_attr /opt/stack/new/nova/nova/objects/instance.py:519
2014-04-08 11:14:26.209 DEBUG nova.network.api [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Updating cache with info: [VIF({'ovs_interfaceid': None, 'network': Network({'bridge': u'br100', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': u'fixed', 'floating_ips': [], 'address': u'10.1.0.2'})], 'version': 4, 'meta': {u'dhcp_server': u'10.1.0.1'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'8.8.4.4'})], 'routes': [], 'cidr': u'10.1.0.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': u'gateway', 'address': u'10.1.0.1'})}), Subnet({'ips': [], 'version': None, 'meta': {u'dhcp_server': None}, 'dns': [], 'routes': [], 'cidr': None, 'gateway': IP({'meta': {}, 'version': None, 'type': u'gateway', 'address': None})})], 'meta': {u'tenant_id': None, u'should_create_bridge': True, u'bridge_interface': u'eth0'}, 'id': u'9751787e-f41c-4299-be13-941c901f6d18', 'label': u'private'}), 'devname': None, 'qbh_params': None, 'meta': {}, 'details': {}, 'address': u'fa:16:3e:d8:87:38', 'active': False, 'type': u'bridge', 'id': u'db1ac48d-805a-45d3-9bb9-786bb5855673', 'qbg_params': None})] update_instance_cache_with_nw_info /opt/stack/new/nova/nova/network/api.py:74
2014-04-08 11:14:27.661 2894 DEBUG nova.virt.driver [-] Emitting event <nova.virt.event.LifecycleEvent object at 0x4932e50> emit_event /opt/stack/new/nova/nova/virt/driver.py:1207
2014-04-08 11:14:27.661 2894 INFO nova.compute.manager [-] Lifecycle event 1 on VM 75da98d7-bbd5-42a2-ad6f-7a66e38977fa
2014-04-08 11:14:27.773 2894 ERROR nova.virt.driver [-] Exception dispatching event <nova.virt.event.LifecycleEvent object at 0x4932e50>: Info cache for instance 75da98d7-bbd5-42a2-ad6f-7a66e38977fa could not be found.
Traceback (most recent call last):
  File ""/opt/stack/new/nova/nova/conductor/manager.py"", line 597, in _object_dispatch
    return getattr(target, method)(context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 151, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance.py"", line 500, in refresh
    self.info_cache.refresh()
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 151, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance_info_cache.py"", line 103, in refresh
    self.instance_uuid)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 112, in wrapper
    result = fn(cls, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance_info_cache.py"", line 70, in get_by_instance_uuid
    instance_uuid=instance_uuid)
InstanceInfoCacheNotFound: Info cache for instance 75da98d7-bbd5-42a2-ad6f-7a66e38977fa could not be found.
2014-04-08 11:14:27.840 2894 INFO nova.virt.libvirt.driver [-] [instance: 75da98d7-bbd5-42a2-ad6f-7a66e38977fa] Instance destroyed successfully.
Raw logs for a failure: http://logs.openstack.org/38/62038/14/check/check-tempest-dsvm-full/86cde16/logs/screen-n-cpu.txt.gz?level=TRACE
Specific failure point: http://logs.openstack.org/38/62038/14/check/check-tempest-dsvm-full/86cde16/logs/screen-n-cpu.txt.gz?level=DEBUG#_2014-04-08_11_14_25_928"
1197,1305083,neutron,b31d28415a6e5b7224d53eb674e4f76e8de738ac,0,0,"Refactoring “The dhcp agent never uses the reuse_existing flag, which is set to true by default.”",reuse_existing is never used by the dhcp agent,"The dhcp agent never uses the reuse_existing flag, which is set to true by default. It can be removed."
1198,1305197,cinder,d09d12ab2ba72a9e7fe42852a7cf837231053590,1,1,,volume manager must call driver.remove_export with...,"There are a couple of places in the volume manager (initialize_connection and terminate_connection) where driver.remove_export() is called with a user context rather than an elevated context.
This appears to break the assumption used in delete_volume() where an elevated context is passed.
This causes the LVM LIO driver to fail with an error removing an export in an initialize_connection() error case:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/amqp.py"", line 462, in _process_data
    **args)
  File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/usr/lib/python2.7/site-packages/cinder/volume/manager.py"", line 801, in initialize_connection
    self.driver.remove_export(context, volume)
  File ""/usr/lib/python2.7/site-packages/cinder/volume/drivers/lvm.py"", line 540, in remove_export
    self.target_helper.remove_export(context, volume)
  File ""/usr/lib/python2.7/site-packages/cinder/volume/iscsi.py"", line 232, in remove_export
    volume['id'])
  File ""/usr/lib/python2.7/site-packages/cinder/db/api.py"", line 232, in volume_get_iscsi_target_num
    return IMPL.volume_get_iscsi_target_num(context, volume_id)
  File ""/usr/lib/python2.7/site-packages/cinder/db/sqlalchemy/api.py"", line 116, in wrapper
    raise exception.AdminRequired()
AdminRequired: User does not have admin privileges"
1199,1305377,neutron,dda6c89202b015628f74101839301424001a6c63,1,1,,Correct flake8 E711 and E712 violations,"Enable flake8 checking for
E711 comparison to False should be 'if cond is False:' or 'if not cond:'
E712 comparison to True should be 'if cond is True:' or 'if cond:'"
1200,1305399,nova,8831f3d0e2e2a81a6b406cc9c8bf89bc15989065,1,1,,Cannot unshelve instance with user volumes,"the steps to reproduce:
1. boot an instance with user volume: nova boot --flavor 11 --image cirros --block-device-mapping /dev/vdb=a6118113-bce9-4e0f-89ce-d2aecb0148f8 test_vm1
2. shelve the instance: nova shelve 958b6615-1a02-46a7-a0cf-a4b253f1b9de
3. unshelve the instance: nova unshelve 958b6615-1a02-46a7-a0cf-a4b253f1b9de
the instance will be in task_state of ""unshelving"", and the error message in log file is:
[-] Exception during message handling: Invalid volume: status must be 'available'
Traceback (most recent call last):
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 243, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 229, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 294, in decorated_function
    function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 271, in decorated_function
    e, sys.exc_info())
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 258, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3593, in unshelve_instance
    do_unshelve_instance()
  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner
    return f(*args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3592, in do_unshelve_instance
    filter_properties, node)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3617, in _unshelve_instance
    block_device_info = self._prep_block_device(context, instance, bdms)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1463, in _prep_block_device
    instance=instance)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1442, in _prep_block_device
    self.driver, self._await_block_device_map_created) +
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 364, in attach_block_devices
    map(_log_and_attach, block_device_mapping)
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 362, in _log_and_attach
    bdm.attach(*attach_args, **attach_kwargs)
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 44, in wrapped
    ret_val = method(obj, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 218, in attach
    volume_api.check_attach(context, volume, instance=instance)
  File ""/opt/stack/nova/nova/volume/cinder.py"", line 229, in check_attach
    raise exception.InvalidVolume(reason=msg)
InvalidVolume: Invalid volume: status must be 'available'"
1201,1305423,nova,f087a6f77ef1338bb8d10943d2a18712220c3c44,1,1,,nova libvirt re-write broken with mulitiple epheme...,"Seem to be experiencing a bug with libvirt.xml device formatting when --ephemeral flag is used after initial booth and then use of nova stop/start or nova reboot --hard.  We are using following libvirt options in nova.conf for storage:
libvirt_images_type=lvm
libvirt_images_volume_group=vglocal
When normally using nova boot with a flavor that has ephemeral defined it create two LVM volumes appropriatly ex.
instance-0000077e_disk
instance-0000077e_disk.local
The instance libvirt.xml contains disk devices entry as follows:
<devices>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-0000077e_disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-0000077e_disk.local""/>
      <target bus=""virtio"" dev=""vdb""/>
    </disk>
If we use ""nova boot --flavor 757c75fa-0b6d-4d4f-a128-27813009bff4 --image caa978e0-acae-4205-a4a4-2cf159c166fd --nic net-id=44f2fb0b-0a7a-475c-8fff-54cd4b37958b --ephemeral size=1 --ephemeral size=1 localdisk-1"" the LVM disks for ephemeral goes through enumeration logic whether there is one or more --ephemeral options
 instance-000007ed_disk
 instance-000007ed_disk.eph0
 instance-000007ed_disk.eph1
The instance libvirt.xml after instance spawn has disk device entries like below and the instances happily boots.
 <devices>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk.eph0""/>
      <target bus=""virtio"" dev=""vdb""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk.eph1""/>
      <target bus=""virtio"" dev=""vdc""/>
    </disk>
If nova stop/start or nova reboot --hard is executed the instance is destroyed and libvirt.xml gets recreated.  At this stage whatever values we passed with --ephemeral are not respected and libvirt.xml revirts to configuration that would have been generated without the use of the --ephemeral option like below where we only have one extra disk and it is not using the enumerated naming.
  <devices>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk.local""/>
      <target bus=""virtio"" dev=""vdb""/>
    </disk>
This causes instances booting to fail at this stage.  The nova block_device_mapping table has records for all 3 devices."
1202,1305656,neutron,1e0ea5217a93fadb915c3dcdf4c260738bed70ac,0,0,Bug in test,test that inherit from BaseTestCase don't need to ...,"In BaseTestCase a clean up is added to stop all patches :
self.addCleanup(mock.patch.stopall)
the tests that inherits from BaseTestCase don't need to stop their patches"
1203,1305957,neutron,f5d33051b6d9b0dd0fe08692758748ed34d50136,1,0,“This change https://review.openstack.org/#/c/69803/3 added new quota resources.”,LBaaS extension doesn't register it's resources to...,"LBaaS extension is using the new resource_helper module to register its resources to resource manager.
This change https://review.openstack.org/#/c/69803/3 added new quota resources.
In order those new quota resources to be registered in quotas engine, register_quota=True argument should be passed to the resource_helper.build_resource_info() function.
neutron/extensions/loadbalancer.py line 351
This bug causes modified tempest quotas test from https://review.openstack.org/#/c/60008 to fail
because new lbaas quota resources are not registered"
1204,1306009,nova,79ab96e34ba5b8dd3e4e542dd3a7f65624b13367,1,1,,nova-api-metadata requires db access when configur...,"When nova-network is being used, nova-api-metadata requires db access which in some configurations means the compute nodes still connect to the database.
It connects to the db when trying to map the fixed ip address to an instance uuid:
fixed_ip = network.API().get_fixed_ip_by_address(ctxt, address)
This should be changed to do an rpc call to conductor."
1205,1306032,cinder,9026db874634a78b8cdd3abb45a75a37f6ededfe,1,0,"""When creating a volume from a snapshot on Windows”",Volumes created from snapshots on Windows are unac...,"When creating a volume from a snapshot on Windows, a shadow copy volume is exported as an iSCSI disk. The issue is that this export is readonly and cannot be mounted to instances. As this export cannot be modified, to make the new volume usable, the solution would be to copy the image to a new path and import it as the desired volume."
1206,1306479,nova,0da4d72ae6aae012f2c65c928bbab013043d09a1,1,1,,ec2 api breaks on non-ascii characters,"When you query the ec2 api, and it returns non-ascii data from the database, It throws something like this
2014-04-11 10:05:19.874 ERROR nova.api.ec2 [req-bc73aa28-0faa-437c-8fb1-e61188658c03 user project] Unexpected error raised: 'ascii' codec can't encode character u'\xe4' in position 1: ordinal not in range(128)
Tha cause seems to be the str() calls when calling xml.createTextNode in the /api/ec2/apirequest.py file.
This should porbably be made safe for non-ascii characters."
1207,1306488,neutron,22bec67395b52a879ad4eb37b150eead9d4bb444,1,1,,invalid gre/vxlan id shouldn't be created successf...,"create vxlan successfully with invalid vxlan id for ml2:
curl -i http://127.0.0.1:9696/v2.0/networks -H ""X-Auth-Token:$token_id"" -H ""Content-Type: application/json"" -X POST -d '{""network"": {""name"": ""vxlan_test"", ""admin_state_up"": true, ""tenant_id"": $tenant_id, ""provider:network_type"": ""vxlan"", ""router:external"": false, ""shared"": false, ""provider:segmentation_id"": -1}}'
response:
HTTP/1.1 201 Created
Content-Type: application/json; charset=UTF-8
Content-Length: 332
X-Openstack-Request-Id: req-1e9af36d-b742-4c76-bfdf-0cea4df6399f
Date: Fri, 11 Apr 2014 09:25:51 GMT
{
    ""network"": {
        ""status"": ""ACTIVE"",
         ""subnets"": [
        ],
         ""name"": ""vxlan_test"",
         ""provider:physical_network"": null,
         ""admin_state_up"": true,
         ""tenant_id"": ""bfed9cd5990c49ad8a42ba36d505c003"",
         ""provider:network_type"": ""vxlan"",
         ""router:external"": false,
         ""shared"": false,
         ""id"": ""5bd9b587-02f8-4432-b0d0-18a126126072"",
         ""provider:segmentation_id"": -1
    }
}"
1208,1306591,swift,07841b42828dc133ba258cf7c7f09d6f8906a96f,1,1,,Proxy server generates error log when cache middle...,"Proxy server generates error log for every request when cache middleware is disabled.
Here I attach a sample.
Apr 11 20:26:11 swift-proxy proxy-server: STDOUT: ERROR:root:ERROR: swift.cache could not be found in env! (txn: tx61382a02457d47d7b8085-005347d153)"
1209,1306718,nova,b3225581befe14f21b909e176d0a4583b297e031,1,1,,Instances become undelete-able if vif unplugging f...,"If an instance's vifs cannot be deleted (because, for example, they were never plugged in the first place), then compute manager will fail trying to delete the instance:
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 2154, in _shutdown_instance
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     block_device_info)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 956, in destroy
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     destroy_disks)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 992, in cleanup
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     self.unplug_vifs(instance, network_info)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 864, in unplug_vifs
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     self.vif_driver.unplug(instance, vif)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/vif.py"", line 783, in unplug
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     self.unplug_ovs(instance, vif)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/vif.py"", line 667, in unplug_ovs
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     self.unplug_ovs_hybrid(instance, vif)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/vif.py"", line 661, in unplug_ovs_hybrid
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     v2_name)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/network/linux_net.py"", line 1317, in delete_ovs_vif_port
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     _ovs_vsctl(['del-port', bridge, dev])
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/network/linux_net.py"", line 1302, in _ovs_vsctl
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     raise exception.AgentError(method=full_args)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher AgentError: Error during following call to agent: ['ovs-vsctl', '--timeout=120', 'del-port', 'br-int', u'qvo81ce661d-1a']"
1210,1306727,nova,aee892e7af2dbe5a600945f98be8f1915e8ac654,1,1,,versions controller requests with a body log ERROR...,"Using Nova trunk (Juno). I'm seeing the following nova-api.log errors when unauthenticated /versions controller POST requests are made with a request body:
-----
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 ERROR nova.api.openstack.wsgi [-] Exception handling resource: index() got an unexpected keyword argument 'body'
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi Traceback (most recent call last):
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/api/openstack/wsgi.py"", line 983, in _process_stack
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi     action_result = self.dispatch(meth, request, action_args)
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/api/openstack/wsgi.py"", line 1070, in dispatch
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi     return method(req=request, **action_args)
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi TypeError: index() got an unexpected keyword argument 'body'
-----
Both the index() and multi() actions in the versions controller are susceptible to this behavior. Ideally we wouldn't be logging stack traces when this happens."
1211,1306922,nova,4f8185549dfe11eb1ce405711593baa1528045ea,1,1,,Bug #1306922 “binding,"After unshelve an instance, found the attached port's binding info didn't get updated.
os@cloudcontroller:~$ nova show vm2
+--------------------------------------+----------------------------------------------------------+
| Property                             | Value                                                    |
+--------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig                    | AUTO                                                     |
| OS-EXT-AZ:availability_zone          | nova                                                     |
| OS-EXT-SRV-ATTR:host                 | xuhj-c1                                                  |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | xuhj-c1                                                  |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000000d                                        |
| OS-EXT-STS:power_state               | 1                                                        |
| OS-EXT-STS:task_state                | -                                                        |
| OS-EXT-STS:vm_state                  | active                                                   |
| OS-SRV-USG:launched_at               | 2014-04-11T07:15:36.000000                               |
| OS-SRV-USG:terminated_at             | -                                                        |
| accessIPv4                           |                                                          |
| accessIPv6                           |                                                          |
| config_drive                         |                                                          |
| created                              | 2014-04-11T06:51:04Z                                     |
| flavor                               | m1.small (2)                                             |
| hostId                               | 493891ff9d0ff1ab3dddfa99ec9aba8fc30bfb2ccbc34e91f9f5c5b8 |
| id                                   | 6c952df5-8893-47c8-b19d-c3f393c7f688                     |
| image                                | ubuntu-core (0237b107-a492-44ea-817e-d94589943c9c)       |
| key_name                             | -                                                        |
| metadata                             | {}                                                       |
| name                                 | vm2                                                      |
| net1 network                         | 12.0.0.7                                                 |
| os-extended-volumes:volumes_attached | []                                                       |
| progress                             | 0                                                        |
| security_groups                      | default                                                  |
| status                               | ACTIVE                                                   |
| tenant_id                            | fac60a38271e4b3bbc4a7dc97aee4e90                         |
| updated                              | 2014-04-11T07:15:36Z                                     |
| user_id                              | d7f959000a764cccbc0e9c634eb82514                         |
+--------------------------------------+----------------------------------------------------------+
os@cloudcontroller:~$ neutron port-show c514acdd-4038-414f-be3a-d30f76b4cba6
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs |                                                                                 |
| binding:host_id       | xuhj-c2                                                                         |
| binding:profile       | {}                                                                              |
| binding:vif_details   | {""port_filter"": true}                                                           |
| binding:vif_type      | bridge                                                                          |
| binding:vnic_type     | normal                                                                          |
| device_id             | 6c952df5-8893-47c8-b19d-c3f393c7f688                                            |
| device_owner          | compute:nova                                                                    |
| extra_dhcp_opts       |                                                                                 |
| fixed_ips             | {""subnet_id"": ""b4bf03ac-5243-404d-bd16-705b4b61643e"", ""ip_address"": ""12.0.0.7""} |
| id                    | c514acdd-4038-414f-be3a-d30f76b4cba6                                            |
| mac_address           | fa:16:3e:49:9b:1f                                                               |
| name                  |                                                                                 |
| network_id            | cea70fce-b954-40da-b325-b0bea5179ca0                                            |
| security_groups       | 83dac46b-fa06-41e6-a028-35f9b7bf4046                                            |
| status                | BUILD                                                                           |
| tenant_id             | fac60a38271e4b3bbc4a7dc97aee4e90                                                |
+-----------------------+---------------------------------------------------------------------------------+
| OS-EXT-SRV-ATTR:host                 | xuhj-c1                                                  |
-------------
| binding:host_id       | xuhj-c2                                                                         |"
1212,1307025,neutron,e7ca2541b13d9e755f443b866708865ac22a8bd1,0,0,Bug in test,Patch in cisco unit test not being correctly clear...,"A recent patch removed the explicit patch stops from most of the unit tests to let the default mock.patch.stopall handle the cleanup[1]. However, there appears to be cases where stopall loses the reference to a patch and doesn't stop it correctly. In this particular case, the nexus test patch to sys.modules is not being stopped correctly so it's causing unit test failures in the gate[2].
1. https://github.com/openstack/neutron/commit/6546ba570367aba7209ec36544ec4cf742e0bd63
2. http://logs.openstack.org/10/86810/5/check/gate-neutron-python27/89dfa5c/testr_results.html.gz"
1213,1307038,neutron,e7ca2541b13d9e755f443b866708865ac22a8bd1,0,0,Bug in test,linux IP lib tests conflicting patches causing spo...,"The following failure is happening sporadically when running the unit tests:
ft1.11275: neutron.tests.unit.test_linux_ip_lib.TestDeviceExists.test_device_exists_StringException: Empty attachments:
  pythonlogging:''
  pythonlogging:'neutron.api.extensions'
  stderr
  stdout
Traceback (most recent call last):
  File ""neutron/tests/unit/test_linux_ip_lib.py"", line 785, in test_device_exists
    _execute.assert_called_once_with('o', 'link', ('show', 'eth0'))
  File ""/home/jenkins/workspace/gate-neutron-python27/.tox/py27/local/lib/python2.7/site-packages/mock.py"", line 845, in assert_called_once_with
    raise AssertionError(msg)
AssertionError: Expected to be called once. Called 0 times."
1214,1307088,nova,de05c2994212e83eba3cedf2544cdc549d9f8184,1,1,,can't attach a read only volume to an instance,"Description of problem:
An attachment of a read only volume to an instance failed. The openstack was installed as AIO, Cinder was configured with Netapp back end. The following error from the nova compute log:
014-04-13 11:28:17.838 25176 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: Invalid input received: Invalid attaching mode 'rw' for volume 3f5828e1-77b2-4302-9cdf-486f7
0834c31.
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 360, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/exception.py"", line 88, in wrapped
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/exception.py"", line 71, in wrapped
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 244, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     pass
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 230, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 272, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     e, sys.exc_info())
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 259, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3876, in attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     bdm.destroy(context)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3873, in attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return self._attach_volume(context, instance, driver_bdm)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3894, in _attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     self.volume_api.unreserve_volume(context, bdm.volume_id)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3886, in _attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     do_check_attach=False, do_driver_attach=True)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/virt/block_device.py"", line 44, in wrapped
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     ret_val = method(obj, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/virt/block_device.py"", line 251, in attach
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     instance['uuid'], self['mount_device'])
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/volume/cinder.py"", line 174, in wrapper
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     res = method(self, ctx, volume_id, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/volume/cinder.py"", line 263, in attach
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     mountpoint)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/cinderclient/v1/volumes.py"", line 266, in attach
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     'mode': mode})
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/cinderclient/v1/volumes.py"", line 250, in _action
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return self.api.client.post(url, body=body)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/cinderclient/client.py"", line 210, in post
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return self._cs_request(url, 'POST', **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/cinderclient/client.py"", line 174, in _cs_request
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/cinderclient/client.py"", line 157, in request
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     raise exceptions.from_response(resp, body)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher InvalidInput: Invalid input received: Invalid attaching mode 'rw' for volume 3f5828e1-77b2-4302-9cdf-486f70834c31.
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher
2014-04-13 11:28:17.858 25176 ERROR oslo.messaging._drivers.common [-] Returning exception Invalid input received: Invalid attaching mode 'rw' for volume 3f5828e1-77b2-4302-9cdf-486f70834c31. to caller
Version-Release number of selected component (if applicable):
openstack-nova-conductor-2014.1-0.13.b3.el7.noarch
openstack-swift-object-1.12.0-1.el7.noarch
openstack-glance-2014.1-0.4.b3.el7.noarch
openstack-packstack-puppet-2014.1.1-0.7.dev1018.el7.noarch
openstack-nova-cert-2014.1-0.13.b3.el7.noarch
python-django-openstack-auth-1.1.4-1.el7.noarch
openstack-swift-1.12.0-1.el7.noarch
openstack-keystone-2014.1-0.4.b3.el7.noarch
openstack-utils-2013.2-3.p1.el7.noarch
openstack-nova-api-2014.1-0.13.b3.el7.noarch
openstack-nova-compute-2014.1-0.13.b3.el7.noarch
openstack-nova-novncproxy-2014.1-0.13.b3.el7.noarch
openstack-dashboard-2014.1-0.5.b3.el7.noarch
openstack-swift-account-1.12.0-1.el7.noarch
openstack-swift-proxy-1.12.0-1.el7.noarch
openstack-puppet-modules-2014.1-5.3.el7.noarch
openstack-cinder-2014.1-0.6.b3.el7.noarch
openstack-nova-common-2014.1-0.13.b3.el7.noarch
openstack-nova-console-2014.1-0.13.b3.el7.noarch
openstack-nova-network-2014.1-0.13.b3.el7.noarch
openstack-swift-container-1.12.0-1.el7.noarch
openstack-packstack-2014.1.1-0.7.dev1018.el7.noarch
openstack-nova-scheduler-2014.1-0.13.b3.el7.noarch
openstack-swift-plugin-swift3-1.7-3.el7.noarch
How reproducible:
100%
Steps to Reproduce:
1. Create a volume from a qcow2 image.
2. Attach the volume to an instance and make changes.
3. Detach the volume from the instance.
4. Set the volume as read only.
5. Attach the volume to an instance.
Actual results:
the attachment fails.
Expected results:
the attachment should succeed, but the volume cannot be changed."
1215,1307295,neutron,b3e52b6b2462cf80dbdb5fa4785e51b0d147316e,1,1,,duplicate entry exception for vxlan-allocation,"I run multiple neutron-servers using haproxy. Here's the exception thrown by all the neutron-servers when services restart:
2014-04-14 11:42:18.315 6457 ERROR neutron.service [-] Unrecoverable error: please check log for details.
2014-04-14 11:42:18.315 6457 TRACE neutron.service Traceback (most recent call last):
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 103, in serve_wsgi
2014-04-14 11:42:18.315 6457 TRACE neutron.service     service.start()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 72, in start
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self.wsgi_app = _run_wsgi(self.app_name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 117, in _run_wsgi
2014-04-14 11:42:18.315 6457 TRACE neutron.service     app = config.load_paste_app(app_name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/common/config.py"", line 145, in load_paste_app
2014-04-14 11:42:18.315 6457 TRACE neutron.service     app = deploy.loadapp(""config:%s"" % config_path, name=app_name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 247, in loadapp
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return loadobj(APP, uri, name=name, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 272, in loadobj
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return context.create()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 710, in create
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return self.object_type.invoke(self)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-04-14 11:42:18.315 6457 TRACE neutron.service     **context.local_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/util.py"", line 56, in fix_call
2014-04-14 11:42:18.315 6457 TRACE neutron.service     val = callable(*args, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/paste/urlmap.py"", line 25, in urlmap_factory
2014-04-14 11:42:18.315 6457 TRACE neutron.service     app = loader.get_app(app_name, global_conf=global_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-04-14 11:42:18.315 6457 TRACE neutron.service     name=name, global_conf=global_conf).create()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 710, in create
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return self.object_type.invoke(self)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-04-14 11:42:18.315 6457 TRACE neutron.service     **context.local_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/util.py"", line 56, in fix_call
2014-04-14 11:42:18.315 6457 TRACE neutron.service     val = callable(*args, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/auth.py"", line 69, in pipeline_factory
2014-04-14 11:42:18.315 6457 TRACE neutron.service     app = loader.get_app(pipeline[-1])
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-04-14 11:42:18.315 6457 TRACE neutron.service     name=name, global_conf=global_conf).create()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 710, in create
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return self.object_type.invoke(self)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 146, in invoke
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return fix_call(context.object, context.global_conf, **context.local_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/util.py"", line 56, in fix_call
2014-04-14 11:42:18.315 6457 TRACE neutron.service     val = callable(*args, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 71, in factory
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return cls(**local_config)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 75, in __init__
2014-04-14 11:42:18.315 6457 TRACE neutron.service     plugin = manager.NeutronManager.get_plugin()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 211, in get_plugin
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return cls.get_instance().plugin
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 206, in get_instance
2014-04-14 11:42:18.315 6457 TRACE neutron.service     cls._create_instance()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return f(*args, **kwargs)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 200, in _create_instance
2014-04-14 11:42:18.315 6457 TRACE neutron.service     cls._instance = cls()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 112, in __init__
2014-04-14 11:42:18.315 6457 TRACE neutron.service     plugin_provider)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 140, in _get_plugin_instance
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return plugin_class()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 106, in __init__
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self.type_manager.initialize()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/managers.py"", line 74, in initialize
2014-04-14 11:42:18.315 6457 TRACE neutron.service     driver.obj.initialize()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/drivers/type_vxlan.py"", line 81, in initialize
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self._sync_vxlan_allocations()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/drivers/type_vxlan.py"", line 172, in _sync_vxlan_allocations
2014-04-14 11:42:18.315 6457 TRACE neutron.service     session.add(alloc)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py"", line 402, in __exit__
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self.commit()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py"", line 314, in commit
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self._prepare_impl()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py"", line 298, in _prepare_impl
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self.session.flush()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 615, in _wrap
2014-04-14 11:42:18.315 6457 TRACE neutron.service     _raise_if_duplicate_entry_error(e, get_engine().name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 559, in _raise_if_duplicate_entry_error
2014-04-14 11:42:18.315 6457 TRACE neutron.service     raise exception.DBDuplicateEntry(columns, integrity_error)
2014-04-14 11:42:18.315 6457 TRACE neutron.service DBDuplicateEntry: (IntegrityError) (1062, ""Duplicate entry '1' for key 'PRIMARY'"") 'INSERT INTO ml2_vxlan_allocations (vxlan_vni, allocated) VALUES (%s, %s)' ((1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0)  ... displaying 10 of 100000 total bound parameter sets ...  (99999, 0), (100000, 0))
2014-04-14 11:42:18.315 6457 TRACE neutron.service
2014-04-14 11:42:18.331 6457 CRITICAL neutron [-] (IntegrityError) (1062, ""Duplicate entry '1' for key 'PRIMARY'"") 'INSERT INTO ml2_vxlan_allocations (vxlan_vni, allocated) VALUES (%s, %s)' ((1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0)  ... displaying 10 of 100000 total bound parameter sets ...  (99999, 0), (100000, 0))"
1216,1307338,nova,6cd183a4bac7ae79bca380c960686ea33892f066,1,1,,Return incorrect message in keypair-show and keypa...,"Reproduce:
1.nova keypair-list
+----------+-------------------------------------------------+
| Name     | Fingerprint                                     |
+----------+-------------------------------------------------+
| root_key | 41:f3:fc:23:07:1d:99:cc:fd:e4:7a:a3:20:ba:78:25 |
+----------+-------------------------------------------------+
2.nova keypair-show root
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-542fa1da-0ab0-4624-b662-7d7c908508e2)
3.nova keypair-delete root
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-2f8587a3-ee5e-4134-ba5d-a2b3f0968cbc)
expected:
1.nova keypair-show root
ERROR: No keypair with a name or ID of 'root' exists.
2.nova keypair-delete root
ERROR: No keypair with a name or ID of 'root' exists."
1217,1307408,nova,aadfffaaa69c6a8939f6c88cfbaeb026c7604163,1,1,,VMWare - Destroy fails when Claim is not successfu...,"If Claim is not successful, compute manager triggers a call to destroy instance.
Destroy fails since the compute node (cluster) is set only after claim is successful.
This issue occurs when multiple parallel nova boot operations are triggered simultaneously.
Snippet from nova-compute.log
2014-04-06 22:48:52.454 [00;32mDEBUG nova.compute.utils [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[00;32m] [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00;32mInsufficient compute resources: Free memory 975.00 MB < requested 2000 MB.[00m [00;33mfrom (pid=9041) notify_about_instance_usage /opt/stack/nova/nova/compute/utils.py:336[00m
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00mTraceback (most recent call last):
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1289, in _build_instance
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m    with rt.instance_claim(context, instance, limits):
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m    return f(*args, **kwargs)
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 122, in instance_claim
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m    overhead=overhead, limits=limits)
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m  File ""/opt/stack/nova/nova/compute/claims.py"", line 95, in __init__
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m    self._claim_test(resources, limits)
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m  File ""/opt/stack/nova/nova/compute/claims.py"", line 148, in _claim_test
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m    ""; "".join(reasons))
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00mComputeResourcesUnavailable: Insufficient compute resources: Free memory 975.00 MB < requested 2000 MB.
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m
2014-04-06 22:48:52.455 [00;32mDEBUG nova.compute.manager [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[00;32m] [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00;32mClean up resource before rescheduling.[00m [00;33mfrom (pid=9041) _reschedule_or_error /opt/stack/nova/nova/compute/manager.py:1401[00m
2014-04-06 22:48:52.455 [01;36mAUDIT nova.compute.manager [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[01;36m] [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [01;36mTerminating instance[00m
2014-04-06 22:48:52.544 [00;32mDEBUG nova.network.api [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[00;32m] [01;35m[00;32mUpdating cache with info: [][00m [00;33mfrom (pid=9041) update_instance_cache_with_nw_info /opt/stack/nova/nova/network/api.py:74[00m
2014-04-06 22:48:52.555 [00;32mDEBUG nova.objects.instance [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[00;32m] [01;35m[00;32mLazy-loading `system_metadata' on Instance uuid b22186ec-9f05-4f7d-a0d6-2276baeb6572[00m [00;33mfrom (pid=9041) obj_load_attr /opt/stack/nova/nova/objects/instance.py:519[00m
2014-04-06 22:48:52.563 [00;32mDEBUG nova.compute.manager [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[00;32m] [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00;32mDeallocating network for instance[00m [00;33mfrom (pid=9041) _deallocate_network /opt/stack/nova/nova/compute/manager.py:1784[00m
2014-04-06 22:48:52.593 [01;31mERROR nova.compute.manager [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[01;31m] [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [01;31mError: Insufficient compute resources: Free memory 975.00 MB < requested 2000 MB.[00m
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mTraceback (most recent call last):
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1289, in _build_instance
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    with rt.instance_claim(context, instance, limits):
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    return f(*args, **kwargs)
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 122, in instance_claim
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    overhead=overhead, limits=limits)
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/claims.py"", line 95, in __init__
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    self._claim_test(resources, limits)
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/claims.py"", line 148, in _claim_test
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    ""; "".join(reasons))
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mComputeResourcesUnavailable: Insufficient compute resources: Free memory 975.00 MB < requested 2000 MB.
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m
2014-04-06 22:48:52.664 [00;32mDEBUG nova.compute.utils [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[00;32m] [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00;32mThe resource None does not exist[00m [00;33mfrom (pid=9041) notify_about_instance_usage /opt/stack/nova/nova/compute/utils.py:336[00m
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mTraceback (most recent call last):
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1202, in _run_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    instance, image_meta, legacy_bdm_in_spec)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1366, in _build_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    filter_properties, bdms, legacy_bdm_in_spec)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1412, in _reschedule_or_error
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    self._log_original_error(exc_info, instance_uuid)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    six.reraise(self.type_, self.value, self.tb)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1407, in _reschedule_or_error
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    bdms, requested_networks)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 2136, in _shutdown_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    requested_networks)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    six.reraise(self.type_, self.value, self.tb)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 2126, in _shutdown_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    block_device_info)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 656, in destroy
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    _vmops = self._get_vmops_for_compute_node(instance['node'])
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 544, in _get_vmops_for_compute_node
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    resource = self._get_resource_for_node(nodename)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 536, in _get_resource_for_node
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    raise exception.NotFound(msg)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mNotFound: The resource None does not exist"
1218,1307416,nova,c58f2393b1c88268179205e349a2638f481bd492,1,1,,Unshelve instance needs handling exceptions,"There are some error cases not handled when we unshelve an instance in the conductor.
   nova/conductor/manager.py#823
if the key 'shelved_image_id' is not defined the current code will raise an KeyError not handled.
Also when the 'shelved_image_id' is set to None (which is not the expected behavior), the error is not correctly handled and the message could be confusing."
1219,1307472,neutron,53e784d27eafe848593467d4d598305f34030848,0,0,"Obsolete code “Remove workaround for bug #1219530""",workaround for bug #1219530 is obsolete,Bug #1219530 has been fixed and workaround for it stands in a way of putting together support for rootwrap daemon mode.
1220,1307560,cinder,0814bff9366974d77d42d03fa11f60fbe469a8c7,1,0,"“LoopingCall has been renamed to FixedIntervalLoopingCall in https://review.openstack.org/#/c/26345/""",Switch to newer naming in oslo loopingcall,LoopingCall has been renamed to FixedIntervalLoopingCall in https://review.openstack.org/#/c/26345/
1221,1307582,nova,3a915546507e9a5617ada90546342a9239735ff8,1,1,""" This reverts commit 79ab96e34ba5b8dd3e4e542dd3a7f65624b13367.”",FixedIp object does not support Neutron,"We recently landed a patch to make the metadata API's base.py use the FixedIP object for address lookups.
This causes problems when using the metadata API with neutron because the Nova FixedIP object class does not yet appear to support neutron.
When using the metadata server today (with Neutron) you'll see the following:
Apr 14 15:36:40 localhost nova-api[3373]: 2014-04-14 15:36:40.035 3505 ERROR nova.api.metadata.handler [-] Failed to get metadata for ip: 172.19.0.5"
1222,1307662,nova,f9dfb5b25c087b5146e0c6a9956b2402aec6458f,0,0,refactoring “We should remove this unused argument.”,Bug #1307662 “xenapi,"The downloading code in xenserver requires an instance argument.  However in practice this argument is not used for debugging or any other purpose.  Downloading in fact can occur without an instance being involved at all.  We should remove this unused argument.
This is in preparation to separate out the download code and leverage it for precaching blueprint work.  This should be treated as a bug on it's own as it's a code refactor/cleanup work."
1224,1308058,cinder,993facb3bf753d23332236dc05b397850bf99144,1,1,"""https://github.com/openstack/cinder/commit/da13c6285bb0aee55cfbc93f55ce2e2b7d6a28f2 - this patch removes the default of None from the getattr call.”",Cannot create volume from glance image without che...,"It is no longer possible to create a volume from an image that does not have a checksum set.
https://github.com/openstack/cinder/commit/da13c6285bb0aee55cfbc93f55ce2e2b7d6a28f2 - this patch removes the default of None from the getattr call.
If this is intended it would be nice to see something more informative in the logs.
2014-04-15 11:52:26.035 19000 ERROR cinder.api.middleware.fault [req-cf0f7b89-a9c1-4a10-b1ac-ddf415a28f24 c139cd16ac474d2184237ba837a04141 83d5198d5f5a461798c6b843f57540d
f - - -] Caught error: checksum
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/api/middleware/fault.py"", line 75, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return req.get_response(self.application)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     application, catch_exc_info=False)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     app_iter = application(self.environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 615, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return self.app(env, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     response = self.app(environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     resp = self.call_func(req, *args, **self.kwargs)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return self.func(req, *args, **kwargs)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/api/openstack/wsgi.py"", line 895, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     content_type, body, accept)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/api/openstack/wsgi.py"", line 943, in _process_stack
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     action_result = self.dispatch(meth, request, action_args)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/api/openstack/wsgi.py"", line 1019, in dispatch
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return method(req=request, **action_args)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/api/v2/volumes.py"", line 346, in create
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     **kwargs)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/volume/api.py"", line 189, in create
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     flow_engine.run()
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/lock_utils.py"", line 54, in wrapper
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 96, in run
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     self._run()
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 111, in _ru
n
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     misc.Failure.reraise_if_any(failures.values())
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 649, in reraise_if_any
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     failures[0].reraise()
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 656, in reraise
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     six.reraise(*self._exc_info)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 34, in _e
xecute_task
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     result = task.execute(**arguments)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/volume/flows/api/create_volume.py"", line 341, in execute
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     self._check_image_metadata(context, image_id, size)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/volume/flows/api/create_volume.py"", line 180, in _check_image_metadata
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     image_meta = self.image_service.show(context, image_id)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/image/glance.py"", line 228, in show
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     base_image_meta = self._translate_from_glance(image)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/image/glance.py"", line 336, in _translate_from_glance
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     image_meta = _extract_attributes(image)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/image/glance.py"", line 434, in _extract_attributes
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     output[attr] = getattr(image, attr)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/python-glanceclient/glanceclient/openstack/common/apiclient/base.py"", line 462, in __getattr__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return self.__getattr__(k)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/python-glanceclient/glanceclient/openstack/common/apiclient/base.py"", line 464, in __getattr__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     raise AttributeError(k)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault AttributeError: checksum
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault
2014-04-15 11:52:26.037 19000 INFO cinder.api.middleware.fault [req-cf0f7b89-a9c1-4a10-b1ac-ddf415a28f24 c139cd16ac474d2184237ba837a04141 83d5198d5f5a461798c6b843f57540df - - -] http://162.13.156.43:8776/v2/83d5198d5f5a461798c6b843f57540df/volumes returned with HTTP 500"
1225,1308253,cinder,aabb0fa1f68974e2fa4828cde1462dc47c429655,1,1,,Reset state doesn't update migration status,"In the case of a failed migration the volume's status and the migration status are both set to error state.  We have the ability to use reset-state to get the volume back to a usable state, however the migration-status isn't updated which makes it impossible to do things like delete the volume or perhaps retry the migration.
We should have the reset-state command clean up outliers like this as well."
1226,1308342,nova,222d44532c65ddf3f26532ced217890628352536,1,1,,Cannot delete vm instance if send duplicate delete...,"I deployed openstack with icehouse rc1 and booted 100 vms on my nodes. After my testing, i  tried to delete my vms at the same time. Then i fount all of my vms` status change to deleting but cannot be deleted. I checked my openstack, the rabbitmq-server crashed . Then i restart rabbitmq-server and my openstack nova services, sended the delete requests again and again, the vms still cannot be deleted. While , in havana, the vms can be deleted if received duplicate delete requests .
I think icehouse should handle duplicate delete requests like havana ."
1227,1308418,nova,fe02cc830f9c9e1dac234164bc1f0caa0e2072d7,1,1,,nova-spiceproxy ignores spicehtml5proxy_host setti...,"Hello Stackers!
I'm trying SPICE Consoles and it is working smoothly! But, I detected a problem, it is ignoring the entry ""spicehtml5proxy_host"" in nova.conf for Dual-Stacked environment (IPv4 / IPv6).
So, it isn't listening on ""::"".
The following setup doesn't work....
---
[spice]
enabled = True
spicehtml5proxy_host = ::
html5proxy_base_url = http://controller.yourdomain.com:6082/spice_auto.html
keymap = en-us
---
Unless I patch the following file:
/usr/lib/python2.7/dist-packages/nova/cmd/spicehtml5proxy.py into this:
---
opts = [
    cfg.StrOpt('spicehtml5proxy_host',
               default='::',
               help='Host on which to listen for incoming requests'),
---
As you guys can see, I replaced the default ""0.0.0.0"" to ""::"" and now SPICE Proxy listens on both IPv4 and IPv6! But, this is a hack...
I don't know why it is ignoring the ""spicehtml5proxy_host = ::"" entry at nova.conf.
BTW, the ""novncproxy_host = ::"" works as expected for NoVNC but, I'm disabling VNC from my cloud in favor of SPICE.
Cheers!
Thiago"
1228,1308421,neutron,30e5dae520535fbd759f4c12e184970afef854de,1,1,copy & paste error,local variable 'physical_network' referenced befor...,"In my case, triggered by running neutron-usage-audit. Fix underway.
Traceback (most recent call last):
  File ""~/bin/neutron-usage-audit"", line 10, in <module>
    sys.exit(main())
  File ""~/neutron/cmd/usage_audit.py"", line 37, in main
    plugin = manager.NeutronManager.get_plugin()
  File ""~/neutron/manager.py"", line 211, in get_plugin
    return cls.get_instance().plugin
  File ""~/neutron/manager.py"", line 206, in get_instance
    cls._create_instance()
  File ""~/neutron/openstack/common/lockutils.py"", line 249, in inner
    return f(*args, **kwargs)
  File ""~/neutron/manager.py"", line 200, in _create_instance
    cls._instance = cls()
  File ""~/neutron/manager.py"", line 112, in __init__
    plugin_provider)
  File ""~/neutron/manager.py"", line 140, in _get_plugin_instance
    return plugin_class()
  File ""~/neutron/plugins/linuxbridge/lb_neutron_plugin.py"", line 263, in __init__
    db.sync_network_states(self.network_vlan_ranges)
  File ""~/neutron/plugins/linuxbridge/db/l2network_db_v2.py"", line 87, in sync_network_states
    'physical_network': physical_network})
UnboundLocalError: local variable 'physical_network' referenced before assignment"
1229,1308544,nova,476dc5efcdb11c9859b062dc69a8c205e69be861,1,1,,Bug #1308544 “libvirt,"If an instance fails during its network creation (for example if the network-vif-plugged event doesn't arrive in time) a subsequent delete will also fail when it tries to delete the vif, leaving the instance in a Error(deleting) state.
This can be avoided by including the ""--if-exists"" option to the ovs=vsctl command.
Example of stack trace:
 2014-04-16 12:28:51.949 AUDIT nova.compute.manager [req-af72c100-5d9b-44f6-b941-3d72529b3401 demo demo] [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] Terminating instance
2014-04-16 12:28:52.309 ERROR nova.virt.libvirt.driver [-] [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] During wait destroy, instance disappeared.
2014-04-16 12:28:52.407 ERROR nova.network.linux_net [req-af72c100-5d9b-44f6-b941-3d72529b3401 demo demo] Unable to execute ['ovs-vsctl', '--timeout=120', 'del-port', 'br-int', u'qvo67a96e96-10']. Exception: Unexpected error while running command.
Command: sudo nova-rootwrap /etc/nova/rootwrap.conf ovs-vsctl --timeout=120 del-port br-int qvo67a96e96-10
Exit code: 1
Stdout: ''
Stderr: 'ovs-vsctl: no port named qvo67a96e96-10\n'
2014-04-16 12:28:52.573 ERROR nova.compute.manager [req-af72c100-5d9b-44f6-b941-3d72529b3401 demo demo] [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] Setting instance vm_state to ERROR
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] Traceback (most recent call last):
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/compute/manager.py"", line 2261, in do_terminate_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self._delete_instance(context, instance, bdms, quotas)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/hooks.py"", line 103, in inner
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     rv = f(*args, **kwargs)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/compute/manager.py"", line 2231, in _delete_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     quotas.rollback()
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     six.reraise(self.type_, self.value, self.tb)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/compute/manager.py"", line 2203, in _delete_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self._shutdown_instance(context, db_inst, bdms)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/compute/manager.py"", line 2145, in _shutdown_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     requested_networks)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     six.reraise(self.type_, self.value, self.tb)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/compute/manager.py"", line 2135, in _shutdown_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     block_device_info)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/driver.py"", line 955, in destroy
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     destroy_disks)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/driver.py"", line 991, in cleanup
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self.unplug_vifs(instance, network_info)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/driver.py"", line 863, in unplug_vifs
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self.vif_driver.unplug(instance, vif)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/vif.py"", line 783, in unplug
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self.unplug_ovs(instance, vif)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/vif.py"", line 667, in unplug_ovs
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self.unplug_ovs_hybrid(instance, vif)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/vif.py"", line 661, in unplug_ovs_hybrid
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     v2_name)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/network/linux_net.py"", line 1318, in delete_ovs_vif_port
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     _ovs_vsctl(['del-port', bridge, dev])
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/network/linux_net.py"", line 1302, in _ovs_vsctl
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     raise exception.AgentError(method=full_args)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] AgentError: Error during following call to agent: ['ovs-vsctl', '--timeout=120', 'del-port', 'br-int', u'qvo67a96e96-10']"
1230,1308565,nova,98e6891dfd4408c56644f55fe3cff88703beb4bf,1,1,,Delete does not clean up pending resize files,"If an instance is deleted while a resize operation is in progress (i.e. before the VM state is RESIZED), the temporary files created during the resize operation (e.g. <instance_id>_resize with libvirt) are not cleaned up.  This would seem to be related to bug 1285000, except in that case the temporary files are on a different node.  Ideally a fix would address both."
1231,1308649,nova,4ab7b6b7ad8b6047ec108c8f98bb1c0f310b9899,1,1,,Attaching Nova volumes fails if open-iscsi daemon ...,"nova-compute traces on first volume attach if open-iscsi is not running:
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] Traceback (most recent call last):
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/compute/manager.py"", line 4142, in _attach_volume
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     do_check_attach=False, do_driver_attach=True)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/block_device.py"", line 44, in wrapped
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     ret_val = method(obj, context, *args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/block_device.py"", line 248, in attach
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     connector)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     six.reraise(self.type_, self.value, self.tb)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/block_device.py"", line 239, in attach
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     device_type=self['device_type'], encryption=encryption)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1224, in attach_volume
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     disk_info)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1183, in volume_driver_method
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     return method(connection_info, *args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 249, in inner
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     return f(*args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/contextlib.py"", line 34, in __exit__
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     self.gen.throw(type, value, traceback)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 212, in lock
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     yield sem
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 249, in inner
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     return f(*args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/libvirt/volume.py"", line 285, in connect_volume
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     for ip, iqn in self._get_target_portals_from_iscsiadm_output(out):
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] ValueError: too many values to unpack
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]
Thats because it runs ""iscsiadm -m discovery -t sendtargets -p"", which prints upon first run:
Loading iscsi modules:                                                                                   done
Starting iSCSI initiator service:                                                                        done
Setting up iSCSI targets:                                                                                unused
192.168.204.82:3260,1 iqn.2010-10.org.openstack:volume-f9b12623-6ce3-4dac-a71f-09ad4249bdd3
192.168.204.82:3260,1 iqn.2010-10.org.openstack:volume-f9b12623-6ce3-4dac-a71f-09ad4249bdd3
aka there is some garbage before the actual output that it looks for. this is due to /etc/iscsid.conf having configured to start the daemon:
from /etc/iscsid.conf:
# Default for upstream open-iscsi scripts (uncomment to activate).
iscsid.startup = /usr/sbin/rcopen-iscsi start"
1232,1308675,neutron,dd2ccd12adfb6c7d4aa500ba4120f767beeed35c,1,1,,netaddr raises ValueError in 0.7.10 and AddrFormat...,netaddr raises ValueError in 0.7.10 and AddrFormatError in 0.7.11
1233,1308715,nova,da1fa5dc93e6e64d397e8d00769e34828669e010,1,1,,Deadlock on quota_usages,"We are getting deadlocks for concurrent quota reservations that we did not see in grizzly:
see https://bugs.launchpad.net/nova/+bug/1283987
The deadlock handling needs to be fixed as per above, but we shouldn't be deadlocking, here. It seems this is due to bad indexes in the database:
mysql> show index from quota_usages;
+--------------+------------+---------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
| Table        | Non_unique | Key_name                        | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |
+--------------+------------+---------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
| quota_usages |          0 | PRIMARY                         |            1 | id          | A         |           8 |     NULL | NULL   |      | BTREE      |         |               |
| quota_usages |          1 | ix_quota_usages_project_id      |            1 | project_id  | A         |           8 |     NULL | NULL   | YES  | BTREE      |         |               |
| quota_usages |          1 | ix_quota_usages_user_id_deleted |            1 | user_id     | A         |           8 |     NULL | NULL   | YES  | BTREE      |         |               |
| quota_usages |          1 | ix_quota_usages_user_id_deleted |            2 | deleted     | A         |           8 |     NULL | NULL   | YES  | BTREE      |         |               |
+--------------+------------+---------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
4 rows in set (0.01 sec)
mysql> explain select * from quota_usages where project_id='foo' and user_id='bar' and deleted=0;
+----+-------------+--------------+------+------------------------------------------------------------+----------------------------+---------+-------+------+------------------------------------+
| id | select_type | table        | type | possible_keys                                              | key                        | key_len | ref   | rows | Extra                              |
+----+-------------+--------------+------+------------------------------------------------------------+----------------------------+---------+-------+------+------------------------------------+
|  1 | SIMPLE      | quota_usages | ref  | ix_quota_usages_project_id,ix_quota_usages_user_id_deleted | ix_quota_usages_project_id | 768     | const |    1 | Using index condition; Using where |
+----+-------------+--------------+------+------------------------------------------------------------+----------------------------+---------+-------+------+------------------------------------+
1 row in set (0.00 sec)
We should have an index on project_id/deleted and project_id/user_id/deleted instead of the current values."
1234,1308811,nova,5427630f735381569066815605c75bccd278def8,1,1,,nova.objects.base imports conductor wrong,"in nova.objects.base it imports conductor
from nova.conductor import api as conductor_api
self._conductor = conductor_api.API()
This bypasses the logic to detemin whether to use conductor RPC service or not.
Should do
from nova import conductor
self._conductor = conductor.API()"
1235,1308819,cinder,3305f9dd061e70736fc4b890a207ecd4d30c3544,1,1,,The status change to retyping when cinder return e...,"Do the below test:
1. Set the maximum number of volumes to 10 for user admin
2. Create type-1 on IBM-svc
3. Create type-7 on IBM-v7k
4. Create 10 volumes on type-1
5. Retype the volume from type-1 to type-7
The cinder will return error:
stack@ubuntu1310:/etc/cinder$ cinder retype --migration-policy on-demand 82cbafa7-949f-4c90-bde9-170507baa32f type-7
ERROR: VolumeLimitExceeded: Maximum number of volumes allowed (12) exceeded (HTTP 413) (Request-ID: req-15e2c0bb-e493-4ceb-a3f1-59d3c5282eff)
6. Check the volume status after that, it was found the status of the volume change to retyping , and keep on retyping after that.
|                  ID                  |   Status  |          Name         | Size | Volume Type | Bootable |
| 82cbafa7-949f-4c90-bde9-170507baa32f |  retyping |          111          |  1   |    type-1   |  false   |
It may because when retyping for migrate, it will have a template volume creation.
If the volumes has reached the maximum number, it can't create the template volume.
But if the retype command failed,   the status for the volume should not change to retyping.
It is better keeping on available."
1236,1308839,nova,aa9383081230b92ecc7c1b176cb3eb62a237949c,1,1,,ProcessExecutionError exception is not defined in ...,"ubuntu@devstack-master:/opt/stack/nova$ grep -r exception.ProcessExecutionError *
nova/virt/libvirt/volume.py:        except exception.ProcessExecutionError as exc:
commit 5e016846708ef62c92dcf607f03c67c36ce5c23f has been fixed all other wrong used places, but this one is added after this change.
[Impact]
Error doesn't exist, if error encountered then wrong error reported.
[Test Case]
grep -r exception.ProcessExecutionError /usr/lib/python2.7/dist-packages/nova/*
[Regression Potential]
Minimal. This change is in an error path already. When error path is encountered, the exception doesn't exist and causes a different error to occur but calling code treats both the same way. This ensures the right error is thrown."
1237,1308927,neutron,9f673c2482528bdde15776c52928dbaceecdf811,1,1,,Bug #1308927 “ofagent,"the flow _provision_local_vlan_inbound_for_tunnel installs needs push_vlan.
while the current code happens to work with older versions of OVS,
the latest OVS correctly rejects the flow."
1238,1309043,nova,1722378c85dbc9dc2506a2f8a84fef7a27254f57,0,0,Bug in test,NetworkCommandsTestCase unit test failing,"Change-Id I663bd06eb50872f16fc9889dde917277739fefce introduced a race condition where if another test doesn't properly reset the _IS_NEUTRON flag, it will fail because it will think that it is using Neutron and error out."
1239,1309144,neutron,966dbb5368d710e2652b18f5b9cb4295f58598da,1,1,,TypeError while building routers,"This stacktrace:
2014-04-17 09:37:06.460 10212 ERROR neutron.openstack.common.loopingcall [req-512e4936-b85a-4019-bc2c-e1b8faf4c8c5 None] in dynamic looping call
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall Traceback (most recent call last):
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall   File ""/opt/stack/neutron/neutron/openstack/common/loopingcall.py"", line 123, in _inner
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall     idle = self.f(*self.args, **self.kw)
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall   File ""/opt/stack/neutron/neutron/plugins/vmware/common/sync.py"", line 647, in _synchronize_state
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall     scan_missing=scan_missing)
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall   File ""/opt/stack/neutron/neutron/plugins/vmware/common/sync.py"", line 401, in _synchronize_lrouters
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall     filters=filters)
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 195, in _get_collection
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall     items = [dict_func(c, fields) for c in query]
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall   File ""/opt/stack/neutron/neutron/db/l3_db.py"", line 106, in _make_router_dict
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall     nw_id = router.gw_port['network_id']
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall TypeError: 'NoneType' object has no attribute '__getitem__'
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall
has been observed during this run:
http://208.91.1.172/logs/neutron/82709/9/412384/"
1240,1309154,nova,94a3b83f9f1fd52a78b9d49b32ddfae40182f852,1,1,,Detach Volume Ordering Has Bad User Experience,"Currently when detaching a volume we do:
1. driver.detach() -> detaches the volume in the hypervisor
2. cinder.detach() -> marks the volume detached in cinder
3. bdm.delete() -> stops the nova side from reporting an attached volume.
This leads to bad UX for two reasons:
a. If the cinder detach fails, the bdm still exists so nova reports a volume attached even when the hypervisor
    has detached it.
b. There is a window where cinder reports the volume available but nova still thinks the attachment exists
I propose we reverse the order of 2. and 3.
This reverses a. so that a detach fail will show the volume as attached in cinder but nova will not show it. The nova side more accurately reflects what nova knows about.
This reverses b. so that there is a window where nova has removed the attachment but cinder still reports it in use. This is a fairly minor difference, but leads to a nicer experience when you are detaching and re-attaching the same volume because it can be reattached once it becomes available.
This also may help with https://bugs.launchpad.net/nova/+bug/1172695"
1241,1309195,neutron,d23bc8fa6e2d8a735a2aa75224b1bc96a3b992f5,1,1,,[OSSA 2014-019] IPv6 prefix shouldn't be added in ...,"SNAT rules with IPv6 prefixes are added into the NAT table, which causes failure with the call to iptables-restore:
Stderr: ""iptables-restore v1.4.18: invalid mask `64' specified\nError occurred at line: 22\nTry `iptables-restore -h' or 'iptables-restore --help' for more information.\n"""
1242,1309239,nova,5aa29b6b0a34d603e882a80cdc8e936f983b1892,0,0,feature “we should always load the info_cache so we can avoid the extra query”,_heal_instance_info_cache should info info_cache a...,The _heal_instance_info cache() is used to heal the cache access instance['info_cache'] so we should always load the info_cache so we can avoid the extra query to get it when accessed.
1243,1309286,neutron,c1827a074654d2e8d07e93945fa605ad3b6c18fb,1,1,,Bug #1309286 “OFAgent,"Port the patch for the following bug of OVS to OFAgent.
https://bugs.launchpad.net/nova/+bug/1240849"
1244,1310135,nova,6e1fd7b9b51498af5f688a41d6636ff90b9a56f5,1,1,,Stopping an instance via the Nova API when using t...,"When using the Ironic Nova driver, a stopped server is still presented as Running even when the server is stopped. Checking via the Ironic API correctly shows the instance as powered down:
stack@ironic:~/logs/screen$ nova list
+--------------------------------------+---------+--------+------------+-------------+-------------------+
| ID                                   | Name    | Status | Task State | Power State | Networks          |
+--------------------------------------+---------+--------+------------+-------------+-------------------+
| 5b43d631-91e1-4384-9b87-93283b3ae958 | testing | ACTIVE | -          | Running     | private=10.1.0.10 |
+--------------------------------------+---------+--------+------------+-------------+-------------------+
stack@ironic:~/logs/screen$ nova stop 5b43d631-91e1-4384-9b87-93283b3ae958
stack@ironic:~/logs/screen$ nova list
+--------------------------------------+---------+---------+------------+-------------+-------------------+
| ID                                   | Name    | Status  | Task State | Power State | Networks          |
+--------------------------------------+---------+---------+------------+-------------+-------------------+
| 5b43d631-91e1-4384-9b87-93283b3ae958 | testing | SHUTOFF | -          | Running     | private=10.1.0.10 |
+--------------------------------------+---------+---------+------------+-------------+-------------------+
stack@ironic:~/logs/screen$ ping 10.1.0.10
PING 10.1.0.10 (10.1.0.10) 56(84) bytes of data.
From 172.24.4.2 icmp_seq=1 Destination Host Unreachable
From 172.24.4.2 icmp_seq=5 Destination Host Unreachable
From 172.24.4.2 icmp_seq=6 Destination Host Unreachable
From 172.24.4.2 icmp_seq=7 Destination Host Unreachable
From 172.24.4.2 icmp_seq=8 Destination Host Unreachable
--- 10.1.0.10 ping statistics ---
9 packets transmitted, 0 received, +5 errors, 100% packet loss, time 8000ms
stack@ironic:~/logs/screen$ ironic node-list
+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+
| UUID                                 | Instance UUID                        | Power State | Provisioning State | Maintenance |
+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+
| 91e81c38-4dce-412b-8a1b-a914d28943e4 | 5b43d631-91e1-4384-9b87-93283b3ae958 | power off   | active             | False       |
+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+"
1245,1310460,neutron,7811c8a8ffce78dc14815203858854c450f1c8f6,1,1,,wrong key “router.interface,"In my testbed, ceilometer throws an exception which is caused by the wrong key format of neutron:
2014-04-01 10:52:39.135 6104 ERROR notification [-] Insert msg to Mongodb occured error:Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/ceilometer/billing/backend.py"", line 74, in insert_data
    self.db[collection].insert(data, safe=True)
  File ""/usr/lib64/python2.6/site-packages/pymongo/collection.py"", line 357, in insert
    continue_on_error, self.__uuid_subtype), safe)
InvalidDocument: key 'router.interface' must not contain '.'
It seems that mongodb doesn't support ""."" in key.
The workaround is to change ""."" with ""_"" in l3_db.py."
1246,1310559,cinder,a221a30b32aa94077f352719b2637b51c1809069,1,1,,Storwize/SVC driver detach volume failed,"The case on our system where create two hosts, one for fc and the other is iscsi
larry@larry-ubuntu:~$ ssh superuser@9.115.247.251  'lshost'
superuser@9.115.247.251's password:
id name                   port_count iogrp_count status
...
7  OpenstackUbun-28670410 1          4           offline
...
9  OpenstackUbun-20533817 2          4           degraded
larry@larry-ubuntu:~/Desktop$ less -R screen-c-vol.2014-04-16-162932.log
larry@larry-ubuntu:~/Desktop$ ssh superuser@9.115.247.251  'lsvdiskhostmap volume-d86758fb-190a-44b9-9442-845c8af93d16'
superuser@9.115.247.251's password:
id  name                                        SCSI_id host_id host_name              vdisk_UID                        IO_group_id IO_group_name
162 volume-d86758fb-190a-44b9-9442-845c8af93d16 0       7       OpenstackUbun-28670410 60050760008F03000C00000000000104 0           io_grp0
2014-04-18 13:55:45.683 ERROR cinder.volume.drivers.ibm.storwize_svc.ssh [req-e31cb4b8-cd98-4415-a8cf-ca525e416524 cbf3ff0436ed4c13a9da2fb2eb4e9277 36e2e0918ba54edaa4ce561926870bca] CLI Exception output:
 command: ['svctask', 'rmvdiskhostmap', '-host', '""OpenstackUbun-20533817""', u'volume-d86758fb-190a-44b9-9442-845c8af93d16']
 stdout:
 stderr: CMMVC5842E The action failed because an object that was specified in the command does not exist.
when  detach the volume, find the host logic error."
1247,1310659,cinder,dcec1b84f4d3462fc408f4152f4c89b8df79d629,1,1,,NetApp Eseries fails to attach lun if lun is alrea...,"In some situations it may be possible for the lun on an eseries controller to be mapped, but for cinder to not know about it.  In this situation, if you then attempt certain operations in cinder (such as upload-to-image or create volume from image), cinder will attempt to attach the lun to the cinder node.  This will fail because the eseries controller returns back an error that the lun is already mapped.
2014-04-18 12:30:04.424 DEBUG cinder.volume.drivers.netapp.eseries.client [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Invoking rest with method: POST, path: /storage-syste
ms/{system-id}/volume-mappings, data: {'mappableObjectId': u'0200000060080E500023BB340000C7C65350FAAF', 'targetId': u'8400000060080E500023C734003024D55350F8AA', 'lun': 1}, use_system: True, timeout: None, verify: False, kwargs: {}. from
 (pid=30195) _invoke /opt/stack/cinder/cinder/volume/drivers/netapp/eseries/client.py:123
2014-04-18 12:30:04.426 DEBUG urllib3.connectionpool [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Setting read timeout to None from (pid=30195) _make_request /usr/lib/pytho
n2.7/dist-packages/urllib3/connectionpool.py:375
2014-04-18 12:30:04.809 DEBUG urllib3.connectionpool [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] ""POST /devmgr/v2/storage-systems/f79b215b-b502-43b7-800c-9b6a08c7086b/volu
me-mappings HTTP/1.1"" 422 None from (pid=30195) _make_request /usr/lib/python2.7/dist-packages/urllib3/connectionpool.py:415
2014-04-18 12:30:04.812 ERROR cinder.volume.driver [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Unable to fetch connection information from backend: Response error - {
  ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""retcode"" : ""105"",
  ""codeType"" : ""symbol""
}.
2014-04-18 12:30:04.812 DEBUG cinder.volume.driver [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Cleaning up failed connect initialization. from (pid=30195) _attach_volume /
opt/stack/cinder/cinder/volume/driver.py:399
2014-04-18 12:30:04.862 ERROR oslo.messaging.rpc.dispatcher [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Exception during message handling: Bad or unexpected response from
the storage volume backend API: Unable to fetch connection information from backend: Response error - {
  ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""retcode"" : ""105"",
  ""codeType"" : ""symbol""
}.
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 719, in copy_volume_to_image
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     payload['message'] = unicode(error)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 713, in copy_volume_to_image
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     image_meta)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/driver.py"", line 355, in copy_volume_to_image
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     attach_info = self._attach_volume(context, volume, properties)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/driver.py"", line 406, in _attach_volume
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     raise exception.VolumeBackendAPIException(data=err_msg)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: Response error - {
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   ""retcode"" : ""105"",
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   ""codeType"" : ""symbol""
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher }.
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher
2014-04-18 12:30:04.865 ERROR oslo.messaging._drivers.common [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Returning exception Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: Response error - {
  ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""retcode"" : ""105"",
  ""codeType"" : ""symbol""
}. to caller"
1248,1310817,nova,f89d13b141eba66487b3d858cd075a47b2de6016,1,1,,Bug #1310817 “VMware,"The VMware Minesweeper CI is occasionally seeing an error when deleting snapshots. The error is:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 280, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 266, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 309, in decorated_function
    e, sys.exc_info())
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 296, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 2692, in backup_instance
    task_states.IMAGE_BACKUP)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 2758, in _snapshot_instance
    update_task_state)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 645, in snapshot
    _vmops.snapshot(context, instance, name, update_task_state)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 873, in snapshot
    self._delete_vm_snapshot(instance, vm_ref, snapshot)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 780, in _delete_vm_snapshot
    self._session._wait_for_task(delete_snapshot_task)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 948, in _wait_for_task
    ret_val = done.wait()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
    return hubs.get_hub().switch()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
    return self.greenlet.switch()
VMwareDriverException: A general system error occurred: concurrent access
Full logs for an affected run can be found here: http://10.148.255.241/logs/85961/2"
1249,1310857,neutron,d6f014d0922e03864fd72efbcde04322711c2510,1,0,“devstack currently has a bug re”,agent attempts to create firewall even if fwaas di...,"Investigating some tempest failures in this area led me to this issue.  devstack currently has a bug re: configuration of fwaas.  This leads the service to be enabled, but the agent does get passed the relevant config files /w fwaas config.  On firewall creation, the following traceback appears while the firewall stays in PENDING_CREATE:
neutron.services.firewall.agents.l3reference.firewall_l3_agent [req-2b7a801e-7358-418e-b4e7-95b7b27aefc2 None] FWaaS RPC failure in create_firewall for fw: f24bd240-04d5-49f1-971c-8ae95e666ef0
neutron.services.firewall.agents.l3reference.firewall_l3_agent Traceback (most recent call last):
neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/opt/stack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 133, in _invoke_driver_for_plugin_api
neutron.services.firewall.agents.l3reference.firewall_l3_agent     self.fwaas_driver.__getattribute__(func_name)(
neutron.services.firewall.agents.l3reference.firewall_l3_agent AttributeError: 'VPNAgent' object has no attribute 'fwaas_driver'
neutron.services.firewall.agents.l3reference.firewall_l3_agent"
1250,1310966,nova,625e48f82b5c7f1d55641b4207afd85b0b798cc2,1,1,,nova network floating IP has direct DB access,"nova-network with floating IPs is currently broken due to a direct DB access
2014-04-22 08:04:32.395 ESC[01;31mERROR oslo.messaging.rpc.dispatcher [ESC[01;36mreq-8fa8da3a-2e61-47e9-a6c6-68f598e979ad ESC[00;36mTestServerAdvancedOps-228418898
TestServerAdvancedOps-892915532ESC[01;31m] ESC[01;35mESC[01;31mException during message handling: nova-computeESC[00m
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 276, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 262, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 329, in decorated_function
    function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 250, in decorated_function
    migration.instance_uuid, exc_info=True)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 237, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 305, in decorated_function
    e, sys.exc_info())
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 292, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3472, in resize_instance
    migration_p)
  File ""/opt/stack/nova/nova/network/api.py"", line 45, in wrapped
    return func(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/network/api.py"", line 492, in migrate_instance_start
    self._get_floating_ip_addresses(context, instance)
  File ""/opt/stack/nova/nova/network/api.py"", line 474, in _get_floating_ip_addresses
    instance['uuid'])
  File ""/opt/stack/nova/nova/db/api.py"", line 700, in instance_floating_address_get_all
    return IMPL.instance_floating_address_get_all(context, instance_uuid)
  File ""/opt/stack/nova/nova/cmd/compute.py"", line 52, in __call__
    raise exception.DBNotAllowed('nova-compute')
DBNotAllowed: nova-compute"
1251,1310991,cinder,fcc17a6b09cbcfaa8aa9db2ee5d187de7f7649aa,1,1,,available snapshot exists which doesn't have snaps...,"Create snapshot from bootable volume.
Let's say cinder process down suddenly, when snapshot status is changed available.
Restarting cinder process, there is available snapshot that doesn't have snapshot metadata.
When the snapshot doesn't have the snapshot metadata, the status is available is not good.(data and status are mismatched)"
1252,1311058,cinder,8ce85e4fd40e9def337daadfdc92aca7b064efff,1,1,,CINDER retype doesn't work with the volumes with n...,"""cinder retype""  seems to work perfectly while retyping a volume having a initial volume-type i.e. while volume is created with volume-type.
but it gets stuck at 'retyping'  state while retyping a volume having a volume-type = None (for the volumes created with no volume-type)
openstack@ubuntu:~$ cinder list
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
|                  ID                                                            |   Status    |  Name  | Size   | Volume Type  | Bootable | Attached to |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
| f508dacd-845d-4d09-b69a-6412ec93213f  | available   | None    |  1       |     None           |  false         |                         |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
openstack@ubuntu:~$ cinder type-list
+------------------------------------------------+---------------+
|                  ID                                                            |          Name     |
+------------------------------------------------+---------------+
| ef8ad765-cea2-4fba-9162-a10d73a6c823  |          abcd        |
+------------------------------------------------+---------------+
openstack@ubuntu:~$ cinder retype  f508dacd-845d-4d09-b69a-6412ec93213f   abcd
openstack@ubuntu:~$ cinder list
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
|                  ID                                                            |   Status    |  Name  | Size   | Volume Type  | Bootable | Attached to |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
| f508dacd-845d-4d09-b69a-6412ec93213f  | retyping   | None    |  1       |     None           |  false        |                         |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+"
1254,1311243,cinder,a5a33a6833b29457d2ea7fc5b8c31f74a7aa4140,1,1,,Bug #1311243 “cinder,"The cinder list versions API call works fine for JSON:
curl -i http://23.253.228.211:8776/ -H ""Accept: application/json""   -H ""X-Auth-Token: $token""
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 298
Date: Tue, 22 Apr 2014 17:35:12 GMT
{""versions"": [{""status"": ""CURRENT"", ""updated"": ""2012-01-04T11:33:21Z"", ""id"": ""v1.0"", ""links"": [{""href"": ""http://23.253.228.211:8776/v1/"", ""rel"": ""self""}]}, {""status"": ""CURRENT"", ""updated"": ""2012-11-21T11:33:21Z"", ""id"": ""v2.0"", ""links"": [{""href"": ""http://23.253.228.211:8776/v2/"", ""rel"": ""self""}]}]}
However, the same call with XML fails:
curl -i http://23.253.228.211:8776/.xml -H ""Accept: application/xml""   -H ""X-Auth-Token: $token""
HTTP/1.1 500 Internal Server Error
Content-Length: 189
Content-Type: application/xml; charset=UTF-8
Date: Tue, 22 Apr 2014 17:37:09 GMT
<computeFault code=""500"" xmlns=""http://docs.openstack.org/volume/api/v1""><message>The server has either erred or is incapable of performing the requested operation.</message></computeFault>
Additionally, the XML call to show v1 details fails:
curl -i http://23.253.228.211:8776/v1/.xml -H ""Accept: application/xml"" -H ""X-Auth-Token:$token""
HTTP/1.1 404 Not Found
Content-Length: 52
Content-Type: text/plain; charset=UTF-8
X-Openstack-Request-Id: req-9ff05fb1-0b20-4cb7-a60f-8d956ec81dd9
Date: Tue, 22 Apr 2014 17:49:59 GMT
404 Not Found
The resource could not be found.
Call for XML to show v2 details fails:
curl -i http://23.253.228.211:8776/v2/.xml -H ""Accept: application/xml""   -H ""X-Auth-Token: $token""
HTTP/1.1 404 Not Found
Content-Length: 52
Content-Type: text/plain; charset=UTF-8
X-Openstack-Request-Id: req-9f96a04b-dc35-4ceb-bb53-ab6ade803075
Date: Tue, 22 Apr 2014 17:58:52 GMT
404 Not Found
The resource could not be found.
Also, the JSON call for show cinder v2 details seems to show cinder v1:
curl -i http://23.253.228.211:8776/v2/ -H ""Accept: application/json""   -H ""X-Auth-Token: $token""
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2012-01-04T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.volume+xml;version=1""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.volume+json;version=1""
         }
      ],
      ""id"":""v1.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8776/v2/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://jorgew.github.com/block-storage-api/content/os-block-storage-1.0.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.rackspacecloud.com/servers/api/v1.1/application.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}"
1255,1311260,neutron,c4f0c4ba967af71d31629beff72a0f1f748202cb,1,1,,SDN-VE controller expects information not in Neutr...,"The SDN-VE controller expects the port-id information for removing router interfaces.  Furthermore, the controller requires the use of string 'null' when the external gateway info is present and set to {} in Neutron update_router requests. The controller cannot accept "":"" as part of the incoming requests and therefore the requests need to be manipulated to replace the "":"" with ""_"" before sending requests to the controller."
1256,1311277,cinder,8f112b270af0965a6f9aafbd83976c1ad22314f8,1,1,,cinder list with metadata filter doesn't work,"cinder list does not work with --metadata filtered
>cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| b67cffdd-e573-4cdc-8362-dea6b1fe49a9 | available |    test2     |  1   |     None    |  false   |             |
| cd2880ae-cfd2-4c9e-8e73-2083d3eb3a90 | available |     test     |  1   |     None    |  false   |             |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
one of them has the readonly metadata attribute setted
 cinder show test
+--------------------------------+--------------------------------------+
|            Property            |                Value                 |
+--------------------------------+--------------------------------------+
|          attachments           |                  []                  |
|       availability_zone        |                 nova                 |
|            bootable            |                false                 |
|           created_at           |      2014-04-21T18:34:40.000000      |
|      display_description       |                 None                 |
|          display_name          |                 test                 |
|           encrypted            |                False                 |
|               id               | cd2880ae-cfd2-4c9e-8e73-2083d3eb3a90 |
|            metadata            |        {u'readonly': u'True'}        |
|     os-vol-host-attr:host      |          jmolle-Controller           |
| os-vol-mig-status-attr:migstat |                 None                 |
| os-vol-mig-status-attr:name_id |                 None                 |
|  os-vol-tenant-attr:tenant_id  |   55088aa5b5054b878b11d765e960c459   |
|              size              |                  1                   |
|          snapshot_id           |                 None                 |
|          source_volid          |                 None                 |
|             status             |              available               |
|          volume_type           |                 None                 |
+--------------------------------+--------------------------------------+
cinder show test2
+--------------------------------+--------------------------------------+
|            Property            |                Value                 |
+--------------------------------+--------------------------------------+
|          attachments           |                  []                  |
|       availability_zone        |                 nova                 |
|            bootable            |                false                 |
|           created_at           |      2014-04-22T16:19:01.000000      |
|      display_description       |                 None                 |
|          display_name          |                test2                 |
|           encrypted            |                False                 |
|               id               | b67cffdd-e573-4cdc-8362-dea6b1fe49a9 |
|            metadata            |                  {}                  |
|     os-vol-host-attr:host      |          jmolle-Controller           |
| os-vol-mig-status-attr:migstat |                 None                 |
| os-vol-mig-status-attr:name_id |                 None                 |
|  os-vol-tenant-attr:tenant_id  |   55088aa5b5054b878b11d765e960c459   |
|              size              |                  1                   |
|          snapshot_id           |                 None                 |
|          source_volid          |                 None                 |
|             status             |              available               |
|          volume_type           |                 None                 |
+--------------------------------+--------------------------------------+
But if I try to get the list with the metadata readonly as True it return that no volume has it setted
cinder list --metadata readonly=True
+----+--------+--------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+----+--------+--------------+------+-------------+----------+-------------+
+----+--------+--------------+------+-------------+----------+-------------+
It is spected that volume test is listed (it has the readonly attribute setted)"
1257,1311960,glance,e5e76fffbddddf8e6cd8e0b0b1173d4cca53fafe,1,1,“ The current implementation can lead to session timeouts”,Handle session timeout in the VMware store,"The current implementation can potentially lead to a timeout when the invoke_api is not triggered (for example PUT/GET: direct HTTP access to the datastore).
We need to recreate the session responses and retry when getting 401 HTTP responses."
1258,1311971,neutron,4539ff26d903174c844d6571533dfe719195e107,1,1,,Bug #1311971 “database exception causes UnboundLocalError in lin... ,"When database exception raises in update_device_down, the linuxbridge-agent doesn't deal with them in a proper way that causes accessing not-existed local variables.
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent Traceback (most recent call last):
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent   File ""/usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 997, in daemon_loop
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent     sync = self.process_network_devices(device_info)
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent   File ""/usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 894, in process_network_devices
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent     resync_b = self.treat_devices_removed(device_info['removed'])
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent   File ""/usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 963, in treat_devices_removed
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent     if details['exists']:
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent UnboundLocalError: local variable 'details' referenced before assignment
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent
2014-04-22 20:35:53.437 494 DEBUG neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent [req-ffc712fc-80af-4837-a068-6e1a076e4ebc None] Loop iteration exceeded interval (2 vs. 51.2715768814)! daemon_loop /usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:1011
2014-04-22 20:35:53.438 494 INFO neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent [req-ffc712fc-80af-4837-a068-6e1a076e4ebc None] Agent out of sync with plugin!"
1259,1312124,neutron,e575fde4cbe9b23cfc103e2de2f4052dd5874b02,1,1,“In downgrade of 4eca4a84f08a_remove_ml2_cisco_cred_db migartion there is a mistake”,Downgrade doesn't work in 4eca4a84f08a_remove_ml2_...,"In downgrade for 4eca4a84f08a_remove_ml2_cisco_cred_db there is a mistake in usage SQLAlchemy String type. Used sa.string instead of sa.String
akamyshnikova@akamyshnikova:/opt/stack/neutron$ neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini downgrade -10
INFO  [alembic.migration] Context impl MySQLImpl.
INFO  [alembic.migration] Will assume non-transactional DDL.
INFO  [alembic.migration] Running downgrade 1dde83e0359e -> 26a933acf533, add_index_psql_cisco
INFO  [alembic.migration] Running downgrade 26a933acf533 -> 30231c78a878, add_index_psql_packetfilter
INFO  [alembic.migration] Running downgrade 30231c78a878 -> 168ce7333432, add_index_psql_metering
INFO  [alembic.migration] Running downgrade 168ce7333432 -> 6be312499f9, add_index_psql_fwaas
INFO  [alembic.migration] Running downgrade 6be312499f9 -> d06e871c0d5, set_not_null_vlan_id_cisco
INFO  [alembic.migration] Running downgrade d06e871c0d5 -> 4eca4a84f08a, set_admin_state_up_not_null_ml2
INFO  [alembic.migration] Running downgrade 4eca4a84f08a -> 33c3db036fe4, Remove ML2 Cisco Credentials DB
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 167, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 81, in do_upgrade_downgrade
    do_alembic_command(config, cmd, revision, sql=CONF.command.sql)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 59, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 150, in downgrade
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 199, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 205, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 103, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 87, in run_migrations_online
    options=build_options())
  File ""<string>"", line 7, in run_migrations
  File ""/usr/local/lib/python2.7/dist-packages/alembic/environment.py"", line 681, in run_migrations
    self.get_context().run_migrations(**kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/migration.py"", line 225, in run_migrations
    change(**kw)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/4eca4a84f08a_remove_ml2_cisco_cred_db.py"", line 53, in downgrade
    sa.Column('credential_id', sa.string(length=255), nullable=True),
AttributeError: 'module' object has no attribute 'string'"
1260,1312132,nova,d0a5e27288940c8e104330d784fed43575187afd,1,1,,"live migrate of stopped VM goes to error, not back...","When the InstanceNotRunning exception is raised during the initial live-migrate, it should really just revert to task_state=None in the same way as if there was NoValidHost, or the compute service was down."
1261,1312392,neutron,17d38f90d1ce28383a5fc061e01f0853cd8debf6,1,1,,Wrong protocol value for SG IPV6 RA rule,"The ingress SG rule for RA has a wrong value for protocol
field."
1262,1312402,neutron,9e1c61b93ab3523bc1b5510775c1ee3331097f21,1,1,,Setting gateway in L3 should use “replace default ...,Just came across this.  I noticed that the ip_lib.py code uses replace instead of add.  Using the common code would be good anyway.
1263,1312467,neutron,fc7cffedbe60ae9da7963373e9072c55700fce5f,1,1,,"On external networks with multiple subnets, router...","This subject came up on IRC here [1]. It relates to the blueprint about pluggable external network connections and so I jumped in.
There are two reasons that using multiple external networks to allow multiple floating ip subnets [2] is not optimal.
- Extra L2 infrastructure needed.
- A neutron router cannot have a gateway connection to more than one external network. So, floating IPs wouldn't be able to float as freely as we'd like them to.
I cracked open devstack and started playing with it. I tried this first just to add a second subnet full of floating IPs.
neutron subnet-create ext-net 10.224.24.0/24 --disable-dhcp
In devstack, I needed to add a ""gateway router"". I did this by adding an IP to the br-ex interface. In a real cloud, we'd need to configure the upstream router as a gateway on the second subnet.
sudo ip addr add 10.224.24.1/24 dev br-ex
At this point, I was able to get a router to host floating IPs on both subnets! Pretty cool! I was very surprised it worked so easily.
There is one bug which this bug report addresses! Traffic between floating IPs on the second subnet went up to the router and then back down. The upstream router sent ICMP redirect packets periodically back to the Neutron router sourcing the traffic. These did the router no good because what it really needed to know was that the IP was on link but the upstream router couldn't tell it that.  Some upstream routers may not be configured to send redirects or route back through the port of origin.
The answer to this is to add an on-link route for each subnet on the external network to each router's gateway interface. This will require an L3 agent change but should not be very difficult.
[1] http://eavesdrop.openstack.org/irclogs/%23openstack-neutron/%23openstack-neutron.2014-04-08.log starting at 2014-04-08T23:23:51 (near the bottom)
[2] http://docs.openstack.org/admin-guide-cloud/content/adv_cfg_l3_agent_multi_extnet.html"
1264,1312482,neutron,f55e451fb790b976d3fbd3a23dd8688bcc584c77,1,1,,scalability problem of router routes update,"Updating router routes takes long time when increasing a number of routes.
The critical problem is that it is CPU bound task of neutron-server and neutron-server can not reply for other request.
I show below a measurement example of neutron-server's CPU usage.
Setting routes to a router. (0 to N)
100 routes: 1 sec
1000 routes: 5 sec
10000 routes: 51 sec
I found validation check of parameter  is inefficient. The following example support it too.
No change but just specify same routes to a router. (N to N, DB is not changed)
100 routes: <1 sec
1000 routes: 4 sec
10000 routes: 52 sec
Remove routes from a router. (N to 0)
100 routes: <1 sec
1000 routes:  8 sec
10000 routes: 750 sec
I found handling of record deletion is bad. It takes N**2 order."
1265,1312718,cinder,38ed0523125efeaf8383b36d1db169b42e2f5eb6,1,1,,Temporary snapshot may remain during creating volu...,"When creating LVM volume by source volume, cinder create source volume's snapshot first, then destination volume and active it, at last copy the volume.
Step:
1. self.create_snapshot
2.self._create_volume
3.self.vg.activate_lv
4.volutils.copy_volume
But If we create or active target volume fail since insufficient free space or other system error for example, the source volume's snapshot wouldn't be remove.
Temporary snapshot would remain, and what is worse we can't delete the source volume successfully."
1266,1312822,neutron,bfdec043f1429ac4aa884e9422861b4e6c1ca815,1,0,“NSX 4.2 GA has tweaked the naming for certain resources. Edge”,Bug #1312822 “NSX,"NSX: change api mapping for Service Cluster to Edge Cluster
NSX 4.2 GA has tweaked the naming for certain resources. Edge
Cluster vs Service CLuster is one of them."
1267,1313116,cinder,c1c10205423db25bc6b69e0e23b4f56aa2dfbd25,1,0,“update version response to remove PDF and WADL links”,Bug #1313116 “cinder,"The links in the GET /v2 response do not work and should be removed. Replace the PDF link with a link to docs.openstack.org.
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2012-01-04T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.volume+xml;version=1""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.volume+json;version=1""
         }
      ],
      ""id"":""v1.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8776/v2/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://jorgew.github.com/block-storage-api/content/os-block-storage-1.0.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.rackspacecloud.com/servers/api/v1.1/application.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}"
1268,1313118,cinder,c1c10205423db25bc6b69e0e23b4f56aa2dfbd25,1,0,,Bug #1313118 “cinder v1,"The links in the GET /v1 response do not work and should be removed. Replace the PDF link with a link to docs.openstack.org.
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2012-01-04T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.volume+xml;version=1""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.volume+json;version=1""
         }
      ],
      ""id"":""v1.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8776/v1/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://jorgew.github.com/block-storage-api/content/os-block-storage-1.0.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.rackspacecloud.com/servers/api/v1.1/application.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}"
1269,1313119,nova,3903090a60ecb0170c070a68e2f1df4a01b0aca1,1,0,,Bug #1313119 “nova,"The links in the GET /v2 response do not work and should be removed. Replace the PDF link with a link to docs.openstack.org.
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2011-01-21T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.compute+xml;version=2""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.compute+json;version=2""
         }
      ],
      ""id"":""v2.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8774/v2/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/2/os-compute-devguide-2.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/2/wadl/os-compute-2.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}
The links in the GET /v3 response do not work either:
{
   ""version"":{
      ""status"":""EXPERIMENTAL"",
      ""updated"":""2013-07-23T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.compute+json;version=3""
         }
      ],
      ""id"":""v3.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8774/v3/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/3/os-compute-devguide-3.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/3/wadl/os-compute-3.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}"
1270,1313801,neutron,ba87499dda0c897aa08afadfc2911ff80bdae2f9,0,0,"Feature, “Colocate strings used in REST client methods as URIs for APIs to Cisco CSR device, to make it easier to find and maintain them.”",Cisco VPN move URI strings to constants,"Colocate strings used in REST client methods as URIs for APIs to Cisco CSR device, to make it easier to find and maintain them. File is:
neutron/services/vpn/device_drivers/cisco_csr_rest_client.py"
1271,1313894,cinder,32bac00ea003015add0d33be262cb3002e4c43af,1,1,,3PAR vlun could be wrong for multi-attach,"When the 3PAR driver creates a vlun and then has to fetch it, the fetching could return the wrong vlun.
When a volume is exported to the same host more than once, we will have multiple LUN ids associated with that volume.
We have to make sure we get the correct LUN for the VLUN we just created.   This is especially important
 in a multi-attach scenario."
1272,1313992,glance,cb93eb60abf10af6fbb1ee438a6e8a51853e6788,1,1,,VMware store should handle upload of files with un...,"Nova needs to upload streamOptimized disks to Glance. These streamOptimized disks are converted/compressed on the fly by
vCenter. Consequently, it is not possible to know the size of the Glance image before upload. Without specifying the size
or size zero, vCenter will reject the request (Broken Pipe)."
1273,1313997,neutron,47e51e7521784f6a2edcfbf71a9aac0237e76e42,1,1,,Bug #1313997 “NSX,"Corner case with networks without a subnet leads to a spurious exception during net-migration, because the validation logic is called twice: before migration and after."
1274,1314007,glance,95d81a0b03703f47d457f26acecb36d7a10e66cb,1,1,,run_tests.sh failed by  “No module named tools,"$ ./run_tests.sh
No virtual environment found...create one? (Y/n) y
Traceback (most recent call last):
  File ""tools/install_venv.py"", line 29, in <module>
    from tools import install_venv_common as install_venv
ImportError: No module named tools
Change ""Enable H304 check"" (I099ed65db9b42223eaa4b66a3a5c6113d1cc56fe) causes it."
1276,1314176,nova,18938d283c5f87ab2e55f9f75626f313e8724bf9,0,0,"Feature ""We need to improve the hacking rule to avoid markers of author for the tag:”",Improve hacking rule for author tag,"We need to improve the hacking rule to avoid markers of author for the tag:
'.. moduleauthor'"
1277,1314472,neutron,717a0a0ab4436e2904360fe4e34226af7d0e412d,1,1,,Bug #1314472 “ofagent,"OFANeutronAgent.__init__() calls setup_rpc(), which spawns a thread to consume RPCs.
self.updated_ports, which is accessed by an RPC handler, port_update(), has not been initialized at this point."
1278,1314677,nova,695191fa89387d96e60120ff32965493c844e7f5,1,1,,nova-cells fails when using JSON file to store cel...,"As recommended in http://docs.openstack.org/havana/config-reference/content/section_compute-cells.html#cell-config-optional-json I'm creating the nova-cells config with the cell information stored in a json file. However, when I do this nova-cells fails to start with this error in the logs:
2014-04-29 11:52:05.240 16759 CRITICAL nova [-] __init__() takes exactly 3 arguments (1 given)
2014-04-29 11:52:05.240 16759 TRACE nova Traceback (most recent call last):
2014-04-29 11:52:05.240 16759 TRACE nova   File ""/usr/bin/nova-cells"", line 10, in <module>
2014-04-29 11:52:05.240 16759 TRACE nova     sys.exit(main())
2014-04-29 11:52:05.240 16759 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/cmd/cells.py"", line 40, in main
2014-04-29 11:52:05.240 16759 TRACE nova     manager=CONF.cells.manager)
2014-04-29 11:52:05.240 16759 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 257, in create
2014-04-29 11:52:05.240 16759 TRACE nova     db_allowed=db_allowed)
2014-04-29 11:52:05.240 16759 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 139, in __init__
2014-04-29 11:52:05.240 16759 TRACE nova     self.manager = manager_class(host=self.host, *args, **kwargs)
2014-04-29 11:52:05.240 16759 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/cells/manager.py"", line 87, in __init__
2014-04-29 11:52:05.240 16759 TRACE nova     self.state_manager = cell_state_manager()
2014-04-29 11:52:05.240 16759 TRACE nova TypeError: __init__() takes exactly 3 arguments (1 given)
I have had a dig into the code and it appears that CellsManager creates an instance of CellStateManager with no arguments. CellStateManager __new__ runs and creates an instance of CellStateManagerFile which runs __new__ and __init__ with cell_state_cls and cells_config_path set. At this point __new__ returns CellStateManagerFile and the new instance's __init__() method is invoked (CellStateManagerFile.__init__) with the original arguments (there weren't any) which then results in the stack trace.
It seems reasonable for CellStateManagerFile to derive the cells_config_path info for itself so I've patched it locally with
=== modified file 'state.py'
--- state.py	2014-04-30 15:10:16 +0000
+++ state.py	2014-04-30 15:10:26 +0000
@@ -155,7 +155,7 @@
             config_path = CONF.find_file(cells_config)
             if not config_path:
                 raise cfg.ConfigFilesNotFoundError(config_files=[cells_config])
-            return CellStateManagerFile(cell_state_cls, config_path)
+            return CellStateManagerFile(cell_state_cls)
         return CellStateManagerDB(cell_state_cls)
@@ -450,7 +450,9 @@
 class CellStateManagerFile(CellStateManager):
-    def __init__(self, cell_state_cls, cells_config_path):
+    def __init__(self, cell_state_cls=None):
+        cells_config = CONF.cells.cells_config
+        cells_config_path = CONF.find_file(cells_config)
         self.cells_config_path = cells_config_path
         super(CellStateManagerFile, self).__init__(cell_state_cls)
Ubuntu: 14.04
nova-cells: 1:2014.1-0ubuntu1
nova.conf:
[DEFAULT]
dhcpbridge_flagfile=/etc/nova/nova.conf
dhcpbridge=/usr/bin/nova-dhcpbridge
logdir=/var/log/nova
state_path=/var/lib/nova
lock_path=/var/lock/nova
force_dhcp_release=True
iscsi_helper=tgtadm
libvirt_use_virtio_for_bridges=True
connection_type=libvirt
root_helper=sudo nova-rootwrap /etc/nova/rootwrap.conf
verbose=True
ec2_private_dns_show_ip=True
api_paste_config=/etc/nova/api-paste.ini
volumes_path=/var/lib/nova/volumes
enabled_apis=ec2,osapi_compute,metadata
auth_strategy=keystone
compute_driver=libvirt.LibvirtDriver
quota_driver=nova.quota.NoopQuotaDriver
[cells]
enable=True
name=cell
cell_type=compute
cells_config=/etc/nova/cells.json
cells.json:
{
    ""parent"": {
        ""name"": ""parent"",
        ""api_url"": ""http://api.example.com:8774"",
        ""transport_url"": ""rabbit://rabbit.example.com"",
        ""weight_offset"": 0.0,
        ""weight_scale"": 1.0,
        ""is_parent"": true
    }
}"
1279,1314850,neutron,07a130be1a1f77fc86076f37afe5b6bae2692a2c,1,1,"""This method was removed from that file in commit 53609f29f3c8fcadc545afb891189253c07b33c3”",'module' object has no attribute 'get_engine',"File ""neutron/lib/python2.7/site-packages/neutron/wsgi.py"", line 98, in start
    session.get_engine(sqlite_fk=True).pool.dispose()
AttributeError: 'module' object has no attribute 'get_engine'
https://review.openstack.org/#/c/77205/
wsgi.py was not updated with these changes and execution results in an exception when get_engine from neutron.openstack.common.db.sqlalchemy.session is invoked."
1280,1314994,neutron,61a7b584238327d346bd4da642f3f8cb4d1dbf9c,1,1,,There are log messages that do not have transtaion...,"Following files have log messages that are not translated:
neutron/agent/securitygroups_rpc.py
neutron/plugins/hyperv/agent/security_groups_driver.py
neutron/plugins/hyperv/agent/utilsfactory.py
neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py"
1281,1315097,neutron,89b01ca24ba95e36568352b4e5c7abf0cd04e4af,0,0,Optimization “Optimize querying for security groups”,get_security_groups_on_port takes a long time slow...,"In Havana neutron, there is a _get_security_groups_on_port(...) which checks that all security groups on port belong to tenant. This method is called every time a port is created. This method calls get_security_groups(...) and with 'service' tenant, this results in getting security groups of all tenants from the database, which is time consuming if there are hundreds of tenants. get_security_groups(...) then applies filter on the security groups returned from db query to select security groups associated with the port. In our environment with around 700 tenants, we saw this method take > 30 seconds and also cause slowdown of other API request processing on this worker instance. This can be made more efficient by supplying 'id' filter so that the database query itself filters on the tenant-id. In our environment, this patch resulted in the method taking 0.1 seconds, down from 30 seconds."
1282,1315137,neutron,5cecf9571f93f89ceaa024cae0a7368767643931,0,0,feature“plumgrid drivers should be dynamically loaded”,plumgrid drivers should be dynamically loaded,In PLUMgrid plugin drivers are not dynamically loaded. They should be loaded by configuration as the rest of the plugins
1286,1315467,neutron,45381fe1c742c75773d97f1c0bd1f3cb1e7a6468,1,1,,Neutron deletes the router interface instead of ad...,"After parsing a lot of log files related to check failure, looks like the q-vpn at the time when I would expect to add
floating ip , it destroys the router's qg- and qr-  interfaces.
However after the floating ip deletion request the q-vpn service restores the qg-, qr- interfaces.
tempest.scenario.test_minimum_basic.TestMinimumBasicScenario.test_minimum_basic_scenario[compute,image,network,volume]
failed in http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/console.html.
admin user: admin/9e02f14321454af6bb27587770f27d9b
admin tenant id: admin/413bb1232bca45069f3a3256839effa1
test user: TestMinimumBasicScenario-1306090821/c8dd95056c0b407e8dd168dbf410a66a
test Tenant:  TestMinimumBasicScenario-993819377/2527b8222e3343bca9f70343e608880c
External Net : public/c29040d3-7e73-4a87-9f73-bb5cbe602afb
External subnet: public-subnet/ee754eb6-6194-4a25-a4cc-f9233d366c1e
Network: TestMinimumBasicScenario-1375749858-network/f029d7a8-54e0-484c-a215-cc34066ae830
Subnet: TestMinimumBasicScenario-1375749858-subnet/7edc72be-1207-4571-95d4-911223885ae7  10.100.0.0/28
Router id:TestMinimumBasicScenario-1375749858-router/08216822-5ee2-4313-be7e-dad2d84147db
Expected interfaces in the qrouter-08216822-5ee2-4313-be7e-dad2d84147db:
* lo 127.0.0.1
* qr-529eddd4-2c 10.100.0.1/28 iface_id: 529eddd4-2ca8-43ec-9cab-29c3a6632604, attached-mac: fa:16:3e:2a:f8:ba, ofport 166
* qg-9be8f502-93 172.24.4.85/24 iface-id: 9be8f502-9360-47dc-9eff-33c8743e7c2b  attached-mac: fa:16:3e:be:a1:54, ofport 37
Floating IP: 172.24.4.87 (Never appears in the q-vpn log)
port: (net/subnet/port)(c29040d3-7e73-4a87-9f73-bb5cbe602afb/ee754eb6-6194-4a25-a4cc-f9233d366c1e/013b0b2d-80ed-403d-b380-6b6895ce34f5)
mac?: fa:16:3e:15:f2:57
floating ip uuid: cd84111e-af6a-4c26-af73-3167419c664a
Instance:
Ipv4: 10.100.0.2 mac: FA:16:3E:18:F1:69
Instance uuid: 8a552eda-2fbd-4972-bfcf-cee7e6472871
iface_id/port_id: 4188c532-3265-4294-8b4e-9bbfe5a482e8
ovsdb_interface_uuid: 9d7b858b-745e-482b-b91a-1e9ae34fc545
intbr tag: 49
dhcp server dev: tap4c6c6e06-e4
ns: qdhcp-f029d7a8-54e0-484c-a215-cc34066ae830
ip: 10.100.0.3 mac: fa:16:3e:a2:f1:ea
intbr tag: 49
Host:
eth0: 10.7.16.229/15 mac: 02:16:3e:52:5d:ff
Router + router interface creation in the logs:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/tempest.txt.gz#_2014-05-01_19_49_46_924
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-q-svc.txt.gz#_2014-05-01_19_49_46_724
Floating IP create:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-n-api.txt.gz#_2014-05-01_19_50_18_447
Floating IP associate:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-n-api.txt.gz#_2014-05-01_19_50_18_814
q-vpn starts destroying the router:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-q-vpn.txt.gz#_2014-05-01_19_50_20_277
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-c0cd93e7-5cb0-403b-a509-8e07b352b89d', 'ipsec', 'whack', '--ctlbase', '/opt/stack/data/neutron/ipsec/c0cd93e7-5cb0-403b-a509-8e07b352b89d/var/run/pluto', '--status']
Exit code: 1
Stdout: ''
Stderr: 'whack: Pluto is not running (no ""/opt/stack/data/neutron/ipsec/c0cd93e7-5cb0-403b-a509-8e07b352b89d/var/run/pluto.ctl"")\n' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:74
2014-05-01 19:50:20.277 19279 DEBUG neutron.openstack.common.lockutils [-] Semaphore / lock released ""sync"" inner /opt/stack/new/neutron/neutron/openstack/common/lockutils.py:252
2014-05-01 19:50:20.277 19279 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-08216822-5ee2-4313-be7e-dad2d84147db', 'ip', '-o', 'link', 'show', 'qr-529eddd4-2c'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:48
2014-05-01 19:50:20.516 19279 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-08216822-5ee2-4313-be7e-dad2d84147db', 'ip', '-o', 'link', 'show', 'qr-529eddd4-2c']
Exit code: 0
Stdout: '605: qr-529eddd4-2c: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN \\    link/ether fa:16:3e:2a:f8:ba brd ff:ff:ff:ff:ff:ff\n'
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:74
2014-05-01 19:50:20.517 19279 DEBUG neutron.agent.linux.utils [-] Running command: ['ip', '-o', 'link', 'show', 'br-int'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:48
2014-05-01 19:50:20.533 19279 DEBUG neutron.agent.linux.utils [-]
Command: ['ip', '-o', 'link', 'show', 'br-int']
Exit code: 0
Stdout: '6: br-int: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN \\    link/ether c6:49:9f:72:d2:4a brd ff:ff:ff:ff:ff:ff\n'
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:74
2014-05-01 19:50:20.534 19279 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--timeout=10', '--', '--if-exists', 'del-port', 'br-int', 'qr-529eddd4-2c'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:48
2014-05-01 19:50:21.061 19279 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--timeout=10', '--', '--if-exists', 'del-port', 'br-int', 'qr-529eddd4-2c']
Exit code: 0
Tempest tries to connect to the VM:
2014-05-01 19:50:19,463 .. 2014-05-01 19:53:48,494 ~ 209 sec
Delete request for the floatingip:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/tempest.txt.gz#_2014-05-01_19_54_24_191
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-n-api.txt.gz#_2014-05-01_19_54_25_047
Neutron touches the qr-529eddd4-2c interface + port,
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-q-vpn.txt.gz#_2014-05-01_19_54_37_886
Delete request for router and router interface:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/tempest.txt.gz#_2014-05-01_19_54_57_423"
1287,1315475,neutron,3758f25c3ca83d9465f7a0041f286156504c7065,1,0,"“The initial OpenDaylight integration with Openstack did not support
vlan isolation so it was not included as a valid type.”",OpenDaylight plugin does not allow vlan network ty...,"The initial OpenDaylight integration with Openstack did not support
vlan isolation so it was not included as a valid type."
1288,1315538,neutron,a3a8a86d7bccb824725d91735bd032237786f5aa,1,1,,Bug #1315538 “NSX,"The following stacktrace has been observed using NSX DHCP:
2014-05-02 14:00:36.295 30957 DEBUG neutron.plugins.vmware.api_client.base [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] [0] Released connection https://192.168.1.13:443. 10 connection(s) available. release_connect
ion /opt/stack/neutron/neutron/plugins/vmware/api_client/base.py:176
2014-05-02 14:00:36.296 30957 DEBUG neutron.plugins.vmware.api_client.eventlet_request [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] [0] Completed request 'POST /ws.v1/lservices-node/20e0dc1c-a1da-455f-8841-3c52d78
6696c/lport': 201 _handle_request /opt/stack/neutron/neutron/plugins/vmware/api_client/eventlet_request.py:152
2014-05-02 14:00:36.296 30957 DEBUG neutron.plugins.vmware.api_client.client [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] Request returns ""<httplib.HTTPResponse instance at 0x4c49368>"" request /opt/stack/neutron/n
eutron/plugins/vmware/api_client/client.py:93
2014-05-02 14:00:36.297 30957 ERROR neutron.api.v2.resource [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] add_router_interface failed
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 87, in resource
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 193, in _handle_action
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     return getattr(self._plugin, name)(*arg_list, **kwargs)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/plugins/base.py"", line 1719, in add_router_interface
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     context, router_id, interface=router_iface_info)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcpmeta_modes.py"", line 157, in handle_router_metadata_access
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     router_id, interface)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/combined.py"", line 89, in handle_router_metadata_access
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     plugin, context, router_id, interface)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/nsx.py"", line 312, in handle_router_metadata_access
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     context, subnet_id, is_enabled)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 294, in lsn_metadata_configure
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     self.lsn_port_metadata_setup(context, lsn_id, subnet)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 225, in lsn_port_metadata_setup
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     lsn_port_id = self.lsn_port_create(self.cluster, lsn_id, data)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 453, in lsn_port_create
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     subnet_info['mac_address'], lsn_id)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 442, in lsn_port_save
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     context, lsn_port_id, subnet_id, mac_addr, lsn_id)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dbexts/lsn_db.py"", line 96, in lsn_port_add_for_lsn
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     with context.session.begin(subtransactions=True):
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource AttributeError: 'NSXCluster' object has no attribute 'session'
This is because the wrong parameter is passed to during the DB operation."
1289,1315542,cinder,f7f9e6228d8b73be6c1a4ad3fd998d02104a7a3b,1,1,,3PAR initialize_connection fails when no NSP is re...,"When a VLUN is created to attach a 3PAR volume to a host/instance and an NSP isn't used, the NSP isn't returned in the _create_vlun call.   We were always assuming the nsp would be returned from the REST call."
1290,1315574,neutron,2d72790bf83473e0c9b73cc290f711fcda7a5803,0,0,Bug in test,Big Switch unit test unnecessary magic mock,The Big Switch unit tests use Magic Mocks to stop external calls (e.g. HTTP and RPC). This is a waste of memory most of the time since no assertions are made on the Mocks.
1291,1315597,nova,523e52f2f754f35b1e2453190f935d1a990464fb,1,0,"“api-version 3 list-extensions"" with a patch https://review.openstack.org/#/c/91942/ ,
the output is the following.
So 'extensions', 'flavors' and 'ips' are not camelcase. This seems inconsistent.”",Some v3 API extension names are not camelcase,"When operating ""nova --os-compute-api-version 3 list-extensions"" with a patch https://review.openstack.org/#/c/91942/ ,
the output is the following.
So 'extensions', 'flavors' and 'ips' are not camelcase. This seems inconsistent.
$ nova --os-compute-api-version 3 list-extensions
+--------------------------+-----------------------------------------------------------------------+-------------------------------+---------+
| Name                     | Summary                                                               | Alias                         | Version |
+--------------------------+-----------------------------------------------------------------------+-------------------------------+---------+
| Consoles                 | Consoles.                                                             | consoles                      | 1       |
| extensions               | Extension information.                                                | extensions                    | 1       |
| FlavorAccess             | Flavor access support.                                                | flavor-access                 | 1       |
| FlavorsExtraSpecs        | Flavors Extension.                                                    | flavor-extra-specs            | 1       |
| FlavorManage             | Flavor create/delete API support.                                     | flavor-manage                 | 1       |
| flavors                  | Flavors Extension.                                                    | flavors                       | 1       |
| ips                      | Server addresses.                                                     | ips                           | 1       |
| Keypairs                 | Keypair Support.                                                      | keypairs                      | 1       |
| AccessIPs                | Access IPs support.                                                   | os-access-ips                 | 1       |
| AdminActions             | Enable admin-only server actions...                                   | os-admin-actions              | 1       |
| AdminPassword            | Admin password management support.                                    | os-admin-password             | 1       |
| Agents                   | Agents support.                                                       | os-agents                     | 1       |
| Aggregates               | Admin-only aggregate administration.                                  | os-aggregates                 | 1       |
| AttachInterfaces         | Attach interface support.                                             | os-attach-interfaces          | 1       |
| AvailabilityZone         | 1. Add availability_zone to the Create Server API....                 | os-availability-zone          | 1       |
| BlockDeviceMapping       | Block device mapping boot support.                                    | os-block-device-mapping       | 1       |
| Cells                    | Enables cells-related functionality such as adding neighbor cells,... | os-cells                      | 1       |
| Certificates             | Certificates support.                                                 | os-certificates               | 1       |
| ConfigDrive              | Config Drive Extension.                                               | os-config-drive               | 1       |
| ConsoleAuthTokens        | Console token authentication support.                                 | os-console-auth-tokens        | 1       |
| ConsoleOutput            | Console log output support, with tailing ability.                     | os-console-output             | 1       |
| CreateBackup             | Create a backup of a server.                                          | os-create-backup              | 1       |
| DeferredDelete           | Instance deferred delete.                                             | os-deferred-delete            | 1       |
| Evacuate                 | Enables server evacuation.                                            | os-evacuate                   | 1       |
| ExtendedAvailabilityZone | Extended Server Attributes support.                                   | os-extended-availability-zone | 1       |
| ExtendedServerAttributes | Extended Server Attributes support.                                   | os-extended-server-attributes | 1       |
| ExtendedStatus           | Extended Status support.                                              | os-extended-status            | 1       |
| ExtendedVolumes          | Extended Volumes support.                                             | os-extended-volumes           | 1       |
| FlavorRxtx               | Support to show the rxtx status of a flavor.                          | os-flavor-rxtx                | 1       |
| HideServerAddresses      | Support hiding server addresses in certain states.                    | os-hide-server-addresses      | 1       |
| Hosts                    | Admin-only host administration.                                       | os-hosts                      | 1       |
| Hypervisors              | Admin-only hypervisor administration.                                 | os-hypervisors                | 1       |
| LockServer               | Enable lock/unlock server actions.                                    | os-lock-server                | 1       |
| MigrateServer            | Enable migrate and live-migrate server actions.                       | os-migrate-server             | 1       |
| Migrations               | Provide data on migrations.                                           | os-migrations                 | 1       |
| Multinic                 | Multiple network support.                                             | os-multinic                   | 1       |
| MultipleCreate           | Allow multiple create in the Create Server v3 API.                    | os-multiple-create            | 1       |
| PauseServer              | Enable pause/unpause server actions.                                  | os-pause-server               | 1       |
| PCIAccess                | Pci access support.                                                   | os-pci                        | 1       |
| Quotas                   | Quotas management support.                                            | os-quota-sets                 | 1       |
| RemoteConsoles           | Interactive Console support.                                          | os-remote-consoles            | 1       |
| Rescue                   | Instance rescue mode.                                                 | os-rescue                     | 1       |
| SchedulerHints           | Pass arbitrary key/value pairs to the scheduler.                      | os-scheduler-hints            | 1       |
| SecurityGroups           | Security group support.                                               | os-security-groups            | 1       |
| ServerActions            | View a log of actions and events taken on an instance.                | os-server-actions             | 1       |
| ServerDiagnostics        | Allow Admins to view server diagnostics through server action.        | os-server-diagnostics         | 1       |
| ServerExternalEvents     | Server External Event Triggers.                                       | os-server-external-events     | 1       |
| ServerPassword           | Server password support.                                              | os-server-password            | 1       |
| ServerUsage              | Adds launched_at and terminated_at on Servers.                        | os-server-usage               | 1       |
| Services                 | Services support.                                                     | os-services                   | 1       |
| Shelve                   | Instance shelve mode.                                                 | os-shelve                     | 1       |
| SuspendServer            | Enable suspend/resume server actions.                                 | os-suspend-server             | 1       |
| Server Metadata          | Server Metadata API.                                                  | server-metadata               | 1       |
| Servers                  | Servers.                                                              | servers                       | 1       |
| Versions                 | API Version information.                                              | versions                      | 1       |
+--------------------------+-----------------------------------------------------------------------+-------------------------------+---------+"
1292,1315809,neutron,14ebdbe77479a72ad1f324663ec795460e03e99a,1,1,,Updation of IKE Policy after the site creation fai...,"Updating the IKE policy after the site creation fails stating ike policy in already in use
Updating IKEPolicy 07fc4f3b-1b44-4d5c-9949-5f7fa3bc6ae5
_PUT-REST: url:          http://<controller_ip>:9696/v2.0/vpn/ikepolicies/07fc4f3b-1b44-4d5c-9949-5f7fa3bc6ae5
_PUT-REST: X-Auth-Token: 86c8be3ce0204aa8bdd1458021eacec4
_PUT-REST: data:         {""ikepolicy"":{""name"": ""IKE1"",""encryption_algorithm"": ""aes-256""}}
{u'NeutronError': {u'message': u'IKEPolicy 07fc4f3b-1b44-4d5c-9949-5f7fa3bc6ae5 is still in use', u'type': u'IKEPolicyInUse', u'detail': u''}}
None"
1293,1315815,neutron,5da44b9bdb65c54f823ce7867386b9ca0c9a6b25,0,0,"Feature ""Radware LBaaS driver should transfer the tenant-id to the back system when creating an ADC service”","Radware LBaaS driver, transfer tenant id to back s...",Radware LBaaS driver should transfer the tenant-id to the back system when creating an ADC service
1294,1316074,nova,9b41af32bbbfc063eabf570fb95cdf516a8bb4a7,1,1,,nova network dhcpbridge has direct DB access,"nova-network is currently broken due to direct DB access in dhcpbridge.
This issue was found using a multihost devstack setup where the non-controller node has an empty sql connection string."
1295,1316152,nova,2ddc905ebba6e6b2bd5be2ca45a14115ef5a6e8f,1,1,,nova cell showing should raise more information on...,"nova cell showing should raise more information on v2 API, instead of ""The resource could not be found. """
1296,1316167,nova,92d92c7fe1d4db4a5c263d4499eae64568765047,0,0,Bug in test,mock.assert_not_called() is not a thing,"We have 14 hits of tests using mock.assert_not_called() which is not a real method for mocks:
nova
nova
tests
api
openstack
test_common.py
395: href_link_mock.assert_not_called()
compute
test_compute_utils.py (2 matches)
764: mock_log.warning.assert_not_called()
775: mock_log.warning.assert_not_called()
image
test_glance.py (10 matches)
548: img.assert_not_called()
556: img.assert_not_called()
690: trans_from_mock.assert_not_called()
708: is_avail_mock.assert_not_called()
709: trans_from_mock.assert_not_called()
754: trans_from_mock.assert_not_called()
791: is_avail_mock.assert_not_called()
792: trans_from_mock.assert_not_called()
848: trans_from_mock.assert_not_called()
920: trans_from_mock.assert_not_called()
virt
baremetal
test_nova_baremetal_deploy_helper.py
339: self.m_mp.assert_not_called()
https://code.google.com/p/mock/issues/detail?id=159
We should be using self.assertFalse(mock.called) instead."
1297,1316190,neutron,1715eb7c8e1f2433df3c081e357f8c40dfe2a28a,1,1,,When IPv6 addresses are added to the dnsmasq host ...,"When an IPv6 subnet is created, with ipv6_ra_mode not set, and ipv6_address_mode set to ""slaac"" - ipv6 addresses are added to the host file that dnsmasq uses, and this causes dnsmasq to cease responding to dhcp clients on the v4 side."
1298,1316382,neutron,0dde14c0cd6ffea8ebff715342852ef17a9c0b70,1,0,"""isn't present in the python 2.6 httplib.”",Bug #1316382 “Big Switch,The HTTPS class with certificate validation in the Big Switch server manager module references a source_address attribute that isn't present in the python 2.6 httplib.
1299,1316486,neutron,61edb744bdaefa72151e99c43d9a6e75e7895f07,0,0,Bug in test,Package is imported instead of module in neutron/t...,This change was wrong https://review.openstack.org/#/c/89628/5/neutron/tests/unit/services/loadbalancer/drivers/netscaler/test_netscaler_driver.py and needs to be taken back. Previous version imported module correctly and there was no need to change.
1300,1316674,nova,4cb139fb99814f04b9e4595d7e2efa34e7992826,1,1,,VCDriver - Stats update does not ignore hosts in m...,"Cluster has 2 hosts, with each host having 24 cores.
Both hosts are active.
Stats update is correct
Snippet of nova-compute.log
-05-07 03:14:28.001 AUDIT nova.compute.resource_tracker [-] Free ram (MB): 57582
2014-05-07 03:14:28.001 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 99
2014-05-07 03:14:28.002 AUDIT nova.compute.resource_tracker [-] Free VCPUS: 48
2014-05-07 03:14:28.021 INFO nova.compute.resource_tracker [-] Compute_service record updated for sagar-devstack:domain-c162(Demo-Pulsar-Cluster-DRS)
2014-05-07 03:14:28.021 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released ""update_available_resource"" from (pid=28072) inner /opt/stack/nova/nova/openstack/common/lockutils.py:252
Put one of the hosts in Maintenance mode, stats update does not ignore host in maintenance mode and the free vCPUS is still 48.
It should be 24"
1301,1316824,cinder,f2a6e77bc75ea72a374ded4dba82cf2e95c11404,0,0,Bug in test,remove hplefthandclient requirement from unit test...,"In order to eliminate the need to have the hplefthandclient in the global-requirements project, we need to remove the hplefthandclient from being imported in all hplefthand driver unit tests in cinder.
It should _at least_ be optional for the tests."
1303,1317075,cinder,da9597aed0186e68dbf1c7304b30e49f8e6a54ff,1,0,"“the behavior described in this bug is specific to F20 (for now).
“",Cannot attach volume after volume snapshot create/...,"lvm backend, with tgtadm ISCSI helper.
1. nova boot test2 --image cirros-0.3.2-x86_64-uec --flavor 42 # wait for 'active'
2. cinder create 1 #  wait for 'available'
3. cinder snapshot-create <vol_id> # wait for create
4. cinder snapshot-delete <snap_id>  # wait for delete
5. nova volume-attach test2 <vol_id> /dev/vdc
6. cinder list # says it is in 'attaching' status for long time (>10s)
7. cinder list #  the volume in 'available' status
The  cinder snapshot-delete <vol_id> causes the volume looses it (a)ctive lvm flag.
 LV                                          VG            Attr       LSize Pool Origin Data%  Move Log Cpy%Sync Convert
volume-c86347f9-3f42-4824-aee1-ae4aa33a2cf9 stack-volumes -wi------- 1.00g
This command has effect also to  the original volume, even if it used on the snapshot:
$ lvchange -y -an  stack-volumes/_snapshot-1b5cfb86-be8e-4ad0-89f5-ecc4e64a5f5d"
1304,1317134,cinder,ffa12c98971e35056a7867cd61ba5a39630cc0a7,1,1,,Volume detach hangs. HP 3par client fails to delet...,"When an HP 3par host in host set, the cinder volume detach will hang. Looking at the log files, the exception shows the following exception after deleting all the vluns, the host entry is a pre-existing one since the host storage is using the same 3par system:
cinder.openstack.common.rpc.amqp HTTPConflict: Conflict (HTTP 409) 77 - host is a member of a set
in master/cinder/volume/drivers/san/hp/hp_3par_common.py
def delete_vlun(self, volume, hostname):
        volume_name = self._get_3par_vol_name(volume['id'])
        vlun = self._get_vlun(volume_name, hostname)
        if vlun is not None:
            # VLUN Type of MATCHED_SET 4 requires the port to be provided
            if self.VLUN_TYPE_MATCHED_SET == vlun['type']:
                self.client.deleteVLUN(volume_name, vlun['lun'], hostname,
                                       vlun['portPos'])
            else:
                self.client.deleteVLUN(volume_name, vlun['lun'], hostname)
        try:
            self._delete_3par_host(hostname)
            self._remove_hosts_naming_dict_host(hostname)
 >>        except hpexceptions.HTTPConflict as ex:
 >>            # host will only be removed after all vluns
 >>           # have been removed
 >>           if 'has exported VLUN' in ex.get_description():
 >>              pass
 >>           else:
 >>               raise
Yes, the problem occurred in Havana. I believe the problem is in Icehouse as well, but I have not tested it yet.
Log info:
ccontrol': 'no-cache',
 'connection': 'close',
 'date': 'Fri, 02 May 2014 17:32:17 GMT',
 'pragma': 'no-cache',
 'server': 'hp3par-wsapi',
 'status': '200'}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:166
2014-05-02 17:31:23.698 22459 DEBUG hp3parclient.http [-] RESP BODY:
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:167
2014-05-02 17:31:23.699 22459 DEBUG hp3parclient.http [-]
REQ: curl -i https://192.168.3.20:8080/api/v1/hosts/kvm1-4 DELETE -H ""X-Hp3Par-Wsapi-Sessionkey: 272d1-c810c89243580a0047fec3b7c713b650-a1d66353"" -H ""Accept: application/json"" -H ""User-Agent: python-3parclient""
 _http_log_req /usr/lib/python2.6/site-packages/hp3parclient/http.py:159
2014-05-02 17:31:23.866 22459 DEBUG hp3parclient.http [-] RESP:{'connection': 'close',
 'content-type': 'application/json',
 'date': 'Fri, 02 May 2014 17:32:17 GMT',
 'server': 'hp3par-wsapi',
 'status': '409'}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:166
2014-05-02 17:31:23.867 22459 DEBUG hp3parclient.http [-] RESP BODY:{""code"":77,""desc"":""host is a member of a set""}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:167
2014-05-02 17:31:23.868 22459 DEBUG hp3parclient.http [-]
REQ: curl -i https://192.168.3.20:8080/api/v1/credentials/272d1-c810c89243580a0047fec3b7c713b650-a1d66353 DELETE -H ""X-Hp3Par-Wsapi-Sessionkey: 272d1-c810c89243580a0047fec3b7c713b650-a1d66353"" -H ""Accept: application/json"" -H ""User-Agent: python-3parclient""
 _http_log_req /usr/lib/python2.6/site-packages/hp3parclient/http.py:159
2014-05-02 17:31:23.887 22459 DEBUG hp3parclient.http [-] RESP:{'cache-control': 'no-cache',
 'connection': 'close',
 'date': 'Fri, 02 May 2014 17:32:17 GMT',
 'pragma': 'no-cache',
 'server': 'hp3par-wsapi',
 'status': '200'}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:166
2014-05-02 17:31:23.888 22459 DEBUG hp3parclient.http [-] RESP BODY:
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:167
2014-05-02 17:31:23.889 22459 DEBUG cinder.volume.drivers.san.hp.hp_3par_common [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Disconnect from 3PAR client_logout /usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py:201
2014-05-02 17:31:23.890 22459 DEBUG cinder.openstack.common.lockutils [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Released file lock ""3par"" at /var/lib/cinder/tmp/cinder-3par for method ""terminate_connection""... inner /usr/lib/python2.6/site-packages/cinder/openstack/common/lockutils.py:239
2014-05-02 17:31:23.891 22459 ERROR cinder.openstack.common.rpc.amqp [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Exception during message handling
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 819, in wrapper
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 658, in terminate_connection
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     self.driver.terminate_connection(volume_ref, connector, force=force)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 233, in terminate_connection
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     wwn=connector['wwpns'])
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1099, in terminate_connection
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     self.delete_vlun(volume, hostname)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 546, in delete_vlun
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     self._delete_3par_host(hostname)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 408, in _delete_3par_host
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     self.client.deleteHost(hostname)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/hp3parclient/client.py"", line 388, in deleteHost
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     reponse, body = self.http.delete('/hosts/%s' % name)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 327, in delete
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     return self._cs_request(url, 'DELETE', **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 231, in _cs_request
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 205, in _time_request
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     resp, body = self.request(url, method, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 199, in request
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     raise exceptions.from_response(resp, body)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp HTTPConflict: Conflict (HTTP 409) 77 - host is a member of a set
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp
2014-05-02 17:31:23.897 22459 ERROR cinder.openstack.common.rpc.common [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Returning exception Conflict (HTTP 409) 77 - host is a member of a set to caller"
1305,1317180,nova,40a790c32ee4ea7b9d79e259545f07409fd618fa,1,1,“The following patch https://github.com/openstack/nova/commit/4c2f36bfe006cb0ef89ca7a706223f30488a182e#diff-5c6ee11140977e63b54542e2ff5763d3R22 caused a”,Hyper-v fails to attach volumes when using v1 volu...,"The following patch https://github.com/openstack/nova/commit/4c2f36bfe006cb0ef89ca7a706223f30488a182e#diff-5c6ee11140977e63b54542e2ff5763d3R22 caused a regression by changing the eventlet.subprocess.Popen with the builtin subprocess.Popen (by using the nova.utils execute method) without changing the way the args were parsed.
In this module, the execution args were parsed separated by whitespaces, which is not allowed by the builtin subprocess.Popen, causing a ""not found"" error. This error is returned for example when attaching a volume, at the point where iscsicli tool is used to login the iSCSI target or portal.
Trace:
http://paste.openstack.org/show/79418/"
1306,1317208,nova,1e2b7fbf73a9d6b9088396cd41091911d35279ef,1,1,,Xen live-migrate with volume attached ends with FI...,"The relevant portion of an example stack trace is below:
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packa
ges/nova/virt/xenapi/vmops.py"", line 2316, in attach_block_device_volumes
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     volume_utils.forget_sr(self._session, sr_uuid_map[sr_re
f])
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/vmops.py"", line 2307, in attach_block_device_volumes
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     hotplug=False)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/volumeops.py"", line 53, in attach_volume
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     vm_ref = vm_utils.vm_ref_or_raise(self._session, instance_name)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/vm_utils.py"", line 2661, in vm_ref_or_raise
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     vm_ref = lookup(session, instance_name)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/vm_utils.py"", line 1743, in lookup
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     vm_refs = session.call_xenapi(""VM.get_by_name_label"", name_label)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/client/session.py"", line 179, in call_xenapi
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     return session.xenapi_request(method, args)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/XenAPI.py"", line 133, in xenapi_request
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     result = _parse_result(getattr(self, methodname)(*full_params))
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/XenAPI.py"", line 203, in _parse_result
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     raise Failure(result['ErrorDescription'])
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher Failure: ['FIELD_TYPE_ERROR', 'label']
attach_block_device_volumes() in nova/virt/xenapi/vmops.py calls attach_volume() in nova/virt/xenapi/volumops.py and passes None for instance_name,  and this causes a failure when looking up the vm_ref."
1307,1317257,swift,b61fce6cbabf2181fed3f0c4bb83a2d3c40db100,1,1,,container auditor blows up if there's a file in de...,"On your saio if you `touch /srv/node1/asdf` and run the container-auditor you get an uncaught exception:
container-6011: UNCAUGHT EXCEPTION#012Traceback (most recent call last):#012  File ""/usr/local/bin/swift-container-auditor"", line 7, in <module>#012    execfile(__file__)#012  File ""/vagrant/swift/bin/swift-container-auditor"", line 23, in <module>#012    run_daemon(ContainerAuditor, conf_file, **options)#012  File ""/vagrant/swift/swift/common/daemon.py"", line 110, in run_daemon#012    klass(conf).run(once=once, **kwargs)#012  File ""/vagrant/swift/swift/common/daemon.py"", line 55, in run#012    self.run_once(**kwargs)#012  File ""/vagrant/swift/swift/container/auditor.py"", line 99, in run_once#012    self._one_audit_pass(reported)#012  File ""/vagrant/swift/swift/container/auditor.py"", line 54, in _one_audit_pass#012    for path, device, partition in all_locs:#012  File ""/vagrant/swift/swift/common/utils.py"", line 1756, in audit_location_generator#012    partitions = listdir(datadir_path)#012  File ""/vagrant/swift/swift/common/utils.py"", line 2195, in listdir#012    return os.listdir(path)#012OSError: [Errno 20] Not a directory: '/srv/node1/asdf/containers'
This may only happen if mount_check is false, otherwise it's probably skipped earlier.  Still, we should probably just skip over files in the devices root."
1308,1317321,nova,524ee4913d9015c2893cca69cefbc80fbe6878bb,0,0,Bug in test,Add extra unit test case for more than 1 ephemeral...,"There was a comment on https://review.openstack.org/#/c/90583/ (after it was approved for merge) to add an extra test case to handle more than 1 ephemeral disk, as well as correct a comment in the code to be more accurate.  The purpose of this bug is to add the extra test case for _get_instance_block_device_info to test more than 1 ephemeral disk and correct one of the comments in _get_instance_block_device_info."
1309,1317573,cinder,a058e90c5ec33e35c3bc88fed644b598e490e9ad,1,1,,Bug #1317573 “logger,"failure to mount gluster storage appears in the cinder volume log as a warning and not an error
since we would basically not be able to create/delete/attach when we fail to mount I think that this should be logged as ERROR and if possible print the trace in the log.
2014-05-08 18:54:45.306 3121 WARNING cinder.volume.drivers.glusterfs [-] Exception during mounting Unexpected error while running command.
Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf mount -t glusterfs 10.35.64.104:/gluster2-cinder-tshefi /var/lib/cinder/mnt/8f2d3277bf483ae9bae356bd866fed5e
Exit code: 1
Stdout: 'Mount failed. Please check the log file for more details.\n'
Stderr: ''"
1310,1317654,nova,23790183d20f59ddcb6025e8497aa16e443bb929,1,1,,default dhcp lease time of 120s is too short,The default dhcp lease time is fairly short (120s). Is this simply a hold-over from the time before force_dhcp_release was the default and dhcp_release was expected throughout or is there another reason for the default to be so brief?
1311,1317804,nova,59a6cf233b538d6666740de4796fce25ed8265aa,1,0,"""The change to use InstanceActionEvent in compute.utils.EventReporter changed the order of how things are done. “",InstanceActionEvent traceback parameter not serial...,"The change to use InstanceActionEvent objects in compute.utils.EventReporter changed the order of how things are done. Before, traceback info were converted to strings before being sent to the conductor. Now, since the object method being used remotes itself, the order becomes the opposite and any captured tracebacks are sent as is, resulting in errors during messaging.
See http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVmFsdWVFcnJvcjogQ2lyY3VsYXIgcmVmZXJlbmNlIGRldGVjdGVkXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjkwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzOTk2MjYzMjYwODZ9"
1313,1317871,neutron,d7318ff7123ea851e9d6953ef2c66f793ae46ca1,1,1,,Bug #1317871 “LBAAS ,"Steps to Reproduce:
Setup onto ICEHouse GA:
Build :2014.1-0ubuntu1~cloud0
Steps to Reproduce:
1. Create a lbaas site. (Pool,member,vip,health monitor)
2. Check the configuration onto the network node under
/var/lib/neutron/lbaas/$Pool_id/
conf  pid  sock
3. Delete all the lbaas and neutron resource created onto the controller.
4. Check the configuration onto the network node.
Actual Results: Directory structure for the pool id remain as its but the files got delete
Expected Results: all the configuration and files/folder should get deleted after deletion of lbaas resource.
Note: Working in Havana GA release."
1314,1317935,neutron,ce59e63249dfcd86e782d056407e38e845cbe19c,1,1,,Neutron does not follow the RFC 3442 spec for DHCP...,"When metadata networking is enabled, neutron will generate a dnsmasq configuration that includes classless routes.
The RFC states that those routes should include the default route if they are defined, and that the DHCP client should ignore the default router.
This causes standards-abiding DHCP clients like dhcpcd to not provide a default gateway when running on nova.
At https://github.com/openstack/neutron/pull/22 you can see an example of how to fix this."
1315,1318104,nova,1236b09076cca3b4b16538b055e52edde5a4feea,1,1,,dhcp isolation via iptables does not work,Attempting to block iptables across the bridge via iptables rules is not working. The iptables rules are never hit. blocking dhcp traffic from exiting the node will need to use ebtables instead.
1316,1318108,cinder,a7f24f0a2395594ead7eb8a5cef894b257cd7e4f,1,0,"Change in requirements “However, SolarisISCSIDriver
    still called the _execute function, which no longer exists in it's
    parent classes.”",SolarisISCSIDriver fails on _execute,"A previous refactor [1] of SolarisISCSIDriver and SanDriver renamed
the `_execute` function to `san_execute`. However, SolarisISCSIDriver
still called the _execute function, which no longer exists in it's
parent classes.
[1] https://review.openstack.org/#/c/38194/"
1317,1318261,neutron,3f9658dcd9b2cccdc0f03088723305c980130cd3,1,1,,Bug #1318261 “OVS Agent,"When neutron Openswitch agent runs with l2pop ON, it is not removing flows from table 0 of br-tun, those flow sthat carry cleaned up tunnel ports by l2-pop.
Impact of this bug:  Explosion of flow rules in br-tun table 0 as tunnel ports come-in and go-away in a scaled environment"
1319,1318891,nova,c75cd9a8b9da86b9d9e7ffd6512fe13b1913fd85,1,1,,detach_pci_devices failed,"when detach a pci device from instance
the method  _detach_pci_devices will check if pci device detached
>      for hdev in [d for d in guest_config.devices
>                        if d.type == 'pci']:
guest_config.devices will have more device not only pci, like disk
in LibvirtConfigGuestDisk has no attribute type
  File ""/home/xiaoding/nova/nova/tests/virt/libvirt/test_libvirt.py"", line 684, in test_detach_pci_devices
    conn._detach_pci_devices(FakeDomain(), pci_devices)
  File ""/home/xiaoding/nova/nova/virt/libvirt/driver.py"", line 2774, in _detach_pci_devices
    if d.type == 'pci']:
AttributeError: 'LibvirtConfigGuestDisk' object has no attribute 'type'
https://review.openstack.org/#/c/93383/"
1320,1319180,nova,de92d65f2981b826e145e82ab2734c19bf1dffa3,1,1,“I am not sure that this is a bug. The default value is None.”,force_config_drive cannot be set to False from nov...,"force_config_drive is not cast to boolean, so the only way to disable this option is not to have it in nova.conf. If it is set to 'false' it is treated as a string and evaluates to true in configdrive.required_by()"
1321,1319182,nova,8ff170dc95bf3101fe38a2624e941bfa3b7c1138,1,1,,Pausing a rescued instance should be impossible,"In the following commands, 'vmtest' is a freshly created virtual machine.
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state                | -
| status                               | ACTIVE
$ nova rescue vmtest
+-----------+--------------+
| Property  | Value
+-----------+--------------+
| adminPass | 2ZxvzZULT4sr
+-----------+--------------+
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state                | -
| status                               | RESCUE
$ nova pause vmtest
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state                | -
| status                               | PAUSED
$ nova unpause vmtest
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state                | -
| status                               | ACTIVE
Here, we would want the vm to be in the 'RESCUE' state, as it was before being paused.
$ nova unrescue vmtest
ERROR (Conflict): Cannot 'unrescue' while instance is in vm_state active (HTTP 409) (Request-ID: req-34b8004d-b072-4328-bbf9-29152bd4c34f)
The 'unrescue' command fails, which seems to confirm that the VM was no longer being rescued.
So, two possibilities:
1) When unpausing, the vm should go back to 'rescued' state
2) Rescued vms should not be allowed to be paused, as is indicated by this graph: http://docs.openstack.org/developer/nova/devref/vmstates.html
Note that the same issue can be observed with suspend/resume instead of pause/unpause, and probably other commands as well.
WDYT ?"
1322,1319361,neutron,f27e5244adcac9a12c04605fbf5c49a086d5fce5,1,1,,Bug #1319361 “Lbaas,"Steps to Reproduce:
1. Create network, subnet
2.Create pool,member vip  and healthmonitor.
neutron lb-pool-list
+--------------------------------------+-------+----------+-------------+----------+----------------+--------+
| id                                   | name  | provider | lb_method   | protocol | admin_state_up | status |
+--------------------------------------+-------+----------+-------------+----------+----------------+--------+
| d488b38e-c8f1-4db6-bb5f-8b2500bef0ed | pool1 | haproxy  | ROUND_ROBIN | HTTP     | True           | ACTIVE |
+--------------------------------------+-------+----------+-------------+----------+----------------+--------+
neutron lb-member-list
+--------------------------------------+-----------+---------------+----------------+--------+
| id                                   | address   | protocol_port | admin_state_up | status |
+--------------------------------------+-----------+---------------+----------------+--------+
| 303f3d66-c51c-4c92-b5da-d43440db9fb2 | 10.10.1.5 |            80 | True           | ACTIVE |
| ac58985b-49de-480e-97e5-bff80a538c87 | 10.10.1.4 |            80 | True           | ACTIVE |
+--------------------------------------+-----------+---------------+----------------+--------+
4. Validate the algorithm and try to send request from server to client .
5. Check the lp-pool-stats for the pool created.
Actual Results:
neutron lb-pool-stats pool1
+--------------------+-------+
| Field              | Value |
+--------------------+-------+
| active_connections | 0     |
| bytes_in           | 1440  |
| bytes_out          | 560   |
| total_connections  | 0     |
+--------------------+-------+
Connections in response for this command always show 0
Expected Results: Active connection should show the value when the connection is active and total connenction should show total number of active connections"
1323,1319615,nova,822f3069cd485215e863bc142b0e05d7bd1d52e6,0,0,"Refactoring, future bug “We should avoid to assign builtin function with other value.”",Don't shadow builtin function type,We should avoid to assign builtin function with other value. That case will lead calling type failure in other place.
1324,1319643,cinder,6791fa41e06beab23bc7832a3bfa9ab28adf1e34,1,1,,Using random.random() should not be used to genera...,"In cinder code : /cinder/transfer/api.py . Below line of code used random.random() to generate a random number, Standard random number generators should not be used to generate randomness used for security reasons. Could we use a crytographic randomness generator to provide sufficient entropy to instead of it?
rndstr = """"
random.seed(datetime.datetime.now().microsecond)
while len(rndstr) < length:
 rndstr += hashlib.sha224(str(random.random())).hexdigest()   ---------------> This line has described issues.
 return rndstr[0:length]"
1325,1319892,neutron,3190e5b672ac21f9c82110d319473f2041bf64f7,1,1,,Change of default netpartition behavior in nuage p...,"Current behavior of nuage plugin assumes a clean slate on the VSD (back-end controller) as part of the neutron startup. There are use-cases where this assumption is invalid. Also, there could exist non default net-partitions on the VSD for which behavior is not the same as for default net-partition. This needs to be fixed."
1326,1319943,nova,5dda3a6ab2becb5dd0b58c088f6daad807e12276,1,1,,libvirt driver's to_xml method logs iscsi auth_pas...,If you have debug logging enabled the libvirt driver's to_xml method logs the iscsi auth_password in plain text.
1327,1320050,cinder,1eda138be81b2405969b80c00f30ba237e250fcd,0,0,"Feature “Brocade FC SAN lookup service should allow customized hosts key and missing policy""",Brocade FC SAN lookup service should allow customi...,"In the BrcdFCSanLookupService, when initialize. The ssh client should be allowed to load customized host key file instead of only load from the OS host key file. Also for the host key missing policy, the code should also allow to be customized by parsing the kwargs instead of hard-code the ""missing policy"". The ""MIssing Policy"" will not stop the man in the middle attack if the known_hosts is not a match, it should allow customized policy being configured in different scenarios to fit the security need."
1328,1320056,cinder,39081787963298be87c1941ab6cba5310f6261c6,0,0,Feature “Cinder utils SSHPool should allow customized ssh host keys and missing policy”,Cinder utils SSHPool should allow customized ssh h...,"In cinder/utils.py, SSHPool is using paramiko.AutoAddPolicy() as default. This may lead security issue without being notified. The utility should allow customized usage when create the pool or session. Also the host_keys file should be allowed to be customized so that any driver utilizing the SSHPool should have their customized security setting or delegate to customer's scenario & configuration to determine the policy and key files."
1329,1320628,nova,cc5388bbe81aba635fb757e202d860aeed98f3e8,1,1,,Double powering-off state confuses the clients and...,"http://logs.openstack.org/52/73152/8/check/check-tempest-dsvm-full/9352c04/console.html.gz#_2014-05-13_18_12_38_547
At client side the only way to know an instance action is doable is to making sure the status  is a permanent status like ACTIVE or SHUTOFF and no action in progress, so the task-state is None.
In the above linked case tempest stopped the instance and the instance reached the  ""SHUTOFF/None"".
'State transition ""ACTIVE/powering-off"" ==> ""SHUTOFF/None"" after 10 second wait'
Cool, at this time we can start the instance right ? No, other attribute needs to be checked.
The start attempt was rewarded with 409 :
 u'Instance 7bc9de3b-1960-476f-b964-2ab2da986ec7 in task_state powering-off. Cannot start while the instance is in this  state'
The below line indicates the  task state, silently moved back to ""SHUTOFF/powering-off""  , before the 'start' attempt.
2014-05-13 18:09:13,610 State transition ""SHUTOFF/powering-off"" ==> ""SHUTOFF/None"" after 1 second wait
Please do not set the 'None' task-state when the 'powering-off' is not finished."
1330,1320774,neutron,0ff8536c0d9b95172c42b1a01d292df58ed2e573,0,0,Bug in test,Non-existent assertion methods being called on moc...,"There are various places in the unit tests where non-existent ""assert"" methods are called on mock objects.
e.g. assert_called_once(), assert_called_twice(), assert_called_one_with(), assert_called_twice()
To reproduce, put the mock.py file in the base neutron directory and edit the __getattr__ methods to check if the name begins with 'assert' and raise an exception if true."
1331,1320775,neutron,6167cb55e2f62a645487d66e52b809c9599b3bb8,1,1,,Bug #1320775 “fwaas,"Steps to Reproduce:
                                   1. create two network connected to the router and each network having a VM
                                   2. create firewall rule of icmp deny
                                   3. attach the firewall rule to the policy
                                   4. Create firewall with that policy and check that firewall is active
                                   5. Try to ping from one vm to another vm.
Actual Results:
                                VM is able to ping even though firewall is active. However the ping fails as expected after creating external gateway to the router.
Expected Results:
                                It should fail since the firewall is active"
1332,1321080,neutron,bb4f44654f6765c4e1fbcf92375c273494151099,1,1,,[OSSA 2014-021] auth token is exposed in meter htt...,"auth token is exposed in meter http.request
# curl -i -X GET -H 'X-Auth-Token: 258ab6539b3b4eae8b3af307b8f5eadd' -H 'Content-Type: application/json' -H 'Accept: application/json' -H 'User-Agent: python-ceilometerclient' http://0.0.0.0:8777/v2/meters/http.request
-----------
snip..
{""counter_name"": ""http.request"", ""user_id"": ""0"", ""resource_id"": ""ip-9-37-74-33:8774"", ""timestamp"": ""2014-05-16T17:42:16.851000"", ""recorded_at"": ""2014-05-16T17:42:17.039000"", ""resource_metadata"": {""request.CADF_EVENT:initiator:host:address"": ""9.44.143.6"", ""request.CADF_EVENT:initiator:credential:token"": ""4724 xxxxxxxx 8478"", ""request.RAW_PATH_INFO"": ""/v2/9af97e383dad44969bd650ebd55edfe0/servers/060c76a5-0031-430d-aa1e-01f9b3db234b"", ""request.REQUEST_METHOD"": ""DELETE"", ""event_type"": ""http.request"", ""request.HTTP_X_TENANT_ID"": ""9af97e383dad44969bd650ebd55edfe0"", ""request.CADF_EVENT:typeURI"": ""http://schemas.dmtf.org/cloud/audit/1.0/event"", ""request.HTTP_X_PROJECT_NAME"": ""ibm-default"", ""host"": ""nova-api"", ""request.SERVER_PORT"": ""8774"", ""request.REMOTE_PORT"": ""55258"", ""request.HTTP_X_USER_ID"": ""0"", ""request.HTTP_X_AUTH_TOKEN"": ""4724d3dd6b984079a58eecf406298478"", ""request.CADF_EVENT:action"": ""delete"", ""request.CADF_EVENT:target:typeURI"": ""service/compute/servers/server"", ""request.HTTP_USER_AGENT"": ""Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Firefox/24.0"",
snip...
auth token is masked in ""request.CADF_EVENT:initiator:credential:token"": ""4724 xxxxxxxx 8478"".
But it is exposed in  ""request.HTTP_X_AUTH_TOKEN"": ""4724d3dd6b984079a58eecf406298478"""
1333,1321220,nova,37520a7dc14971b2d244f37febdb9fb13edbfd2f,1,1,,[EC2] StartInstance response missing instanceset i...,"Startinstance response elements shown as below:
""<StartInstancesResponse xmlns=""""http://ec2.amazonaws.com/doc/2013-10-15/"""">
  <requestId>req-5970ccd7-c763-456c-89f0-5b55ea18880b</requestId>
  <return>true</return>
</StartInstancesResponse>
""
But as per the AWS API reference doc, the response elements shown be as below:
==
<StartInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2014-02-01/"">
  <requestId>59dbff89-35bd-4eac-99ed-be587EXAMPLE</requestId>
  <instancesSet>
    <item>
      <instanceId>i-10a64379</instanceId>
      <currentState>
          <code>0</code>
          <name>pending</name>
      </currentState>
      <previousState>
          <code>80</code>
          <name>stopped</name>
      </previousState>
    </item>
  </instancesSet>
</StartInstancesResponse>
===
here, <instanceSet> information missing in the response elements."
1334,1321239,nova,502fa4875a3975990cbdf84fc0f846f7ede8fa92,1,1,“The <instanceSet> information missing in the response elements.”,[EC2] StopInstance response missing instanceset in...,"Stoptinstance response elements shown as below:
Sample Request to stop the specified instance:
===
https://ec2.amazonaws.com/?Action=StopInstances
&InstanceId.1=i-10a64379
&AUTHPARAMS
==
Response elements are:
==
"":<StopInstancesResponse xmlns=""""http://ec2.amazonaws.com/doc/2013-10-15/"""">
  <requestId>req-30edb813-5802-4fa2-8a83-9dbcb751264e</requestId>
  <return>true</return>
</StopInstancesResponse>
""
But as per the AWS API reference doc, the response elements shown be as below:
==
<StopInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2014-02-01/"">
  <requestId>59dbff89-35bd-4eac-99ed-be587EXAMPLE</requestId>
  <instancesSet>
    <item>
      <instanceId>i-10a64379</instanceId>
      <currentState>
          <code>64</code>
          <name>stopping</name>
      </currentState>
      <previousState>
          <code>16</code>
          <name>running</name>
      </previousState>
  </instancesSet>
</StopInstancesResponse>
===
The <instanceSet> information missing in the response elements."
1335,1321298,nova,6dd5cc503cc05c00c5f9d831480539c67f6e2a48,1,1,,Periodic task cause errors in _finish_resize,"In the event that an end user sets resize_confirm_window to something small (say 1 in this example) there is a possibility that the periodic task can run in nova/compute/manager.py:ComputeManager._finish_resize() after the migration has been updated but before the instances has been updated.
http://git.openstack.org/cgit/openstack/nova/tree/nova/compute/manager.py#n3570
One possible solution to this would be to reverse the order, and update the instance before updating the migration, in which case the migration will get updated in _confirm_resize: http://git.openstack.org/cgit/openstack/nova/tree/nova/compute/manager.py#n5018"
1336,1321334,nova,310231541661adaa30d92d357f7590c88dc905b3,1,1,,Non bootable volumes can be used as a bootable sou...,"The following command results in a successful vm creation:
nova boot --boot-volume <non-bootable-volume-id> --flavor <test-flavor> <test-name>
we should validate that the --boot-volume passed is actually a bootable volume and if it's not we should return an error explaining what went wrong"
1337,1321352,neutron,0f877f2594d415513856af3c528275fce2228ac1,1,1,,Nova notification introduces a hard dependency on ...,The nova notification patch introduces a hard dependency on novaclient when it is a runtime-configurable dependency. The import from novaclient should be conditional on the appropriate nova notification options being enabled in the config.
1338,1321381,nova,803d59e9f409a59403b2ce55040acc02e14eee28,1,1,“Commit a141206e9dfd31955b9b31d9e5a7f73bbd8510ca ensured that user defined ports were not deleted when an instance is deleted or rescheduled. The problem with this commit is that it did not reset the 'device_id' and ‘device_owner' of the port.”,Bug #1321381 “VMware,"On Havana, when an instance errors out and gets rescheduled, attached ports are currently not being cleaned up and is causing NVS/OVS to have multiple ports with the same UUID and MAC.
This is currently occurring with the VC Driver + Neutron and is blocking VMware Minesweeper CI."
1339,1321640,nova,513c6bbd36563e57a85d33f9c94f4a20ab7c00f4,1,1,,Bug #1321640 “[HyperV],"If we use config-drive (whether set --config-drive=true in boot command or set force_config_drive=always in nova.conf), there is bug for config-drive when resize or migrate instances on hyperv.
You can see from current nova codes:
https://github.com/openstack/nova/blob/master/nova/virt/hyperv/migrationops.py#L269
when finished migration, there is no code to attach configdrive.iso or configdrive.vhd to the resized instance. compared to boot instance (https://github.com/openstack/nova/blob/master/nova/virt/hyperv/vmops.py#L226). Although this commit https://review.openstack.org/#/c/55975/ handled coping configdrive to resized or migrated instance, there is no code to attach it after resized or migrated."
1340,1321653,nova,9e8657839844ecef3348dad51aca6411f05da99d,1,1,,Got 500  when adding list type host to an aggregat...,"Steps to reproduce as admin:
1. create an aggregate  (ID 1)
$ nova aggregate-create foo
2. curl -i ""http://127.0.0.1:8774/v2/""`keystone token-get | awk '/ tenant_id /{print $4}'`""/os-aggregates/1/action"" -X POST -H ""Content-Type: application/json"" -H ""X-Auth-Token: ""`keystone token-get | awk '/ id /{print $4}'` -d '{""add_host"": {""host"": [""host-2"", ""host-1""]}}
HTTP/1.1 500 Internal Server Error
Content-Length: 128
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-f6ea35a8-029a-444a-9741-7c6abd27f294
Date: Wed, 21 May 2014 08:34:57 GMT
{""computeFault"": {""message"": ""The server has either erred or is incapable of performing the requested operation."", ""code"": 500}
Expected solution:
A:  Response with  400(Bad Request) when the ""host"" value is not the expected type.
B:  Add multiple hosts by  a single request"
1341,1321864,neutron,52301e4727091f867c42b18b316d4c4aacffea31,1,1,,allowed address pairs - unnecessary and incomplete...,"Current code intends to disallow assigning a fixed ip to a port when that ip
overlaps with one of the addresses in the allowed-address-pairs list. However, it is an unnecessary check and also the current code does not enforce it in all cases.
Cases where it enforces:
1) A port-update with allowed-address-pairs list containing an IP address which is *exactly* same as one of the fixed-ips on the port. For example, for a port with fixed-ip of 10.10.8.6, the following fails:
$> neutron port-update  58062310-ee5a-4b14-b554-5df699064bc9 --allowed-address-pairs type=dict list=true ip_address=10.10.8.6
400-{u'NeutronError': {u'message': u""Port's Fixed IP and Mac Address match an address pair entry."", u'type': u'AddressPairMatchesPortFixedIPAndMac', u'detail': u''}}
2) A port-update with a fixed-ip which is exactly same as one of the allowed
IP addresses in the allowed-address-pairs list. For example, for a port
with allowed-address-pairs with """", the following fails:
$> neutron port-show 58062310-ee5a-4b14-b554-5df699064bc9 | grep allowed
| allowed_address_pairs | {""ip_address"": ""10.10.8.6"", ""mac_address"": ""fa:16:3e:7f:3c:06""}                    |
$> neutron port-update 58062310-ee5a-4b14-b554-5df699064bc9 -- --fixed-ips type=dict list=true ip_address=10.10.8.6
400-{u'NeutronError': {u'message': u""Port's Fixed IP and Mac Address match an address pair entry."", u'type': u'AddressPairMatchesPortFixedIPAndMac', u'detail': u''}}
However, allowed-address-pairs can work with IP CIDRs and the overlap check
is *not* properly enforced when IP addresses are specified in the CIDR notation.
Case where the current code is incomplete:
Same as case (1) above but IP address specified in cidr notation. In this case, the code does not check for overlaps.
$> neutron port-update  58062310-ee5a-4b14-b554-5df699064bc9 --allowed-address-pairs type=dict list=true ip_address=10.10.8.6/32
Updated port: 58062310-ee5a-4b14-b554-5df699064bc9
$> neutron port-show 58062310-ee5a-4b14-b554-5df699064bc9 | grep allowed
| allowed_address_pairs | {""ip_address"": ""10.10.8.6/32"", ""mac_address"": ""fa:16:3e:7f:3c:06""}                 |
Functionally, it is incorrect to allow overlaps in one type of inputs and not allow in other types of input.
On the other hand, if we fix this bug and check overlaps in all cases, then the API will become hard to use. For example, if a fixed IP 10.10.1.1 exists on a port and we want to allow addresses in 10.10.1.0/24 cidr on that port, then one has to configure a list of 8 cidrs ([10.10.1.0/32, 10.10.1.2/31,
10.10.1.4/30, ..., 10.10.1.128/25]) on the allowed-address-pairs.
In any case, this is an unnecessary check as the overlap does not have any negative effect. Allowed-address-pairs is ADDING on to what is allowed because of the fixed IPs. So, there is no possibility of conflict. The check will probably make sense if we are maintaining denied addresses (instead of allowed addresses).
My suggestion is to remove this check entirely.
https://review.openstack.org/#/c/94508/"
1342,1321872,nova,e0228eb7e739956978f002f58edf35bc4d698022,1,1,,Resize/migrate stopped instance fails with Neutron...,"I originally thought this was bug 1306342 but that's a different issue, that was when not having neutron configured properly for calling back to nova and nova would timeout on spawn waiting for a notification from neutron that networking was setup.
This is a different issue where resize/migrate fails if you started from a stopped instance and using neutron.  In this case, the _create_domain_and_network method in the libvirt driver passes in power_on=False since the instance was stopped before the resize/migration.  The virtual interface isn't plugged in that case so we're waiting on a neutron event that's not going to happen, and we hit the eventlet timeout which then tries to destroy the non-running domain, and that fails with a libvirtError telling you that the domain isn't running in the first place.
The fix is to check the power_on flag in _create_domain_and_network and if it's False, don't wait for neutron events, same as if vifs_already_plugged=False is passed in."
1343,1322025,nova,e15ce7735e492f9eb0914efb621211e315ea40d9,1,1,,[EC2] Terminateinstance returns incorrect current ...,"TerminateInstance returns the currentstate name and previousstate name are same.
In the below sample response elements show the currnentstate name and previoustate name as ""running"".
Ideally the currentstate name should be ""terminated"".
==
<TerminateInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2013-10-15/"">
  <requestId>req-c15f5c7d-2551-4a08-b8b8-255462a09592</requestId>
  <instancesSet>
    <item>
      <instanceId>i-00000001</instanceId>
      <currentState>
        <code>16</code>
        <name>running</name>
      </currentState>
      <previousState>
        <code>16</code>
        <name>running</name>
      </previousState>
    </item>
  </instancesSet>
</TerminateInstancesResponse>
=="
1344,1322076,neutron,09dd5eb9c81033fe83b10d4051b2f06125250185,0,0,"Resfactoring ""fwaas plugin doesn't need to handle delete firewall rule operation
“",fwaas plugin doesn't need to handle delete firewal...,"If firewall rule is attached to firewall policy, it would raise FirewallRuleInUse excpetion in DB ops, else it is a pure DB delete ops. So it is useless to handle delete_firewall_rule ops in fwaas plugin."
1345,1322105,neutron,c0c3c8361771091c1ef1d4906c2552f9d92e7715,1,1,,NVP FWaaS occurs error when removing a rule which ...,"Bugs reproduce process:
1. create a firewall rule and attache it to a firewall policy
2. create two firewalls with the firewall policy attached alternatively on two routers
3. remove the firewall rule from the firewall policy
it would occur the following error:
 Traceback (most recent call last):
  File ""/home/stack/neutron/neutron/api/v2/resource.py"", line 87, in resource
    result = method(request=request, **args)
  File ""/home/stack/neutron/neutron/api/v2/base.py"", line 201, in _handle_action
    return getattr(self._plugin, name)(*arg_list, **kwargs)
  File ""/home/stack/neutron/neutron/plugins/vmware/plugins/service.py"", line 1077, in remove_rule
    context, fwr['id'], edge_id)
  File ""/home/stack/neutron/neutron/plugins/vmware/vshield/edge_firewall_driver.py"", line 270, in delete_firewall_rule
    vcns_rule_id = rule_map.rule_vseid
AttributeError: 'NoneType' object has no attribute 'rule_vseid'
2014-05-22 16:21:22,244     INFO [neutron.plugins.vmware.vshield.tasks.tasks] TaskManager terminated
}}}
Traceback (most recent call last):
  File ""/home/stack/neutron/neutron/tests/unit/vmware/vshield/test_fwaas_plugin.py"", line 650, in test_remove_rule_with_firewalls
    expected_body=attrs)
  File ""/home/stack/neutron/neutron/tests/unit/db/firewall/test_db_firewall.py"", line 295, in _rule_action
    self.assertEqual(res.status_int, expected_code)
  File ""/home/stack/neutron/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 322, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/stack/neutron/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 412, in assertThat
    raise MismatchError(matchee, matcher, mismatch, verbose)
MismatchError: 500 != 200
It is because when deleting the corresponding vcns_edge_firewallrule_binding entry, it query based on id instead of (edge_id, id) which leads to deleting the other rule_binding entry."
1346,1322139,neutron,be340d1bab015c47650687f97393c6c9015fb537,1,1,“fix openvswitch requirement check” (b53c094d83df881156aa646002f95c9643b1b7a5),VXLAN kernel requirement check for openvswitch age...,"on RHEL7 beta agent set to use VXLAN tunneling does not start.
I'm using rdo packages, if you want I can check the upstream version somewhere, but seems that code that checks the version is still in github.
in openvswitch-agent.log I see:
2014-05-21 13:53:21.762 1814 ERROR neutron.plugins.openvswitch.agent.ovs_neutron_agent [req-ce4eadcb-4cbb-4c09-a404-98ecb5383fa5 None] Agent terminated
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 231, in _check_ovs_version
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     ovs_lib.check_ovs_vxlan_version(self.root_helper)
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 551, in check_ovs_vxlan_version
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     'kernel', 'VXLAN')
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 529, in _compare_installed_and_required_version
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     raise SystemError(msg)
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent SystemError: Unable to determine kernel version for Open vSwitch with VXLAN support. To use VXLAN tunnels with OVS, please ensure that the version is 1.10 or newer!
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent
It seems that the minimum kernel version required to use VXLAN is set to 3.13 (shouldn't 3.9 be enough ?). RHEL7 ships only 3.10. Vxlan module however is present and working, and even if the only module properly working is in 3.13 kernel, the check doesn't take into consideration backported features."
1347,1322180,nova,00ac56a100ac7329a01b5f1fa336b99e64778eaf,1,0,"""Legacy bdm in incoming parameters contains a number of devices specified by their names (e.g. ""vdb"" or “/dev/vdc""). Current code considers ""vda"" as root device and doesn't work on “/dev/vda”""",Fail to launch an instance from volume by legacy b...,"Launting an instance from bootable volume passing legacy bdm is available only using vda (no /dev/ prefix) as root device name. This is weird restriction. It prevents to create consistent instance data, because root_device_name instance attribute has /dev/ prefix, but device_name bdm attribute doesn't.
Environment: DevStack
Steps to reproduce:
1 Create bootable volume
$ cinder create --image-id xxx 1
Note: I used cirros-0.3.2-x86_64-uec ami image.
2 Boot instance from the volume by legacy bdm.
$ nova boot --flavor m1.nano --block-device-mapping /dev/vda=yyy:::1 inst
3 Wait instance status Active, go to instance console and look to 'No bootable device' message.
The reason is in _get_bdm_image_metadata in nova/compute/api.py. There only 'vda' devices are processed as root device for legacy bdm."
1348,1322195,nova,a55f41492e5ce9bbc2f2ef3435a7e7e65bf6cb3e,1,1,,Admin creates volume backed instance snapshot in i...,"For instance booted from volume with legacy bdm and image (this method is documented as workaround in http://docs.openstack.org/grizzly/openstack-ops/content/attach_block_storage.html) admin user creates instance snapshot in the image tenant rather than current tenant.
Created snapshot cannot be used.
Environment: DevStack
Steps to reproduce:
1 Create bootable volume from public image from not current tenant.
For example, use demo tenant in DevStack.
$ cinder create --image-id xxx 1
Note: I used cirros-0.3.2-x86_64-uec ami image.
2 Boot an instance from the volume passing the original image.
$ nova boot --flavor m1.nano --image xxx --block-device-mapping /dev/vda=yyy inst
3 Create instance snapshot under admin user
$ nova image-create inst snap
4 List images and make sure there is no the created snapshot.
$ glance image-list
5 List images from the original image tenant and found the snapshot.
$ glance --os-tenant-name nnn image-list
snapshot_volume_backed in nova/compute/api.py receives image in image_meta parameter, cleans some attributes, but forgets to deal something with owner attribute."
1349,1322379,cinder,bfbc29f19037feb89408206625849bde14a7e84a,1,1,,NetApp netapp_server_port config is ignored,"The netapp_server_port setting in cinder.conf is ignored by both the NetApp NFS and iSCSI drivers. Ironically it is marked as a required flag.
NetAppDirectNfsDriver._get_client() creates a client from NaServer but no port argument is passed to init(). NaServer.set_transport_type() sets the port based on the configured transport_type (URI scheme), http=80 and https=443. netapp_server_port appears to be largely unused.
A quick work-around is to set the client port from self.configuration.netapp_server_port in NetAppDirectNfsDriver._get_client() immediately after creating the client. This may not be the cleanest solution but it solves the immediate problem I had.
There is a similar problem in NetAppDirectISCSIDriver._create_client().
It seems to me that a better solution would be to add a port argument to NaServer.init() and fall back to the scheme-based defaults if no arg is supplied. That may have ramifications that I am unaware of."
1350,1322410,nova,1b46f86148c0366770065fc481984adc6bce5693,0,0,Refactorings “remove uncessary log.exception in api layer”,"Bug #1322410 “remove uncessary log.exception in api layer "" ","There are some LOG.exception in api layer, which is not necessary , we should downgrade them"
1351,1322702,nova,0f28fbef8bedeafca0bf488b84f783568fefc960,1,0,"“This is really a bug in libvirt, not openstack”, “This patch fixes a bug in libvirt driver get_host_capabilities where some features can be duplicated”",libvirt get_host_capabilities() duplicates feature...,"get_host_capabilities() in libvirt driver seems to have a bug that will result in duplicated features.
def get_host_capabilities(self):
        """"""Returns an instance of config.LibvirtConfigCaps representing
           the capabilities of the host.
        """"""
        if not self._caps:
            xmlstr = self._conn.getCapabilities()
            self._caps = vconfig.LibvirtConfigCaps()
            self._caps.parse_str(xmlstr)
            if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
                try:
                    features = self._conn.baselineCPU(
                        [self._caps.host.cpu.to_xml()],
                        libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
                    # FIXME(wangpan): the return value of baselineCPU should be
                    #                 None or xml string, but libvirt has a bug
                    #                 of it from 1.1.2 which is fixed in 1.2.0,
                    #                 this -1 checking should be removed later.
                    if features and features != -1:
                        self._caps.host.cpu.parse_str(features)
                except libvirt.libvirtError as ex:
                    error_code = ex.get_error_code()
                    if error_code == libvirt.VIR_ERR_NO_SUPPORT:
                        LOG.warn(_LW(""URI %(uri)s does not support full set""
                                     "" of host capabilities: "" ""%(error)s""),
                                     {'uri': self.uri(), 'error': ex})
                    else:
                        raise
        return self._caps
The _caps.parse_str() is called in sequence for both capabilites and expand features. Since capabilities will have certain features in a VM, and these will be repeated again in the expand features, the _caps.host.cpu.features will end up with duplicated features. This will cause cpu compare to fail later.
(nova)root@overcloud-novacompute0-un6ckrnp5tzl:~# python
Python 2.7.6 (default, Mar 22 2014, 22:59:38)
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import libvirt
>>> conn = libvirt.open(""qemu:///system"")
>>> from nova.virt.libvirt import config as vconfig
>>> caps = vconfig.LibvirtConfigCaps()
>>> xmlstr = conn.getCapabilities()
>>> caps.parse_str(xmlstr)
>>> features = conn.baselineCPU([caps.host.cpu.to_xml()], libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
>>> caps.host.cpu.parse_str(features)
>>> for f in caps.host.cpu.features:
...     print f.name
...
hypervisor
popcnt
hypervisor
popcnt
pni
sse2
sse
fxsr
mmx
pat
cmov
pge
sep
apic
cx8
mce
pae
msr
tsc
pse
de
fpu
>>>"
1352,1322926,nova,50636a881b60f112027494bcd84af66888db8c1c,1,1,,Hyper-V driver volumes are attached incorrectly wh...,Hyper-V can change the order of the mounted drives when rebooting a host and thus passthrough disks can be assigned to the wrong instance resulting in a critical scenario.
1353,1322958,cinder,c08211b19e2b9c38e537e155d11ffced37af0e85,1,1,,Bug #1322958 “UnboundLocalError,"When the driver was not initialized, I uploaded a volume as  image into glance, I got some error as follow:
[01;31mException during message handling: local variable 'volume' referenced before assignment^[[00m
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    incoming.message))
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return self._do_dispatch(endpoint, method, ctxt, args)
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    result = getattr(endpoint, method)(ctxt, **new_args)
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 721, in copy_volume_to_image
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    if (volume['instance_uuid'] is None and
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mUnboundLocalError: local variable 'volume' referenced before assignment
As we can see the code from cinder/volume/manager.py,  the method require_driver_initialized occurs exception so that the method volume_get  will be not executed,  and the variable 'volume' is not defined.
 def copy_volume_to_image(self, context, volume_id, image_meta):
        payload = {'volume_id': volume_id, 'image_id': image_meta['id']}
        try:
            # NOTE(flaper87): Verify the driver is enabled
            # before going forward. The exception will be caught
            # and the volume status updated.
            utils.require_driver_initialized(self.driver)
            volume = self.db.volume_get(context, volume_id)
            image_service, image_id = \
                glance.get_remote_image_service(context, image_meta['id'])
            self.driver.copy_volume_to_image(context, volume, image_service,
                                             image_meta)
            LOG.debug(_(""Uploaded volume %(volume_id)s to ""
                        ""image (%(image_id)s) successfully""),
                      {'volume_id': volume_id, 'image_id': image_id})
        except Exception as error:
            with excutils.save_and_reraise_exception():
                payload['message'] = unicode(error)
        finally:
            if (volume['instance_uuid'] is None and
                    volume['attached_host'] is None):
                self.db.volume_update(context, volume_id,
                                      {'status': 'available'})
            else:
                self.db.volume_update(context, volume_id,
                                      {'status': 'in-use'})"
1354,1323005,neutron,10bf897bf3a10681f5a8769f387599271d57dadb,1,1,,Metadata workers has a bad default for production,"The Neutron metadata agent supports multiple workers to increase scalability. By default we create a single worker, and the various deployment tools (Should) increase to 4/8/16/x. Any number other than 1 would be a better default."
1355,1323173,nova,75a0ec684c4a972ad7703399b64ddf9300a7acf3,1,1,,Record wrong action name when migrate instance,"Actions are recorded in database while users take atcions in specific
instance. These actions can be listed by comand 'nova instance-action-list'.
Resize and migrate use same code path, but always record the action as
'resize' even user migrate instance."
1356,1323259,neutron,744c1bd2f1ae1caf3cd6d0c07d61b034a21204bc,0,0,"Feature ""To enable reuse of the ovs agent with the Intel® DPDK Accelerated Open vSwitch the proposal is to change the generation of the agent_id to use the hostname instead of the mac address of the br-int.”",Open vSwitch Agent ID initalisation prevents reuse...,"Problem description
===================
In the Open vSwitch agent, the Agent id is currently based off the mac address of the br-int.
Userspace Open vSwitch derivatives such as the Intel® DPDK Accelerated Open vSwitch do not currently create a tap device in the kernel to back the ovs bridges local port.
This limitation prevents reuse of the OpenVSwitch agent between both switches.
By allowing integration of high throughput vSwitch implementations via existing agents, NFV workloads can be enabled in OpenStack without significant extension of the current codebase.
Proposed change
===============
To enable reuse of the ovs agent with the Intel® DPDK Accelerated Open vSwitch the proposal is to change the  generation of the  agent_id to use the hostname instead of the mac address of the br-int.
-        mac = self.int_br.get_local_port_mac()
-        self.agent_id = '%s%s' % ('ovs', (mac.replace("":"", """")))
+        self.agent_id = 'ovs-agent-%s' % socket.gethostname()
For several plugins such as the nec,mlnx,hyperv and onconvergence agents the hostname is used to create the agent id.
Using the hostname will normalise the agent_id between these 5 neutron agents.
at present the Agent_id is only used in log messages. by using the hostname instead of mac of br-int log readability will also be improved as it will be easier to identify which node the log is from if log aggregation is preformed across a cluster.
Patch
===============
initial patch submitted for review
https://review.openstack.org/#/c/95138/1"
1357,1323322,neutron,0f7cfee155c0e3e216b4a1425ec1fbdde6eeb296,1,1,Feature “Disallow regular user to update firewall's shared attribute”,Bug #1323322 “fwaas,"DESCRIPTION:
Shared attribute is not shown when creating firewall.
I understand that, admin can only create shared firewall since it will affect other tenants also
In that case, creating shared firewall is prohibited correctly however I am able to update the firewall from tenant by shared = true
This should not be allowed
Steps to Reproduce:
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2 --shared true
Invalid values_specs true
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2 --shared false
Invalid values_specs false
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2 --shared
{""NeutronError"": {""message"": ""Policy doesn't allow create_firewall to be performed."", ""type"": ""PolicyNotAuthorized"", ""detail"": """"}}
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2
Created a new firewall:
+--------------------+--------------------------------------+
| Field              | Value                                |
+--------------------+--------------------------------------+
| admin_state_up     | True                                 |
| description        |                                      |
| firewall_policy_id | 7436f673-e1e8-4acf-b8a2-38e70a020105 |
| id                 | 476dfe06-07f0-404b-8e92-aae953257af9 |
| name               | f2                                   |
| status             | PENDING_CREATE                       |
| tenant_id          | bf4fbb928d574829855ebfd9e5d0e58c     |
+--------------------+--------------------------------------+
root@IGA-OSC:~# fwu f2 --shared true --------------------------------------------------------------------> able to update
Updated firewall: f2
root@IGA-OSC:~# fws f2
+--------------------+--------------------------------------+
| Field              | Value                                |
+--------------------+--------------------------------------+
| admin_state_up     | True                                 |
| description        |                                      |
| firewall_policy_id | 7436f673-e1e8-4acf-b8a2-38e70a020105 |
| id                 | 476dfe06-07f0-404b-8e92-aae953257af9 |
| name               | f2                                   |
| status             | ACTIVE                               |
| tenant_id          | bf4fbb928d574829855ebfd9e5d0e58c     |
+--------------------+--------------------------------------+
Actual Results:
Able to update the shared attribute of tenant's firewall
Expected Results:
tenant's firewall should not be able to update the shared attribute"
1358,1323403,nova,68e008b21b463a261461a0f7bbfaa92f4e9a7e92,1,1,,ec2-api crashes on describing volume backed snapsh...,"If volume backed snapshot is available for an user euca-describe-images crashes due to 'unknown server error'.
Environment: DevStack
Steps to reproduce:
1 Create a volume backed snapshot
$ cinder create --image-id xxx 1
$ nova boot --flavor m1.nano --image xxx --block-device-mapping /dev/vda=yyy:::1 inst
2 List images to ensure the created snapshot is available.
$ glance image-list
3 Describe images by euca2ools
$ euca-describe-images
TypeError: Unknown error occurred.
nova-api.log
2014-05-26 23:16:18.070 ERROR nova.api.ec2 [req-105138c5-1b82-42ff-a5fd-4588a4262763 demo demo] Unexpected TypeError raised: int() argument must be a string or a number,
not 'NoneType'
2014-05-26 23:16:18.071 DEBUG nova.api.ec2.faults [req-105138c5-1b82-42ff-a5fd-4588a4262763 demo demo] EC2 error response: TypeError: Unknown error occurred. ec2_error_response /opt/stack/nova/nova/api/ec2/faults.py:29"
1359,1323715,neutron,2bd97d205b98292be709b0257bda5fc489eb643a,0,0,Bug in test,network tests fail on policy check after upgrade f...,"Lots of tempest tests fail after upgrade
http://logs.openstack.org/51/94351/3/check/check-grenade-dsvm-neutron/ac837a8/logs/testr_results.html.gz
2014-05-26 21:47:20.109 364 INFO neutron.wsgi [req-7c96bf86-6845-4143-92d0-2bb32f5767d7 None] (364) accepted ('127.0.0.1', 60250)
2014-05-26 21:47:20.110 364 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:619
2014-05-26 21:47:20.110 364 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:678
2014-05-26 21:47:20.110 364 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token _cache_get /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:1041
2014-05-26 21:47:20.111 364 DEBUG keystoneclient.middleware.auth_token [-] Storing token in cache _cache_put /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:1151
2014-05-26 21:47:20.111 364 DEBUG keystoneclient.middleware.auth_token [-] Received request from user: 47d465f7c2e44c048f63066dff93093c with project_id : d3e7af8cf42d4613beb315dc19444d40 and roles: _member_  _build_user_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:940
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] No route matched for GET /ports.json __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:97
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] Matched GET /ports.json __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] Route path: '/ports{.format}', defaults: {'action': u'index', 'controller': <wsgify at 87437776 wrapping <function resource at 0x5358500>>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] Match dict: {'action': u'index', 'controller': <wsgify at 87437776 wrapping <function resource at 0x5358500>>, 'format': u'json'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2014-05-26 21:47:20.122 364 DEBUG neutron.policy [req-ee5c1651-9d0c-43c9-974d-a0c888c08468 None] Unable to find ':' as separator in tenant_id. __call__ /opt/stack/new/neutron/neutron/policy.py:243
2014-05-26 21:47:20.123 364 ERROR neutron.policy [req-ee5c1651-9d0c-43c9-974d-a0c888c08468 None] Unable to verify match:%(tenant_id)s as the parent resource: tenant was not found
2014-05-26 21:47:20.123 364 TRACE neutron.policy Traceback (most recent call last):
2014-05-26 21:47:20.123 364 TRACE neutron.policy   File ""/opt/stack/new/neutron/neutron/policy.py"", line 239, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.policy     parent_res, parent_field = do_split(separator)
2014-05-26 21:47:20.123 364 TRACE neutron.policy   File ""/opt/stack/new/neutron/neutron/policy.py"", line 234, in do_split
2014-05-26 21:47:20.123 364 TRACE neutron.policy     separator, 1)
2014-05-26 21:47:20.123 364 TRACE neutron.policy ValueError: need more than 1 value to unpack
2014-05-26 21:47:20.123 364 TRACE neutron.policy
2014-05-26 21:47:20.123 364 ERROR neutron.api.v2.resource [req-ee5c1651-9d0c-43c9-974d-a0c888c08468 None] index failed
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/api/v2/resource.py"", line 87, in resource
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/api/v2/base.py"", line 309, in index
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     return self._items(request, True, parent_id)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/api/v2/base.py"", line 264, in _items
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     request.context, obj_list[0])
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/api/v2/base.py"", line 145, in _exclude_attributes_by_policy
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     might_not_exist=True):
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/policy.py"", line 346, in check
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     return policy.check(*(_prepare_check(context, action, target)))
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 169, in check
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     result = rule(target, creds)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 732, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     return _rules[self.match](target, creds)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 732, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     return _rules[self.match](target, creds)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 366, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     if rule(target, cred):
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/policy.py"", line 261, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     reason=err_reason)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource PolicyCheckError: Failed to check policy tenant_id:%(tenant_id)s because Unable to verify match:%(tenant_id)s as the parent resource: tenant was not found
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource"
1360,1323718,neutron,43c1f98f074549611db90b6c333d2c54d7e4d4b3,0,0,"Feature “Additionally, the patch improves”",OVS capabilities test failure masks exception,"neutron/agent/linux/ovs_lib:ofctl_arg_supported executes an ovs-ofctl command and catches the general Exception, concluding that the feature is not supported. However, unexpected exceptions may be raised, swallowed and never logged. So, for example, if an OVS agent starts up and arp_responder is True, then the capabilities test for it may fail in an unexpected way. The result is that arp_responder will not be supported, the agent will start and the exception will not be printed, masking a potential bug.
Additionally, the capability test should be moved to the offline sanity command."
1361,1323741,cinder,571ac18e7c4109fc9728a0ee27f04887f1d8aeea,1,0,"“The 'num_iscsi_scan_tries' option was renamed to 'num_volume_device_scan_tries', but setting this ISER option still sets the old configuration field which is never read.”",LVM iSER driver scan_tries not set correctly,"The LVMISERDriver takes an option 'num_iser_scan_tries', but it appears that this is not being processed correctly.
The 'num_iscsi_scan_tries' option was renamed to 'num_volume_device_scan_tries', but setting this ISER option still sets the old configuration field which is never read.
(I have not tested this behavior, just found via code inspection.)"
1362,1323769,neutron,e588ee5b470bdca25abc634db3144d2aa9b84554,1,0,“NEC plugin: Bump L3RPC callback version to 1.1”,Bug #1323769 “nec plugin,"In nec plugin with l3-agent (icehouse) AttributeError: No such RPC function 'update_floatingip_statuses' occurs.
update_floatingip_statuses was implemented in Icehouse and RPC callback version related to L3RpcCallbackMixin was bumped to 1.1, but the version of L3RpcCallback in NEC plugin was not bumped to 1.1 yet.
update_floatingip_statues RPC call from l3-agent expects RPC version 1.1."
1363,1323813,nova,bb23a3730e5b2281e0a223a86c89f2a0bfcb4897,1,0,"“According to AWS docs
(http://docs.aws.amazon.com/AWSEC2/latest/APIReference/
api-error-codes.html)
“",Invalid ec2 error code for absent volumes and snap...,"ec2-api returns InvalidVolumeID.NotFound and InvalidSnapshotID.NotFound for absent volumes and snapshots.
But AWS returns InvalidVolume.NotFound and InvalidSnapshot.NotFound as it is documented in http://docs.aws.amazon.com/AWSEC2/latest/APIReference/api-error-codes.html
For example this affects Tempest. Tempest expects correct (AWS version) errors in waitXXXStatus functions and raises an error if other error cames for absent objects. So it make difficult writting tests."
1364,1323867,neutron,accd56fea86a0cd5f1706cbdd5b1bbc31c4cf479,0,0,Feature “Add new behavior to control update and delete operations “,Control update and delete operations for cisco-net...,"Add new behavior to control update and delete operations for the cisco-network-profile resource extension.
The new behavior is to allow update and delete operations only if there are no neutron networks associated with a particular cisco-network-profile instance."
1365,1323975,glance,10691597011e8cb462d28c156b39742329bd0339,0,0,Refactoing “remove default=None for config options”,do not use default=None for config options,In the cfg module default=None is set as the default value. It's not necessary to set it again when defining config options.
1366,1324120,neutron,84dfaa8a87cce660aa20619bd93263e645bbb2d0,1,1,“Problem is the line https://github.com/openstack/neutron/blob/master/neutron/plugins/vmware/plugins/base.py#L1012. A flat network will return an object instead of 0”,Bug #1324120 “NSX,"2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session RuntimeError: maximum recursion depth exceeded
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session
Problem is the line https://github.com/openstack/neutron/blob/master/neutron/plugins/vmware/plugins/base.py#L1012. A flat network will return an object instead of 0"
1367,1324131,neutron,c892a0f398b664bacf8a2a0302f51e3d9b96619d,0,0,“Radware LBaaS driver should support HA backend”,Radware LBaaS driver should support HA backend,"Radware LBaaS driver should be able to work with a backend that was configured in HA  mode.
The driver should try and call the other node in the HA pair and see if it is active."
1368,1324194,neutron,82683feff64b5b0dff36f983526421e608f13d94,1,1,“This is caused by a missing network_id in the port body”,keyerror while updating dhcp port,"When an update for a dhcp port fails, a keyerror on missing network_id while attempting to log a trace masks the underlying exception. An example is here:
http://logs.openstack.org/91/94891/1/check/check-tempest-dsvm-neutron/c87b4fc/logs/screen-q-svc.txt.gz?level=TRACE
This is because on update, the dhcp agent only sends either this:
https://github.com/openstack/neutron/blob/master/neutron/agent/linux/dhcp.py#L770
or this:
https://github.com/openstack/neutron/blob/master/neutron/agent/linux/dhcp.py#L787
This is somewhat a corner case, but it would be good to address the issue to see what actually went wrong."
1369,1324400,nova,de58feb796d63dd4eea1fd49e66102686fcf4037,1,0,"“Since nova.virt.driver:LibvirtDriver.get_guest_config prepends instance root_device_name with 'dev' prefix, root_device_name may not coincide with device_name in block device mapping structure.”",Invalid EC2 instance  type for a volume backed ins...,"Since nova.virt.driver:LibvirtDriver.get_guest_config prepends instance root_device_name with 'dev' prefix, root_device_name may not coincide with device_name in block device mapping structure.
In this case describe instances operation reports wrong instance type: instance-store instead of ebs.
Environment: DevStack
Steps to reproduce:
1 Create volume backed instance passing vda as root device name
$ cinder create --image-id xxx 1
$ nova boot --flavor m1.nano --block-device-mapping vda=yyy:::1 inst
Note. I used cirros ami image.
2 Describe instances
$ euca-describe-instances
Look on instace type. It must be ebs, but it is instance-store in the output.
Note. If euca-describe-instance crashes on ebs instnce, apply https://review.openstack.org/#/c/95580/"
1370,1324450,neutron,c1ed203ccb816ac0a3a0e015d2790ed3aee04564,0,0,"""add delete operations”",add delete operations for the ODL MechanismDriver,"The delete operations (networks, subnets and ports) haven't been managed since the 12th review of the initial support.
It seems sync_single_resource only implements create and update operations."
1371,1325116,cinder,759b3dcefc708aade88fd9c5906f86de9c7c67a8,1,1,,hp_lefthand_rest_proxy no handler for logger durin...,"No handlers could be found for logger when running lefthand tests.
Here's an example from https://jenkins03.openstack.org/job/gate-cinder-python26/477/console:
2014-05-30 17:27:15.848 | No handlers could be found for logger ""cinder.volume.drivers.san.hp.hp_lefthand_rest_proxy"""
1372,1325128,nova,4a60c6a655006b2882331844664fac5cf67c5f34,1,1,"“CVE-2014-3517""",[OSSA 2014-024] nova metadata does not use a const...,"Here:
https://github.com/openstack/nova/blob/HEAD/nova/api/metadata/handler.py#L173
a constant time comparison should be used, more information on this type of attack here: http://codahale.com/a-lesson-in-timing-attacks/
An example constant time comparison in Python can be found here: https://github.com/django/django/blob/master/django/utils/crypto.py#L80 or via the PyCA cryptography library: https://cryptography.io/en/latest/hazmat/primitives/constant-time/"
1373,1325148,nova,897a4b754daae840100e63433d928bb62a81fbdc,0,0,“Remove useless”,Remove useless codes for server_group,"Some codes about 'extract_members' from 'create_server_group' is written in original bp/patch's reviewing (https://review.openstack.org/#/c/62557/29).
But as the design variation, it can't be specified members in create API (see it in PS29 vs PS30).
But some codes are still kept after this feature merged.
So it's necessary to remove them."
1374,1325184,neutron,903e2a8cd1dd9169048d1ad9dd8a566b2ae52395,0,0,Bug in test,add unit tests for the ODL MechanismDriver,"All the operations (create, update or delete) haven't been covered by unit tests.
Bug #1324450 about the delete operations would have been caught."
1375,1325246,nova,9047165b7fed691eb89ef7322b7f4fe9b453d184,0,0,Bug in test,test_objects.test_versions fails with different ha...,"After rebasing with community I'm seeing this failure, but I haven't changed the Network object or any of it's base classes.
======================================================================
FAIL: nova.tests.objects.test_objects.TestObjectVersions.test_versions
tags: worker-0
----------------------------------------------------------------------
Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
INFO [migrate.versioning.api] 215 -> 216...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 216 -> 217...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 217 -> 218...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 218 -> 219...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 219 -> 220...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 220 -> 221...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 221 -> 222...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 222 -> 223...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 223 -> 224...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 224 -> 225...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 225 -> 226...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 226 -> 227...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 227 -> 228...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 228 -> 229...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 229 -> 230...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 230 -> 231...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 231 -> 232...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 232 -> 233...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 233 -> 234...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 234 -> 235...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 235 -> 236...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 236 -> 237...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 237 -> 238...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 238 -> 239...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 239 -> 240...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 240 -> 241...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 241 -> 242...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 242 -> 243...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 243 -> 244...
INFO [migrate.versioning.api] done
}}}
Traceback (most recent call last):
  File ""nova/tests/objects/test_objects.py"", line 941, in test_versions
    self._test_versions_cls(obj_name)
  File ""nova/tests/objects/test_objects.py"", line 937, in _test_versions_cls
    'has been bumped, and then update this hash') % obj_name)
  File ""/opt/stack/nova/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/opt/stack/nova/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = '1.1-faba26d0290395456f9a040584c4364b'
actual    = '1.1-6d5f3c575cfc4b25db53ee5f071207ce'
: Network object has changed; please make sure the version has been bumped, and then update this hash
======================================================================
FAIL: process-returncode
tags: worker-0
----------------------------------------------------------------------
Binary content:
  traceback (test/plain; charset=""utf8"")
Ran 2 tests in 3.262s (+2.164s)
FAILED (id=100, failures=2)
error: testr failed (1)
Looks like it's also happening in the check queue:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiTmV0d29yayBvYmplY3QgaGFzIGNoYW5nZWQ7IHBsZWFzZSBtYWtlIHN1cmUgdGhlIHZlcnNpb24gaGFzIGJlZW4gYnVtcGVkLCBhbmQgdGhlbiB1cGRhdGUgdGhpcyBoYXNoXCIgQU5EIChidWlsZF9uYW1lOmdhdGUtbm92YS1weXRob24yNiBPUiBidWlsZF9uYW1lOmdhdGUtbm92YS1weXRob24yNykgQU5EIHRhZ3M6Y29uc29sZSIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiMTcyODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQwMTU2MjMwMzg3Mn0=
10 hits in 48 hours, all failures, check queue only but different changes."
1376,1325481,cinder,7ef73d325bac3a57970be1fc0cfe8b0d871f533e,1,1,“comment misspelled about volume_clear in cinder/volume/driver.py (volume misspelled)”,comment misspelled about volume_clear,comment misspelled about volume_clear in cinder/volume/driver.py (volume misspelled)
1377,1325608,neutron,0759ed18a95b438bf775d6905256119f1cee75db,0,0,"“https://github.com/openstack/neutron/commit/634fd1d23fb241bc4990275d5a4da0c3ab66e2de""",base L3's create_router should process extensions,"Change:
https://github.com/openstack/neutron/commit/634fd1d23fb241bc4990275d5a4da0c3ab66e2de
Tweaked base create_router in l3_db.py to process the extensions or not; however it was chosen at the time, to set the process_extensions flag to False. This effectively disable the ability for extensions made to create_router's method to handle extension's attributes correctly.
A more appropriate value should be True, so that extension attributes can be shown during the create process."
1378,1325771,neutron,7b9ed4edeef7d599c164c8682fbbb36066d98abb,0,0,"“it doesn't have a consistency hash yet, so it is currently setting the HTTP header to ‘False'.""",Bug #1325771 “BSN,"When the Big Switch plugin first starts, it doesn't have a consistency hash yet, so it is currently setting the HTTP header to 'False'. This requires special-casing on the backend to handle. It should just be blank."
1379,1325982,neutron,eb8973dc34901406f4ca8e47956cde5c43ca54c6,1,1,"“Validate expected parameters in add/remove router interfaces
    The add and remove router interface methods check that interface_info
    is not empty but don't check if it contains any of expected parameters:”",Bug #1325982 “l3_db add/remove_router_interface,"L3_NAT_db_mixin  add_router_interface() and remove_router_interface() methods check that interface_info is not empty but don't check that it contains any of expected parameters - port_id or subnet_id:
 if not interface_info:
   msg = _(""Either subnet_id or port_id must be specified"")
   raise n_exc.BadRequest(resource='router', msg=msg)
Expected parameters should be explicitly checked."
1380,1326007,neutron,03277a80d573161abd0d1cb81ec647b53e140063,0,0,Refactoring “Removing check for overlap with fixed ips”,allowed address pairs - overlap check,"This bug is relate with the following bug:
https://bugs.launchpad.net/neutron/+bug/1321864
The fix patch for the bug delete overlap check for fixed ip and allowed ip address range.
I think that we also need to remove the following code if we do not need to check fixed_ip and allowed address pair overlap:
https://github.com/openstack/neutron/blob/master/neutron/db/allowedaddresspairs_db.py
51 for fixed_ip in port['fixed_ips']:
52 if ((fixed_ip['ip_address'] == address_pair['ip_address'])
53 and (port['mac_address'] ==
54 address_pair['mac_address'])):
55 raise addr_pair.AddressPairMatchesPortFixedIPAndMac()
https://github.com/openstack/neutron/blob/master/neutron/extensions/allowedaddresspairs.py
35class AddressPairMatchesPortFixedIPAndMac(nexception.InvalidInput):
36 message = _(""Port's Fixed IP and Mac Address match an address pair entry."")"
1381,1326173,neutron,63d271d4e4a8bab947f90f457ed176e46135d39e,1,1,,Bug #1326173 “BSN,"The capabilities check in the BigSwitch server manager tries to call json.loads on an object that has already been decoded and fails. This
causes the servers to have an empty capability list so none of the newer features are leveraged."
1382,1326183,nova,232cbfe67ffb7696f115830c711a960af5fa0828,1,1,,Bug #1326183 “detach interface fails as instance info cache is c... ,"Performing attach/detach interface on a VM sometimes results in an interface that can't be detached from the VM.
I could triage it to the corrupted instance cache info due to non-atomic update of that information.
Details on how to reproduce the bug are as follows. Since this is due to a race condition, the test can take quite a bit of time before it hits the bug.
Steps to reproduce:
1) Devstack with trunk with the following local.conf:
disable_service n-net
enable_service q-svc
enable_service q-agt
enable_service q-dhcp
enable_service q-l3
enable_service q-meta
enable_service q-metering
RECLONE=yes
# and other options as set in the trunk's local
2) Create few networks:
$> neutron net-create testnet1
$> neutron net-create testnet2
$> neutron net-create testnet3
$> neutron subnet-create testnet1 192.168.1.0/24
$> neutron subnet-create testnet2 192.168.2.0/24
$> neutron subnet-create testnet3 192.168.3.0/24
2) Create a testvm in testnet1:
$> nova boot --flavor m1.tiny --image cirros-0.3.2-x86_64-uec --nic net-id=`neutron net-list | grep testnet1 | cut -f 2 -d ' '` testvm
3) Run the following shell script to attach and detach interfaces for this vm in the remaining two networks in a loop until we run into the issue at hand:
--------
#! /bin/bash
c=10000
netid1=`neutron net-list | grep testnet2 | cut -f 2 -d ' '`
netid2=`neutron net-list | grep testnet3 | cut -f 2 -d ' '`
while [ $c -gt 0 ]
do
   echo ""Round: "" $c
   echo -n ""Attaching two interfaces... ""
   nova interface-attach --net-id $netid1 testvm
   nova interface-attach --net-id $netid2 testvm
   echo ""Done""
   echo ""Sleeping until both those show up in interfaces""
   waittime=0
   while [ $waittime -lt 60 ]
   do
       count=`nova interface-list testvm | wc -l`
       if [ $count -eq 7 ]
       then
           break
       fi
       sleep 2
       (( waittime+=2 ))
   done
   echo ""Waited for "" $waittime "" seconds""
   echo ""Detaching both... ""
   nova interface-list testvm | grep $netid1 | awk '{print ""deleting "",$4; system(""nova interface-detach testvm ""$4 "" ; sleep 2"");}'
   nova interface-list testvm | grep $netid2 | awk '{print ""deleting "",$4; system(""nova interface-detach testvm ""$4 "" ; sleep 2"");}'
   echo ""Done; check interfaces are gone in a minute.""
   waittime=0
   while [ $waittime -lt 60 ]
   do
       count=`nova interface-list testvm | wc -l`
       echo ""line count: "" $count
       if [ $count -eq 5 ]
       then
           break
       fi
       sleep 2
       (( waittime+=2 ))
   done
   if [ $waittime -ge 60 ]
   then
      echo ""bad case""
      exit 1
   fi
   echo ""Interfaces are gone""
   ((  c-- ))
done
---------
Eventually the test will stop with a failure (""bad case"") and the interface remaining either from testnet2 or testnet3 can not be detached at all."
1383,1326256,neutron,8fb69a4633e87446eadd7f0173a0168f78604cbe,1,1,,Dnsmasq config files syntax issue when dhcp_domain...,"A previous fix as already been done to manage an empty dhcp_domain:
https://bugs.launchpad.net/neutron/+bug/1099625
Hence in case of an empty dhcp_domain  it remains a bug on Dnsmasq config files.
With an empty dhcp_domain Dnsmasq launch command line will be:
 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=tapbdb96782-bb --except-interface=lo --pid-file=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/host --addn-hosts=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/addn_hosts --dhcp-optsfile=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/opts --leasefile-ro --dhcp-range=set:tag0,20.0.0.0,static,86400s --dhcp-lease-max=256 --conf-file=
""addn_hosts"" file contains:
20.0.0.3	host-20-0-0-3. host-20-0-0-3
20.0.0.4	host-20-0-0-4. host-20-0-0-4
""host"" file contains:
fa:16:3e:bf:e1:e4,host-20-0-0-3.,20.0.0.3
fa:16:3e:5f:88:81,host-20-0-0-4.,20.0.0.4
=> for both ""addn_hosts"" and ""host"" files the hostname (2nd parameter) is ended with an extra dot char.
(it should be ""host-20-0-0-3"" instead of ""host-20-0-0-3."" )
So generated files should be:
""addn_hosts"" file:
20.0.0.3	host-20-0-0-3 host-20-0-0-3
20.0.0.4	host-20-0-0-4 host-20-0-0-4
""host"" file:
fa:16:3e:bf:e1:e4,host-20-0-0-3,20.0.0.3
fa:16:3e:5f:88:81,host-20-0-0-4,20.0.0.4"
1384,1326429,swift,7056ec6a16fd8707564ec4b0a05cab461ee2a80d,1,1,,formpost middleware crashes for uploads larger 2 G...,"If a user tries to upload a file that is larger than 2GiB the upload fails due to an error OverflowError.
Actually this happens in python/socket.py, but it can be fixed either in swift/common/utils.py line 2406 or swift/common/middleware/formpost.py line 245. In both cases it's possible to use a long(-1) as the default instead of None (which would be converted to an int(-1) later on).
Note that this happens on 64bit systems."
1385,1326666,neutron,ff007a57efa82bc73cbe4a1e60b1cde42c7efa82,0,0,“dead code in ovs/ofagent agents”,dead code in ovs/ofagent agents,"""class Port"" seems like a leftover from days when agents had direct database accesses."
1386,1326778,nova,77dd352933ca4272bfc376bd2f9ea3ff781a9bf8,1,1,"“This reverts commit 6dd5cc5.""",Bug #1326778 “resize test fails with “Returning 400 to user,"Fails here:
http://logs.openstack.org/93/96293/2/gate/gate-tempest-dsvm-postgres-full/a644174/console.html
The error in the n-api log is here:
http://logs.openstack.org/93/96293/2/gate/gate-tempest-dsvm-postgres-full/a644174/logs/screen-n-api.txt.gz#_2014-06-05_06_50_26_679
From the console log, it looks like it goes into an error state after going to verify_resize state:
2014-06-05 06:50:26.714 |     2014-06-05 06:50:26,573 Request (MigrationsAdminTest:test_list_migrations_in_flavor_resize_situation): 200 GET http://127.0.0.1:8774/v2/7d6640f8f8e34866b5bd00e109fe90b7/servers/88591e95-69a2-4e34-a294-90b79d8f0d55 0.103s
2014-06-05 06:50:26.714 |     2014-06-05 06:50:26,573 State transition ""RESIZE/resize_finish"" ==> ""VERIFY_RESIZE/None"" after 16 second wait
2014-06-05 06:50:26.714 |     2014-06-05 06:50:26,681 Request (MigrationsAdminTest:test_list_migrations_in_flavor_resize_situation): 400 POST http://127.0.0.1:8774/v2/7d6640f8f8e34866b5bd00e109fe90b7/servers/88591e95-69a2-4e34-a294-90b79d8f0d55/action 0.106s
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiTWlncmF0aW9uc0FkbWluVGVzdFwiIEFORCBtZXNzYWdlOlwiSFRUUCBleGNlcHRpb24gdGhyb3duXFw6IEluc3RhbmNlIGhhcyBub3QgYmVlbiByZXNpemVkXCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1hcGkudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDE5NzIzNDQyMjh9
8 hits in 7 days, looks like this started on 6/5.  Fails in check and gate queues."
1387,1326781,glance,5cf0659d53eba85f7591565b7f18714e39c1713a,1,1,,v2 api returns 200 with blank response (no image d...,"v2 api returns 200 with blank response (no image data) for download_image policy
If you have enabled download_image policy in policy.json to ""role:admin"" then it should return 403 error if user other admin role is calling image-download api.
Presently it is returning 200 with blank response (no image data). If you enable cache filter, then it returns 403 error correctly.
Steps to reproduce:
1. Ensure following flavor is set in glance-api.conf
   [paste-deploy]
   flavor = keystone+cachemanagement
2. Disable cache
   a. Open /etc/glance/glance-api-paste.ini file.
   b. Remove cahce from following sections.
     [pipeline:glance-api-caching]
     [pipeline:glance-api-cachemanagement]
     [pipeline:glance-api-keystone+caching]
     [pipeline:glance-api-keystone+cachemanagement]
     [pipeline:glance-api-trusted-auth+cachemanagement]
   c. Save and exit from file.
   d. Restart the g-api (glance-api) service.
3. Ensure that 'download_image' policy is set in policy.json
   ""download_image"": ""role:admin""
4. Download image using v2 api for role other than admin
   a. source openrc normal_user normal_user
   b. glance --os-image-api-version 2 image-download <image-id>
   Output:
   -------
   ''
   glance-api screen log:
   ----------------------
 2014-06-05 12:45:00.711 24883 INFO glance.wsgi.server [-] Traceback (most recent call last):
   File ""/usr/lib/python2.7/dist-packages/eventlet/wsgi.py"", line 395, in handle_one_response
  for data in result:
   File ""/mnt/stack/glance/glance/notifier.py"", line 228, in get_data
  for chunk in self.image.get_data():
   File ""/mnt/stack/glance/glance/api/policy.py"", line 233, in get_data
  self.policy.enforce(self.context, 'download_image', {})
   File ""/mnt/stack/glance/glance/api/policy.py"", line 143, in enforce
  exception.Forbidden, action=action)
   File ""/mnt/stack/glance/glance/api/policy.py"", line 131, in _check
  return policy.check(rule, target, credentials, *args, **kwargs)
   File ""/mnt/stack/glance/glance/openstack/common/policy.py"", line 183, in check
  raise exc(*args, **kwargs)
 Forbidden: You are not authorized to complete this action.
 2014-06-05 12:45:00.711 24883 INFO glance.wsgi.server [-] 10.146.146.4 - - [05/Jun/2014 12:45:00] ""GET /v2/images/63826dea-e281-4ffe-821b-f598c747ba54/file HTTP/1.1"" 200 0 0.062499"
1388,1326926,nova,7f53a8c91e623c69855caadf6cf65703a7e5b799,1,1,"“This only shows up in master so it's a regression in Juno only, this is the patch that broke it:
https://review.openstack.org/#/c/92285/""",Conductor passing GlanceImageService instead of no...,"After image-api partial refactor, some calls still use a glance service instance to call compute_utils.get_instance_metadata which expect the object to have a 'get()' method.
Since that method is not present in GlanceImageService, and exception is thrown and the image metadata cannot be retrieved.
Sample calling  _cold_migration(..)
2014-06-05 15:45:13.138 WARNING nova.compute.utils [req-7a86365f-f01a-4d49-b1c3-595e8dc9bd24 admin admin] [instance: 290d3587-b69a-48d8-b5c0-307259e2f590] Can't access image 40c33532-0aed-4acc-8d7a-2a45698e1f2d: 'GlanceImageService' object has no attribute 'get'"
1390,1327145,nova,c2aa27d6f86d9ead2a9653b07de425669351d93d,1,1,“Change https://review.openstack.org/#/c/62314/ mistakenly made the conversion of the instance object in compute/rpcapi.py rescue_image()”,rescue_instance RPC has reverted to passing a dict...,Change https://review.openstack.org/#/c/62314/  mistakenly made the conversion of the instance object in compute/rpcapi.py   rescue_image() unconditional on the RPC version.   From 3.9 onwards this should be passed as an object
1391,1327406,nova,46e88320e6e6231550f3e2b40312c51f55e059f5,1,1,,The One And Only network is variously visible,"I am testing with the templates in https://review.openstack.org/#/c/97366/
I can create a stack.  I can use `curl` to hit the webhooks to scale up and down the old-style group and to scale down the new-style group; those all work.  What fails is hitting the webhook to scale up the new-style group.  Here is a typescript showing the failure:
$ curl -X POST 'http://10.10.0.125:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3A39675672862f4bd08505bfe1283773e0%3Astacks%2Ftest4%2F3cd6160b-d8c5-48f1-a527-4c7df9205fc3%2Fresources%2FNewScaleUpPolicy?Timestamp=2014-06-06T19%3A45%3A27Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=35678396d987432f87cda8e4c6cdbfb5&SignatureVersion=2&Signature=W3aJQ6SR7O5lLOxLEQndbzNB%2FUhefr1W7qO9zNZ%2BHVs%3D'
<ErrorResponse><Error><Message>The request processing has failed due to an internal error:Remote error: ResourceFailure Error: Nested stack UPDATE failed: Error: Resource CREATE failed: NotFound: No Network matching {'label': u'private'}. (HTTP 404)
[u'Traceback (most recent call last):\n', u'  File ""/opt/stack/heat/heat/engine/service.py"", line 61, in wrapped\n    return func(self, ctx, *args, **kwargs)\n', u'  File ""/opt/stack/heat/heat/engine/service.py"", line 911, in resource_signal\n    stack[resource_name].signal(details)\n', u'  File ""/opt/stack/heat/heat/engine/resource.py"", line 879, in signal\n    raise failure\n', u""ResourceFailure: Error: Nested stack UPDATE failed: Error: Resource CREATE failed: NotFound: No Network matching {'label': u'private'}. (HTTP 404)\n""].</Message><Code>InternalFailure</Code><Type>Server</Type></Error></ErrorResponse>
The original sin looks like this in the heat engine log:
2014-06-06 17:39:20.013 28692 DEBUG urllib3.connectionpool [req-2391a9ea-46d6-46f0-9a7b-cf999a8697e9 ] ""GET /v2/39675672862f4bd08505bfe1283773e0/os-networks HTTP/1.1"" 200 16 _make_request /usr/lib/python2.7/dist-packages/urllib3/connectionpool.py:415
2014-06-06 17:39:20.014 28692 ERROR heat.engine.resource [req-2391a9ea-46d6-46f0-9a7b-cf999a8697e9 None] CREATE : Server ""my_instance"" Stack ""test1-new_style-qidqbd5nrk44-43e7l57kqf5w-4t3xdjrfrr7s"" [20523269-0ebb-45b8-ad59-75f55607f3bd]
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource Traceback (most recent call last):
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource   File ""/opt/stack/heat/heat/engine/resource.py"", line 383, in _do_action
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource     handle())
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource   File ""/opt/stack/heat/heat/engine/resources/server.py"", line 493, in handle_create
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource     nics = self._build_nics(self.properties.get(self.NETWORKS))
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource   File ""/opt/stack/heat/heat/engine/resources/server.py"", line 597, in _build_nics
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource     network = self.nova().networks.find(label=label_or_uuid)
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource   File ""/opt/stack/python-novaclient/novaclient/base.py"", line 194, in find
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource     raise exceptions.NotFound(msg)
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource NotFound: No Network matching {'label': u'private'}. (HTTP 404)
Private debug logging reveals that in the scale-up case, the call to ""GET /v2/{tenant-id}/os-networks HTTP/1.1"" returns with response code 200 and an empty list of networks.  Comparing with the corresponding call when the stack is being created shows no difference in the calls --- because the normal logging omits the headers --- even though the results differ (when the stack is being created, the result contains the correct list of networks).  Turning on HTTP debug logging in the client reveals that the X-Auth-Token headers differ."
1393,1327476,nova,8f8b6e656a6ef06bf0b99068a07a7d194783fc9b,1,1,"""Commit 59a6cf233b538d6666740de4796fce25ed8265aa added code to handle
    serializing non-str exception traceback objects, but didn't account for
    unicode. “",Bug #1327476 “AttributeError,"Saw this in CI -> http://logs.openstack.org/11/98511/2/check/check-grenade-dsvm/5d60567/logs/new/screen-n-cpu.txt.gz?level=ERROR
2014-06-06 23:00:29.533 ERROR oslo.messaging._drivers.common [req-4ae02289-de3e-4700-968c-7611980e0346 ServerActionsTestXML-67143133 ServerActionsTestXML-1032052660] Returning exception 'unicode' object has no attribute 'tb_frame'
Traceback (most recent call last):
  File ""/opt/stack/new/nova/nova/conductor/manager.py"", line 602, in _object_dispatch
    return getattr(target, method)(context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance_action.py"", line 119, in wrapper
    kwargs['exc_tb'] = ''.join(traceback.format_tb(exc_tb))
  File ""/usr/lib/python2.7/traceback.py"", line 76, in format_tb
    return format_list(extract_tb(tb, limit))
  File ""/usr/lib/python2.7/traceback.py"", line 95, in extract_tb
    f = tb.tb_frame
AttributeError: 'unicode' object has no attribute 'tb_frame'
 to caller
2014-06-06 23:00:29.534 ERROR oslo.messaging._drivers.common [req-4ae02289-de3e-4700-968c-7611980e0346 ServerActionsTestXML-67143133 ServerActionsTestXML-1032052660] ['Traceback (most recent call last):\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n    incoming.message))\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', '  File ""/opt/stack/new/nova/nova/exception.py"", line 88, in wrapped\n    payload)\n', '  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', '  File ""/opt/stack/new/nova/nova/exception.py"", line 71, in wrapped\n    return f(self, context, *args, **kw)\n', '  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 286, in decorated_function\n    pass\n', '  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', '  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 272, in decorated_function\n    return function(self, context, *args, **kwargs)\n', '  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 336, in decorated_function\n    function(self, context, *args, **kwargs)\n', '  File ""/opt/stack/new/nova/nova/compute/utils.py"", line 437, in __exit__\n    exc_tb=exc_tb, want_result=False)\n', '  File ""/opt/stack/new/nova/nova/objects/instance_action.py"", line 121, in wrapper\n    return fn.__get__(None, cls)(*args, **kwargs)\n', '  File ""/opt/stack/new/nova/nova/objects/base.py"", line 144, in wrapper\n    args, kwargs)\n', '  File ""/opt/stack/new/nova/nova/conductor/rpcapi.py"", line 355, in object_class_action\n    objver=objver, args=args, kwargs=kwargs)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 150, in call\n    wait_for_reply=True, timeout=timeout)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 89, in _send\n    timeout=timeout)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 386, in send\n    return self._send(target, ctxt, message, wait_for_reply, timeout)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 379, in _send\n    raise result\n', 'AttributeError: \'unicode\' object has no attribute \'tb_frame\'\nTraceback (most recent call last):\n\n  File ""/opt/stack/new/nova/nova/conductor/manager.py"", line 602, in _object_dispatch\n    return getattr(target, method)(context, *args, **kwargs)\n\n  File ""/opt/stack/new/nova/nova/objects/instance_action.py"", line 119, in wrapper\n    kwargs[\'exc_tb\'] = \'\'.join(traceback.format_tb(exc_tb))\n\n  File ""/usr/lib/python2.7/traceback.py"", line 76, in format_tb\n    return format_list(extract_tb(tb, limit))\n\n  File ""/usr/lib/python2.7/traceback.py"", line 95, in extract_tb\n    f = tb.tb_frame\n\nAttributeError: \'unicode\' object has no attribute \'tb_frame\'\n\n']
2014-06-06 23:01:15.378 ERROR nova.virt.libvirt.driver [req-a0da2719-553b-4654-8c45-215b49ce1d3f ServerActionsTestJSON-1623844969 ServerActionsTestJSON-1569101731] An error occurred while trying to launch a defined domain with xml: <domain type='qemu'>
I feel like this is the remote end of the error?  Or maybe it's an oslo bug, idk."
1394,1327497,nova,3ea14e8a70a946dbb162ecafa848e4f2fa29772a,1,1,,live-migration fails when FC multipath is used,"I tried live-migration against VM with multipath access to FC bootable volume and FC data volume.
After checking the code, I found the reason is that
1. /dev/dm-<NUM> is used, which is subject to change in the destination Compute Node since it is not unique across nodes
2. multipath_id in connnection_info is not maintained properly and may be lost during connection refreshing
The fix would be
1. Like iSCSI multipath, use /dev/mapper/<multipath_id> instead of /dev/dm-<NUM>
2. Since multipath_id is unique for a volume no matter where it is attached, add logic to preserve this information."
1395,1327974,neutron,25a5a8526cf8182fbb6d7f8acf224ee1666da5c2,0,0,Bug in test,hyperv unit test agent failure,"The hyperv unit appear to not properly mock all cases of report_state calls so an occasional exception will be thrown on an unrelated patch.[1]
1. http://logs.openstack.org/01/96201/6/gate/gate-neutron-python27/2b0de5e/console.html"
1396,1327975,neutron,a8d664d95f22a713d54c8ea30471dd3a4e976924,1,0,“The name of the synchronized queue class is queue instead of Queue in Python3.”,Use import from six.moves to import the queue modu...,The name of the synchronized queue class is queue instead of Queue in Python3.
1397,1328162,neutron,58e6bb5893186517edafe1a4d51710c1362bc9cc,1,1,,Tempest fails to delete firewall in 300 seconds,"Similar to bug #1314313 but this is another failure.
In some tempest runs a test fails to delete firewall within 300 seconds.
That happens because at the point firewall agent sends deleting confirmation to neutron server, firewall object is already updated to a state unexpected by deleting method.
Example of the issue:
http://logs.openstack.org/18/97218/2/gate/gate-tempest-dsvm-neutron/e03d166/console.html#_2014-06-07_10_33_34_506"
1398,1328222,neutron,2d9488be71244cd7fcee624764b2f6c0cc5fe3a1,1,1,"""Sync function is missing network information”",Bug #1328222 “BigSwitch,The Big Switch full topology synchronization function isn't including all of the information about a network. It's only including the ports and floating IP addresses. The subnets are missing as well as the tenant of the network.
1399,1328288,neutron,1222366f6dadf7ce2a810c919344054196134db8,1,1,,[mos] openvswitch agent fails with bridges longer ...,"The openvswitch agent will try to construct veth pairs with names longer than the maximum allowed (15) and fail. VMs will then have no external connectivity.
This happens in cases where the bridge name is very long (e.g. int-br-bonded)."
1400,1328321,neutron,c8bdff1533e54787d9a3dd98fe57a1e2e0d82e73,1,1,,Bug #1328321 “Big Switch,"The consistency watch dog is calling the wrong method for a health check which is raising an exception. However, since it is in a greenthread, the exception is silently discarded so the watchdog dies without any indication."
1401,1328331,neutron,09e706b210513968cac2bad78fe81ee9cbb5b5be,1,1,,Bug #1328331 “Big Switch,"The Big Switch servermanager records the consistency hash to the database every time it gets updated but it does not retrieve the latest value from the database whenever it includes it in an HTTP request. This is fine in single neutron server deployments because the cached version on the object is always the latest, but this isn't always the case in HA deployments where another server updates the consistency DB."
1402,1328362,neutron,b1b83e42ae20f510eea452b9c7fb7673374f2a79,0,0,"Refactoring “ extensions: remove 'check_env' method
    The method is not documented or used.”",ext.check_env is not used,check_env method for extension descriptor is not documented or used.
1403,1328539,nova,077e3c770ebeebd037ce882863a6b5dcefd644cf,1,1,,Fixed IP allocation doesn't clean up properly on f...,"If fixed IP allocation fails, for example because nova's network interfaces got renamed after a reboot, nova will loop continuously trying, and failing, to create a new instance. For every attempted spawn the instance will end up with an additional fixed IP allocated to it. This is because the code is associating the IP, but not disassociating it if the function fails."
1404,1328694,nova,801a37de197cdd98f264caa2bbe9c6ebddec070b,1,1,“ This reverts commit 8057b66c0b7ac87863f175d850414bc0ed260ab2.”,FloatingIp pollster spamming n-api logs,"Noticed this here:
http://logs.openstack.org/02/99002/1/check/check-tempest-dsvm-full/5fca6a7/logs/screen-n-api.txt.gz?level=TRACE
Showing up a ton since 6/8:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiQ2F1Z2h0IGVycm9yOiBQYXJlbnQgaW5zdGFuY2VcIiBBTkQgbWVzc2FnZTpcImlzIG5vdCBib3VuZCB0byBhIFNlc3Npb247IGxhenkgbG9hZCBvcGVyYXRpb24gb2YgYXR0cmlidXRlICdmaXhlZF9pcCcgY2Fubm90IHByb2NlZWRcIiBBTkQgdGFnczpcInNjcmVlbi1uLWFwaS50eHRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNjA0ODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQwMjQzNDU3MTcwNX0=
Assuming this is the culprit given it merged on 6/8 and is related to floating ip's which is what shows up in that n-api log mess.
https://review.openstack.org/83676"
1405,1328870,nova,0a7527c71228c8e776ad40cedd2cf137fd99f43d,1,1,,Hyper-V cannot attach volumes when the host is an ...,The domain name gets added to the initiator name used by the host when it's an AD member. The method which automatically gets the initiator name when this is not set in the registry does not take this into account. Trying to use a wrong initiator name will lead to an exception when trying to log in to the according iSCSI target.
1406,1328889,neutron,d4e5373240b97727ce9c4ec6636119bd5849bace,0,0,Bug in test,DHCP Unit test uses invalid IPv6 prefix,"Unit tests for neutron's linux dhcp section use an invalid prefix and host address.
https://github.com/openstack/neutron/blob/master/neutron/tests/unit/test_linux_dhcp.py#L129"
1408,1329099,neutron,3eef1ae80fcd7d9fe82b5c7485702991992b2766,1,1,"“Add a config parameter to Cisco N1kv plugin to determine the total
    number of active REST calls that can be made to the VSM.”",Control active number of REST calls to the control...,Add a config parameter to Cisco N1kv neutron plugin to determine and control the number of active REST calls to the VSM (controller)
1409,1329156,cinder,c04709b127d6f2dea32c5faa37cc1e27f6792cd9,0,0,“Remove cinder.context warning logging”,Lots of cinder.context warning in scheduler log,"There have been quite a few warning log in cinder scheduler log, something like:
2014-06-11 09:41:39.441 3074 WARNING cinder.context [-] Arguments dropped when creating context: {'user': u'1284c7ced03d4b17830b0c1911b0cfb2', 'tenant': u'a28cee075f3f49528afd48a79646533a', 'user_identity': u'1284c7ced03d4b17830b0c1911b0cfb2 a28cee075f3f49528afd48a79646533a - - -'}
http://logs.openstack.org/25/77125/8/check/check-tempest-dsvm-full/bd92be7/logs/screen-c-sch.txt.gz?level=WARNING#_2014-06-11_09_26_45_485
It's quite annoy and not really helpful."
1410,1329426,neutron,13fe102fadf8d25b0927ee172b0fb3f681c56bf4,1,1,“Add missing keyword raise to get_profile_binding function”,Cisco n1kv neutron plugin typo in get_profile_bind...,Need to fix typo where raise keyword is missing in get_profile_binding
1411,1329546,neutron,90fedbe44ca6bfccce5d71465532fbdc85ee3814,1,1,,Upon rebuild instances might never get to Active s...,"VMware mine sweeper for Neutron (*) recently showed a 100% failure rate on tempest.api.compute.v3.servers.test_server_actions
Logs for two instances of these failures are available at [1] and [2]
The failure manifested as an instance unable to go active after a rebuild.
A bit of instrumentation and log analysis revealed no obvious error on the neutron side - and also that the instance was actually in ""running"" state even if its task state was ""rebuilding/spawning""
N-API logs [3] revealed that the instance spawn was timing out on a missed notification from neutron regarding VIF plug - however the same log showed such notification was received [4]
It turns out that, after rebuild, the instance network cache had still 'active': False for the instance's VIF, even if the status for the corresponding port was 'ACTIVE'. This happened because after the network-vif-plugged event was received, nothing triggered a refresh of the instance network info. For this reason, the VM, after a rebuild, kept waiting for an even which obviously was never sent from neutron.
While this manifested only on mine sweeper - this appears to be a nova bug - manifesting in vmware minesweeper only because of the way the plugin synchronizes with the backend for reporting the operational status of a port.
A simple solution for this problem would be to reload the instance network info cache when network-vif-plugged events are received by nova. (But as the reporter knows nothing about nova this might be a very bad idea as well)
[1] http://208.91.1.172/logs/neutron/98278/2/413209/testr_results.html
[2] http://208.91.1.172/logs/neutron/73234/34/413213/testr_results.html
[3] http://208.91.1.172/logs/neutron/73234/34/413213/logs/screen-n-cpu.txt.gz?level=WARNING#_2014-06-06_01_46_36_219
[4] http://208.91.1.172/logs/neutron/73234/34/413213/logs/screen-n-cpu.txt.gz?level=DEBUG#_2014-06-06_01_41_31_767
(*) runs libvirt/KVM + NSX"
1412,1329559,nova,f33a25a3c40722644c774395b38fd7a7ed0246e1,1,1,"“This was intentionally done in
https://review.openstack.org/#/c/58829/ . We also intentionally ignore
duplicate requests to delete an instance if its already being deleted
(https://review.openstack.org/#/c/55444/).""",Cannot delete an instance that failed a previous d...,"Currently we have a situation where if an instance fails to delete,
instead of having its state reverted, like we do in most places we set
it to error,deleting. This was intentionally done in
https://review.openstack.org/#/c/58829/ . We also intentionally ignore
duplicate requests to delete an instance if its already being deleted
(https://review.openstack.org/#/c/55444/). The combination of these two
things means that if an instance fails to delete for some reason a
tenant is unable to delete that instance.
It turns out this is really bad because instances in deleting state
count against quota, so the tenant slowly looses usable quota.
To fix this, allow duplicate delete calls to go through if the instance
is in error state."
1413,1329614,nova,7e54641fc1f760423c2f1210787903e17ea1cc06,0,0,Bug in test,test_uri_length_limit follows proxy and hits wrong...,"test_uri_length_limit honours HTTP_PROXY (if set).  This means the test queries to http://localhost:$port/ hit the wrong localhost when $HTTP_PROXY is another host, resulting in a meaningless test scenario, and apparent failures with HTTP status codes other than REQUEST_URI_TOO_LARGE (414)."
1414,1329882,nova,c28956a6872b34a93891c985c93aad5e242563b6,1,1,"“This looks like a clear regression caused by https://review.openstack.org/#/c/82455/.""",Snapshot is deleted after instance is terminated,NovaImageBuilder runs Anaconda and other installers inside a nova instance.  After the install the instance is shut down and a snapshot is taken.  After the snapshot finishes being saved the instance is terminated.  In the last week I have noticed that the snapshot gets deleted also.
1415,1330065,nova,c5de5e7ab166e29304a01d7e310ae6ed32d22090,1,1,"“Also during spawing, if a datastore in maintenance mode gets choosen, since it had the largest disk space, the spawn would fail.”",VMWare - Driver does not ignore Datastore in maint...,"A datastore can be in maintenance mode. The driver does not ignore it both in stats update and while spawing instances.
During stats update, a wrong stats update is returned if a datastore is in maintenance mode.
Also during spawing, if a datastore in maintenance mode gets choosen, since it had the largest disk space, the spawn would fail.
The driver should ignore datastore in maintenance mode"
1416,1330135,neutron,f89a6bd30b6f99bc39f266ae8d3880380379f8b9,1,1,,Bug #1330135 “Big Switch,"If the consistency watchdog encounters an exception, it is uncaught and it kills the greenthread so it no longer works until the neutron server is restarted."
1417,1330431,nova,c606043c64b31ba3289002dd9ac90b7566e1bca2,1,1,,wrong lock name when operating instance events,"We use wrong lock name at here:
    def prepare_for_instance_event(self, instance, event_name):
        """"""Prepare to receive an event for an instance.
        This will register an event for the given instance that we will
        wait on later. This should be called before initiating whatever
        action will trigger the event. The resulting eventlet.event.Event
        object should be wait()'d on to ensure completion.
        :param instance: the instance for which the event will be generated
        :param event_name: the name of the event we're expecting
        :returns: an event object that should be wait()'d on
        """"""
        @utils.synchronized(self._lock_name)
        def _create_or_get_event():
            if instance.uuid not in self._events:
                self._events.setdefault(instance.uuid, {})
            return self._events[instance.uuid].setdefault(
                event_name, eventlet.event.Event())
        LOG.debug('Preparing to wait for external event %(event)s',
                  {'event': event_name}, instance=instance)
        return _create_or_get_event()
We should invoke self._lock_name, not pass it as name.
So will get log message as below:
2014-06-16 17:44:59.022 DEBUG nova.openstack.common.lockutils [req-97211458-bae1-473b-a3ad-47fd153ae30a admin admin] Got semaphore ""<function _lock_name at 0x7fe6a7edec08>"" from (pid=30672) lock /opt/stack/nova/nova/openstack/common/lockutils.py:168
Same problem for pop_instance_event and clear_events_for_instance"
1418,1330503,nova,dd6fb1246ff2789bd78b772b45e1fcac21eda67a,1,1,,Restarting destination compute manager during resi...,"During compute manager startup init_host is called. One of the functions there is to delete instance data that doesn't belong to this host i.e. _destroy_evacuated_instances. But this function only checks if the local instance belongs to the host or not. It doesn't check the task_state or vm_state.
If at this time a resize migration is taking place and the destination compute manager is restarted it might destroy the resizing instance. Alternatively, if the resize has completed (vm_state = RESIZED) but has not been confirmed/reverted, then a restart of the source compute manager might destroy the original instance.
A similar bug concerning just the migrating state is outlined here: https://bugs.launchpad.net/nova/+bug/1319797 and a fix is proposed here: https://review.openstack.org/#/c/93903
It was intended to have that fix deal with resize migrating instances as well as those just in the migrating state but as pointed out in a review comment this solution will work for migrating but a fix for resize would require further changes so I have raised this bug to highlight that."
1419,1330856,nova,da9891a30b3a3eac39458d829abdd5bc50cf876e,1,1,,Confusing fault reasion when the flavors disk size...,"Fedora-x86_64-20-20140407-sda has 2 GiB virtual size.
$ nova boot fed_1G_2  --image Fedora-x86_64-20-20140407-sda --flavor 1 --key-name mykey
$ nova show fed_1G_2
+--------------------------------------+------------------------------------------------------------------------------------------+
| Property                             | Value                                                                                    |
+--------------------------------------+------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                                   |
| OS-EXT-AZ:availability_zone          | nova                                                                                     |
| OS-EXT-STS:power_state               | 0                                                                                        |
| OS-EXT-STS:task_state                | -                                                                                        |
| OS-EXT-STS:vm_state                  | error                                                                                    |
| OS-SRV-USG:launched_at               | -                                                                                        |
| OS-SRV-USG:terminated_at             | -                                                                                        |
| accessIPv4                           |                                                                                          |
| accessIPv6                           |                                                                                          |
| config_drive                         |                                                                                          |
| created                              | 2014-06-17T07:35:43Z                                                                     |
| fault                                | {""message"": ""No valid host was found. "", ""code"": 500, ""created"": ""2014-06-17T07:35:44Z""} |
| flavor                               | m1.tiny (1)                                                                              |
| hostId                               | a904a292f4eb7f6735bef786c4a240a0b9240a6bc4f002519cb0e2b7                                 |
| id                                   | 3c908a54-9682-40ad-8f12-a5bf64066660                                                     |
| image                                | Fedora-x86_64-20-20140407-sda (085610a8-77ae-4bc8-9a28-3bcc1020e06e)                     |
| key_name                             | mykey                                                                                    |
| metadata                             | {}                                                                                       |
| name                                 | fed_1G_2                                                                                 |
| os-extended-volumes:volumes_attached | []                                                                                       |
| private network                      | 10.1.0.5                                                                                 |
| security_groups                      | default                                                                                  |
| status                               | ERROR                                                                                    |
| tenant_id                            | 1d26ad7003cf47e5b0107313be4832c3                                                         |
| updated                              | 2014-06-17T07:35:44Z                                                                     |
| user_id                              | bf52e56b9ca14648b391c5b6d490a0c1                                                         |
+--------------------------------------+------------------------------------------------------------------------------------------+
$ # nova flavor-list
+-----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
| ID  | Name      | Memory_MB | Disk | Ephemeral | Swap_MB | VCPUs | RXTX_Factor | Is_Public |
+-----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
| 1   | m1.tiny   | 512       | 1    | 0         |         | 1     | 1.0         | True      |
| 2   | m1.small  | 2048      | 20   | 0         |         | 1     | 1.0         | True      |
| 3   | m1.medium | 4096      | 40   | 0         |         | 2     | 1.0         | True      |
| 4   | m1.large  | 8192      | 80   | 0         |         | 4     | 1.0         | True      |
| 42  | m1.nano   | 64        | 0    | 0         |         | 1     | 1.0         | True      |
| 451 | m1.heat   | 1024      | 0    | 0         |         | 2     | 1.0         | True      |
| 5   | m1.xlarge | 16384     | 160  | 0         |         | 8     | 1.0         | True      |
| 84  | m1.micro  | 128       | 0    | 0         |         | 1     | 1.0         | True      |
+-----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
Many images requires minimum 2,5,10 Gib as minimum disk size, the 1 used by m1.tiny is frequently not enough.
It might be increased to 10 or the original 0 should be restored.
This bug is about why I see 'message"": ""No valid host was found. "", ""code"": 500, ""created"": ""2014-06-17T07:35:44Z""'
when ""Flavor's disk is too small for requested image'"" was raised on the n-cpu side.
It is confusing, 'No valid host was found' type of messages sounds like there is no  n-cpu running, or all of them full.
instance: f62b56da-d1fa-4dc2-ae37-42b8fde3d3a5] Instance failed to spawn
 Traceback (most recent call last):
   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 2064, in _build_resources
     yield resources
   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 1966, in _build_and_run_instance
     block_device_info=block_device_info)
   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 2233, in spawn
     admin_pass=admin_password)
   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 2607, in _create_image
     project_id=instance['project_id'])
   File ""/opt/stack/new/nova/nova/virt/libvirt/imagebackend.py"", line 182, in cache
     *args, **kwargs)
   File ""/opt/stack/new/nova/nova/virt/libvirt/imagebackend.py"", line 374, in create_image
     prepare_template(target=base, max_size=size, *args, **kwargs)
   File ""/opt/stack/new/nova/nova/openstack/common/lockutils.py"", line 249, in inner
     return f(*args, **kwargs)
   File ""/opt/stack/new/nova/nova/virt/libvirt/imagebackend.py"", line 172, in fetch_func_sync
     fetch_func(target=target, *args, **kwargs)
   File ""/opt/stack/new/nova/nova/virt/libvirt/utils.py"", line 658, in fetch_image
     max_size=max_size)
   File ""/opt/stack/new/nova/nova/virt/images.py"", line 110, in fetch_to_raw
     raise exception.FlavorDiskTooSmall()
 FlavorDiskTooSmall: Flavor's disk is too small for requested image."
1420,1330981,nova,9283379849906f74047e47a679326e08e923fecc,1,1,,Cannot attach volumes to LXC instances,"Volumes cannot be attach to any LXC instances
since it's root device cannot be parsed properly.
This later causes a failure in device name generation in get_next_device_name(),
when attempting to generate a name for the attached volume.
The generated device name, will not be recognized by get_dev_prefix_for_disk_bus()
when trying to select a disk bus, nor libvirt will be able to attach the volume
with an unrecognized device name.
When creating a LXC instance, the /dev/nbd1 or /dev/loop0 devices will be saved as
instance root device in _create_domain()
Later, when attaching the volume, block_device.match_device will be called from compute_utils.get_next_device_name(),
The formed device will be named as /dev/na (for /dev/nbdX)
Which will not be recognized in blockinfo.get_dev_prefix_for_disk_bus()
Even if it will be recognized, libvirt wont be able to attach a volume named /dev/na"
1421,1331170,nova,5b27fe7de22aef53b82402f15b076887bc52670a,1,1,"""Unfortunately, that can result in live migrations failing if your environment is using different versions of the host OS on compute noes as the destination node may not be able to support the machine type used when the VM was originally started.""",Live migration fails in heterogeneous host OS envi...,"The libvirt driver currently does not set the machine type for a KVM guest by default.  When not specified, libvirt will use the newest one it knows about.  Unfortunately, that can result in live migrations failing if your environment is using different versions of the host OS on compute noes as the destination node may not be able to support the machine type used when the VM was originally started.
A simple solution to this is to provide a new option which allows you to specify the default machine type on a per compute node basis (nova.conf option).  By using this option, you can ensure that VMs are started with a machine type that will allow it to be live migrated to other nodes in the deployment."
1422,1331249,neutron,24a19bfd362996cd9833a167d6eb86fdcf743b75,0,0,REFACTORING “Big Switch: Remove unnecessary initialization code”,Bug #1331249 “Big Switch,"Both the Big Switch Plugin and ML2 driver allow a server_timeout param in the initialization methods that can never be set by a user so it doesn't serve a purpose [1][2]. With those removed, the entire __init__ method of the base class can be removed as well.
1. https://github.com/openstack/neutron/blob/d379170109982a53544d01566ba9231d66b24ed4/neutron/plugins/bigswitch/plugin.py#L171
2. https://github.com/openstack/neutron/blob/1a116d24a955c9e45fa8a29998d09da0350be4ab/neutron/plugins/ml2/drivers/mech_bigswitch/driver.py#L46"
1423,1331317,nova,32cc13365be9baba7a1d50fae397f1b597ff505b,0,0,Bug in test,test_create_instance_with_neutronv2_fixed_ip_alrea...,"In test_create_instance_with_neutronv2_fixed_ip_already_in_use of both
v2 and v3, these tests pass wrong parameters ""fixed-ip"" instead of
valid parameter ""fixed_ip"".
The purposes of these tests are to check the behaviors when passing
in-use addresses, but current tests contain two negative factors and
we cannot test the purposes now."
1424,1331456,neutron,3867174bc82b7fd85dd79bc0cc5625a15df2d8fb,0,0,feature. “Need to add the same ability to auto_schedule_networks.”,Fix dhcp agent scheduler to be resistant to race c...,"DHCP ChanceScheduler has _schedule_bind_network method which can detect duplicate entry.
Need to add the same ability to auto_schedule_networks.
That might be important for the case when api server and rpc_server run in different processes."
1425,1331566,neutron,cf92bbead9b25ee40a3336bd76110b3733a4f7ee,1,1,,Neutron nova notifier does not support use of tena...,The neutron nova notifier (neutron/notifiers/nova.py) hardcodes the project_id to the novaclient as 'None'.  This prevents the ability to use a nova admin tenant name in place of a nova admin tenant id in the event that the tenant id is not known/available at the time that the neutron service is being configured.  An example of such a case is with tripleo (see related bug https://bugs.launchpad.net/tripleo/+bug/1293782).
1426,1331569,neutron,1b46d36269adc5c64c8cd55a2d9f2c47e42b341f,1,1,"""ip_lib is currently used to list the bridges
in the Open vSwitch neutron agent.

use of ip_lib blocks reuse of the Open vSwitch agent
with userspace only open vSwitchs implementations.” (52e281c736ce0ecac33f5807fa2c5c953925eda9)",Open vSwitch Agent should use ovs_lib to list brid...,"Problem description
===================
In the Open vSwitch  neutron agent, ip_lib is currently used to list the bridges.
as userspace only vswiches do not create bridge local ports in the kernel ip_lib will not correctly detect the bridge configuration.
Proposed Change
==============
use of ip_lib blocks reuse of the Open vSwitch agent with userspace only open vSwitchs implementations.
to enable this use case ovs_lib.get_bridges should be used instead."
1427,1331839,neutron,79f6ccd1b5b67c23d438332e9d8641d07426a9a1,1,1,,vpn agent fails to remove iptables rule on vpn-sit...,"The following warning appears when deleting VPNaaS' vpn-site-connection object:
2013-12-15 13:57:04.274 6899 WARNING neutron.agent.linux.iptables_manager [-] Tried to remove rule that was not there: 'POSTROUTING' u'-s 10.35.214.0/24 -d 10.35.7.0/24 -m policy --dir out --pol ipsec -j ACCEPT ' True False"
1428,1332041,neutron,e8b9a11fd728d287a1c64aa025ebba009bf5025e,1,1,,type_tunnel.TunnelRpcCallbackMixin  __init__ metho...,"The init method in type_tunnel.TunnelRpcCallbackMixin expects extra parameters[1] not passed to the rest of the RPC mixins so it requires a custom __init__ method in the ML2 plugin[2].
1. https://github.com/openstack/neutron/blob/caf60442247ef0a13595db2691733f3b7589d24f/neutron/plugins/ml2/rpc.py#L54
2. https://github.com/openstack/neutron/blob/0755e7b379232285e434d827eeb854260a1db595/neutron/plugins/ml2/drivers/type_tunnel.py#L90"
1430,1332198,nova,5120c4f7c2670eaa71898fe6941029bbb0081949,1,1,,Volumes are not detached when a build fails,"When a build fails in the driver spawn method attached volumes are not detached.  If the instance goes to ERROR and is later deleted everything gets cleaned up appropriately.  If the instance is rescheduled then the next compute will fail with:
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] Traceback (most recent call last):
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 1786, in _prep_block_device
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     self.driver, self._await_block_device_map_created) +
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 368, in attach_block_devices
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     map(_log_and_attach, block_device_mapping)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 366, in _log_and_attach
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     bdm.attach(*attach_args, **attach_kwargs)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 45, in wrapped
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     ret_val = method(obj, context, *args, **kwargs)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 218, in attach
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     volume_api.check_attach(context, volume, instance=instance)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/volume/cinder.py"", line 249, in check_attach
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     raise exception.InvalidVolume(reason=msg)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] InvalidVolume: Invalid volume: status must be 'available'
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]
2014-06-18 20:09:02.002 11008 ERROR nova.compute.manager [req-e76e85f6-0520-4372-b47d-a80744c912a7 None] [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] Failure prepping block device
which stops the build and properly stops a reschedule.
Cinder volumes need to be detached on a build failure."
1431,1332290,neutron,a8d67485ab498a647b1f50184755c47b18e97e2c,1,0,“It should be a configurable parameter.”,Bug #1332290 “Cisco,The http timeout parameter used in the Cisco n1kv client module is a constant defined in cisco_constants module. It should be a configurable parameter.
1432,1332334,neutron,60c1b9e227fdfcae8e3660ffbd5062eb6a4abb33,1,1,"“If the user sets the consistency watchdog polling interval to 0, it will poll as fast as possible instead of disabling the watchdog as expected”",Bug #1332334 “Big Switch,"If the user sets the consistency watchdog polling interval to 0, it will poll as fast as possible instead of disabling the watchdog as expected."
1433,1332382,nova,66721eb2c0f53fc4260b2f0aa9a3811da0f7ddbd,1,1,,block device mapping timeout in compute,"When booting instances passing in block-device and increasing the volume size , instances can go in to error state if the volume takes longer to create than the hard code value set in:
nova/compute/manager.py
  def _await_block_device_map_created(self, context, vol_id, max_tries=180,
                                        wait_between=1):
Here is the command used to repro:
nova boot --flavor ca8d889e-6a4e-48f8-81ce-0fa2d153db16 --image 438b3f1f-1b23-4b8d-84e1-786ffc73a298
--block-device source=image,id=438b3f1f-1b23-4b8d-84e1-786ffc73a298,dest=volume,size=128
--nic net-id=5f847661-edef-4dff-9f4b-904d1b3ac422 --security-groups d9ce9fe3-983f-42a8-899e-609c01977e32
Test_Image_Instance
max_retries should be made configurable.
Looking through the different releases, Grizzly was 30, Havana was 60 , IceHouse is 180.
Here is a traceback:
2014-06-19 06:54:24.303 17578 ERROR nova.compute.manager [req-050fc984-cfa2-4c34-9cde-c8aeea65e6ed
d0b8f2c3cf70445baae994004e602e11 1e83429a8157489fb7ce087bd037f5d9] [instance:
74f612ea-9722-4796-956f-32defd417000] Instance failed block device setup
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
Traceback (most recent call last):
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1394,
in _prep_block_device
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
self._await_block_device_map_created))
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 283,
in attach_block_devices
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
block_device_mapping)
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 238,
in attach
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
wait_func(context, vol['id'])
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 909,
in _await_block_device_map_created
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
attempts=attempts)
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
VolumeNotCreated: Volume 8489549e-d23e-45c2-ae6e-7fdb1a9c30d0 did not finish
being created even after we waited 65 seconds or 60 attempts.
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]"
1434,1332412,neutron,48e9c8b79bea9c9a65ceb1c24528a89db6d313d2,1,0,“probably due to oslo.messaging changes.”,Could not send notification to notifications,"""Could not send notification to notifications"" errors started happening today.
probably due to oslo.messaging changes.
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging Traceback (most recent call last):
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/notify/_impl_messaging.py"", line 47, in notify
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     version=self.version)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 96, in _send_notification
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     self._driver.send_notification(target, ctxt, message, version)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 394, in send_notification
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     envelope=(version == 2.0), notify=True)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 355, in _send
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     rpc_amqp.pack_context(msg, context)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqp.py"", line 212, in pack_context
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     context_d = six.iteritems(context.to_dict())
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/six.py"", line 553, in iteritems
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     return iter(d.iteritems(**kw))
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging AttributeError: 'Context' object has no attribute 'iteritems'
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIkNvdWxkIG5vdCBzZW5kIG5vdGlmaWNhdGlvbiB0byBub3RpZmljYXRpb25zXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjE3MjgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDMyNDIyNjczOTd9"
1436,1332571,neutron,e299349b3be240d799822daedfa2730fd96839d3,0,0,Feature? “They should be replaced by a unique constant equals to 15 to ensure consistency”,Multiple constants define linux interface maximum ...,"Multiple constants define linux interface maximum length (15):
* neutron.agent.linux.utils: DEVICE_NAME_LEN in get_interface_mac (=15)
* neutron.agent.linux.ip_lib: VETH_MAX_NAME_LENGTH (=15)
* neutron.plugins.common.constants: MAX_DEV_NAME_LEN (=16 incorrect value)
They should be replaced by a unique constant equals to 15 to ensure consistency."
1437,1332660,nova,9b559c31b781689fb66551f29a0cb8d10c7bac94,0,0,“Update statistics from computes if RBD ephemeral is used”,Update statistics from computes if RBD ephemeral i...,"If we use RBD as the backend for ephemeral drives, compute nodes still calculate their available disk size looking back to the local disks.
This is the path how they do it:
* nova/compute/manager.py
    def update_available_resource(self, context):
        """"""See driver.get_available_resource()
        Periodic process that keeps that the compute host's understanding of
        resource availability and usage in sync with the underlying hypervisor.
        :param context: security context
        """"""
        new_resource_tracker_dict = {}
        nodenames = set(self.driver.get_available_nodes())
        for nodename in nodenames:
            rt = self._get_resource_tracker(nodename)
            rt.update_available_resource(context)
            new_resource_tracker_dict[nodename] = rt
....................
    def _get_resource_tracker(self, nodename):
        rt = self._resource_tracker_dict.get(nodename)
        if not rt:
            if not self.driver.node_is_available(nodename):
                raise exception.NovaException(
                        _(""%s is not a valid node managed by this ""
                          ""compute host."") % nodename)
            rt = resource_tracker.ResourceTracker(self.host,
                                                  self.driver,
                                                  nodename)
            self._resource_tracker_dict[nodename] = rt
        return rt
* nova/compute/resource_tracker.py
    def update_available_resource(self, context):
        """"""Override in-memory calculations of compute node resource usage based
        on data audited from the hypervisor layer.
        Add in resource claims in progress to account for operations that have
        declared a need for resources, but not necessarily retrieved them from
        the hypervisor layer yet.
        """"""
        LOG.audit(_(""Auditing locally available compute resources""))
        resources = self.driver.get_available_resource(self.nodename)
* nova/virt/libvirt/driver.py
    def get_local_gb_info():
        """"""Get local storage info of the compute node in GB.
        :returns: A dict containing:
             :total: How big the overall usable filesystem is (in gigabytes)
             :free: How much space is free (in gigabytes)
             :used: How much space is used (in gigabytes)
        """"""
        if CONF.libvirt_images_type == 'lvm':
            info = libvirt_utils.get_volume_group_info(
                                 CONF.libvirt_images_volume_group)
        else:
            info = libvirt_utils.get_fs_info(CONF.instances_path)
        for (k, v) in info.iteritems():
            info[k] = v / (1024 ** 3)
        return info
It would be nice to have something like ""libvirt_utils.get_rbd_info"" which could be used in case CONF.libvirt_images_type == 'rbd'"
1438,1332719,neutron,f807023246501b77e792371d98b612b208f5e8e3,1,0,"“However it is necessary to support all versions of NOS, hence a run time check of the NOS version should be made and the appropriate template should be used.”",brocade ml2 mechanism does not support VDX/NOS ver...,"In order to support VDX/NOS version greater than 4.1.0, NETCONF template needs to be enhanced. However it is necessary to support all versions of NOS, hence a run time check of the NOS version should be made and the appropriate template should be used."
1439,1333103,neutron,02cfe35694dc69c08d858c756db7221c32bf9f3c,1,1, “Clarify message when no probes are cleared”,wrong_info_is_displayed_during_no_probe_neutron_de...,"Take a case, when there is no probe present. In this case if we try to clear probes using following CLI -
#neutron-debug probe-clear
it should ideally give a message like - ""Nothing to delete/clear"" or ""No probe is present to clear""
But when I tries above command following is traced -
2014-06-23 11:48:43.924 12056 INFO neutron.common.config [-] Logging enabled!
2014-06-23 11:48:43.925 12056 WARNING neutron.agent.common.config [-] Deprecated: DEFAULT.root_helper is deprecated! Please move root_helper configuration to [AGENT] section.
2014-06-23 11:48:43.925 12056 WARNING neutron.agent.common.config [-] Deprecated: DEFAULT.root_helper is deprecated! Please move root_helper configuration to [AGENT] section.
2014-06-23 11:48:43.927 12056 DEBUG neutron.debug.commands.ClearProbe [-] run(Namespace(request_format='json')) run /opt/stack/neutron/neutron/debug/commands.py:105
2014-06-23 11:48:43.928 12056 DEBUG neutronclient.client [-]
REQ: curl -i http://10.0.9.40:9696/v2.0/ports.json?device_owner=network%3Aprobe&device_owner=compute%3Aprobe&device_id=openstack -X GET -H ""X-Auth-Token: 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-xM-dDkVpSvK15tuQ8nMxnOwa3dwqF5Gmql9A3icH7V1cjnNZfDDwNyvkzvQPoaX5A8gNxKKZ7+4y6LZO0hTc+ib1ikVAjLm--7VG13owGMD2Qd5r5+QJtpPmFujYQaqPIktTJQRFlILvoJlfu+Fo7eidwWDjvIViI0XmcWVBsAJBIzAIH8hmW9w-HYUc0H6E4wK+UXRXLpfCowgNYae30ZBYM4MeWeveXvduVh1iqFFSnWqcqCuKfbiz6RBqWbRWQbcuEw9pXfnQHGTfiQui020qxiuvfFMMiYRVs6-HHwaybUl13RwBEY5MK9FLk8CzqdZB16aHv85djA=="" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -H ""User-Agent: python-neutronclient""
 http_log_req /opt/stack/python-neutronclient/neutronclient/common/utils.py:175
2014-06-23 11:48:44.038 12056 DEBUG neutronclient.client [-] RESP:200 CaseInsensitiveDict({'date': 'Mon, 23 Jun 2014 06:18:44 GMT', 'content-length': '13', 'content-type': 'application/json; charset=UTF-8', 'x-openstack-request-id': 'req-afbd1b2f-ab5e-4441-8477-bede40a652fe'}) {""ports"": []}
 http_log_resp /opt/stack/python-neutronclient/neutronclient/common/utils.py:184
2014-06-23 11:48:44.039 12056 INFO neutron.debug.commands.ClearProbe [-] All Probes deleted
Concentrate in the last of traces --->   ""ClearProbe [-] All Probes deleted""
I dont think it should be like this. In my opinion information ""All probe deleted"" should be modified as per condition if probes are present or not."
1441,1333252,cinder,ea56cf255ac956d23eba5bb4f6e7312b944eb5fb,1,0,“Update _resize_volume_file() to support appropriate permissions”,IBMNAS driver fails to resize volume after setting...,"Based on the discussion of drivers setting inappropriate permissions (https://wiki.openstack.org/wiki/OSSN/OSSN-0014), ibmnas driver inherits  _set_rw_permissions_for_all() from nfs.py which sets 666 to the volumes.
Changes are expected to be made in nfs.py such that it sets 600 or 660 permissions, while doing so ibmnas driver fails performing the operation  _resize_volume_file(). This can be fixed by performing resize operation using root user."
1442,1333315,nova,04add2ed0d8b0341ea347ac98834f75cf5b2be1d,1,0,“Neutronv2 api does not support neutron without port quota”,Nova boot fails with quota activated in neutron bu...,"When nova is deployed with neutron, nova boot fails if quota are activated but not for ports
""Failing"" usecase:
In neutron.conf
[quotas]
quota_items = network,subnet,router,floatingip
neutron quota-show
+---------------------+-------+
| Field               | Value |
+---------------------+-------+
| floatingip          | 2     |
| network             | 5     |
| router              | 1     |
| security_group      | 10    |
| security_group_rule | 100   |
| subnet              | 4     |
+---------------------+-------+
nova boot fails because neutronv2 api expect neutron.show_quota(tenant_id=context.project_id)['quota'].get('port') to return an int but instead None is returned"
1443,1333325,glance,cb7b189f4cd425cad3cbea6ca71986216416cec7,0,0,"“This is a sync of processutils and lockutils from oslo-incubator
along with their dependencies.”",glance-api workers should default to number of CPU...,"The docs recommend setting the 'workers' option equal to the number of CPUs on the host but defaults to 1.  I proposed a change to devstack to set workers=`nproc` but it was decided to move this into glance itself:
https://review.openstack.org/#/c/99739/
Note that nova changed in Icehouse to default to number of CPUs available also, and Cinder will most likely be doing the same for it's osapi_volume_workers option.
This will have a DocImpact and probably UpgradeImpact is also necessary since if you weren't setting the workers value explicitly before the change you'll now have `nproc` glance API workers by default after restarting the service."
1444,1333370,cinder,6f24ca2496e37f3f75496c0e4191e71e55fd7b5d,0,0,"“ This moves and renames nova.utils.cpu_count() utility method from nova
    to oslo. Added in commit 75c96a48fc7e5dfb59d8258142b01422f81b0253, this”",Set osapi_volume_workers equal to number of CPUs,"I started a change in devstack here but we decided to move this into Cinder itself:
https://review.openstack.org/#/c/98492/
Nova made the same change for it's API and conductor workers in Icehouse:
https://review.openstack.org/#/c/69266/"
1445,1333654,nova,4f8ccd7b95c27180a1cfe689e3c6f46bde5f803b,1,1,,Timeout waiting for vif plugging callback for inst...,"The neutron full job is exhibiting a rather high number of cases where network-vif-plugged timeout are reported.
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIlRpbWVvdXQgd2FpdGluZyBmb3IgdmlmIHBsdWdnaW5nIGNhbGxiYWNrIGZvciBpbnN0YW5jZVwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiIxNzI4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDAzNjA5MTk0NDg4LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
95.78% of this kind of messages appear for the neutron full job. However, only a fraction of those cause build failures, but that's because the way the tests are executed.
This error is currently being masked by another bug as tempest tries to get the console log of a VM in error state: https://bugs.launchpad.net/tempest/+bug/1332414
This bug will target both neutron and nova pending a better triage.
Fixing this is of paramount importance to get the full job running.
Note: This is different from https://bugs.launchpad.net/nova/+bug/1321872 and https://bugs.launchpad.net/nova/+bug/1329546"
1446,1334024,nova,0d781d41fef329294891fdda0c1dac4add5ce7c3,1,1,"""The suffix .rescue is missed here and hence, original disk.config is overwritten.”",Nova rescue fails for libvirt driver with config d...,"I am using config drive to boot VMs. In icehouse, I observed that nova rescue fails and leaves the VM in SHUTOFF state.
Short error log:
instances/270e299b-90b2-46d5-bf9a-e7f6efe3742e/disk.config.rescue': No such file or directory
Difference in Havana and Icehouse code path:
# Havana
# Config drive
 if configdrive.required_by(instance):
      LOG.info(_('Using config drive'), instance=instance)
      extra_md = {}
      if admin_pass:
          extra_md['admin_pass'] = admin_pass
      for f in ('user_name', 'project_name'):
         if hasattr(context, f):
             extra_md[f] = getattr(context, f, None)
         inst_md = instance_metadata.InstanceMetadata(instance,
                content=files, extra_md=extra_md, network_info=network_info)
         with configdrive.ConfigDriveBuilder(instance_md=inst_md) as cdb:
             configdrive_path = basepath(fname='disk.config')
             LOG.info(_('Creating config drive at %(path)s'),
                      {'path': configdrive_path}, instance=instance)
def basepath(fname='', suffix=suffix): << Adds suffix .rescue to disk.config.
    return os.path.join(libvirt_utils.get_instance_path(instance),
                                fname + suffix)
# Icehouse:
# Config drive
if configdrive.required_by(instance):
    LOG.info(_('Using config drive'), instance=instance)
    extra_md = {}
    if admin_pass:
        extra_md['admin_pass'] = admin_pass
    for f in ('user_name', 'project_name'):
        if hasattr(context, f):
            extra_md[f] = getattr(context, f, None)
        inst_md = instance_metadata.InstanceMetadata(instance,
                content=files, extra_md=extra_md, network_info=network_info)
        with configdrive.ConfigDriveBuilder(instance_md=inst_md) as cdb:
            configdrive_path = self._get_disk_config_path(instance)
            LOG.info(_('Creating config drive at %(path)s'),
                         {'path': configdrive_path}, instance=instance)
@staticmethod
def _get_disk_config_path(instance):
    return os.path.join(libvirt_utils.get_instance_path(instance),
                        'disk.config')
The suffix .rescue is missed here and hence, original disk.config is overwritten.
Following change fixed the issue for me:
configdrive_path = self._get_disk_config_path(instance, suffix)
@staticmethod
def _get_disk_config_path(instance, suffix=''):
    return os.path.join(libvirt_utils.get_instance_path(instance),
                            'disk.config' + suffix)"
1447,1334086,cinder,8d6e9cf0fc564942dce14ce8f133c5d0ee06c4f4,1,0,“Commit 971a63bd9cf675a00bce1244ec101577b5c17cac has added a new parameter 'optional_args' to execute method of QuotaReserveTask and QuotaCommitTask. So where those Task used need change to provide the new parameter.”,Import existing volume failed due to missing 'opti...,"Import existing volume failed with below ERROR:
2014-06-24 13:04:54.407 110362 ERROR oslo.messaging.rpc.dispatcher [req-7aff78e5-92c2-4864-8388-7884068e3360 86341fc3271b4e2da241dd3b603c52f5 0807a39b955048ed88c123c9b3a0485b - - -] Exception during message handling: taskflow.patterns.linear_flow.Flow: volume_manage_existing_manager; 7 requires ['optional_args'] but no other entity produces said requirements
Commit 971a63bd9cf675a00bce1244ec101577b5c17cac has added a new parameter 'optional_args' to execute method of QuotaReserveTask and QuotaCommitTask. So where those Task used need change to provide the new parameter.
This commit has changed volume create task flow as below to provide the new parameter.  We need the samiliar change to get_flow method in cinder\volume\flows\manager\manage_existing.py
diff --git a/cinder/volume/api.py b/cinder/volume/api.py
index 2fd65bf..8e4201c 100644
--- a/cinder/volume/api.py
+++ b/cinder/volume/api.py
@@ -174,6 +174,7 @@ class API(base.Base):
             'scheduler_hints': scheduler_hints,
             'key_manager': self.key_manager,
             'backup_source_volume': backup_source_volume,
+            'optional_args': {'is_quota_committed': False}
         }
Commit comment of b5c17cac for reference:
     Made provision for providing optional arguments
    The 'quota_committed' attribute of 'RequestContext' object is
    a transient property, so it will not be saved in the taskflow
    persistent storage. The updated value of 'quota_committed'
    attribute will not be available while resuming/reverting the
    flow, if cinder api-service is down/stopped after committing
    the quota.
    Since this 'quota_committed' attribute is not used anywhere
    in cinder project other than in create-volume taskflow api, so
    removed 'quota_committed' from RequestContext and made
    provision to pass it as an optional argument which will be
    passed to api-flow via create_what dictionary, in order to
    make it persistent and use it as and when needed."
1448,1334142,nova,c250696ba12ead71e741cf8f608ce5212b0cc7bb,1,1,"“The git commit 873aad92944f8840e772d65eda4b3320d65a9ce7[1] has moved
    ""brctl addif"" command, but the commit did not move its error check.”",A server creation fails due to adding interface fa...,"http://logs.openstack.org/72/61972/27/gate/gate-tempest-dsvm-full/ed1ab55/logs/testr_results.html.gz
pythonlogging:'': {{{
2014-06-25 06:45:11,596 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 202 POST http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers 0.295s
2014-06-25 06:45:11,674 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 200 GET http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.077s
2014-06-25 06:45:12,977 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 200 GET http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.300s
2014-06-25 06:45:12,978 25675 INFO     [tempest.common.waiters] State transition ""BUILD/scheduling"" ==> ""BUILD/spawning"" after 1 second wait
2014-06-25 06:45:14,150 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 200 GET http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.171s
2014-06-25 06:45:14,153 25675 INFO     [tempest.common.waiters] State transition ""BUILD/spawning"" ==> ""ERROR/None"" after 3 second wait
2014-06-25 06:45:14,221 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 400 POST http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f/action 0.066s
2014-06-25 06:45:14,404 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 204 DELETE http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.182s
}}}
Traceback (most recent call last):
  File ""tempest/api/compute/servers/test_delete_server.py"", line 97, in test_delete_server_while_in_verify_resize_state
    resp, server = self.create_test_server(wait_until='ACTIVE')
  File ""tempest/api/compute/base.py"", line 247, in create_test_server
    raise ex
BadRequest: Bad request
Details: {'message': 'The server could not comply with the request since it is either malformed or otherwise incorrect.', 'code': '400'}"
1449,1334164,nova,48de2895b9a550a0944b31212349275605a4061d,1,1,,Bug #1334164 “nova error migrating VMs with floating ips,"Seeing this in conductor logs when migrating a VM with a floating IP assigned:
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 1019, in network_migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     migration)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 527, in network_migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     self.network_api.migrate_instance_start(context, instance, migration)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/nova/network/api.py"", line 94, in wrapped
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     return func(self, context, *args, **kwargs)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/nova/network/api.py"", line 543, in migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     self.network_rpcapi.migrate_instance_start(context, **args)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/nova/network/rpcapi.py"", line 350, in migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     floating_addresses=floating_addresses)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/client.py"", line 150, in call
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     wait_for_reply=True, timeout=timeout)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/transport.py"", line 90, in _send
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     timeout=timeout)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 409, in send
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 402, in _send
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     raise result
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher AttributeError: 'FixedIP' object has no attribute '_sa_instance_state'"
1450,1334264,neutron,60fc1f64fabd86c38846c76785255c523a98b331,1,1,"“Previously ovs-agent exited with exit code 1 after SIGTERM was received.
    SIGTERM should shutdown agent gracefully and exit code should be 0.” (320b9bd196a87f58229000c6a545a4911340c9ae)",openvswitch agent exits with exit code 1 when hand...,"SIGTERM should be handled properly and agent should exit with 0.
https://github.com/openstack/neutron/blob/master/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py#L1444"
1451,1334278,nova,2e6b2404155156ca336dadeacc8874645ca07bfc,1,1,,limits with tenant parameter returns wrong maxTota...,"When querying for the ""absolute limits"" of a specific tenant
the maxTotal* values reported aren't correct.
How to reproduce:
for example using devstack...
OS_TENANT_NAME=demo (11b2b129994844798c98f437d9809a9c)
OS_USERNAME=demo
$nova absolute-limits
+-------------------------+-------+
| Name                    | Value |
+-------------------------+-------+
| maxServerMeta           | 128   |
| maxPersonality          | 5     |
| maxImageMeta            | 128   |
| maxPersonalitySize      | 10240 |
| maxTotalRAMSize         | 1000  |
| maxSecurityGroupRules   | 20    |
| maxTotalKeypairs        | 100   |
| totalRAMUsed            | 128   |
| maxSecurityGroups       | 10    |
| totalFloatingIpsUsed    | 0     |
| totalInstancesUsed      | 2     |
| totalSecurityGroupsUsed | 1     |
| maxTotalFloatingIps     | 10    |
| maxTotalInstances       | 10    |    <-----------------------
| totalCoresUsed          | 2     |
| maxTotalCores           | 10    |   <-----------------------
+-------------------------+-------+
OS_TENANT_NAME=admin (b0f08277004b43aab516ae7dbf36ff51)
OS_USERNAME=admin
$nova absolute-limits
+-------------------------+--------+
| Name                    | Value  |
+-------------------------+--------+
| maxServerMeta           | 128    |
| maxPersonality          | 5      |
| maxImageMeta            | 128    |
| maxPersonalitySize      | 10240  |
| maxTotalRAMSize         | 151200 |
| maxSecurityGroupRules   | 20     |
| maxTotalKeypairs        | 100    |
| totalRAMUsed            | 1152   |
| maxSecurityGroups       | 10     |
| totalFloatingIpsUsed    | 0      |
| totalInstancesUsed      | 18     |
| totalSecurityGroupsUsed | 1      |
| maxTotalFloatingIps     | 10     |
| maxTotalInstances       | 30     |
| totalCoresUsed          | 18     |
| maxTotalCores           | 30     |
+-------------------------+--------+
$nova absolute-limits --tenant 11b2b129994844798c98f437d9809a9c
+-------------------------+--------+
| Name                    | Value  |
+-------------------------+--------+
| maxServerMeta           | 128    |
| maxPersonality          | 5      |
| maxImageMeta            | 128    |
| maxPersonalitySize      | 10240  |
| maxTotalRAMSize         | 151200 |
| maxSecurityGroupRules   | 20     |
| maxTotalKeypairs        | 100    |
| totalRAMUsed            | 128    |
| maxSecurityGroups       | 10     |
| totalFloatingIpsUsed    | 0      |
| totalInstancesUsed      | 2      |
| totalSecurityGroupsUsed | 1      |
| maxTotalFloatingIps     | 10     |
| maxTotalInstances       | 30     |     <-------------------
| totalCoresUsed          | 2      |
| maxTotalCores           | 30     |    <-------------------
+-------------------------+--------+
note: arrows show the wrong values.
Seems that maxTotal* shows the values for the current tenant and not what is specified by ""--tenant""
as expected.
tested in havana and icehouse-1"
1452,1334345,nova,9a01e62693a28a73120544b27ee2104558e0250e,1,1,,Bug #1334345 “Unexpected task state,"It appears that nova compute is expecting (in a number of cases) an instance to be in powering-off state, but the state is actually none.  This appears to have a wide range of failure modes.
I'm seeing this ~2800 times in the last 7 days
http://logstash.openstack.org/#eyJzZWFyY2giOiJ0YWdzOlwic2NyZWVuLW4tY3B1LnR4dFwiIEFORCBtZXNzYWdlOlwib3Nsby5tZXNzYWdpbmcucnBjLmRpc3BhdGNoZXIgVW5leHBlY3RlZFRhc2tTdGF0ZUVycm9yOiBVbmV4cGVjdGVkIHRhc2sgc3RhdGU6IGV4cGVjdGluZyAodSdwb3dlcmluZy1vZmYnLCkgYnV0IHRoZSBhY3R1YWwgc3RhdGUgaXMgTm9uZVwiIEFORCBidWlsZF9zdGF0dXM6XCJGQUlMVVJFXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwibW9kZSI6IiIsImFuYWx5emVfZmllbGQiOiIiLCJzdGFtcCI6MTQwMzcxMTkxODcxNH0=
Example traceback:
UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None
 to caller
2014-06-25 05:01:31.058 ERROR oslo.messaging._drivers.common [req-ae166761-731e-4ead-b9da-eb8e8b1cf6af None None]
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/new/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/new/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 278, in decorated_function
    LOG.info(_(""Task possibly preempted: %s"") % e.format_message())
  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 272, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 336, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 314, in decorated_function
    kwargs['instance'], e, sys.exc_info())
  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 302, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 2346, in stop_instance
    instance.save(expected_task_state=task_states.POWERING_OFF)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 187, in wrapper
    ctxt, self, fn.__name__, args, kwargs)
  File ""/opt/stack/new/nova/nova/conductor/rpcapi.py"", line 354, in object_action
    objmethod=objmethod, args=args, kwargs=kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
    retry=self.retry)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
    timeout=timeout, retry=retry)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 401, in send
    retry=retry)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 392, in _send
    raise result
UnexpectedTaskStateError_Remote: Unexpected task state: expecting (u'powering-off',) but the actual state is None
Traceback (most recent call last):
  File ""/opt/stack/new/nova/nova/conductor/manager.py"", line 404, in _object_dispatch
    return getattr(target, method)(context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 196, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance.py"", line 473, in save
    columns_to_join=_expected_cols(expected_attrs))
  File ""/opt/stack/new/nova/nova/db/api.py"", line 780, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 164, in wrapper
    return f(*args, **kwargs)
  File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 2229, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 2280, in _instance_update
    actual=actual_state, expected=expected)
UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None"
1453,1334412,neutron,a630b3bae84091f0ff1029a87e3fc27eb9d7f5e3,0,0,"Feature. “Since nova notifications introduce a run-time dependency on novaclient, there should be an external sanity check added that tests the running systems ability to import neutron.notifiers.nova.”",Add an external sanity check for nova notification...,"Since nova notifications introduce a run-time dependency on novaclient, there should be an external sanity check added that tests the running systems ability to import neutron.notifiers.nova."
1454,1334651,nova,2f3d774eb51eac9a370813362047a2cb3d124d86,1,1,,Nova api service outputs error messages when SIGHU...,"When SIGHUP signal is send to nova-api service, it stops all the nova-api processes and while restarting the nova-api processes, it throws AttributeError: 'WSGIService' object has no attribute 'reset'.
2014-06-24 15:52:55.185 CRITICAL nova [-] AttributeError: 'WSGIService' object has no attribute 'reset'
2014-06-24 15:52:55.185 TRACE nova Traceback (most recent call last):
2014-06-24 15:52:55.185 TRACE nova   File ""/usr/local/bin/nova-api"", line 10, in <module>
2014-06-24 15:52:55.185 TRACE nova     sys.exit(main())
2014-06-24 15:52:55.185 TRACE nova   File ""/opt/stack/nova/nova/cmd/api.py"", line 56, in main
2014-06-24 15:52:55.185 TRACE nova     launcher.launch_service(server, workers=server.workers or 1)
2014-06-24 15:52:55.185 TRACE nova   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 340, in launch_service
2014-06-24 15:52:55.185 TRACE nova     self._start_child(wrap)
2014-06-24 15:52:55.185 TRACE nova   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 324, in _start_child
2014-06-24 15:52:55.185 TRACE nova     launcher.restart()
2014-06-24 15:52:55.185 TRACE nova   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 145, in restart
2014-06-24 15:52:55.185 TRACE nova     self.services.restart()
2014-06-24 15:52:55.185 TRACE nova   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 478, in restart
2014-06-24 15:52:55.185 TRACE nova     restart_service.reset()
2014-06-24 15:52:55.185 TRACE nova AttributeError: 'WSGIService' object has no attribute 'reset'
2014-06-24 15:52:55.185 TRACE nova
Steps to reproduce:
1. Run nova-api service as daemon.
2. Send SIGHUP signal to nova-api service
   kill -1 <parent_process_id_of_nova_api>"
1455,1334708,neutron,3709a1be92257daccba3f2506bea6d63f732564a,1,1,,NSX - Floating IP status does not transition corre...,"This patch verifies Floating IP status is updated correctly according to blue print.
https://review.openstack.org/#/c/102700/
https://blueprints.launchpad.net/neutron/+spec/fip-op-status
VMWWare-Minesweeper fails consistently. The Jenkins gates pass ok
Please check"
1457,1334774,glance,834e1f2150c6f2afd92036e6b7f53afee4006682,1,0,"“upload_utils is concatenating str and Message objects which doesn't work
on python 2, so change the str to a unicode object.”",test_quota unit tests failing with UnicodeError fr...,"I'm seeing something like this in many unrelated changes:
2014-06-26 15:55:57.452 | FAIL: glance.tests.unit.test_quota.TestImageLocationQuotas.test_add_too_many_image_locations
2014-06-26 15:55:57.452 | tags: worker-0
2014-06-26 15:55:57.452 | ----------------------------------------------------------------------
2014-06-26 15:55:57.452 | Traceback (most recent call last):
2014-06-26 15:55:57.452 |   File ""glance/tests/unit/test_quota.py"", line 568, in test_add_too_many_image_locations
2014-06-26 15:55:57.452 |     self.assertIn('Attempted: 2, Maximum: 1', str(exc))
2014-06-26 15:55:57.452 |   File ""glance/openstack/common/gettextutils.py"", line 333, in __str__
2014-06-26 15:55:57.452 |     raise UnicodeError(msg)
2014-06-26 15:55:57.452 | UnicodeError: Message objects do not support str() because they may contain non-ascii characters. Please use unicode() or translate() instead.
http://logs.openstack.org/63/102863/1/check/gate-glance-python27/6ad16a3/console.html
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVW5pY29kZUVycm9yOiBNZXNzYWdlIG9iamVjdHMgZG8gbm90IHN1cHBvcnQgc3RyKCkgYmVjYXVzZSB0aGV5IG1heSBjb250YWluIG5vbi1hc2NpaSBjaGFyYWN0ZXJzLiBQbGVhc2UgdXNlIHVuaWNvZGUoKSBvciB0cmFuc2xhdGUoKSBpbnN0ZWFkLlwiIEFORCB0YWdzOmNvbnNvbGUgQU5EIHByb2plY3Q6XCJvcGVuc3RhY2svZ2xhbmNlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDM4MDQ5ODkwMDZ9
There is a gettextutils sync here: https://review.openstack.org/#/c/91047/
But that didn't hit this in the check queue, but hit it after being approved."
1458,1334922,neutron,54ffa26ce8e13ce0f77baae7b29bb793eb98f845,0,0,Bug in test,agent functional tests reference incorrect MAX_LEN...,"File ""neutron/neutron/tests/functional/agent/linux/base.py"", line 59, in create_resource
    name = self.get_rand_name(n_const.DEV_NAME_MAX_LEN, name_prefix)
AttributeError: 'module' object has no attribute 'DEV_NAME_MAX_LEN'"
1459,1334938,nova,7028f074202a59111f461b9fa935f44b6d118f41,1,1,,established floating ip connection won't get disco...,"Established connections via floating ip won't get disconnected after we disassociating that floating ip.
As mentioned in maillist by Vish, we should move clean_conntrack() from migration code to  remove_floating_ip.
https://github.com/openstack/nova/blob/master/nova/network/floating_ips.py#L575"
1460,1334965,nova,d7d4ef350c971c442954d772b639f8b673630896,1,1,,the headroom infomation  is wrong in the method of...,"If multiple resources beyond the quotas ,  the headroom information  about the resources beyond the quotas is incomplete(quota.py).
for example:
If the lens of  path and content of the  injected files  exceed the limits, the above situation will appear.
2014-06-27 12:14:04.241 4547 INFO nova.quota [req-a94185e5-779f-4455-ba45-230eb70fb774 None] overs: ['injected_file_content_bytes', 'injected_file_path_bytes']
2014-06-27 12:14:04.241 4547 INFO nova.quota [req-a94185e5-779f-4455-ba45-230eb70fb774 None] headroom: {'injected_file_path_bytes': 255}"
1461,1335076,nova,f8ae852c1a267a15f6b70026ad40d5d219fc0d33,1,1,"""This should be a coherent message.”",Exception raised by attach interface is problemati...,The exception raised is inappropriate. It just returns the instance object. This should be a coherent message.
1462,1335193,nova,0ca33df5660849ce305f9e9756007d95fcbbfa2b,1,0,"“    In newer kernel versions, the network devices added for lxc containers
    do not get checksums added. “",libvirt lxc needs iptables checksum added for dhcp...,"I tested a devstack today with libvirt-lxc, and was unable to get a dhcp address in cirros 0.3.2.
The reason is that cirros's udhcpc seems to ignore the response if it doesn't have checksums.
the appropriate mangle rule would be written if /dev/vhost-net , but with newer kernels this is also happening on the lxc network devices.
It seems the sane thing to do at this point is just to drop the protection based on '/dev/vhost-net' presence.
--
Related bugs:
 * https://bugs.launchpad.net/ubuntu/+source/isc-dhcp/+bug/930962
 * https://bugzilla.redhat.com/show_bug.cgi?id=910619#c6"
1463,1335295,cinder,f50be86330e81cb9dd61b8a2a635fa1a784b702a,1,1,"“However, this config option was accidentally dropped in https://review.openstack.org/#/c/67657/20/cinder/service.py

“",osapi_volume_worker config option was removed,"osapi_volume_worker was introduced when enabling multi-process API support for Cinder.  However, this config option was accidentally dropped in https://review.openstack.org/#/c/67657/20/cinder/service.py"
1464,1335315,nova,825321cd7af9fc033d05f7bfbf730b50e4c6533c,1,0,"""old-world style instance and failing on the task_state attribute access.”",Bug #1335315 “Cells,"Set admin password fails in the child cell service with the following:
2014-06-24 20:45:08.403 29591 DEBUG nova.openstack.common.policy [req- None] Rule compute
:set_admin_password will be now enforced enforce /opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/openstack/common/
policy.py:288
2014-06-24 20:45:08.404 29591 ERROR nova.cells.messaging [req- None] Error processing mes
sage locally: 'dict' object has no attribute 'task_state'
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging Traceback (most recent call last):
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ells/messaging.py"", line 200, in _process_locally
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ells/messaging.py"", line 1289, in _process_message_locally
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ells/messaging.py"", line 692, in run_compute_api_method
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return fn(message.ctxt, *args, **method_info['method_kwargs'])
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 201, in wrapped
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 191, in inner
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return function(self, context, instance, *args, **kwargs)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 172, in inner
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return f(self, context, instance, *args, **kw)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 2712, in set_admin_password
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging instance.task_state = task_states.UPDATING_PASSWORD
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging AttributeError: 'dict' object has no attribute 'task_state'
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging"
1465,1335821,neutron,5b34d41828a2a1260b1b92f6bfd91e1264aea7a8,0,0,Bug in test,Unit tests use inconsistent default tenant id,"Meters and security groups use 'test_tenant' while all other resources use 'test-tenant'.
This means that a test that creates multiple types of resources (While using the the default tenant id) would find that some resources were created under one tenant, and other resources under another tenant.
For example, a test creates a network, subnet, port, security group and meter label + rule. Listing all resources that belong to the 'test-tenant' would return a partial list, leading to confusing results."
1466,1335859,nova,aaa9d6a8a28335b8dac4e1d4045571aa565e3196,0,0,Bug in test,Wrong assert in nova.tests.virt.vmwareapi.test_vmo...,"bad assertion in nova.tests.virt.vmwareapi.vmwareapi.test_vmops.py:640
self.assertTrue(3, len(mock_mkdir.mock_calls)) should be replaced with assertEqual"
1467,1335959,neutron,8479c212b79e61aa9b694ec1368a1073898a09d5,1,0,"""This most likely is due to a recent commit: https://bugs.launchpad.net/neutron/+bug/1316190""",Neutron dhcp agent variable 'mode' referenced befo...,UnboundLocalError: local variable 'mode' referenced before assignment. This most likely is due to a recent commit: https://bugs.launchpad.net/neutron/+bug/1316190
1468,1336127,nova,6ddd9f93f82427ce909c7773f7a806361035a0b2,1,1,,The volumes will be deleted when  creating a virtu...,"when specifying a volume or an image with a user volume to create a virtual machine, if the virtual machine fails to be created for the first time with the parameter delete_on_termination being set “true”, the specified volume or the user volume will be deleted, which causes that the rescheduling fails.
for example:
1. upload a image
| 62aa6627-0a07-4ab4-a99f-2d99110db03e | cirros-0.3.2-x86_64-uec | ACTIVE
2.create a boot volume by the above image
cinder create --image-id 62aa6627-0a07-4ab4-a99f-2d99110db03e --availability-zone nova 1
| b821313a-9edb-474f-abb0-585a211589a6 | available | None | 1 | None | true | |
3. create a virtual machine
nova boot --flavor m1.tiny --nic net-id=28216e1d-f1c2-463b-8ae2-330a87e800d2 tralon_disk1 --block-device-mapping vda=b821313a-9edb-474f-abb0-585a211589a6::1:1
ERROR (BadRequest): Block Device Mapping is Invalid: failed to get volume b821313a-9edb-474f-abb0-585a211589a6. (HTTP 400) (Request-ID: req-486f7ab5-dc08-404e-8d4c-ac570d4f4aa1)
4. use the ""cinder list"" to find that the volume b821313a-9edb-474f-abb0-585a211589a6 has been deleted
+----+--------+------+------+-------------+----------+-------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+----+--------+------+------+-------------+----------+-------------+
+----+--------+------+------+-------------+----------+-------------+"
1470,1336175,neutron,66abc6e5392047d4919a95cbafc9282f6a14c684,1,0,“commit 9d13ea88 had an argument mismatch which causes the following:”,ofagent argument mismatch,"commit 9d13ea88 had an argument mismatch which causes the following:
TypeError: _get_ports() takes exactly 2 arguments (1 given)"
1471,1336196,neutron,69dfbd9468282f9ce16cc5b0cb1bcb121632a0d8,0,0,Bug in test,Inconsistent keyword for automatic deletion of res...,"Ports, networks and subnets have a do_delete=True parameter. By default, these resources are deleted at the end of the context manager scope. All other resources use a different semantic: no_delete=False.
This causes confusing situations such as:
with self.subnet(network, do_delete=False) as subnet:
    with self.security_group(no_delete=True) as sg:
        pass
I personally fell to the pitfall of using do_delete for the security group and was surprised when it wasn't deleted at the end of the scope.
Finally, the double negative of no_delete=False is confusing and should be avoided."
1472,1336198,neutron,fd772afb697b70bf9da009976681a2a67d6fa5f1,0,0,Bug in test,Metering test plugin includes initialization code ...,Currently the metering test case includes setUp code as well as the tests themselves. Splitting up the test case class into two allows other test cases to inherit the metering plugin class without inherting (And implicitly re-running) the tests themselves.
1473,1336207,neutron,dc6b07d4a37ef0db9906187611fec8e8753803cd,1,1,“CVE=2014-3555),[OSSA 2014-025] There is no quota for allowed addr...,"Hi all,
There is no quota for allowed address pair, user can create unlimited allowed address pair, in the backend, there will be at least 1 iptables rule for one allowed address pair.  I tested if we use the attachment script to add about 10,000 allowed address pair. It will cost 30 sec to reflesh iptables rules in kernel...  I think that bad man can use this api to attack compute nodes. This will make the compute nodes crash or very slow only if we add enough allowed address pair rules...
Thanks.
Liping Mao"
1474,1336251,neutron,ee9fe2458f813b5d367bc7263c7a30a3af46aa2b,1,1,“Remove db lock and add missing contexts”,Bug #1336251 “Big Switch,"The new consistency hash table seems to be causing deadlocks.
http://openstack-ci-gw.bigswitch.com/logs/refs-changes-13-92013-4/BSN_PLUGIN/logs/screen/screen-q-svc.log.gz"
1475,1336556,neutron,b98dda29c9bf2587c2eddad0231e337a1cb4ce02,1,1,“Fix missing migration default value”,migration default value not being applied to dbase...,"I have a private repo that is a few weeks behind the upstream, and was having problems stacking. Initially it was failing in one of the migrations, so I changed the SQLAlchemy version to 0.8.4 and alembic to 0.6.5 and retried (clean database).
What I saw was that the migration worked, but later a router create failed.  It was complaining that the enable_snat field was not specified and did not have a default value. Checking in the migrations, I saw that 128e042a2b68_ext_gw_mode.py adds this field and sets the default value to True. However, it was not doing that.
I changed the migration file to use server_default=sa.text(""true"") and now the migration works. BTW, it was the only migration file using 'default=True'.
Not sure if we should just apply this change to the existing migration file or if some other action is needed."
1476,1336566,neutron,fc01ddaaa5f1170b746f47837104352bfe36eaa5,1,1,,unboundlocalerror during sync_routers_task,"I observed this stacktrace:
http://paste.openstack.org/show/85277/
It looks like there may be circumstances where routers do not get initialized. It's better to be more defensive in this regard."
1477,1336596,neutron,73b4239fbef5e52494cf3ffc3293dbb79b52d91c,0,0,Feature. “This change addresses the proper clean up of tables. i.e.”,Bug #1336596 “Cisco N1k,"During rollback operations, the resource is cleaned up from the neutron database but leaves a few stale entries in the n1kv specific tables.
Vlan/VXLAN allocation tables are inconsistent during network rollbacks.
VM-Network table is left inconsistent during port rollbacks.
Explicitly clearing ProfileBinding table entry (during network profile rollbacks) is not required as delete_network_profile internally takes care of it."
1478,1336648,cinder,f66f9e5805070e5ad2ba4688c6c4571ef1ec395a,1,1,,iSER transport protocol is Broken,"when configuring ISER  ( at /etc/cinder/cinder.conf: volume_driver=cinder.volume.drivers.lvm.LVMISERDriver)
volumes cannot be attached.
this happens also in Icehouse (stable).
i will add more logs soon."
1479,1336767,nova,2fee082e9267c75a1e676361d975e52ac9ad86f8,1,1,,Instance disappeared during wait for destroy shoul...,"Currently Nova logs an error if a libvirt domain disappears during while waiting for it to be destroyed, but the code actually treats this (correctly) as a recoverable situation since the end result is the required one.  Hence this should be logged as a warning not  an error.
This may help wit some of the gate failures:  https://bugs.launchpad.net/nova/+bug/1300279"
1480,1336970,glance,8c161b6a4b0a7617ee224b23ada0a368e97eaae7,1,0,“commit cb93eb60abf10af6fbb1ee438a6e8a51853e6788 changed the VMware store to use chunked encoding when the size is not known”,VMware store to use content-length when available,"commit cb93eb60abf10af6fbb1ee438a6e8a51853e6788 changed the VMware store to use chunked encoding when the size is not known (or zero).
We still need to use Content-Length when we know the image size."
1481,1337088,cinder,87cd23419bdd5724e742eda21bc23876b558e0d1,1,1,,Volume detach will be attempted if volume stats is...,"In begin_detaching, the detach will be attempted if volume state is ""in-use"" OR attach_status is ""attached"" but the error message states that detach will only be attempted if volume state is ""in-use"" AND attach_status is ""attached"""
1482,1337236,nova,488f88c4d021bf429f62ca46dd8299a70d31505e,1,1,,Bug #1337236 “vmware,"First use vcenter driver to spawn some instances to one of the datastores that esxi is binding to.  Later this datastore became unavailable due to certain reason(power off or network problem).  Then when restart nova-compute, found that compute service will exit with errors.  This will openstack compute not usable.
2014-07-03 01:38:13.961 3634 DEBUG nova.compute.manager [req-11bc0618-8696-464d-8820-7565db8f44c3 None None] [instance: 9428cf95-5
37f-48f6-b79e-faa981f6066d] NV-AC7AA80 Checking state _get_power_state /usr/lib/python2.6/site-packages/nova/compute/manager.py:10
54
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/lib/python2.6/site-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/service.py"", line 480, in run_service
    service.start()
  File ""/usr/lib/python2.6/site-packages/nova/service.py"", line 180, in start
    self.manager.init_host()
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1037, in init_host
    self._init_instance(context, instance)
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 865, in _init_instance
    try_reboot, reboot_type = self._retry_reboot(context, instance)
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 963, in _retry_reboot
    current_power_state = self._get_power_state(context, instance)
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1056, in _get_power_state
    return self.driver.get_info(instance)[""state""]
  File ""/usr/lib/python2.6/site-packages/nova/virt/vmwareapi/driver.py"", line 862, in get_info
    return _vmops.get_info(instance)
  File ""/usr/lib/python2.6/site-packages/nova/virt/vmwareapi/vmops.py"", line 1376, in get_info
    max_mem = int(query['summary.config.memorySizeMB']) * 1024
KeyError: 'summary.config.memorySizeMB'"
1483,1337349,nova,4431eec1c94c4a353b45e5d873854b3fb1eaa11b,1,1,,Nova qemu hypervisor host smbios serial number is ...,"Erwan Velu from eNovance reported a vulnerability in OpenStack Nova.
The hypervisor is passing host system uuid (-smbios version) to guests, and this happen to be a critical info leak.
The defect have been pinpointed to:
 https://github.com/openstack/nova/blob/master/nova/virt/libvirt/driver.py#L3054
From a simple virtual machine, this may allow numerous info leak like:
    Allow compute hardware enumeration from guests
    Deduce service tag and get all hardware configuration
    Ability to know if two instances are on the same compute
Dell hardware is particulary impacted as :
    - the uuid encodes the service tag
    - the service tag can be used on support site to determine:
    - detailled hardware configuration
    - date & country where the hw was shipped
    - date & type of support contract
    - amount of servers bought during this shipment
If there is no use case for this, we should scrambled that piece of information."
1484,1337526,cinder,06dd7f28d1d3ec8619df0d25ddfe977d538897b3,1,1,“the field volume_image_metadata is missing from the response.’,The volume_image_metadata field is missing from vo...,"When listing all volumes, the field volume_image_metadata is missing from the response.
When enabling debug, this error is logged:
Problem retrieving volume image metadata. It will be skipped. Error: Entity '<class 'cinder.db.sqlalchemy.models.VolumeGlanceMetadata'>' has no property 'project_id' _get_all_images_metadata /usr/lib/python2.7/dist-packages/cinder/api/contrib/volume_image_metadata.py:43
If code is updated to log the whole traceback instead, this can be found:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/cinder/api/contrib/volume_image_metadata.py"", line 39, in _get_all_images_metadata
    all_metadata = self.volume_api.get_volumes_image_metadata(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/volume/api.py"", line 686, in get_volumes_image_metadata
    db_data = self.db.volume_glance_metadata_get_all(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/api.py"", line 552, in volume_glance_metadata_get_all
    return IMPL.volume_glance_metadata_get_all(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 137, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 2447, in volume_glance_metadata_get_all
    return _volume_glance_metadata_get_all(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 137, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 2436, in _volume_glance_metadata_get_all
    session=session).\
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 195, in model_query
    query = query.filter_by(project_id=context.project_id)
  File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 1249, in filter_by
    for key, value in kwargs.iteritems()]
  File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/util.py"", line 1218, in _entity_descriptor
    (description, key)
InvalidRequestError: Entity '<class 'cinder.db.sqlalchemy.models.VolumeGlanceMetadata'>' has no property 'project_id'
The problem is caused by a filtering done on project_id.
VolumeGlanceMetadata does not have the project_id field, filtering should be done on the Volume associated to the VolumeGlanceMetadata instead."
1485,1337796,cinder,4caca0dfae45dd6c61ddc1b4a2233d2ce1a11685,1,1,,Cinder api service outputs error messages when SIG...,"When SIGHUP signal is send to cinder-api service, it stops all the cinder-api processes and while restarting the cinder-api processes, it throws AttributeError: 'WSGIService' object has no attribute 'reset'.
Steps to reproduce:
1. Run cinder-api service as daemon.
2. Send SIGHUP signal to cinder-api service
   kill -1 <parent_process_id_of_cinder_api>"
1486,1337821,nova,28b37c1a707f5e958221b4ee28c4832d081eb706,1,1,,VMDK Volume attach fails while attaching to an ins...,"I have booted an instance from a volume, successfully booted,
now another volume, i try to attach to same instance, it is failing.
see the stack trace..
2014-07-04 08:56:11.391 TRACE oslo.messaging.rpc.dispatcher     raise exception.InvalidDevicePath(path=root_device_name)
2014-07-04 08:56:11.391 TRACE oslo.messaging.rpc.dispatcher InvalidDevicePath: The supplied device path (vda) is invalid.
2014-07-04 08:56:11.391 TRACE oslo.messaging.rpc.dispatcher
2014-07-04 08:56:11.396 ERROR oslo.messaging._drivers.common [req-648122d5-fd39-495b-a3a7-a96bd32091d6 admin admin] Returning exception The supplied device path (vda) is invalid. to caller
2014-07-04 08:56:11.396 ERROR oslo.messaging._drivers.common [req-648122d5-fd39-495b-a3a7-a96bd32091d6 admin admin] ['Traceback (most recent call last):\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n    incoming.message))\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 401, in decorated_function\n    return function(self, context, *args, **kwargs)\n', '  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped\n    payload)\n', '  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', '  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped\n    return f(self, context, *args, **kw)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 286, in decorated_function\n    pass\n', '  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function\n    return function(self, context, *args, **kwargs)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 314, in decorated_function\n    kwargs[\'instance\'], e, sys.exc_info())\n', '  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 302, in decorated_function\n    return function(self, context, *args, **kwargs)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 4201, in reserve_block_device_name\n    return do_reserve()\n', '  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner\n    return f(*args, **kwargs)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 4188, in do_reserve\n    context, instance, bdms, device)\n', '  File ""/opt/stack/nova/nova/compute/utils.py"", line 106, in get_device_name_for_instance\n    mappings[\'root\'], device)\n', '  File ""/opt/stack/nova/nova/compute/utils.py"", line 155, in get_next_device_name\n    raise exception.InvalidDevicePath(path=root_device_name)\n', 'InvalidDevicePath: The supplied device path (vda) is invalid.\n']
The reason behind this issue is: because of the root device_name being set 'vda' in the case of boot from volume, The future volume attaches to the VM fail saying ""The supplied device path (vda) is invalid"""
1487,1338451,nova,9d4b49c542e2076c8a572d1e8c6d50e255efe087,1,1,,shelve api does not work in the nova-cell environm...,"If you run nova shelve api in nova-cell environment It throws following error:
Nova cell (n-cell-child) Logs:
2014-07-06 23:57:13.445 ERROR nova.cells.messaging [req-a689a1a1-4634-4634-974a-7343b5554f46 admin admin] Error processing message locally: save() got an unexpected keyword argument 'expected_task_state'
2014-07-06 23:57:13.445 TRACE nova.cells.messaging Traceback (most recent call last):
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/cells/messaging.py"", line 200, in _process_locally
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     resp_value = self.msg_runner._process_message_locally(self)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/cells/messaging.py"", line 1287, in _process_message_locally
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     return fn(message, **message.method_kwargs)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/cells/messaging.py"", line 700, in run_compute_api_method
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     return fn(message.ctxt, *args, **method_info['method_kwargs'])
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/compute/api.py"", line 192, in wrapped
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     return func(self, context, target, *args, **kwargs)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/compute/api.py"", line 182, in inner
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     return function(self, context, instance, *args, **kwargs)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/compute/api.py"", line 163, in inner
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     return f(self, context, instance, *args, **kw)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/compute/api.py"", line 2458, in shelve
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     instance.save(expected_task_state=[None])
2014-07-06 23:57:13.445 TRACE nova.cells.messaging TypeError: save() got an unexpected keyword argument 'expected_task_state'
2014-07-06 23:57:13.445 TRACE nova.cells.messaging
Nova compute log:
2014-07-07 00:05:19.084 ERROR oslo.messaging.rpc.dispatcher [req-9539189d-239b-4e74-8aea-8076740
31c2f admin admin] Exception during message handling: 'NoneType' object is not iterable
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _
dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _
dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _
do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/conductor/manager.py"", line 351, in notify_usage_exists
    system_metadata, extra_usage_info)
  File ""/opt/stack/nova/nova/compute/utils.py"", line 250, in notify_usage_exists
    ignore_missing_network_data)
  File ""/opt/stack/nova/nova/notifications.py"", line 285, in bandwidth_usage
    macs = [vif['address'] for vif in nw_info]
TypeError: 'NoneType' object is not iterable
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dis
t-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 280, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     pass
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 266, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 330, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 308, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 296, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 3847, in shelve_instance
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     self.conductor_api.notify_usage_exists(
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/conductor/api.py"", line 206, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     system_metadata, extra_usage_info)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 320, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     extra_usage_info=extra_usage_info_p)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     retry=self.retry)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     timeout=timeout, retry=retry)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 401, in send
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     retry=retry)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 392, in _send
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     raise result
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/conductor/manager.py"", line 351, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     system_metadata, extra_usage_info)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/utils.py"", line 250, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     ignore_missing_network_data)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/notifications.py"", line 285, in bandwidth_usage
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     macs = [vif['address'] for vif in nw_info]
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.093 ERROR oslo.messaging._drivers.common [req-9539189d-239b-4e74-8aea-807674031c2f admin admin] Returning exception 'NoneType' object is not iterable
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/conductor/manager.py"", line 351, in notify_usage_exists
    system_metadata, extra_usage_info)
  File ""/opt/stack/nova/nova/compute/utils.py"", line 250, in notify_usage_exists
    ignore_missing_network_data)
  File ""/opt/stack/nova/nova/notifications.py"", line 285, in bandwidth_usage
    macs = [vif['address'] for vif in nw_info]
TypeError: 'NoneType' object is not iterable
 to caller
Shelve api is failing in nova-cell environment because the compute_api shelve/unshelve
methods expect an Instance object, but cell is still passing the sqlalchemy form.
Also 'info_cache' and 'metadata' are not present in instance-object and shelve requires these
properties to be present in Instance object."
1488,1338479,neutron,3ca85460a178a18dea60399fd369ed84b17721dc,1,0,"“ In SQLite Integer can be stored in 1, 2, 3, 4, 6, or 8 bytes depending
    on the magnitude of the value. “",Unhelpful error message when updating quota,"When updating network quota using the following command:
neutron quota-update --network 10000000000
the client ouputs:
""Request Failed: internal server error while processing your request.""
This request fails since the parameter exceeds the integer range. An error message like ""Request Failed: quota limit exceeds integer range"" would be more friendly to users than just raising a single internal server error."
1489,1338481,cinder,4469591c76888bad263662df424c3f266ded8d48,1,1,,Fix error log level in restore-backup routine,"When doing backup-restore operation, it needs volume's size is not less than backup's size.
But in backup-restore routine, when volume's size is bigger than backup's size, it logs the
the  info of volume's size and backup's size to warn level, which I think it should log the info
to info level.
This bug fix it."
1490,1338853,neutron,31d67cfdee5b9f1ed77df2c9cda1c849fd7d6181,1,1,,Remove tables that store the mapping between neutr...,"Nuage plugin stores a mapping of neutron and VSD id's for every neutron resource.
This bug is to remove the mapping to avoid storing redundant data and also avoid the
upgrade and out of sync issues."
1491,1339235,nova,aa1792eb4c1d10e9a192142ce7e20d37871d916a,1,1,"“Commit cc5388bbe81aba635fb757e202d860aeed98f3e8 added locks to
    stop_instance and the _sync_power_states periodic task to try and fix a”",Bug #1339235 “UnexpectedTaskStateError,"This is showing up all over the n-cpu logs on teardown of tempest tests:
UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None
For example:
http://logs.openstack.org/06/103206/4/check/check-tempest-dsvm-postgres-full/b5e8f3c/logs/screen-n-cpu.txt.gz?level=TRACE
We have nearly 40K hits on this in logstash in 7 days:
message:""UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None"" AND tags:""screen-n-cpu.txt""
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVW5leHBlY3RlZFRhc2tTdGF0ZUVycm9yOiBVbmV4cGVjdGVkIHRhc2sgc3RhdGU6IGV4cGVjdGluZyAodSdwb3dlcmluZy1vZmYnLCkgYnV0IHRoZSBhY3R1YWwgc3RhdGUgaXMgTm9uZVwiIEFORCB0YWdzOlwic2NyZWVuLW4tY3B1LnR4dFwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA0ODQxMjQ3MDk4fQ==
This is the interesting traceback from the compute manager:
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/exception.py"", line 88, in wrapped
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/exception.py"", line 71, in wrapped
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 272, in decorated_function
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     LOG.info(_(""Task possibly preempted: %s"") % e.format_message())
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 266, in decorated_function
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 330, in decorated_function
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 308, in decorated_function
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 296, in decorated_function
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 2356, in stop_instance
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     instance.save(expected_task_state=task_states.POWERING_OFF)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/objects/base.py"", line 187, in wrapper
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     ctxt, self, fn.__name__, args, kwargs)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/conductor/rpcapi.py"", line 349, in object_action
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     objmethod=objmethod, args=args, kwargs=kwargs)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     retry=self.retry)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     timeout=timeout, retry=retry)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 404, in send
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     retry=retry)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 395, in _send
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     raise result
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher UnexpectedTaskStateError_Remote: Unexpected task state: expecting (u'powering-off',) but the actual state is None"
1493,1339462,nova,6086db51b0c3acaf2b88617e3e739e06d894d761,1,0,"Platform. “Current Nova vmwareapi cannot configure guest VM's SCSI controller type as “Paravirtual"".""",Bug #1339462 “vmware,"nova icehouse Ubuntu14.04
Version: 1:2014.1-0ubuntu1.2
Current Nova vmwareapi cannot configure guest VM's SCSI controller type as ""Paravirtual"".
(though this requires vmwaretools running on the guest VM)"
1494,1339775,glance,6b8d72df08e4b70b8de4b9797ae1c03be64e0072,1,1,,Some v2 exceptions raise unicodeError,"In v2, for some exceptions the exception is not converted to a string properly resulting in:
2014-07-09 15:36:38,714 INFO msg = _(""Image exceeds the storage quota: %s"") % e
2014-07-09 15:36:38,714 INFO File ""glance/openstack/common/gettextutils.py"", line 333, in __str__
2014-07-09 15:36:38,714 INFO raise UnicodeError(msg)
in some cases 'utils.exception_to_str' is used to cast the exception.
We should do this in all cases for consistency/to avoid the stack trace"
1495,1339855,neutron,e8e2392321e0fd3e2b8a0345b725f2e8df854a34,1,1,,Raise NotImplementedError instead of NotImplemente...,"NotImplementedError is the name of the exception (https://docs.python.org/2/library/exceptions.html).
NotImplemented is the name of a constant (https://docs.python.org/2/library/constants.html).
It makes no sense to raise a constant. The exception should be raised instead."
1496,1339968,neutron,d144cb8ff422e16c5917c87471a46fd095fd1856,1,1,,Bug #1339968 “duplicate dhcp port when deleting original dhcp po... ,"my use case is as follows:
1.delete dhcp port
2.restart dhcp-agent
then, there will be two dhcp ports."
1498,1340145,neutron,cbd89e75d34c048c15c52dc828b1df7443c540d6,0,0,feature? ”Right now CommonDBMixin resides in db_base_plugin_v2.py so those plugins require to import it.”,Extract CommonDBMixin into a separate module,"Several service plugins are inheritig from CommonDBMixin which has a few utulity methods.
Right now CommonDBMixin resides in db_base_plugin_v2.py so those plugins require to import it.
In some cases it is undesirable and can lead to cycles in imports,
so CommonDBMixin needs to be extracted into a different module"
1499,1340159,nova,80df9f5ec53cc71c1ec51a8590921ae5b776ec22,1,1,,Resize to zero disk flavor is not allowed,"When old flavor's root_gb is not equal 0 and new flavor's root_gb is 0,    the resize() in nova.compute.api will raise CannotResizeDisk.
https://github.com/openstack/nova/blob/master/nova/compute/api.py#L2368
    def resize(self, context, instance, flavor_id=None,
        if not flavor_id:
            LOG.debug(""flavor_id is None. Assuming migration."",
                      instance=instance)
            new_instance_type = current_instance_type
        else:
            new_instance_type = flavors.get_flavor_by_flavor_id(
                    flavor_id, read_deleted=""no"")
            if (new_instance_type.get('root_gb') == 0 and
                current_instance_type.get('root_gb') != 0):
                reason = _('Resize to zero disk flavor is not allowed.')
                raise exception.CannotResizeDisk(reason=reason)"
1500,1340315,cinder,15a93312641bb6121d20c7749481104a1ab34bd0,1,1,,Bug #1340315 “VMware,"The minimum disk size of a backing VM is set to 1KB, but the VIM APIs fail the operation if the disk size is less than 1MB."
1501,1340431,neutron,ff1ce62a0b9f504fd1dd46fb569ab85f5b56e4ff,1,1,,Bug #1340431 “NSX,"when the transport type for a network gateway connection is vlan, the neutron code does not validate that the segmentation id is between 0 and 4095.
The requests is then sent to NSX where it fails. However a 500 error is returned to the neutron API user because of the backend failure.
The operation should return a 400 and possibly not go at all to the backend."
1502,1340552,nova,bd5c5d5bbd808b4f83da58dce433cac711575bee,1,1,"""The feature above was put in by this commit: https://github.com/openstack/nova/commit/dc716bd0ce77b56f4aabe54d6633b7f3bf9b0a5d. I agree with your proposed fix.”",Volume detach error when use NFS as the cinder bac...,"Tested Environment
--------------------------
OS: Ubuntu 14.04 LST
Cinder NFS driver:
volume_driver=cinder.volume.drivers.nfs.NfsDriver
Error description
--------------------------
I used NFS as the cinder storage backend and successfully attached multiple volumes to nova instances.
However, when I tried to detach one them, I found following error on nova-compute.log.
-----------Error log------
2014-07-07 17:48:46.175 3195 ERROR nova.virt.libvirt.volume [req-a07d077f-2ad1-4558-91fa-ab1895ca4914 c8ac60023a794aed8cec8552110d5f12 fdd538eb5dbf48a98d08e6d64def73d7] Couldn't unmount the NFS share 172.23.58.245:/NFSThinLun2
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Traceback (most recent call last):
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/volume.py"", line 675, in disconnect_volume
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume     utils.execute('umount', mount_path, run_as_root=True)
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume   File ""/usr/local/lib/python2.7/dist-packages/nova/utils.py"", line 164, in execute
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume     return processutils.execute(*cmd, **kwargs)
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume   File ""/usr/local/lib/python2.7/dist-packages/nova/openstack/common/processutils.py"", line 193, in execute
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume     cmd=' '.join(cmd))
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume ProcessExecutionError: Unexpected error while running command.
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Command: sudo nova-rootwrap /etc/nova/rootwrap.conf umount /var/lib/nova/mnt/16a381ac60f3e130cf26e7d6eb832cb6
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Exit code: 16
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Stdout: ''
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Stderr: 'umount.nfs: /var/lib/nova/mnt/16a381ac60f3e130cf26e7d6eb832cb6: device is busy\numount.nfs: /var/lib/nova/mnt/16a381ac60f3e130cf26e7d6eb832cb6: device is busy\n'
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume
-----------End of the Log--
For NFS volumes, every time you detach a volume, nova tries to umount the device path.
/nova/virt/libvirt/volume.py in
Line 632: class LibvirtNFSVolumeDriver(LibvirtBaseVolumeDriver):
Line 653: 	def disconnect_volume(self, connection_info, disk_dev):
Line 661:		utils.execute('umount', mount_path, run_as_root=True)
This works when the device path is not busy.
If the device path is busy (or in use), it should output a message to log and continue.
The problem is, Instead of output a log message, it raise exception and that cause the above error.
I think the reason is, the ‘if’ statement at Line 663 fails to catch the device busy massage from the content of the exc.message. It looking for the ‘target is busy’ in the exc.message, but umount error code returns ‘device is busy’.
Therefore, current code skip the ‘if’ statement and run the ‘else’ and raise the exception.
How to reproduce
--------------------------
(1)	Prepare a NFS share storage and set it as the storage backend of you cinder
(refer http://docs.openstack.org/grizzly/openstack-block-storage/admin/content/NFS-driver.html)
In cinder.conf
volume_driver=cinder.volume.drivers.nfs.NfsDriver
nfs_shares_config=<path to your nfs share list file>
(2)	Create 2 empty volumes from cinder
(3)	Create a nova instance and attach above 2 volumes
(4)	Then, try to detach one of them.
You will get the error in nova-compute.log “Couldn't unmount the NFS share <your NFS mount path on nova-compute>”
Proposed Fix
--------------------------
I’m not sure about any other OSs who outputs the ‘target is busy’ in the umount error code.
Therefore, first fix comes to my mind is fix the ‘if’ statement to:
Before fix;
if 'target is busy' in exc.message:
After fix;
if 'device is busy' in exc.message:"
1503,1340570,neutron,40417e67820b38dc5492211554ac2ac1cd272f09,0,0,Bug in test,ovs agent test is blocking to sleep,The tests for the daemon loop in the ovs tunnel are not mocking out the polling call so they take 30+ seconds each.
1504,1340696,neutron,8a50e13a31e99148cebd5580f77cb92b8808fffc,1,0,“After migrating to oslo.messaging we don't need to have debug level in oslo.messaging but it's good to have communication logged.” (Change in the enviroment),rpc communication is not logged in debug,"Currently even though service is running in with debug=True, rpc messages are not logged. Previously in icehouse we could see those logs. It helps with debugging when we can see the communication between agents and server."
1505,1340778,neutron,0540847c89a463f22438c6da6a0d03dc5a86d89f,1,1,,common/log.py creates its own logger,Using log decorator from neutron.common.log causes creating new logger and not respecting logging level of decorated methods.
1506,1340885,nova,ae744b4b60169f653d6bafba237593388266cb90,1,1,,Can't unset a flavor-key,"I am able to set a flavor-key but not unset it.  devstack sha1=fdf1cffbd5d2a7b47d5bdadbc0755fcb2ff6d52f
ubuntu@d8:~/devstack$ nova help flavor-key
usage: nova flavor-key <flavor> <action> <key=value> [<key=value> ...]
Set or unset extra_spec for a flavor.
Positional arguments:
  <flavor>     Name or ID of flavor
  <action>     Actions: 'set' or 'unset'
  <key=value>  Extra_specs to set/unset (only key is necessary on unset)
ubuntu@d8:~/devstack$ nova flavor-key m1.tiny set foo=bar
ubuntu@d8:~/devstack$ nova flavor-show m1.tiny
+----------------------------+----------------+
| Property                   | Value          |
+----------------------------+----------------+
| OS-FLV-DISABLED:disabled   | False          |
| OS-FLV-EXT-DATA:ephemeral  | 0              |
| disk                       | 1              |
| extra_specs                | {""foo"": ""bar""} |
| id                         | 1              |
| name                       | m1.tiny        |
| os-flavor-access:is_public | True           |
| ram                        | 512            |
| rxtx_factor                | 1.0            |
| swap                       |                |
| vcpus                      | 1              |
+----------------------------+----------------+
ubuntu@d8:~/devstack$ nova flavor-key m1.tiny unset foo
ubuntu@d8:~/devstack$ nova flavor-show m1.tiny
+----------------------------+----------------+
| Property                   | Value          |
+----------------------------+----------------+
| OS-FLV-DISABLED:disabled   | False          |
| OS-FLV-EXT-DATA:ephemeral  | 0              |
| disk                       | 1              |
| extra_specs                | {""foo"": ""bar""} |
| id                         | 1              |
| name                       | m1.tiny        |
| os-flavor-access:is_public | True           |
| ram                        | 512            |
| rxtx_factor                | 1.0            |
| swap                       |                |
| vcpus                      | 1              |
+----------------------------+----------------+"
1507,1341014,neutron,487b98a73f6be41697dd4f59a245f1a7f338a72f,1,1,"I think this has a BIC …. “This patch deletes all VSM credentials on neutron
start up before adding the newer VSM credentials” (b034f9de18ff0fa131a7b8ac77eeef52f23adaf5)",Update VSM credential correctly,"Today if we modify the VSM credential in the cisco_plugins.ini, the
older VSM ip address remains in the db, and all requests are sent to
the older VSM. This patch deletes all VSM credentials on neutron
start up before adding the newer VSM credentials. Hence making sure
that there is only one VSM IP address and credential in the db."
1509,1341459,nova,916aec717c2c1a6096e2f1884e4ad81a30ae70e6,1,1,,block device isn't resized after swap to larger vo...,"libvirt support swap volume. But if the new volume is larger than the old one, the block device isn't resized. The instance can't see the extra space.
$ nova show vm3
+--------------------------------------+----------------------------------------------------------------+
| Property                             | Value                                                          |
+--------------------------------------+----------------------------------------------------------------+
| OS-DCF:diskConfig                    | AUTO                                                           |
| OS-EXT-AZ:availability_zone          | nova                                                           |
| OS-EXT-SRV-ATTR:host                 | os3                                                            |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | os3                                                            |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000039                                              |
| OS-EXT-STS:power_state               | 1                                                              |
| OS-EXT-STS:task_state                | -                                                              |
| OS-EXT-STS:vm_state                  | active                                                         |
| OS-SRV-USG:launched_at               | 2014-07-14T01:43:31.000000                                     |
| OS-SRV-USG:terminated_at             | -                                                              |
| accessIPv4                           |                                                                |
| accessIPv6                           |                                                                |
| config_drive                         |                                                                |
| created                              | 2014-07-14T01:43:23Z                                           |
| flavor                               | m1.nano (42)                                                   |
| hostId                               | c8e8cab21e9e22dbc3779fd171e77f44940ba1c81161dc114ba4ad85       |
| id                                   | ccda09b7-6c50-40b0-ba7c-0c5c3f0cbb7e                           |
| image                                | cirros-0.3.2-x86_64-uec (da82a342-aeac-407a-bf9d-cf28bf68dc6b) |
| key_name                             | -                                                              |
| metadata                             | {}                                                             |
| name                                 | vm3                                                            |
| net1 network                         | 10.0.0.66                                                      |
| os-extended-volumes:volumes_attached | [{""id"": ""756d0869-2ef2-4537-90c8-66df9657135f""}]               |
| progress                             | 0                                                              |
| security_groups                      | sg1, default                                                   |
| status                               | ACTIVE                                                         |
| tenant_id                            | fdbb1e8f23eb40c89f3a677e2621b95c                               |
| updated                              | 2014-07-14T06:34:57Z                                           |
| user_id                              | 158d3c971e244f479593c86ff751bf8f                               |
+--------------------------------------+----------------------------------------------------------------+
$ cinder  list
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| 13097504-5b0c-4581-b1a5-9e05f616b89d | available | vol3 |  2   |     None    |  false   |                                      |
| 756d0869-2ef2-4537-90c8-66df9657135f |   in-use  | vol1 |  1   |     None    |  false   | ccda09b7-6c50-40b0-ba7c-0c5c3f0cbb7e |
| f0da7609-486d-49b1-bdf3-029bcbe56268 | available | vol2 |  1   |     None    |  false   |                                      |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
Then login guest OS:
$ sudo fdisk -l
.....
Disk /dev/vdc: 1073 MB, 1073741824 bytes
9 heads, 8 sectors/track, 29127 cylinders, total 2097152 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0xd6091017
   Device Boot      Start         End      Blocks   Id  System
/dev/vdc1            2048     2097151     1047552   83  Linux
Swap the volume to larger one.
$ nova volume-update vm3 756d0869-2ef2-4537-90c8-66df9657135f 13097504-5b0c-4581-b1a5-9e05f616b89d
vm3 attached with the vol3
$ cinder list
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| 13097504-5b0c-4581-b1a5-9e05f616b89d |   in-use  | vol3 |  2   |     None    |  false   | ccda09b7-6c50-40b0-ba7c-0c5c3f0cbb7e |
| 756d0869-2ef2-4537-90c8-66df9657135f | available | vol1 |  1   |     None    |  false   |                                      |
| f0da7609-486d-49b1-bdf3-029bcbe56268 | available | vol2 |  1   |     None    |  false   |                                      |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
Check the guest again:
$ sudo fdisk -l
....
Disk /dev/vdc: 1073 MB, 1073741824 bytes
9 heads, 8 sectors/track, 29127 cylinders, total 2097152 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0xd6091017
   Device Boot      Start         End      Blocks   Id  System
/dev/vdc1            2048     2097151     1047552   83  Linux
The device size isn't changed."
1510,1341465,neutron,f676d9215958b1af1105d4ff4671ce6cefd83eb7,1,1,Resgression “after recently merged commit 9d13ea884bff749b3975acb5eb5630e5aca4a665”,Bug #1341465 “ofagent,"after recently merged commit 9d13ea884bff749b3975acb5eb5630e5aca4a665, non tapXXX device handling is broken.
(the corresponding gerrit review is https://review.openstack.org/#/c/100404/)"
1512,1341738,nova,70feb102ae86d3421e46a07710e4cbab35408d02,1,1,,Multiple delete_on_terminate volumes may not get d...,"There's no catching of exceptions in _cleanup_volumes in compute/manager.py so a raised exception will short circuit cleaning up later volumes.
    def _cleanup_volumes(self, context, instance_uuid, bdms):
        for bdm in bdms:
            LOG.debug(""terminating bdm %s"", bdm,
                      instance_uuid=instance_uuid)
            if bdm.volume_id and bdm.delete_on_termination:
                self.volume_api.delete(context, bdm.volume_id)"
1514,1341954,cinder,6a41fe9c5c98a14a355fa81b41aae2c4b0ce0a7b,1,1,,suds client subject to cache poisoning by local at...,"The suds project appears to be largely unmaintained upstream. The default cache implementation stores pickled objects to a predictable path in /tmp. This can be used by a local attacker to redirect SOAP requests via symlinks or run a privilege escalation / code execution attack via a pickle exploit.
cinder/requirements.txt:suds>=0.4
gantt/requirements.txt:suds>=0.4
nova/requirements.txt:suds>=0.4
oslo.vmware/requirements.txt:suds>=0.4
The details are available here -
https://bugzilla.redhat.com/show_bug.cgi?id=978696
(CVE-2013-2217)
Although this is an unlikely attack vector steps should be taken to prevent this behaviour. Potential ways to fix this are by explicitly setting the cache location to a directory created via tempfile.mkdtemp(), disabling cache client.set_options(cache=None), or using a custom cache implementation that doesn't load / store pickled objects from an insecure location."
1515,1342046,cinder,ed2f3ad04410e1236eba95ef6d9025aa901ddbf3,1,1,,GET /v2/​{tenant_id}​/volumes?all_tenants=0 lists ...,"When a user with admin role tries to list volumes belong to a given tenant, it does not work when all_tenants=0 is specified.
Requesting with all_tenants=0 should get the same response as requesting without all_tenants parameter.
Details:
OpenStack Release Version: IceHouse
Issue a REST API with an admin token (user's role in the specified tenant is admin).
- GET /v2/​{tenant_id}​/volumes
  Lists volumes in the specified tenant.
- GET /v2/​{tenant_id}​/volumes?all_tenants=0
  Lists volumes in all tenants.
- GET /v2/​{tenant_id}​/volumes?all_tenants=1
  Lists volumes in all tenants."
1516,1342055,nova,2f121680eba9404dc723872cde4d16dea90a0c3f,1,1,,Suspending and restoring a rescued instance restor...,"If you suspend a rescued instance, resume returns it to the ACTIVE state rather than the RESCUED state."
1517,1342274,glance,adeca091242eed0b691b88f6bcaac8c8044120f6,0,0,Deprecated,auth_token middleware in keystoneclient is depreca...,The auth_token middleware in keystoneclient is deprecated and will only get security updates. Projects should use the auth_token middleware in keystonemiddleware.
1518,1342756,neutron,6762acab9ca70bad843cdee542d816ad8e3cb908,1,1,,cisco_ml2_apic_contracts table needs to set nullab...,"In DB migrations the following is detected:
INFO  [alembic.autogenerate.compare] Detected NULL on column 'cisco_ml2_apic_contracts.tenant_id'"
1519,1342880,neutron,a84a8a5d83f545d0c00b7a83ad91b52dc31c1194,1,1,,Bug #1342880 “Exception during message handling,"q-svc frequently tries to iterate on None Type.
The job can succeed even if this issue happens.
message: ""Exception during message handling"" AND message:""NoneType"" AND message:""object is not iterable"" AND filename:""logs/screen-q-svc.txt""
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIkV4Y2VwdGlvbiBkdXJpbmcgbWVzc2FnZSBoYW5kbGluZ1wiIEFORCBtZXNzYWdlOlwiTm9uZVR5cGVcIiBBTkQgbWVzc2FnZTpcIm9iamVjdCBpcyBub3QgaXRlcmFibGVcIiBBTkQgZmlsZW5hbWU6XCJsb2dzL3NjcmVlbi1xLXN2Yy50eHRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiODY0MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA1NTMzMDE3NzE4LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
 [req-ef892503-3f93-4c68-adeb-17394b66406c ] Exception during message handling: 'NoneType' object is not iterable
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 63, in sync_routers
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     self._ensure_host_set_on_ports(context, plugin, host, routers)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 76, in _ensure_host_set_on_ports
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     interface)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 84, in _ensure_host_set_on_port
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     {'port': {portbindings.HOST_ID: host}})
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/plugins/ml2/plugin.py"", line 870, in update_port
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     need_notify=need_port_update_notify)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/plugins/ml2/plugin.py"", line 302, in _bind_port_if_needed
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     plugin_context, port_id, binding, bind_context)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher"
1520,1342919,nova,963ad71af4750e28745b6de262da11816b403801,1,1,,instances rescheduled after building network info ...,"This is weird - Ironic has used the mac from a different node (which quite naturally leads to failures to boot!)
nova list | grep spawn
| 6c364f0f-d4a0-44eb-ae37-e012bbdd368c | ci-overcloud-NovaCompute3-zmkjp5aa6vgf  | BUILD  | spawning   | NOSTATE     | ctlplane=10.10.16.137 |
 nova show 6c364f0f-d4a0-44eb-ae37-e012bbdd368c | grep hyperv
 | OS-EXT-SRV-ATTR:hypervisor_hostname  | b07295ee-1c09-484c-9447-10b9efee340c                     |
 neutron port-list | grep 137
 | 272f2413-0309-4e8b-9a6d-9cb6fdbe978d |                    | 78:e7:d1:23:90:0d | {""subnet_id"": ""a6ddb35e-305e-40f1-9450-7befc8e1af47"", ""ip_address"": ""10.10.16.137""} |
ironic node-show b07295ee-1c09-484c-9447-10b9efee340c | grep wait
 | provision_state        | wait call-back                                                         |
ironic port-list | grep 78:e7:d1:23:90:0d  # from neutron
| 33ab97c0-3de9-458a-afb7-8252a981b37a | 78:e7:d1:23:90:0d |
ironic port-show 33ab97c0-3de9-458a-afb7-8252a981
+------------+-----------------------------------------------------------+
| Property   | Value                                                     |
+------------+-----------------------------------------------------------+
| node_uuid  | 69dc8c40-dd79-4ed6-83a9-374dcb18c39b                      |  # Ruh-roh, wrong node!
| uuid       | 33ab97c0-3de9-458a-afb7-8252a981b37a                      |
| extra      | {u'vif_port_id': u'aad5ee6b-52a3-4f8b-8029-7b8f40e7b54e'} |
| created_at | 2014-07-08T23:09:16+00:00                                 |
| updated_at | 2014-07-16T01:23:23+00:00                                 |
| address    | 78:e7:d1:23:90:0d                                         |
+------------+-----------------------------------------------------------+
ironic port-list | grep 78:e7:d1:23:9b:1d  # This is the MAC my hardware list says the node should have
| caba5b36-f518-43f2-84ed-0bc516cc89df | 78:e7:d1:23:9b:1d |
# ironic port-show caba5b36-f518-43f2-84ed-0bc516cc
+------------+-----------------------------------------------------------+
| Property   | Value                                                     |
+------------+-----------------------------------------------------------+
| node_uuid  | b07295ee-1c09-484c-9447-10b9efee340c                      |  # and tada right node
| uuid       | caba5b36-f518-43f2-84ed-0bc516cc89df                      |
| extra      | {u'vif_port_id': u'272f2413-0309-4e8b-9a6d-9cb6fdbe978d'} |
| created_at | 2014-07-08T23:08:26+00:00                                 |
| updated_at | 2014-07-16T19:07:56+00:00                                 |
| address    | 78:e7:d1:23:9b:1d                                         |
+------------+-----------------------------------------------------------+"
1521,1343024,neutron,6186e0cabe5ba854cddd68351e314933f826ee33,0,0,"""The OVS core plugin will be removed in Juno 2 but the embrane plugin code is still using it and will stop working.”",remove ovs dependency in embrane plugin,"The OVS core plugin will be removed in Juno 2 but the embrane plugin code is still using it and will stop working.
This bug tracks changing the dependency to ML2"
1522,1343142,cinder,6b7b705be4592c7a5406b1fda62fe83f73d869f6,1,1,,IBM storwize_svc can not get the right host,"IBM storwize_svc can not get the right host in following case:
Assume I have a SVC host using FC protocol. I configured 3 remote WWPNs for it, for example: rwwpn-1, rwwpn-2, rwwpn-3, and the host doesn't support iscsi so it has no iscsi_name property. When I try to attach a volume and do initialize_connection, I can get the host by only one of the 3 remote WWPNs, for example rwwpn-1, but I can not the host by the other two ones."
1523,1343200,nova,daf278c0c8b6cc917b84f514fec65758adf17028,0,0,Feature. “We should add notification for server group operations.”,Add notifications when operating server groups,"Currently, there is no notifications when operating server groups, such as create/delete/update etc. This caused 3rd party cannot know the operation result on time.
We should add notification for server group operations."
1525,1343689,nova,97f81734c030f7f704369b37e1fe122c7040b9a6,1,1,"“ Revert ""Add missing image to instance booted from volume""
    This reverts commit c3191cf0ba5ad3dc2df8da2a2bf5c9d270fde9d9.”",tempest.scenario.test_volume_boot_pattern.TestVolu...,"Not certain where the bug is yet, but filing here to have a recheck target.
Noticed two separate patches fail gating in different jobs but on the same test (tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern):
http://logs.openstack.org/30/104730/1/gate/gate-tempest-dsvm-neutron/975e16b/
http://logs.openstack.org/28/102628/4/check/check-grenade-dsvm-partial-ncpu/d2e1673/
Common error in n-api seems to be:
2014-07-17 23:33:31.537 ERROR nova.api.openstack [req-cd950aba-e19c-4c6b-918b-bdfd41d2afce TestVolumeBootPattern-908565911 TestVolumeBootPattern-1581069975] Caught error: 'image_id'
which just started popping up today:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiQ2F1Z2h0IGVycm9yOiAnaW1hZ2VfaWQnXCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1hcGkudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDU2NDE3OTMwMzl9"
1526,1343750,neutron,8e31122d36ce5c9d367696921ed92c50cb062b5f,1,1,"“ This commit fixes the regression.”,”    The recently merged DVR change (https://review.openstack.org/#/c/102332/)
    assumes that port_id is always a complete uuid “",RYU CI fails with PortNotFound,"recently merged ""RPC additions to support DVR"" change make ofagent CI fail.
https://review.openstack.org/#/c/102332/
http://180.37.183.32/ryuci/32/102332/38/check/check-tempest-dsvm-ofagent/d88f439/logs/screen-q-svc.txt.gz
2014-07-18 03:53:27.765 16087 ERROR oslo.messaging.rpc.dispatcher [req-a13fe10b-aae5-458a-a44f-564563368b56 ] Exception during message handling: Port 7d526916-5c could not be found
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/plugins/ml2/rpc.py"", line 191, in update_device_up
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     l3plugin.dvr_vmarp_table_update(rpc_context, port_id, ""add"")
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_dvr_db.py"", line 439, in dvr_vmarp_table_update
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     port_dict = self._core_plugin._get_port(context, port_id)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 109, in _get_port
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     raise n_exc.PortNotFound(port_id=id)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher PortNotFound: Port 7d526916-5c could not be found
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher"
1527,1343778,nova,83b37aeb5da8af6e305098a7e698d058f431f332,1,1,“API missing decorator expected_errors.”,V3 servers core API missing decorator expected_err...,"Most of v3 servers core API missing decorator expected_errors.
All the v3 api should be under the protection of expected_errors"
1528,1343854,neutron,f9c285fd617403ab89c857e3d8bab0d06d19d3ba,1,1,“Missing token in Neutron context”,Missing token in Neutron context but Nova/Cinder c...,"Missing token in Neutron context but Nova/Cinder context has.  Store the toke into the context can make the third party plugin or driver integrate the authentication with KeyStone.
And Phillip Toohill [mailto:<email address hidden>] also ask for that.
Currently, Neutron did not pass the token to the context. But Nova/Cinder did that. It's easy to do that, just 'copy' from Nova/Cinder.
1.  How Nova/Cinder did that
class NovaKeystoneContext(wsgi.Middleware)
///or CinderKeystoneContext for cinder
              auth_token = req.headers.get('X_AUTH_TOKEN',
                                     req.headers.get('X_STORAGE_TOKEN'))
              ctx = context.RequestContext(user_id,
                                     project_id,
                                     user_name=user_name,
                                     project_name=project_name,
                                     roles=roles,
                                     auth_token=auth_token,
                                     remote_address=remote_address,
                                     service_catalog=service_catalog)
2.  Neutron not passed token. Also not good for the third part network infrastructure to integrate the authentication with KeyStone.
class NeutronKeystoneContext(wsgi.Middleware)
.................
##### token not get from the header and not passed to context. Just change here like what Nova/Cinder did.
        context.Context(user_id, tenant_id, roles=roles,
                              user_name=user_name, tenant_name=tenant_name,
                              request_id=req_id)
        req.environ['neutron.context'] = ctx"
1529,1344036,nova,52de9395e5fe4f328f6dab0b35d660a700787c76,1,0,“Fixes Hyper-V agent force_hyperv_utils_v1 flag issue”,Hyper-V agent generates exception when force_hyper...,"WMI root\virtualization namespace v1 (in Hyper-V) has been removed from Windows Server / Hyper-V Server 2012 R2, according to:
http://technet.microsoft.com/en-us/library/dn303411.aspx
Because of this, setting the force_hyperv_utils_v1 option on the Windows Server 2012 R2 nova compute agent's nova.conf will cause exceptions, since it will try to use the removed root\virtualization namespace v1.
Logs:
http://paste.openstack.org/show/87125/"
1530,1344072,cinder,a074fec5f5d27ba66602a5b6133e5bfd49808c57,1,1,"""The following commit 4fdcbff96790753a4c1a508600e5d78b2c3b7172 introduced a”",Hard coded references from gettextutils.py should ...,The following commit 4fdcbff96790753a4c1a508600e5d78b2c3b7172 introduced a few hard coded references to '/home/jsbryant/cinder-dev/gettextutilsSync/' in cinder/openstack/common/gettextutils.py.  This should be removed.
1531,1344178,swift,e1fb64a7a6c675d909fe76757ae6671e51de9a51,1,1,"“Pass disk usage options when using —all""",Disk usage options are not applied when using --al...,"It is currently possible to have swift-recon report on disk usage by two means:
swift-recon -d OR
swift-recon --all (which includes -d)
Disk usage provides two options:
--human-readable to provide usage in MB/GB/TB++ rather than bytes
--top <integer> to list devices with the most usage
These options are currently usable when using -d but not --all.
This is because the options are not passed to disk_usage when using all:
https://github.com/openstack/swift/blob/d317888a7eae276ae9dddf26a9030d01f6ba00fe/swift/cli/recon.py#L930
Versus when using -d:
https://github.com/openstack/swift/blob/d317888a7eae276ae9dddf26a9030d01f6ba00fe/swift/cli/recon.py#L965"
1532,1346092,nova,13e2bd02a5b50973f95eb3d8fc0af4e0702e3381,0,0,"Feature “Move libvirt RBD utilities to a new file
    This will make it easier to share rbd-related code with cinder and glance.”",RBD helper utils in libvirt driver code need to be...,"The libvirt imagebackend.py file has alot of helper APIs for dealing with the RBD utilities. It is desirable that these all be isolated in a standalone rbd.py file, to be called by the imagebackend.py  This will make the core logic in imagebackend.py easier to follow and make the rbd helpers easier to test."
1533,1346191,nova,5c3f212343df997daa48f1f4a1cdd2a29099c288,1,1,,libvirt _live_snapshot & _swap_volume functions re...,"In the nova/virt/libvirt/driver.py file, the '_live_snapshot' and '_swap_volume' methods have the following code flow
  xml = dom.XMLDesc(0)
  dom.undefine()
  dom.blockRebase()
  dom.defineXML(xml)
The reason for this is that 'blockRebase' requires the guest to be transient, so we must temporarily delete the persistent config and then re-create it later.
Unfortunately this code is using the wrong XML document when re-creating the persistent config.  'dom.XMLDesc(0)' will return the guest XML document based on the current guest state. Since the guest is running in both these cases, it will get getting the *live* XML instead of the persistent XML.
So these methods are deleting the persistent XML and replacing it with the live XML. These two different XML documents are not guaranteed to contain the same information.
As a second problem, it is not requesting inclusion of security information, so any SPICE/VNC password set in the persistent XML is getting lost
The fix is to replace
  dom.XMLDesc(0)
with
  dom.XMLDesc(libvirt.VIR_DOMAIN_XML_INACTIVE |
                               libvirt.VIR_DOMAIN_XML_SECURE)
in the _live_snapshot and _swap_volume functions."
1534,1346245,neutron,4e8f4155fc559a5d49a70aee7882d7e0e117563a,1,1,“b7a8863760e_rm_cisco_vlan_bindin”,Incorrect downgrade in migration b7a8863760e_rm_ci...,Downgrade in migration b7a8863760e_rm_cisco_vlan_bindin fails http://paste.openstack.org/show/87396/
1535,1346372,neutron,8748a3ee68e60778c90b8b83181bc28a7e8fe9d1,1,1,,The default value of quota_firewall_rule should no...,"the default value of ""quota_firewall_rule"" is ""-1"", and this means unlimited. There will be potential security issue if openstack admin do not modify this default value.
A bad tenant User can create unlimited firewall rules to ""attack"" network node, in the backend, we will have a large number of iptables rules. This will make the network node crash or very slow.
So I suggest we use another number but not ""-1"" here."
1536,1346444,neutron,5247f5cdf15bad4c62bbf854e30716fcf00a1d2a,0,0,Bug in test. “ Implement ModelsMigrationsSync test from oslo.db”,DB migrations need unit tests,"Now that the DB healing https://review.openstack.org/96438 is merged, the DB migrations need unit tests."
1537,1346466,cinder,dbab8843ef33e0bb6745ae3ad5951fd86446c1a3,1,1,,Olso messaging API error with publish_errors set t...,"I observed that when you have publish_error enabled and and the notification driver set as below:
notification_driver = cinder.openstack.common.notifier.rpc_notifier
publish_errors = true
You will get the following error when a purposely causing a error in cinder(for example, create volume from image and set the volume size something small that the image would not fit in)
2014-07-21 11:46:51.527 ERROR oslo.messaging.rpc.dispatcher [req-e3015168-bf95-4cea-af97-b13ed9edc141 231fdb65b4134c9b864767e851ad72db 5304a70aac9540a5b445cb6a6f62c917] Exception during message handling: info() takes exactly 4 arguments (3 given)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 337, in create_volume
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     _run_flow()
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 330, in _run_flow
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     flow_engine.run()
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 89, in run
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     for _state in self.run_iter():
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 130, in run_iter
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self._change_state(states.FAILURE)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/openstack/common/excutils.py"", line 82, in __exit__
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 120, in run_iter
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     for state in runner.run_iter(timeout=timeout):
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/runner.py"", line 130, in run_iter
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     misc.Failure.reraise_if_any(failures)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 788, in reraise_if_any
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     failures[0].reraise()
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 795, in reraise
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     six.reraise(*self._exc_info)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 48, in _revert_task
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     result = task.revert(**kwargs)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 193, in revert
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     LOG.error(_(""Volume %s: create failed""), volume_id)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 1449, in error
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self.logger.error(msg, *args, **kwargs)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 1178, in error
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self._log(ERROR, msg, args, **kwargs)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 1271, in _log
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self.handle(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 1281, in handle
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self.callHandlers(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 1321, in callHandlers
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     hdlr.handle(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 749, in handle
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self.emit(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 197, in main
    self._resolve_links()
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 212, in _resolve_links
    f(self, *ca, **ckw)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_executors/impl_eventlet.py"", line 47, in complete
    thread.wait()
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
    return self._exit_event.wait()
  File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 120, in wait
    current.throw(*self._exc)
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 129, in <lambda>
    yield lambda: self._dispatch_and_reply(incoming)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 143, in _dispatch_and_reply
    exc_info=exc_info)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1178, in error
    self._log(ERROR, msg, args, **kwargs)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1271, in _log
    self.handle(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1281, in handle
    self.callHandlers(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1321, in callHandlers
    hdlr.handle(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 749, in handle
    self.emit(record)
  File ""/opt/stack/cinder/cinder/openstack/common/log_handler.py"", line 29, in emit
    dict(error=record.msg))
TypeError: info() takes exactly 4 arguments (3 given)""/opt/stack/cinder/cinder/openstack/common/log_handler.py"", line 29, in emit
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     dict(error=record.msg))
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher TypeError: info() takes exactly 4 arguments (3 given)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher
I see that in /opt/stack/cinder/cinder/openstack/common/log_handler.py that that the notifier.info(...) code(line #28) is not passing in the context as the first parameter as expected in the API  thus the takes 4 or 3 given error above.
rpc.get_notifier('error.publisher').info('error_notification',  dict(error=record.msg))"
1538,1346496,cinder,2e4837971ff512342fb47d3b43a57b67d0ab36d3,1,1,Wrong logic?,The Cinder Windows storage driver must not require...,"The Cinder storage driver is currently depending on Hyper-V WMI features for VHD conversions.
This must be urgently removed and replaced with Win32 calls available on any supported Windows SKU as otherwise CI tests cannot be performed and the Windows Storage Server edition cannot be supported."
1540,1346638,neutron,9ea42f50d3136885ffb1f7b23a7d831165f25247,1,0,"""Now that the DB is healed, neutron-db-manage revision --autogenerate needs to be updated.”",neutron-db-manage --autogenerate needs update afte...,"Now that the DB is healed, neutron-db-manage revision --autogenerate needs to be updated.
The template should do unconditional upgrade/downgrade.
The env.py should include all models from head to compare against the schema."
1541,1346673,neutron,d98ca642402bafbaa15ac3df588ed34d58d8456b,0,0,Bug in test,fixtures in neutron.tests.base blow away default d...,"Really trying to narrow this one down fully, and just putting this up because this is as far as I've gotten.
Basically, the lines in neutron/tests/base.py:
  line 159:        self.addCleanup(CONF.reset)
  line 182:    self.useFixture(self.messaging_conf)
cause cfg.CONF to get totally wiped out in the ""database"" config.  I don't yet understand why this is the case.
if you then run any test that extends BaseTestCase, and then run neutron/tests/unit/test_db_plugin.py -> NeutronDbPluginV2AsMixinTestCase in the same process, these two tests fail:
Traceback (most recent call last):
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/tests/unit/test_db_plugin.py"", line 3943, in setUp
    self.plugin = importutils.import_object(DB_PLUGIN_KLASS)
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/openstack/common/importutils.py"", line 38, in import_object
    return import_class(import_str)(*args, **kwargs)
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
I'm getting this error running tox on a subset of tests, however it's difficult to reproduce as the subprocesses have to work out just right.
To reproduce, just install nose and do:
.tox/py27/bin/nosetests -v neutron.tests.unit.test_db_plugin:DbModelTestCase neutron.tests.unit.test_db_plugin:NeutronDbPluginV2AsMixinTestCase
That is, DbModelTestCase is a harmless test but because it runs base.BaseTestCase first, cfg.CONF gets blown away.
I don't know what the solution should be here, cfg.CONF shouldn't be reset but I don't know what ""messaging_conffixture.ConfFixture"" is or how ""CONF.reset"" was supposed to work as it blows away DB config.  The cfg.CONF in the first place seems to get set up via this path:
  <string>(7)exec2()
  /Users/classic/dev/redhat/openstack/neutron/neutron/tests/unit/test_db_plugin.py(26)<module>()
-> from neutron.api import extensions
  /Users/classic/dev/redhat/openstack/neutron/neutron/api/extensions.py(31)<module>()
-> from neutron import manager
  /Users/classic/dev/redhat/openstack/neutron/neutron/manager.py(20)<module>()
-> from neutron.common import rpc as n_rpc
  /Users/classic/dev/redhat/openstack/neutron/neutron/common/rpc.py(22)<module>()
-> from neutron import context
  /Users/classic/dev/redhat/openstack/neutron/neutron/context.py(26)<module>()
-> from neutron import policy
  /Users/classic/dev/redhat/openstack/neutron/neutron/policy.py(55)<module>()
-> cfg.CONF.import_opt('policy_file', 'neutron.common.config')
  /Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/config/cfg.py(1764)import_opt()
-> __import__(module_str)
  /Users/classic/dev/redhat/openstack/neutron/neutron/common/config.py(135)<module>()
-> max_overflow=20, pool_timeout=10)
> /Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/options.py(145)set_defaults()
-> conf.register_opts(database_opts, group='database')
e.g. oslo.db set_defaults() sets it up."
1542,1346900,neutron,e416a5420f45391c180f84098f4d2c02ecf857b9,1,1,,Table tz_network_bindings uses nullable on primary...,Primary key cannot be null therefore setting NULL on column that is primary key is wrong and then generates wrong migration script using autogenerate.
1543,1347028,nova,9d64a827b9c2c5f332b3e57f6cb818d3f4735d23,1,1,,block_device mapping identifies ephemeral disks in...,"Ephemeral drives are destinaton == local, but the new bdm code bases it on source instead.  This leads to improper errors:
$ nova boot --flavor m1.tiny --block-device source=blank,dest=volume,bus=virtio,size=1,bootindex=0 test
ERROR (BadRequest): Ephemeral disks requested are larger than the instance type allows. (HTTP 400) (Request-ID: req-53247c8e-d14e-43e2-b01e-85b49f520e61)
The code is here:
https://github.com/openstack/nova/blob/106fb458c7ac3cc17bb42d1b83ec3f4fa8284e71/nova/block_device.py#L411
This should be checking destination_type == 'local' instead of source type."
1544,1347156,nova,77a7d14542600f2badcdf048fe6b586a0ff27e30,1,1,,Bug #1347156 “deleting floating-ip in nova-network does not free... ,"It seems that when you allocate a floating-ip in a tenant with nova-network, its quota is never returned after calling 'nova floating-ip-delete' ecen though 'nova floating-ip-list' shows it gone. This behavior applies to each tenant individually. The gate tests are passing because they all run with tenant isolation. But this problem shows in the nightly run without tenant isolation:
http://logs.openstack.org/periodic-qa/periodic-tempest-dsvm-full-non-isolated-master/2bc5ead/console.html"
1545,1347355,nova,536bcbd2ce0822bec757f7ec949b9f70df20c966,1,1,,Extra image metadata didn't assgin to volume based...,"curl -i 'http://cloudcontroller:8774/v2/fdbb1e8f23eb40c89f3a677e2621b95c/servers/e2461ba7-3624-4d43-a456-acb87a0fb6f9/action' -X POST -H ""X-Auth-Project-Id: admin"" -H ""User-Agent: python-novaclient"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -H ""X-Auth-Token: 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-2CM0-JKKjNf-NG9X3NpXPDt1mvaP1brXvzoq0W9IhxvE5THBLyrVrzEk5s+cZlDfo6QqyPtRqGgs80WNdOe3UQ8pL14E+SPoc3QIv66G6on6wOL9JK7KAtVbcfE9ucCgZmLI9hQGzn7J5GyXHzP0dRNRxEw+39P4pKOdTLj7dIQA7-PGEHdajFkMVWIIkD--c+G8pojKzZjESrGWpJh8wzmH8Awkcr5qEvvkEqEXqHKsf4ILdX2d90C4nfYYRDYpggVaVn5H8eK2z4dYkuUdRAf03c7kUzE9n12PabhSxssFNtdF-Dm8VSM0eF1BkBYByA"" -d '{""createImage"": {""name"": ""snapx2"", ""metadata"": {""a"": ""b""}}}'
os@os2:~/devstack$ glance image-show 6c2ba6d6-e906-41b6-8b28-3b4c9fa80d59
+---------------------------------+----------------------------------------------------------------------------------+
| Property                        | Value                                                                            |
+---------------------------------+----------------------------------------------------------------------------------+
| Property 'bdm_v2'               | True                                                                             |
| Property 'block_device_mapping' | [{""guest_format"": null, ""boot_index"": 0, ""no_device"": null, ""snapshot_id"":       |
|                                 | ""2fb5111f-b618-4d8b-b47f-142ed4762064"", ""delete_on_termination"": null,           |
|                                 | ""disk_bus"": ""virtio"", ""image_id"": null, ""source_type"": ""snapshot"",               |
|                                 | ""device_type"": ""disk"", ""volume_id"": null, ""destination_type"": ""volume"",          |
|                                 | ""volume_size"": null}]                                                            |
| Property 'checksum'             | 4eada48c2843d2a262c814ddc92ecf2c                                                 |
| Property 'container_format'     | ami                                                                              |
| Property 'disk_format'          | ami                                                                              |
| Property 'image_id'             | da82a342-aeac-407a-bf9d-cf28bf68dc6b                                             |
| Property 'image_name'           | cirros-0.3.2-x86_64-uec                                                          |
| Property 'kernel_id'            | 3b6a8424-1454-4394-810f-2adc95c6e326                                             |
| Property 'min_disk'             | 0                                                                                |
| Property 'min_ram'              | 0                                                                                |
| Property 'ramdisk_id'           | aa76f1bd-8167-4586-8185-4347dc4951e0                                             |
| Property 'root_device_name'     | /dev/vda                                                                         |
| Property 'size'                 | 25165824                                                                         |
| created_at                      | 2014-07-23T03:16:18                                                              |
| deleted                         | False                                                                            |
| id                              | 6c2ba6d6-e906-41b6-8b28-3b4c9fa80d59                                             |
| is_public                       | False                                                                            |
| min_disk                        | 0                                                                                |
| min_ram                         | 0                                                                                |
| name                            | snapx2                                                                           |
| owner                           | fdbb1e8f23eb40c89f3a677e2621b95c                                                 |
| protected                       | False                                                                            |
| size                            | 0                                                                                |
| status                          | active                                                                           |
| updated_at                      | 2014-07-23T03:16:18                                                              |
+---------------------------------+----------------------------------------------------------------------------------+"
1546,1347452,neutron,46501246c202f1f424f0d2a05769926303a39c89,1,1,,Start message from OVS agent not received by serve...,"When OVS agent starts, it will send server a message with ""start_flag"" to tell server it's up, which is done by the _report_state method. Currently _report_state uses cast method to send rpc message, so there's no guarantee that server receives the ""start"" message. In our test, sometimes this message did miss. As a result, server didn't update the start time of the agent so fdb entries didn't send by the l2pop driver."
1547,1347499,nova,9d64a827b9c2c5f332b3e57f6cb818d3f4735d23,0,0,"Feature ""Allow empty volumes to be created”","block-device source=blank,dest=volume is allowed a...","This is a spin-off of https://bugs.launchpad.net/nova/+bug/1347028
As per the example given there -  currently source=blank, destination=volume will not work. We should either make it create an empty volume and attach it, or disallow it in the API."
1548,1347777,nova,b856bc5e348cc69923e3ee815303804323730026,1,1,,The compute_driver option description does not inc...,"The description of the option ""compute_driver"" should include hyperv.HyperVDriver along with the other supported drivers
https://github.com/openstack/nova/blob/aa018a718654b5f868c1226a6db7630751613d92/nova/virt/driver.py#L35-L38"
1549,1347963,neutron,72f66917fe79996b9715c47eae8e9d479b210c2d,0,0,Bug in test,Network profile tests have incorrect create reques...,"Some unit test cases in the unit test module for cisco n1kv plugin has incorrect create resource calls causing breakage in build.
This was exposed via a fix for bug/1330095"
1550,1348056,neutron,6b8a5f0e1d26d2721e7ad7fc67099ff8b880d9ec,1,1,,Neutron network API throws error code 500 when an ...,"The neutron network API currently throws a error code 500 for an invalid input against the VLAN field.
The error can be reproduced by having the following JSON request body:
{
            ""network"": {
                ""admin_state_up"": ""false"",
                ""provider:segmentation_id"": ""abc"",
                ""name"": ""Network1"",
                ""provider:physical_network"": ""XYZ"",
                ""provider:network_type"": ""vlan""
            }
        }
An error code 400 should be thrown much like how it is thrown for the other fields - if they correspond to incorrect values."
1551,1348288,nova,c363dae6a2b878db6801b502cced1fcc6aad2d0c,1,1,,Resource tracker should report virt driver stats,"sha1 Nova at: 106fb458c7ac3cc17bb42d1b83ec3f4fa8284e71
sha1 ironic at: 036c79e38f994121022a69a0bc76917e0048fd63
The ironic driver passes stats to nova's resource tracker in get_available_resources(). Sometimes these appear to get through to the database without modification, sometimes they seem to be replaced entirely by other stats generated by the resource tracker. The correct behaviour should be to combine the two.
As an example, the following query on the compute_nodes table in nova's database shows the contents for a tripleo system (all nodes are ironic):
mysql> select hypervisor_hostname, stats from compute_nodes;
+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| hypervisor_hostname                  | stats                                                                                                                                                                       |
+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 4e014e26-2f90-4a91-a6f0-c1978df88369 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| fadb50bf-26ec-420c-a13f-f182e38569d6 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ffe5a5bf-7151-468c-b9bb-980477e5f736 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 752966ea-17f8-4d6d-87a4-03c91cb65354 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| f2f0ecb1-6234-4975-808f-a17534c9ae6c | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 9adf4551-24f0-43a7-9267-a20cfa309137 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""}                                                                                       |
| 1bd13fc5-4938-4781-9680-ad1e0ccec77c | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 88a39f5d-6174-47c9-9817-13d08bf2e079 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ec6b5dc6-de38-4e23-a967-b87c10da37e3 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ac52fd79-e0b9-4749-b794-590d5c181b4a | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| a1b81342-ed57-4310-8d5b-a2aa48718f1f | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 0588e463-748a-4248-9110-6e18988cfa4e | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 8f73d8dc-5d8c-47b0-a866-b829edc3667f | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| bac38b1d-f7f9-4770-9195-ff204a0c05c3 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""}                                                                                       |
| 62cc33f7-701b-47f6-8f50-3f7c1ca0f0a3 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| af7f79bf-b2c1-405b-9bc7-5370b93b08cf | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""}                                                                                       |
| 4615c72a-9ea0-433e-8c52-308163112f89 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""}                                                                                       |
| 680e6aa7-9a84-41de-94ba-b761d48b4087 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 2c2d5b87-1be2-4e47-aabe-6822c569446c | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 6f653502-d8ed-4763-b418-3ccfcc430c24 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 006aff97-d3e6-49c8-93f0-4f4c5af1231d | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| addf8ff8-52fe-49da-a4b2-5688554e9161 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| b4b7f7ad-4adc-4dc9-9afb-a9966e2be141 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| e2cb81ca-314f-4436-80fd-e154ca3e9ccc | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 7a7266a9-d72e-49be-b51a-4053ed251b41 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| dc5c63b6-d576-46c9-aa16-0537450cdbd8 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""}                                                                                       |
| 18794409-10d0-4946-9356-66cd5ab8472e | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 8135a1be-c8d8-4cea-a381-8ab8be8b15c7 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ed281d00-c16a-474d-8adb-ef525a9045fa | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
29 rows in set (0.00 sec)
The nodes with ironic stats have not had any instances created on them, the ones with resource tracker stats but no ironic stats have instances on them. This is shown by the following:
ironic node-list
+--------------------------------------+--------------------------------------+-------------+-----------------+-------------+
| uuid                                 | instance_uuid                        | power_state | provision_state | maintenance |
+--------------------------------------+--------------------------------------+-------------+-----------------+-------------+
| dc5c63b6-d576-46c9-aa16-0537450cdbd8 | None                                 | power off   | None            | False       |
| fadb50bf-26ec-420c-a13f-f182e38569d6 | a703ae98-2398-445b-91bf-48f368c5b82a | power on    | active          | False       |
| 9adf4551-24f0-43a7-9267-a20cfa309137 | None                                 | power off   | None            | False       |
| f2f0ecb1-6234-4975-808f-a17534c9ae6c | 308b1375-f8b8-42e5-bf20-143303975135 | power on    | active          | False       |
| 1bd13fc5-4938-4781-9680-ad1e0ccec77c | 4e8c16c4-d0f6-43b8-8c8f-d110d89ac16f | power on    | active          | False       |
| a1b81342-ed57-4310-8d5b-a2aa48718f1f | 01cbe85a-1b12-40ca-ac99-5e7b062b1b50 | power on    | active          | False       |
| 18794409-10d0-4946-9356-66cd5ab8472e | 4d764e7e-a123-4390-9c73-0c05a52f5f23 | power on    | active          | False       |
| ec6b5dc6-de38-4e23-a967-b87c10da37e3 | 87636b3b-b7b0-4e79-bd5f-5189eb5b1134 | power on    | active          | False       |
| ac52fd79-e0b9-4749-b794-590d5c181b4a | 03c06c2f-b606-4ddd-a205-573ea036b5b8 | power on    | active          | False       |
| 4e014e26-2f90-4a91-a6f0-c1978df88369 | 0189aa0e-5d05-4f6d-8a77-d5869b5c79f2 | power on    | active          | False       |
| ed281d00-c16a-474d-8adb-ef525a9045fa | 11e03336-fb12-4448-a8de-b38f82d8b282 | power on    | active          | False       |
| bac38b1d-f7f9-4770-9195-ff204a0c05c3 | None                                 | power off   | None            | False       |
| 62cc33f7-701b-47f6-8f50-3f7c1ca0f0a3 | ab08556b-ce7f-46f3-bb10-91771426d977 | power on    | active          | False       |
| 8135a1be-c8d8-4cea-a381-8ab8be8b15c7 | 4051cc97-bb28-4dbf-b018-d9a61e73a269 | power on    | active          | False       |
| 6f653502-d8ed-4763-b418-3ccfcc430c24 | 855e24b6-f9ae-441b-8520-42d4df9f8703 | power on    | active          | False       |
| 2c2d5b87-1be2-4e47-aabe-6822c569446c | 141e6055-3e2c-4376-aa8b-39ad5c63e8bc | power on    | active          | False       |
| 8f73d8dc-5d8c-47b0-a866-b829edc3667f | a62cab3f-56f7-47f2-813b-23a3255cad15 | power on    | active          | False       |
| 0588e463-748a-4248-9110-6e18988cfa4e | 8fe18a1b-d753-4558-a9e1-b24f552f8e12 | power on    | active          | False       |
| e2cb81ca-314f-4436-80fd-e154ca3e9ccc | 839a77e1-2a9e-4db4-94d9-d68903fe028c | power on    | active          | False       |
| ffe5a5bf-7151-468c-b9bb-980477e5f736 | 205b6dbf-5f75-4d2c-a3b0-1be45d93d493 | power on    | active          | False       |
| 752966ea-17f8-4d6d-87a4-03c91cb65354 | 09d2ad8c-f813-4b1a-9501-530462240657 | power on    | active          | False       |
| 006aff97-d3e6-49c8-93f0-4f4c5af1231d | aa6c8024-ba80-4dc8-810c-c6fae57218c7 | power on    | active          | False       |
| 7a7266a9-d72e-49be-b51a-4053ed251b41 | a76d7b2f-cf8c-4673-96e4-dcd9f2ea3bb5 | power on    | active          | False       |
| 88a39f5d-6174-47c9-9817-13d08bf2e079 | 9a2904d2-d364-4084-a40a-3fbc65d90059 | power on    | active          | False       |
| addf8ff8-52fe-49da-a4b2-5688554e9161 | a86316c0-bdf0-4d6e-81ba-f44da63c906c | power on    | active          | False       |
| b4b7f7ad-4adc-4dc9-9afb-a9966e2be141 | 03b0d70d-42c4-426e-adab-09df927f30bb | power on    | active          | False       |
| 680e6aa7-9a84-41de-94ba-b761d48b4087 | 808dbf2e-8f40-4d6d-9d7b-2c74e3194a6d | power on    | active          | False       |
| 4615c72a-9ea0-433e-8c52-308163112f89 | None                                 | power off   | None            | False       |
| af7f79bf-b2c1-405b-9bc7-5370b93b08cf | None                                 | power off   | None            | False       |
+--------------------------------------+--------------------------------------+-------------+-----------------+-------------+"
1552,1348306,neutron,c3c9f580393aea658571b00b3afd0b729dffe89b,1,1,,L3 agent restart disrupts fip namespace causing co...,"When the L3 agent restarts, it does not preserve the link local addresses used for each router.  For this reason, it has to reassign them and rewire everything.  This is very disruptive to network connectivity.  Connectivity should be preserved as much as possible.
This was an expected backlog item for the new DVR feature."
1553,1348479,neutron,6bd147df43b1d352230d94f52e0fb4c56e7885d6,1,1,,_extend_extra_router_dict does not handle boolean ...,"Method:
https://github.com/openstack/neutron/blob/master/neutron/db/l3_attrs_db.py#L50
is used to add extension attributes to the router object during the handling of the API response. When attributes are unspecified, the router is extended with default values.
In the case of boolean attributes things don't work as they should, because a default value as True takes over on the right side of the boolean expression on:
https://github.com/openstack/neutron/blob/master/neutron/db/l3_attrs_db.py#L56
The end user so is led to believe that the server did not honor the request, when effectively it did."
1554,1348584,nova,5e4a5f0d8c62ca6e94ae6db16e9fbe0428805158,1,1,,KeyError in nova.compute.api.API.external_instance...,"The fix for bug 1333654 ensured events for instance without host are not accepted.
However, the instances without the host are still being passed to the compute API layer.
This is likely to result in keyerrors as the one found here: http://logs.openstack.org/51/109451/2/check/check-tempest-dsvm-neutron-full/ad70f74/logs/screen-n-api.txt.gz#_2014-07-25_01_41_48_068
The fix for this bug should be straightforward."
1555,1348623,nova,a4fd236baa2fc544f174773e32a3f91ec52a4fe5,1,0,“Upgrade-impact: the hyervisor type will intentionally no longer distinguish libvirt Xen from XenAPI within a compute cloud. The driver name should be used instead”,XenAPI and Baremetal drivers use bogus hypervisor ...,"The XenAPI driver reports a hypervisor type of 'xapi' for supported instances. This is confusing the hypervisor type, which should be 'xen', with the management API type which is 'xapi'.
The Baremetal driver reports a hypervisor type of 'baremetal' for supported instances. This is confusing the hypervisor type with the nova driver type. There is no hypervisor concept with the bare metal driver, things just run natively, so the type should be 'native'."
1557,1348629,nova,8fcd1b2c73dad8e2a7b4d299da270934fd5328cc,1,1,It does not meet req “bogus”,Baremetal driver reports bogus vm_mode of 'baremet...,"The Baremetal driver reports a 'vm_mode' of 'baremetal' for supported instance types. This is bogus because the baremetal driver is running OS using the native machine ABI, which is represented by vm_mode.HVM"
1558,1348766,neutron,b8cf45218714937681d7df2e5b9d7d440cb2edd9,1,1,,Bug #1348766 “Big Switch,"The configuration hash db is updated on every response from the backend including errors that contain an empty hash. This is causing the hash to be wiped out if a standby controller is contacted first, which opens a narrow time window where the backend could become out of sync. It should only update the hash on successful REST calls."
1559,1348787,cinder,aad4e2437736270af7164df4fc34174bf3fb49f9,0,0,Feature or bug “Split this into a separate file to reduce overhead (and things that can break)”,cinder-manage loads more modules than necessary,"cinder-manage loads up more stuff than it needs to, should probably look at reducing this since it's an admin command.
For one, it loads up the paramiko library since it's used in utils.py.  This seems rather unnecessary.
(Noticed because the python crypto module loaded by paramiko was throwing warnings to my terminal about linking against a newer libgmp.)"
1561,1349147,nova,b986d3d0b1ea91631df8c7b51e389a8150d497fb,0,0,Bug in test,Bug #1349147 “test_db_api unit tests fail with,"http://logs.openstack.org/62/104262/7/gate/gate-nova-python27/3adf0e2/console.html
2014-07-25 16:27:18.188 | Traceback (most recent call last):
2014-07-25 16:27:18.188 |   File ""nova/tests/db/test_db_api.py"", line 1236, in test_security_group_get_no_instances
2014-07-25 16:27:18.188 |     security_group = db.security_group_get(self.ctxt, sid)
2014-07-25 16:27:18.188 |   File ""nova/db/api.py"", line 1269, in security_group_get
2014-07-25 16:27:18.188 |     columns_to_join)
2014-07-25 16:27:18.188 |   File ""nova/db/sqlalchemy/api.py"", line 167, in wrapper
2014-07-25 16:27:18.188 |     return f(*args, **kwargs)
2014-07-25 16:27:18.188 |   File ""nova/db/sqlalchemy/api.py"", line 3668, in security_group_get
2014-07-25 16:27:18.188 |     query = _security_group_get_query(context, project_only=True).\
2014-07-25 16:27:18.188 |   File ""nova/db/sqlalchemy/api.py"", line 3635, in _security_group_get_query
2014-07-25 16:27:18.188 |     read_deleted=read_deleted, project_only=project_only)
2014-07-25 16:27:18.189 |   File ""nova/db/sqlalchemy/api.py"", line 237, in model_query
2014-07-25 16:27:18.189 |     session = kwargs.get('session') or get_session(use_slave=use_slave)
2014-07-25 16:27:18.189 |   File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 765, in __call__
2014-07-25 16:27:18.189 |     return mock_method(*params, **named_params)
2014-07-25 16:27:18.189 |   File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 1002, in __call__
2014-07-25 16:27:18.189 |     expected_method = self._VerifyMethodCall()
2014-07-25 16:27:18.189 |   File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 1049, in _VerifyMethodCall
2014-07-25 16:27:18.189 |     expected = self._PopNextMethod()
2014-07-25 16:27:18.189 |   File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 1035, in _PopNextMethod
2014-07-25 16:27:18.189 |     raise UnexpectedMethodCallError(self, None)
2014-07-25 16:27:18.189 | UnexpectedMethodCallError: Unexpected method call get_session.__call__(use_slave=False) -> None
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVW5leHBlY3RlZE1ldGhvZENhbGxFcnJvcjogVW5leHBlY3RlZCBtZXRob2QgY2FsbCBnZXRfc2Vzc2lvbi5fX2NhbGxfXyh1c2Vfc2xhdmU9RmFsc2UpIC0+IE5vbmVcIiBBTkQgcHJvamVjdDpcIm9wZW5zdGFjay9ub3ZhXCIgQU5EIHRhZ3M6XCJjb25zb2xlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6ImN1c3RvbSIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJmcm9tIjoiMjAxNC0wNy0xM1QxNjo0MDo1NiswMDowMCIsInRvIjoiMjAxNC0wNy0yN1QxNjo0MDo1NiswMDowMCIsInVzZXJfaW50ZXJ2YWwiOiIwIn0sInN0YW1wIjoxNDA2NDc5MzkzMjc0LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
8 hits in 2 weeks, check and gate, all failures, looks like it started around 7/21."
1562,1349268,nova,26871aa60ba663d56951fcd449167bfadbe01522,1,1,,Bug #1349268 “OverLimit,"The instance will be ERROR when booting instance from volume, if the volume quota is not enough. And there is even no useful error message to show to the user. Following is the related nova-compute.log:
2014-07-27 17:56:19.372 17060 ERROR nova.compute.manager [req-4e876b97-be8a-486b-98e2-7d707266755d 98fa3fd418914a9288b5560e1bb6944e 5254621adfd949a9a3b975f68119e269] [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] Instance failed block device setup
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] Traceback (most recent call last):
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1690, in _prep_block_device
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     self.driver, self._await_block_device_map_created))
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 363, in attach_block_devices
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     map(_log_and_attach, block_device_mapping)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 361, in _log_and_attach
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     bdm.attach(*attach_args, **attach_kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 311, in attach
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     '', '', image_id=self.image_id)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/nova/volume/cinder.py"", line 303, in create
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     item = cinderclient(context).volumes.create(size, **kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/cinderclient/v1/volumes.py"", line 187, in create
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     return self._create('/volumes', body, 'volume')
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/cinderclient/base.py"", line 153, in _create
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     resp, body = self.api.client.post(url, body=body)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/cinderclient/client.py"", line 209, in post
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     return self._cs_request(url, 'POST', **kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/cinderclient/client.py"", line 173, in _cs_request
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     **kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/cinderclient/client.py"", line 156, in request
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     raise exceptions.from_response(resp, body)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] OverLimit: VolumeLimitExceeded: Maximum number of volumes allowed (10) exceeded (HTTP 413) (Request-ID: req-07dcc4c4-182f-4d73-b054-806f31cb7e71)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]
2014-07-27 17:56:19.693 17060 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: Block Device Mapping is Invalid.
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return func(*args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 88, in wrapped
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 71, in wrapped
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 282, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     pass
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 268, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 335, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     function(self, context, *args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 311, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     e, sys.exc_info())
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 298, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2077, in run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     do_run_instance()
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 249, in inner
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return f(*args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2076, in do_run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     legacy_bdm_in_spec)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1209, in _run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     notify(""error"", fault=e)  # notify that build failed
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1193, in _run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     instance, image_meta, legacy_bdm_in_spec)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1347, in _build_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     LOG.exception(msg, instance=instance)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1304, in _build_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     context, instance, bdms)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1707, in _prep_block_device
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     raise exception.InvalidBDM()
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher InvalidBDM: Block Device Mapping is Invalid.
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher"
1564,1349475,cinder,09f6fe636416ac63e1463d14d469af8f93a6ddf0,1,1,,Bug #1349475 “Solidfire,"When a volume is transferred from one project to another, this transferred volume cannot be attached to any instance due to wrong AUTH information.  Nova-compute threw exception when trying to attach the volume:
2014-07-25 00:03:57,358.358 42302 ERROR nova.compute.manager [req-04e6cad9-0d3a-4c3c-a9b3-a9d3a0247872 6a08f3e43965436fb028eda1005ec77b b67a57cc0b9d443eac6d2ff8a494b088] [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Failed to attach volume c946565f-d87a-48df-a6f4-baa3bb9d2300 at /dev/vdc
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Traceback (most recent call last):
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/compute/manager.py"", line 3690, in _attach_volume
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] encryption=encryption)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 1083, in attach_volume
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] disk_info)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 1042, in volume_driver_method
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] return method(connection_info, *args, **kwargs)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] return f(*args, **kwargs)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/volume.py"", line 287, in connect_volume
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] self._run_iscsiadm(iscsi_properties, (""--rescan"",))
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/volume.py"", line 218, in _run_iscsiadm
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] check_exit_code=check_exit_code)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/utils.py"", line 177, in execute
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] return processutils.execute(*cmd, **kwargs)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/openstack/common/processutils.py"", line 178, in execute
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] cmd=' '.join(cmd))
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] ProcessExecutionError: Unexpected error while running command.
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Command: sudo nova-rootwrap /etc/nova/rootwrap.conf iscsiadm -m node -T iqn.2010-01.com.SF:ogav.uuid-c946565f-d87a-48df-a6f4-baa3bb9d2300.200982 -p ISCSITARGET:3260 --rescan
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Exit code: 255
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Stdout: ''
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Stderr: 'iscsiadm: No portal found.\n'
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682]
How to reproduce:
1) Create a volume X on Solidfire with project A;
2) Transfer volume X to project B;
3) attach volume X to any instance;
In solidfire.py:accept_transfer(), the volume metadata in the backend is changed but the AUTH (CHAP username secret) remains unchanged, which causes the mismatch of the ownership and corresponding AUTH info.  Cinder volume manager will have to be changed as well to adopt cases like this (volume info must be updated to DB once driver completes accept_transfer)."
1565,1349638,neutron,3eee50510fed29a7a8d97d4193a1c3c0a209a712,1,0,"""With the vendor's version of L3 Router Service plugins that assumption may not be true and hence the invocation of this method throws an exception.”",DVR vmarp table update causes exception in L3 serv...,"DVR implementation assumes that dvr_vmarp_table_update() method is supported by L3 Router service plugin. With the vendor's version of L3 Router Service plugins that assumption may not be true and hence the invocation of this method throws an exception.
I noticed this during the testing of Arista's  L3 router plugin. I notice that other similar plugins from other vendors are on their way, and will hit this issue as well."
1566,1349767,neutron,cd3f4f7d24c92475ca698906c57f60375861b83f,1,1,,lbaas is not backward compatible with havana confi...,In Havana user_group was taken from default section while currently it needs to be in [haproxy] section.
1568,1349808,cinder,7fd2eea9710ca96415ee7d08349ae981a262f44f,1,0,""" Update ref used for notifications
“",volume has wrong status in notification-info when ...,"Volume's status have update to 'available' in database when volume extended successfully, but in 'notification-info' it still being 'extending'.
2014-07-28 14:45:40.449 3580 INFO cinder.volume.manager [req-2391ae35-c1dc-461c-84f9-9dd0ed89bbd0 06d7b23a60ab4341bbc8701fb121de26 236be19c86d7461295b122f3ef16cf27 - - -] volume d514c152-890b-47ae-ba87-2cb47a34fdcb: extended successfully
2014-07-28 14:45:40.513 3580 INFO oslo.messaging._drivers.impl_zmq [req-2391ae35-c1dc-461c-84f9-9dd0ed89bbd0 06d7b23a60ab4341bbc8701fb121de26 236be19c86d7461295b122f3ef16cf27 - - -] 'notifications-info' {'event_type': 'volume.resize.end',
 'message_id': 'b59eeffa-cb58-4930-81b0-8086fa92eba3',
 'payload': {'availability_zone': u'nova',
             'created_at': '2014-07-15 03:57:03',
             'display_name': u'vol22',
             'launched_at': '2014-07-15 03:57:12',
             'size': 3,
             'snapshot_id': None,
             'status': u'extending',
             'tenant_id': u'236be19c86d7461295b122f3ef16cf27',
             'user_id': u'06d7b23a60ab4341bbc8701fb121de26',
             'volume_id': u'd514c152-890b-47ae-ba87-2cb47a34fdcb',
             'volume_type': None},
 'priority': 'INFO',
 'publisher_id': 'volume.controller1',
 'request_id': u'req-2391ae35-c1dc-461c-84f9-9dd0ed89bbd0',
 'timestamp': '2014-07-28 06:45:40.512994'}"
1569,1349810,neutron,bd40fbe304a18f8a231b519a0e1708b5ba63ad6f,1,0,"“Heal migration fix bug https://bugs.launchpad.net/neutron/+bug/1336177.""","Bug #1349810 “Wrong order of tables for dropping in downgrade "" ",Heal migration fix bug https://bugs.launchpad.net/neutron/+bug/1336177. Now table ml2_brocadenetworks has foreign key and downgrade of 492a106273f8_brocade_ml2_mech_dri fails. Error log: http://paste.openstack.org/show/88898/
1570,1349888,nova,339a97d0f2d17f531cfc79e09cd8b8bc75ce6e2a,1,1,,[SRU] Attempting to attach the same volume multipl...,"[Impact]
 * Ensure attching already attached volume to second instance does not
   interfere with attached instance volume record.
[Test Case]
 * Create cinder volume vol1 and two instances vm1 and vm2
 * Attach vol1 to vm1 and check that attach was successful by doing:
   - cinder list
   - nova show <vm1>
   e.g. http://paste.ubuntu.com/12314443/
 * Attach vol1 to vm2 and check that attach fails and, crucially, that the
   first attach is unaffected (as above). You can also check the Nova db as
   follows:
   select * from block_device_mapping where source_type='volume' and \
       (instance_uuid='<vm1>' or instance_uuid='<vm2>');
   from which you would expect e.g. http://paste.ubuntu.com/12314416/ which
   shows that vol1 is attached to vm1 and vm2 attach failed.
 * finally detach vol1 from vm1 and ensure that it succeeds.
[Regression Potential]
 * none
---- ---- ---- ----
nova assumes there is only ever one bdm per volume. When an attach is initiated a new bdm is created, if the attach fails a bdm for the volume is deleted however it is not necessarily the one that was just created. The following steps show how a volume can get stuck detaching because of this.
$ nova list
c+--------------------------------------+--------+--------+------------+-------------+------------------+
| ID                                   | Name   | Status | Task State | Power State | Networks         |
+--------------------------------------+--------+--------+------------+-------------+------------------+
| cb5188f8-3fe1-4461-8a9d-3902f7cc8296 | test13 | ACTIVE | -          | Running     | private=10.0.0.2 |
+--------------------------------------+--------+--------+------------+-------------+------------------+
$ cinder list
+--------------------------------------+-----------+--------+------+-------------+----------+-------------+
|                  ID                  |   Status  |  Name  | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------+------+-------------+----------+-------------+
| c1e38e93-d566-4c99-bfc3-42e77a428cc4 | available | test10 |  1   |     lvm1    |  false   |             |
+--------------------------------------+-----------+--------+------+-------------+----------+-------------+
$ nova volume-attach test13 c1e38e93-d566-4c99-bfc3-42e77a428cc4
+----------+--------------------------------------+
| Property | Value                                |
+----------+--------------------------------------+
| device   | /dev/vdb                             |
| id       | c1e38e93-d566-4c99-bfc3-42e77a428cc4 |
| serverId | cb5188f8-3fe1-4461-8a9d-3902f7cc8296 |
| volumeId | c1e38e93-d566-4c99-bfc3-42e77a428cc4 |
+----------+--------------------------------------+
$ cinder list
+--------------------------------------+--------+--------+------+-------------+----------+--------------------------------------+
|                  ID                  | Status |  Name  | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+--------+--------+------+-------------+----------+--------------------------------------+
| c1e38e93-d566-4c99-bfc3-42e77a428cc4 | in-use | test10 |  1   |     lvm1    |  false   | cb5188f8-3fe1-4461-8a9d-3902f7cc8296 |
+--------------------------------------+--------+--------+------+-------------+----------+--------------------------------------+
$ nova volume-attach test13 c1e38e93-d566-4c99-bfc3-42e77a428cc4
ERROR (BadRequest): Invalid volume: status must be 'available' (HTTP 400) (Request-ID: req-1fa34b54-25b5-4296-9134-b63321b0015d)
$ nova volume-detach test13 c1e38e93-d566-4c99-bfc3-42e77a428cc4
$ cinder list
+--------------------------------------+-----------+--------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  |  Name  | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------+------+-------------+----------+--------------------------------------+
| c1e38e93-d566-4c99-bfc3-42e77a428cc4 | detaching | test10 |  1   |     lvm1    |  false   | cb5188f8-3fe1-4461-8a9d-3902f7cc8296 |
+--------------------------------------+-----------+--------+------+-------------+----------+--------------------------------------+
2014-07-29 14:47:13.952 ERROR oslo.messaging.rpc.dispatcher [req-134dfd17-14da-4de0-93fc-5d8d7bbf65a5 admin admin] Exception during message handling: <type 'NoneType'> can't be decoded
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 406, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 291, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     pass
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 277, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 319, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 307, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 4363, in detach_volume
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     self._detach_volume(context, instance, bdm)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 4309, in _detach_volume
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     connection_info = jsonutils.loads(bdm.connection_info)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/jsonutils.py"", line 176, in loads
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return json.loads(strutils.safe_decode(s, encoding), **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/strutils.py"", line 134, in safe_decode
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     raise TypeError(""%s can't be decoded"" % type(text))
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher TypeError: <type 'NoneType'> can't be decoded
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher"
1571,1349895,neutron,a5765179201ee03ed26bb8f3bcda9f75bbdb191c,0,0,Feature. “If the Port is not found it should create a new Port.”,Radware LBaaS driver create extra Neutron Ports,"The method ' _create_port_for_pip' should try and check if a Neutron Port exists and reuse it.
If the Port is not found it should create a new Port.
See: https://github.com/openstack/neutron/blob/master/neutron/services/loadbalancer/drivers/radware/driver.py#L616"
1572,1349898,neutron,de5fa0dd327b1a81cb980d7553001dca244ae7a2,1,1,“port_bound forgets to normalize port name.”,Bug #1349898 “ofagent,"port_bound forgets to normalize port name.
it causes port_unbound fail to find Port for non ""tap"" prefixed ports."
1573,1349936,cinder,8f8a8a6ffe14cf7bb461dc26d178bf8acc8ddf12,1,1,It performs additional filtering not done in the original solution (63f5798e8795f80f3612fb0699eb668e20ae321c),Listing volumes has poor performance (due to glanc...,"Listing volumes with a non-admin user has poor performance.
This is caused by the query built to retrieve glance metadata associated to a volume which is sub-optimal: all rows of volume_glance_metadata table are returned."
1574,1350089,neutron,86a0adefe326116ae42c0e01a8fa2c2849bfa2c2,1,1,,Distributed Router Gateway Clear does not delete t...,"When Distributed Routers are created with Interface Ports and after a Gateway is set to the router,  the plugin will create the ""csnat"" interface ports with ""device_owner"" as ""router_csnat_interface"".
These ports should be deleted when the Gateway is cleared  or when the interfaces are removed from the  particular router.
In the current plugin code, these ""interface"" ports are not deleted when a Gateway is cleared. But the ports are deleted when the ""router interfaces are removed"".
This needs to be fixed.
Since we don't clean up the ports there may be an odd chance of having unused ports in the ""Service-node""."
1575,1350119,neutron,7a7291794eb718fc0a6105efacf88b2ee5594043,1,1,"“This code is incorrect: https://github.com/openstack/neutron/blob/master/neutron/db/l3_dvr_db.py#L297""",get_agent_gw_ports_exist_for_network in l3_dvr_db....,"This code is incorrect:  https://github.com/openstack/neutron/blob/master/neutron/db/l3_dvr_db.py#L297
The result is that no port is found by the query.  Because of this, a new port gets created each time the L3 agent restarts and tries to find the external gw port for the DVR router."
1576,1350252,neutron,748d4fdaf62667f39ce65e5053792b54dd684698,0,0,Bug in test,test_l3_plugin fails when run as single test,"tox  -e py27 neutron.tests.unit.test_l3_plugin
fails as follows. This is bacause necessary oslo config isn't initialized properly by L3AgentDbIntTestCase and L3AgentDbSepTestCase
The error log follows.
 $ tox -e py27 neutron.tests.unit.test_l3_plugin
py27 develop-inst-nodeps: /home/yamahata/openstack/tacker/neutron-l3-plugin/upstream/neutron-l3-db-refacotr-0
py27 runtests: commands[0] | python -m neutron.openstack.common.lockutils python setup.py testr --slowest --testr-args=neutron.tests.unit.test_l3_plugin
running testr
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --list
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpNLNI_l
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpBl_vq6
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpO3NkaL
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpc3FrWL
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmppz6MIP
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmprcHfnW
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpVW99Uk
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpMC3U19
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpiqP4jI
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpK6j9JS
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmp_cFhYl
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpNXI0Mi
======================================================================
FAIL:
neutron.tests.unit.test_l3_plugin.L3AgentDbSepTestCase.test_l3_agent_routers_query_floatingips
tags: worker-10
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2073, in setUp
    self.core_plugin = TestNoL3NatPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: neutron.tests.unit.test_l3_plugin.L3AgentDbSepTestCase.test_router_gateway_op_agent
tags: worker-10
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2073, in setUp
    self.core_plugin = TestNoL3NatPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: neutron.tests.unit.test_l3_plugin.L3AgentDbIntTestCase.test_l3_agent_routers_query_floatingips
tags: worker-1
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2060, in setUp
    self.core_plugin = TestL3NatIntPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: neutron.tests.unit.test_l3_plugin.L3AgentDbSepTestCase.test_l3_agent_routers_query_ignore_interfaces_with_moreThanOneIp
tags: worker-1
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2073, in setUp
    self.core_plugin = TestNoL3NatPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: process-returncode
tags: worker-10
----------------------------------------------------------------------
Binary content:
  traceback (test/plain; charset=""utf8"")
======================================================================
FAIL: process-returncode
tags: worker-1
----------------------------------------------------------------------
Binary content:
  traceback (test/plain; charset=""utf8"")
Ran 304 (-13543) tests in 27.382s (-600.855s)
FAILED (id=17, failures=6 (-2))
error: testr failed (1)
ERROR: InvocationError: '/neutron/.tox/py27/bin/python -m neutron.openstack.common.lockutils python setup.py testr --slowest --testr-args=neutron.tests.unit.test_l3_plugin'
__________________________________________________________________ summary __________________________________________________________________
ERROR:   py27: commands failed"
1577,1350268,nova,4dcfa79058a407eb7a6ce83f4d804df19ec95026,1,1,,allocate_fixed_ip should cleanup with correct para...,"in nova-network , when allocate_fixed_ip failed for some unknown reason
it will add
cleanup.append(fip.disassociate)
to cleanup the stuffs it did when handle exception
but the function is following in objects/fixed_ips.py
def disassociate(self, context):
so the cleanup function will not be executed correctly
try:
                        f()
                    except Exception:
                        LOG.warn(_('Error cleaning up fixed ip allocation. '
                                   'Manual cleanup may be required.'),
                                 exc_info=True)"
1578,1350320,neutron,25c5291addcd3ed0ef9df567a828866f1681e014,1,1,,Use correct section for log message if interface_d...,If import of interface_driver in lbaas namespace_driver fails then message for logger raises exception because of taking interface_driver from haproxy section while actual interface_driver is in default section.
1579,1350355,nova,03d34c975586788dc25249b5e0b962fc0634008c,1,1,,Bug #1350355 “nova-network,"Client side of the network RPC API sets 'requested_networks' parameter in deallocate_for_instance() call [1]
while server side expects 'fixed_ips' parameter [2]
[1] https://github.com/openstack/nova/blob/master/nova/network/rpcapi.py#L183
[2] https://github.com/openstack/nova/blob/master/nova/network/manager.py#L555"
1580,1350387,neutron,825a90bdb4ff2b6c08ef6717d350cc8b9d654120,0,0,Refactoring “The Open vSwitch and Linuxbridge plugins are being removed from the Neutron tree in Juno.”,Remove the Cisco Nexus monolithic plugin from the ...,"The Open vSwitch and Linuxbridge plugins are being removed from the Neutron tree in Juno.
See https://bugs.launchpad.net/neutron/+bug/1323729
The Cisco Nexus monolithic plugin does not work without the Open vSwitch plugin, so it also needs to be removed from the tree. The Cisco monolithic plugin contains code for both the Nexus hardware switches and the N1KV virtual switch. The N1KV code will remain in the tree (it does not depend on the OVS plugin).
Note: the Cisco Nexus is now supported in Neutron via the Nexus mechanism driver in the ML2 plugin."
1581,1350469,neutron,62317a159ad98ef9ba5208f2fa7708a88692c46f,1,1,,Floating ip association and deletion not working i...,"In certain scenarios explained below, floating ip association
and deletion does not work correctly
1) Create a neutron port and assign a floating ip to it. Now boot
     a vm using the neutron port. The vm boots correctly but
     the floating ip is not assigned to the vm.
2) Boot a vm and assign a floating ip to it. Now delete the
     floating ip. The floating ip is disassociated from the vm
     and deleted from neutron but is not deleted from Nuage
     VSD."
1582,1350485,neutron,dc658273e7cfe72e50dad40203db8bb0d9bb2188,1,1,,Fix L2Pop mech driver for dvr to handle DVR interf...,"L2Pop mech driver does not apply L2Pop rules to DVR router interfaces correctly (this might have been caused by competing changes for blueprint ofagent-l2pop)
The patch for this bug will enable DVR Router interface ports to be handled correctly in order to address the network island problem.
Network island is a situation where VMs belonging to two different networks are hosted on two different compute nodes.  The compute nodes themselves won't have VMs on the other network except for the one they are hosting.
This fix will enable DVR east-west traffic passthrough for cases where router and its interfaces are added after VMs have already been spawned/active on such routed networks."
1583,1350504,cinder,dca3c8323cf8cf12aa8ce4ba21f647ce631e8153,1,1,CVE,[OSSA 2014-033] GlusterFS driver uses unsafe qcow2...,"Concern about this was raised by Duncan Thomas.
The GlusterFS Cinder driver uses ""qemu-img info"" to guess at whether a volume file is a raw image or a qcow2 image.
This is unsafe because if a user writes a qcow2 header into a volume, Cinder will interpret it as a qcow2-formatted image.   It is believed this can lead to data being extracted from files on the Cinder volume host by writing a qcow2 header with a backing file pointer referencing a path to a file, and then cloning the volume.  (Other similar paths may exist.)
To fix this, Cinder needs to track the file format of any file being processed this way and use ""qemu-img convert -f <source_format>"" when performing operations like volume clone, which disables qemu-img's auto format detection.
This seems to affect the GlusterFS driver, but it is possible that other attack vectors exist.  The convert_image() method in cinder/image/image_utils.py is used in assorted places and does not specify a source format, so other uses of it will need to be examined for safety.
Fixing this in the GlusterFS driver is not simple: since volume/snapshot qcow2 chains are manipulated by Nova as well as Cinder, we will need to have Nova pass information back to Cinder when an operation such as volume_snapshot_delete is performed, indicating the resulting format of any files modified.
Since the above is a large effort, it may be possible to mitigate this in the short-term by having Cinder enforce some rules about whether a backing file pointer is valid before performing an operation on the file.  For the GlusterFS driver that would be: must start with 'volume-<x>' and not contain '/', since our valid usage of this only points to another file named volume-<id>.<id> and does not use paths.
This attack hasn't yet been demonstrated to work, but this is a commonly known problem when processing qcow2 files."
1584,1350542,nova,d2ab9079ada5135598a89aa65f3ef8b63bf28003,1,1,,resource tracker reports negative value for free h...,"When overcommiting on hard disk usage, the audit logs report negative amounts of free disk space. While technically correct, it may confuse the user, or make the user think there is something wrong with the tracking.
The patch to fix this will be in a similar vein to https://review.openstack.org/#/c/93261/"
1585,1350699,cinder,ce8893f4d3fae402494328bfbd07ea6642aae836,0,0,Bug in test,FakeBackupService doesn't implement verify,"Testcase test_import_record_with_verify uses 'FakeBackupService' but there is no
verfiy function implemented in that class."
1586,1350846,nova,984f4192ede99951078e2f6091e4c3e26cdb0e01,1,1,,add/delete fixed ip fails with nova-network,"nova add-fixed-ip <server> <network> fails with following in compute log:
2014-07-31 06:01:48.697 ERROR oslo.messaging.rpc.dispatcher [req-6e04dd42-1ebe-4aa3-a37b-e84bb60b3413 admin demo] Exception during message handling: 'dict' object has no attribute 'get_meta'
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 414, in decorated_function
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 327, in decorated_function
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 315, in decorated_function
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 3737, in add_fixed_ip_to_instance
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     self._inject_network_info(context, instance, network_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 4091, in _inject_network_info
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     network_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 5383, in inject_network_info
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     self.firewall_driver.setup_basic_filtering(instance, nw_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/firewall.py"", line 286, in setup_basic_filtering
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     self.nwfilter.setup_basic_filtering(instance, network_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/firewall.py"", line 123, in setup_basic_filtering
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     if subnet.get_meta('dhcp_server'):
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher AttributeError: 'dict' object has no attribute 'get_meta'
same happens with remove-fixed-ip call"
1587,1350937,neutron,1a0be0e05f8d299820b60cd888d74ed5ed73e265,1,1,(1d8afc7593fa04b8f804e13711ee36ea5bda0957)“Move from Python logging to Openstack loggingReplacing usage of python standard logging module with Openstack common logging module.”,neutron should use openstack logging everywhere,"In several places in the source tree we imports logging
https://github.com/openstack/neutron/blob/master/neutron/plugins/vmware/plugins/base.py#L16
but instead it should do:
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L52
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L63
arosen@arosen-desktop:/opt/stack/neutron/neutron$ grep -R ""import logging"" *
common/utils.py:import logging as std_logging
db/migration/alembic_migrations/heal_script.py:import logging
plugins/cisco/models/virt_phy_sw_v2.py:import logging
plugins/cisco/common/cisco_credentials_v2.py:import logging as LOG
plugins/cisco/network_plugin.py:import logging
plugins/cisco/nexus/cisco_nexus_snippets.py:import logging
plugins/cisco/nexus/cisco_nexus_plugin_v2.py:import logging
plugins/cisco/nexus/cisco_nexus_network_driver_v2.py:import logging
plugins/ml2/drivers/cisco/nexus/nexus_snippets.py:import logging
plugins/vmware/plugins/base.py:import logging
service.py:import logging as std_logging
tests/base.py:import logging
tests/unit/cisco/test_network_plugin.py:import logging
tests/unit/db/metering/test_db_metering.py:import logging
tests/unit/db/firewall/test_db_firewall.py:import logging
tests/unit/db/loadbalancer/test_db_loadbalancer.py:import logging
tests/unit/ml2/test_helpers.py:import logging
tests/unit/test_servicetype.py:import logging
tests/unit/vmware/apiclient/test_api_eventlet_request.py:import logging"
1588,1350942,neutron,b57d52aeb951bcfb9a2966ba7dc1b3f8b640bd8f,0,0,"feature “Use oslo.db create_engine instead of SQLAlchemy
    oslo.db may set additional options to engines that we may be interested in.”",Use oslo.db create_engine instead of SQLAlchemy,"oslo.db may set additional options to engines that we may be interested in.
This will also ease the switch to mysql-connector if that gets approved."
1589,1351002,nova,d96fbffcab5fafe48f4a91ea6f0a24310d63d9c7,1,1,,live migration (non-block-migration) with shared i...,"Note: the reproduction case below has been fixed by not blocking migration on config drives.  However, the underlying issue of NFS not being marked
as shared storage still stands, since the 'is_shared_block_storage' data
is used elsewhere as well.
---------------------------------------------------------------------------
To reproduce:
1. Set up shared instance storage via NFS and use one of the file-based image backends
2. Boot an instance with a config drive
3. Attempt to live migrate said instance w/o doing a block migration
The issue is caused by the following lines in nova/virt/libvirt/driver.py:
        if not (is_shared_instance_path and is_shared_block_storage):
            # NOTE(mikal): live migration of instances using config drive is
            # not supported because of a bug in libvirt (read only devices
            # are not copied by libvirt). See bug/1246201
            if configdrive.required_by(instance):
                raise exception.NoLiveMigrationForConfigDriveInLibVirt()
The issue, I believe, was caused by commit bc45c56f1, which separated checks for shared instance directories and shared block storage backends like Ceph.  The issue is that if a deployer is not using Ceph, the call to self.image_backend.backend().is_shared_block_storage() returns False.  However, is_shared_block_storage should not even be considered if the image backend is a file-based one."
1590,1351020,nova,6cef3c9b75cedb0fe6ff901128452415128d60b0,1,1,,FloatingIP fails to load from database when not as...,"A FloatingIP can be not associated with an FixedIP, which will cause its fixed_ip field in the database model to be None. Currently, FloatingIP's _from_db_object() method always assumes it's non-None and thus tries to load a FixedIP from None, which fails."
1591,1351066,neutron,6994ade726f4a33753a11b2b2c24454160c96d31,1,1,,Neutron floatingip delete does not delete the fip_...,"Neutron floatingip-delete ""id"" does not delete the associated ""fip_agent_gw_port""."
1592,1351123,neutron,0b4e42fe11bf918e18ea8f240d9055b3967b60bb,1,1,“Fix DB Duplicate error”,DB Duplicate error while scheduling distributed ro...,"I observed this error here:
http://logs.openstack.org/77/108177/10/experimental/check-tempest-dsvm-neutron-dvr/9e67d95/logs/screen-q-svc.txt.gz?level=TRACE#_2014-07-31_15_05_48_864
And in few other places. This seems to be triggered (for instance) during the following testcase:
tempest.scenario[...]test_server_connectivity_pause_unpause
It looks like the scheduling process fails because of a duplicate entry to the agent/router binding table. This might be an effect of fix:
https://review.openstack.org/#/c/73234/"
1593,1351127,nova,6a9fe989e8d20ba43ed1a2bf318bc41b745f318e,0,0,Bug in test,Exception in string format operation,"""tox -e docs"" shows the following error
2014-07-31 22:20:04.961 23265 ERROR nova.exception [req-ad8a37c6-85ae-4218-b8e6-d15eda5a1553 None] Exception in string format operation
2014-07-31 22:20:04.961 23265 TRACE nova.exception Traceback (most recent call last):
2014-07-31 22:20:04.961 23265 TRACE nova.exception   File ""/Users/dims/openstack/nova/nova/exception.py"", line 118, in __init__
2014-07-31 22:20:04.961 23265 TRACE nova.exception     message = self.msg_fmt % kwargs
2014-07-31 22:20:04.961 23265 TRACE nova.exception KeyError: u'flavor_id'
2014-07-31 22:20:04.961 23265 TRACE nova.exception
2014-07-31 22:20:04.962 23265 ERROR nova.exception [req-ad8a37c6-85ae-4218-b8e6-d15eda5a1553 None] reason:
2014-07-31 22:20:04.962 23265 ERROR nova.exception [req-ad8a37c6-85ae-4218-b8e6-d15eda5a1553 None] code: 404"
1594,1351350,nova,a507d42cf5d9912c2b3622e84afb8b7d3278595b,0,0,Refactorings,Warnings and Errors in Document generation,"Just pick any recent docs build and you will see a ton of issues:
Example from:
http://logs.openstack.org/46/111146/1/check/gate-nova-docs/4f3e8c4/console.html
2014-08-01 03:40:18.805 | /home/jenkins/workspace/gate-nova-docs/nova/api/openstack/compute/contrib/hosts.py:docstring of nova.api.openstack.compute.contrib.hosts.HostController.index:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.806 | /home/jenkins/workspace/gate-nova-docs/nova/api/openstack/compute/contrib/hosts.py:docstring of nova.api.openstack.compute.contrib.hosts.HostController.index:5: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.806 | /home/jenkins/workspace/gate-nova-docs/nova/api/openstack/compute/plugins/v3/hosts.py:docstring of nova.api.openstack.compute.plugins.v3.hosts.HostController.index:6: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.806 | /home/jenkins/workspace/gate-nova-docs/nova/compute/resource_tracker.py:docstring of nova.compute.resource_tracker.ResourceTracker.resize_claim:7: ERROR: Unexpected indentation.
2014-08-01 03:40:18.807 | /home/jenkins/workspace/gate-nova-docs/nova/compute/resource_tracker.py:docstring of nova.compute.resource_tracker.ResourceTracker.resize_claim:8: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.847 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/api.py:docstring of nova.db.sqlalchemy.api.instance_get_all_by_filters:23: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.847 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/api.py:docstring of nova.db.sqlalchemy.api.instance_get_all_by_filters:24: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.847 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/api.py:docstring of nova.db.sqlalchemy.api.instance_get_all_by_filters:31: ERROR: Unexpected indentation.
2014-08-01 03:40:18.848 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/utils.py:docstring of nova.db.sqlalchemy.utils.create_shadow_table:6: ERROR: Unexpected indentation.
2014-08-01 03:40:18.848 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:15: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.849 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:15: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.849 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:18: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.849 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:18: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.850 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:23: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.850 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:23: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.850 | <autodoc>:0: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.851 | <autodoc>:0: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.851 | /home/jenkins/workspace/gate-nova-docs/nova/image/api.py:docstring of nova.image.api.API.get_all:8: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.852 | /home/jenkins/workspace/gate-nova-docs/nova/keymgr/key_mgr.py:docstring of nova.keymgr.key_mgr.KeyManager.copy_key:9: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.852 | /home/jenkins/workspace/gate-nova-docs/nova/notifications.py:docstring of nova.notifications.info_from_instance:6: WARNING: Field list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.852 | /home/jenkins/workspace/gate-nova-docs/nova/objects/base.py:docstring of nova.objects.base.NovaObject.obj_make_compatible:10: ERROR: Unexpected indentation.
2014-08-01 03:40:18.853 | /home/jenkins/workspace/gate-nova-docs/nova/objects/base.py:docstring of nova.objects.base.NovaObject.obj_make_compatible:11: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.853 | /home/jenkins/workspace/gate-nova-docs/nova/objects/instance.py:docstring of nova.objects.instance.Instance.save:9: ERROR: Unexpected indentation.
2014-08-01 03:40:18.854 | /home/jenkins/workspace/gate-nova-docs/nova/objects/instance.py:docstring of nova.objects.instance.Instance.save:10: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.854 | /home/jenkins/workspace/gate-nova-docs/nova/objects/instance.py:docstring of nova.objects.instance.InstanceList.get_active_by_window_joined:9: WARNING: Field list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.854 | /home/jenkins/workspace/gate-nova-docs/nova/objects/pci_device.py:docstring of nova.objects.pci_device.PciDevice:31: ERROR: Unexpected indentation.
2014-08-01 03:40:18.887 | /home/jenkins/workspace/gate-nova-docs/nova/objects/pci_device.py:docstring of nova.objects.pci_device.PciDevice:33: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.887 | /home/jenkins/workspace/gate-nova-docs/nova/openstack/common/network_utils.py:docstring of nova.openstack.common.network_utils.set_tcp_keepalive:6: ERROR: Unexpected indentation.
2014-08-01 03:40:18.888 | /home/jenkins/workspace/gate-nova-docs/nova/openstack/common/report/report.py:docstring of nova.openstack.common.report.report.ReportSection:16: ERROR: Unexpected indentation.
2014-08-01 03:40:18.888 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_manager.py:docstring of nova.pci.pci_manager.PciDevTracker.get_free_devices_for_requests:5: ERROR: Unexpected indentation.
2014-08-01 03:40:18.889 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.889 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request:11: ERROR: Unexpected indentation.
2014-08-01 03:40:18.889 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request:16: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.890 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request.get_pci_requests_from_flavor:17: ERROR: Unexpected indentation.
2014-08-01 03:40:18.891 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request.get_pci_requests_from_flavor:25: ERROR: Unexpected indentation.
2014-08-01 03:40:18.891 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request.get_pci_requests_from_flavor:27: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.892 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.892 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:4: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.892 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:10: ERROR: Unexpected indentation.
2014-08-01 03:40:18.893 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:11: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.893 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/pci_passthrough_filter.py:docstring of nova.scheduler.filters.pci_passthrough_filter.PciPassthroughFilter:9: ERROR: Unexpected indentation.
2014-08-01 03:40:18.893 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/pci_passthrough_filter.py:docstring of nova.scheduler.filters.pci_passthrough_filter.PciPassthroughFilter:10: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.894 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.894 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.894 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.895 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.927 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.928 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:10: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.930 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:10: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.933 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:20: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.933 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/plugins/v3/test_servers.py:docstring of nova.tests.api.openstack.compute.plugins.v3.test_servers.ServersAllExtensionsTestCase:11: ERROR: Unexpected indentation.
2014-08-01 03:40:18.934 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/plugins/v3/test_servers.py:docstring of nova.tests.api.openstack.compute.plugins.v3.test_servers.ServersAllExtensionsTestCase:13: ERROR: Unexpected indentation.
2014-08-01 03:40:18.934 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/test_servers.py:docstring of nova.tests.api.openstack.compute.test_servers.ServersAllExtensionsTestCase:11: ERROR: Unexpected indentation.
2014-08-01 03:40:18.935 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/test_servers.py:docstring of nova.tests.api.openstack.compute.test_servers.ServersAllExtensionsTestCase:13: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.936 | /home/jenkins/workspace/gate-nova-docs/nova/tests/compute/test_resource_tracker.py:docstring of nova.tests.compute.test_resource_tracker.NoInstanceTypesInSysMetadata:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.937 | /home/jenkins/workspace/gate-nova-docs/nova/tests/compute/test_resource_tracker.py:docstring of nova.tests.compute.test_resource_tracker.NoInstanceTypesInSysMetadata:4: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.938 | /home/jenkins/workspace/gate-nova-docs/nova/tests/db/test_migrations.py:docstring of nova.tests.db.test_migrations:21: ERROR: Unexpected indentation.
2014-08-01 03:40:18.940 | /home/jenkins/workspace/gate-nova-docs/nova/tests/db/test_migrations.py:docstring of nova.tests.db.test_migrations:22: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.941 | /home/jenkins/workspace/gate-nova-docs/nova/tests/db/test_migrations.py:docstring of nova.tests.db.test_migrations:24: ERROR: Unexpected indentation.
2014-08-01 03:40:18.942 | /home/jenkins/workspace/gate-nova-docs/nova/tests/image_fixtures.py:docstring of nova.tests.image_fixtures.get_image_fixtures:10: SEVERE: Unexpected section title.
2014-08-01 03:40:18.942 |"
1595,1351466,neutron,b4bf668ec40ee6a329740b587fa4a651a6e0dca2,1,1,“Remove the reference for now.”,Bug #1351466 “can't copy '.../cisco_cfg_agent.ini',"Started roughly 1800 UTC this evening
2014-08-01 19:36:06.878 | error: can't copy 'etc/neutron/plugins/cisco/cisco_cfg_agent.ini': doesn't exist or not a regular file
http://logs.openstack.org/70/111370/1/check-tripleo/check-tripleo-ironic-undercloud-precise-nonha/3bc75ae/console.html"
1596,1351810,nova,062b1f8c0f6ba09ab6764ea512c3615fc93aaf08,0,0,"Refactoring “Move _is_mapping logic to more central place
“",Move _is_mapping logic to more central place,"This bug is a follow-up to Nikola Dipanov's comment in https://review.openstack.org/#/c/109834/2/nova/compute/manager.py.
The logic to identify volumes is currently a nested function in _default_block_device_names, named _is_mapping.  It should be moved to a more general place so others could utilize it."
1597,1352102,nova,674954f731bf4b66356fadaa5baaeb58279c5832,1,1,"“Commit da66d50010d5b1ba1d7fc9c3d59d81b6c01bb0b0 restricted
    attaching external networks to admin clients. This patch changes
    it to a policy based check instead with the default setting being
    admin only. “",users are unable to create ports on provider netwo...,"after commit da66d50010d5b1ba1d7fc9c3d59d81b6c01bb0b0 my users are unable to boot vm attached to provider networks, this is a serious regression for me as we mostly use provider networks.
bug which originated the commit https://bugs.launchpad.net/ubuntu/+source/nova/+bug/1284718"
1598,1352428,nova,86a4bd7d9b34057f04ebbe2d2131f033d33d082b,1,1,,HyperV “Shutting Down,"The method which gets VM related information can fail if the VM is in an intermediary state such as ""Shutting down"".
The reason is that some of the Hyper-V specific vm states are not defined as possible states.
This will result into a key error as shown bellow:
http://paste.openstack.org/show/90015/"
1599,1352509,glance,d67a94b60261a856ab52e57a2490afaae6c5d91d,1,1,"""Change 4e0c563b8f3a5ced8f65fcca83d341a97729a5d4 was incomplete and missed a variable in the RBD store.”",Bug #1352509 “RBD store,"See https://bugs.launchpad.net/glance/+bug/1336168
Change 4e0c563b8f3a5ced8f65fcca83d341a97729a5d4 was incomplete and missed a variable in the RBD store."
1600,1352595,nova,81348368c70cd39c6241e7da6d33629e577494f5,1,0,"""In libvirt driver directory, rbd.py confict with global rbd library
    which is imported in rbd.py,”",nova boot fails when using rbd backend,"Trace ends with:
TRACE nova.compute.manager [instance: c1edd5bf-ba48-4374-880f-1f5fa2f41cd3]  File ""/opt/stack/nova/nova/virt/libvirt/rbd.py"", line 238, in exists
TRACE nova.compute.manager [instance: c1edd5bf-ba48-4374-880f-1f5fa2f41cd3]     except rbd.ImageNotFound:
TRACE nova.compute.manager [instance: c1edd5bf-ba48-4374-880f-1f5fa2f41cd3] AttributeError: 'module' object has no attribute 'ImageNotFound'
It looks like the above module tries to do a ""import rbd"" and ends up importing itself again instead of the global library module.
A quick fix would be renaming the file to rbd2.py and changing the references in driver.py and imagebackend.py, but maybe there is a better solution?"
1601,1352659,nova,257183a8a9130f2b444f7f96ec8582da89684528,1,1,,race in server show api,"Because of the instance object lazy loading its possible to get into situations where the API code is half way through assembling data to return to the client when the instance disappears underneath it. We really need to ensure everything we will need is retreived up front so we have a consistent snapshot view of the instance
[req-5ca39eb3-c1d2-433b-8dac-1bf5f338ce1f ServersAdminNegativeV3Test-1453501114 ServersAdminNegativeV3Test-364813115] Unexpected exception in API method
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/api/openstack/extensions.py"", line 473, in wrapped
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/api/openstack/compute/plugins/v3/servers.py"", line 410, in show
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     return self._view_builder.show(req, instance)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/api/openstack/compute/views/servers.py"", line 268, in show
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     _inst_fault = self._get_fault(request, instance)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/api/openstack/compute/views/servers.py"", line 214, in _get_fault
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     fault = instance.fault
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/objects/base.py"", line 67, in getter
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     self.obj_load_attr(name)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/objects/instance.py"", line 520, in obj_load_attr
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     expected_attrs=[attrname])
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/objects/base.py"", line 153, in wrapper
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     result = fn(cls, context, *args, **kwargs)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/objects/instance.py"", line 310, in get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     use_slave=use_slave)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/db/api.py"", line 676, in instance_get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     columns_to_join, use_slave=use_slave)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 167, in wrapper
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 1715, in instance_get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     columns_to_join=columns_to_join, use_slave=use_slave)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 1727, in _instance_get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     raise exception.InstanceNotFound(instance_id=uuid)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions InstanceNotFound: Instance fcff276a-d410-4760-9b98-4014024b1353 could not be found.
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions
  http://logs.openstack.org/periodic-qa/periodic-tempest-dsvm-nova-v3-full-master/a278802/logs/screen-n-api.txt"
1602,1352698,neutron,f9c285fd617403ab89c857e3d8bab0d06d19d3ba,0,0,Feature “it makes sense for neutron context to include auth token.”,neutron context doesn't include auth token,"Discussed with the thread starting with
http://lists.openstack.org/pipermail/openstack-dev/2014-July/040644.html
Neutron context isn't populated with auth token unlike nova, glance.
Since there are several (potential) users for it. servicevm project, routervm implementation(cisco csr1kv, vyatta vrouter),
it makes sense for neutron context to include auth token."
1603,1352768,nova,34558259058e19f9c4b2a96cbb97770883460ded,1,1,"""this code review, https://review.openstack.org/#/c/104262/ brings an error in log because of the line at 137:”",Bug #1352768 “virt,"In this code review, https://review.openstack.org/#/c/104262/  brings an error in log because of the line at 137:
+ LOG.info(_LI(""Unable to force TCG mode, libguestfs too old?""),
+ ex)
Error is:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 851, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 685, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 724, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 649, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Logged from file guestfs.py, line 139
To fix this issue, we just need to add %s"
1604,1352786,neutron,c5e186f466a4347b72868db8adbde9450215f107,1,1,"“this was caused because the l3 service plugin was
    erroneously calling a method on self”",router-update fails with 500 for dvr routers,"1.create a dvr with name say dvr1
2.add router gateway
3.now perform router update neutron router-update dvr1 --name dvr2
Actual Results:
Request Failed: internal server error while processing your request"" error is seen,trace related to L3RouterPlugin logs are seen
Expected Results:
router name should be updated"
1607,1352907,neutron,cfea218390605e2fe34b225ffa75b8b5c141f0b9,1,1,,response of normal user update the “shared,"I used a normal user to create a network successfully,then I wanted to update the ""shared"" property of the network.
It failed,and response 404 erorr,the message is :The resource could not be found.But I have created the network,it is so strange.
I check the policy.json of neutron, the rule is: ""update_network:shared"": ""rule:admin_only"", so the normal user can't update it.
So the error information is wrong.
Check the code:
    def update(self, request, id, body=None, **kwargs):
        """"""Updates the specified entity's attributes.""""""
      ......
      ......
        try:
            policy.enforce(request.context,
                           action,
                           orig_obj)
        except exceptions.PolicyNotAuthorized:
            # To avoid giving away information, pretend that it
            # doesn't exist
            msg = _('The resource could not be found.')
            raise webob.exc.HTTPNotFound(msg)
I think we couldn't provide the wrong response information to avoid giving away information,and there isn't any information that need to avoid giving away here, So I think it is a bug.
I suggest to modify the code like this:
       try:
            policy.enforce(request.context,
                           action,
                           orig_obj)
        except exceptions.PolicyNotAuthorized:
            # To avoid giving away information, pretend that it
            # doesn't exist
            # msg = _('The resource could not be found.')
            raise webob.exc.HTTPForbidden(exceptions.PolicyNotAuthorized.message)"
1608,1353006,neutron,608003408b3aba6a8290427257460287d8a4ce96,1,1,,AttributeError when setting external gw in DVR cas...,"This has been found on master:
1) create a DVR router: neutron router-create test --distributed
2) set external gateway: neutron router-gateway-set test public
3) observe stacktrace: http://paste.openstack.org/show/90577/ in L3 Agent's log
This does not seem to upset the L3 Agent and its ability to work correctly, but it would be good to eradicate the trace."
1609,1353112,neutron,b1b97dd721980bd06a97b9a9ad0fb15d95370164,1,1,“extension is missed out and needs a fix.”,template attribute missing a support for subnet an...,One can instantiate subnet and router based on the template provided to inherit most if not all template properties. This is a very useful feature for all Nuage's current and future customers. extension is missed out and needs a fix.
1610,1353131,nova,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,1,1,,Failed to commit reservations  in gate,"From: http://logs.openstack.org/31/105031/14/gate/gate-tempest-dsvm-full/c05b927/console.html
2014-08-05 02:54:01.131 | Log File Has Errors: n-cond
2014-08-05 02:54:01.132 | *** Not Whitelisted *** 2014-08-05 02:25:47.799 ERROR nova.quota [req-19feeaa2-e1d4-419b-a7bb-a19bb7000b1d AggregatesAdminTestJSON-2075387658 AggregatesAdminTestJSON-270189725] Failed to commit reservations [u'ceaa6ce7-db8d-4ba6-871a-b29c59f4a338', u'10d7550d-d791-44dd-8396-2fa6eaea7c20', u'e7a322e2-948d-45f7-892f-7ea4d9aa0e7c']
There are a number of errors happening in that file that arent whitelisted.
This one *seems* to be a possible cause of others.as there is then a number of InstanceNotFound errors."
1611,1353180,neutron,01b2eedd05fb91aa8970c64b753d831257243598,1,1,"“following commit was made https://review.openstack.org/#/c/81334.
That has introduce a problem where neutron-db-manage does not work with some of the options.”",neutron-db-manage current/history/branches/stamp/r...,"In order to fix bug (https://bugs.launchpad.net/neutron/+bug/1288358) following commit was made https://review.openstack.org/#/c/81334.
That has introduce a problem where neutron-db-manage does not work with some of the options.
For example:
root@openstack-ubuntu:/opt/stack/neutron# neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file <X> --config-file <Y> current
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 175, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 63, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 233, in current
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 203, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 212, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 125, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 94, in run_migrations_online
    set_mysql_engine(neutron_config.command.mysql_engine)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/config/cfg.py"", line 2344, in __getattr__
    raise NoSuchOptError(name)
oslo.config.cfg.NoSuchOptError: no such option: mysql_engine
root@openstack-ubuntu:/opt/stack/neutron# neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file <X> --config-file <Y> stamp icehouse
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 175, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 91, in do_stamp
    sql=CONF.command.sql)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 63, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 258, in stamp
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 203, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 212, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 125, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 94, in run_migrations_online
    set_mysql_engine(neutron_config.command.mysql_engine)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/config/cfg.py"", line 2344, in __getattr__
    raise NoSuchOptError(name)
oslo.config.cfg.NoSuchOptError: no such option: mysql_engine"
1612,1353271,neutron,338171c114743c0e03def04ef551231f0d9c93f8,1,1,,404 Error when retrieving metadata using DVR route...,"This happens on master.
Steps to reproduce:
- usual devstack config will do
- boot a VM
- ssh to the VM
- curl http://169.254.169.254 from the console
- 404 Not Found error is reported
I would expect the metadata server response to come across correctly."
1613,1353309,neutron,ca00b1739431fb9fe8f81919dc4644f4f467fa9d,1,0,"“The following commit bumped the RPC version of l3-agent.
L3-agent tried to get service plugin list when it starts.
commit d6f014d0922e03864fd72efbcde04322711c2510”",l3 agent is failing with unsupported version endpo...,"my devstack script is failing with the above mentioned error. It is causing my CI to fail.
the stack trace is
2014-08-06 12:31:44.935 TRACE neutron Traceback (most recent call last):
2014-08-06 12:31:44.935 TRACE neutron   File ""/usr/local/bin/neutron-l3-agent"", line 10, in <module>
2014-08-06 12:31:44.935 TRACE neutron     sys.exit(main())
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 1787, in main
2014-08-06 12:31:44.935 TRACE neutron     manager=manager)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/service.py"", line 264, in create
2014-08-06 12:31:44.935 TRACE neutron     periodic_fuzzy_delay=periodic_fuzzy_delay)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/service.py"", line 197, in __init__
2014-08-06 12:31:44.935 TRACE neutron     self.manager = manager_class(host=host, *args, **kwargs)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 1706, in __init__
2014-08-06 12:31:44.935 TRACE neutron     super(L3NATAgentWithStateReport, self).__init__(host=host, conf=conf)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 430, in __init__
2014-08-06 12:31:44.935 TRACE neutron     self.plugin_rpc.get_service_plugin_list(self.context))
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 142, in get_service_plugin_list
2014-08-06 12:31:44.935 TRACE neutron     version='1.3')
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/common/log.py"", line 36, in wrapper
2014-08-06 12:31:44.935 TRACE neutron     return method(*args, **kwargs)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/common/rpc.py"", line 170, in call
2014-08-06 12:31:44.935 TRACE neutron     context, msg, rpc_method='call', **kwargs)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/common/rpc.py"", line 196, in __call_rpc_method
2014-08-06 12:31:44.935 TRACE neutron     return func(context, msg['method'], **msg['args'])
2014-08-06 12:31:44.935 TRACE neutron   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
2014-08-06 12:31:44.935 TRACE neutron     retry=self.retry)
2014-08-06 12:31:44.935 TRACE neutron   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-08-06 12:31:44.935 TRACE neutron     timeout=timeout, retry=retry)
2014-08-06 12:31:44.935 TRACE neutron   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 404, in send
2014-08-06 12:31:44.935 TRACE neutron     retry=retry)
2014-08-06 12:31:44.935 TRACE neutron   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 395, in _send
2014-08-06 12:31:44.935 TRACE neutron     raise result
2014-08-06 12:31:44.935 TRACE neutron RemoteError: Remote error: UnsupportedVersion Endpoint does not support RPC version 1.3
Please help."
1614,1353414,neutron,03c4dc3573f640d06b80d686a389f1f3438a6d25,0,0,Refactoing “Remove duplicated check for router connect to external”,Bug #1353414 “duplicated check for router connect to external ne... ,"In the function neutron.db.l3_db.get_assoc_data(),
we get router_id first, then check whether this router is connecting to external network.
But the function we used to get router_id -- neutron.db.l3_db._get_router_for_floatingip(), has already checked this.
As it names says, this function is designed for get router_id, which can connect to floating network."
1615,1353506,cinder,140956515327494a53de6ad09c35690624248f0a,0,0,Bug in test,NetApp ESeries unit test breaks tests using python...,"The unit test for the NetApp E-Series volume driver (cinder/tests/test_netapp_eseries_iscsi.py) is missing cleanup operations to ensure other tests using the python-requests library are not affected by its setup.
By assigning a mock object to requests.Session during setup and not restoring the original during teardown, the test causes any following code trying to invoke requests.Session.send to fail with the message:
AttributeError: type object 'FakeEseriesHTTPSession' has no attribute 'send'
Attached is a test case (test_requests_mock.py) that when run alone will pass but fails when being executed after the NetApp E-Series test in the same run:
cinder$ ./run_tests.sh -V cinder.tests.test_requests_mock
[...]
Ran 1 test in 5.007s
OK
cinder$ ./run_tests.sh -V cinder.tests.test_requests_mock cinder.tests.test_netapp_eseries_iscsi
[...]
AttributeError: type object 'FakeEseriesHTTPSession' has no attribute 'send'
Ran 15 tests in 5.030s
FAILED (failures=1)"
1616,1353697,neutron,771327adbe9e563506f98ca561de9ded4d987698,1,0,“ML2 RPC base version is wrongly bumped to 1.1 and it breaks hyper-v agent with ML2 plugin.”,Bug #1353697 “Hyper-V agent raises UnsupportedRpcVersion,"The Hyper-V agent raises:
2014-08-06 10:42:37.096 2052 ERROR neutron.openstack.common.rpc.amqp [req-46340a1a-9143-45c9-b645-2612d41f20a6 None] Exception during message handling
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\site-packages\neutron\openstack\common\rpc\amqp.py"", line 462, in _process_data
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp     **args)
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\site-packages\neutron\openstack\common\rpc\dispatcher.py"", line 178, in dispatch
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp     raise rpc_common.UnsupportedRpcVersion(version=version)
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp UnsupportedRpcVersion: Specified RPC version, 1.1, not supported by this endpoint.
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp
The issue does not affect functionality, but it creates a lot of noise in the logs since the error is logged at each iteration."
1617,1353885,neutron,be81901b615b45d6aed4287df8c285a1f0aa72b0,1,0,"API change. “The L2Pop feature seems to have broken due to OVS Concurrent DeferredBridge implementation approved here:
https://review.openstack.org/77578
“",L2Pop on OVS broken due to DeferredBridge introduc...,"The L2Pop feature seems to have broken due to OVS Concurrent DeferredBridge implementation approved here:
https://review.openstack.org/77578
On the OVS Agent logs on compute hosts, could see that dynamic add_tunnel_port creation is failing:
2014-08-07 01:05:37.618 ^[[01;31mERROR oslo.messaging.rpc.dispatcher [^[[01;36mreq-0561d1e7-87a9-43cf-b7d4-c60e560d34f5 ^[[00;36mNone None^[[01;31m] ^[[01;35m^[[01;31mException during message handling: add_tunnel_port^[[00m
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    incoming.message))
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return self._do_dispatch(endpoint, method, ctxt, args)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    result = getattr(endpoint, method)(ctxt, **new_args)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/common/log.py"", line 36, in wrapper
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return method(*args, **kwargs)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/agent/l2population_rpc.py"", line 45, in add_fdb_entries
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    self.fdb_add(context, fdb_entries)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 347, in fdb_add
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    agent_ports, self.tun_br_ofports)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/common/log.py"", line 36, in wrapper
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return method(*args, **kwargs)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/agent/l2population_rpc.py"", line 179, in fdb_add_tun
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    lvm.network_type)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1061, in setup_tunnel_port
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    network_type)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1017, in _setup_tunnel_port
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    ofport = br.add_tunnel_port(port_name,
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/agent/linux/ovs_lib.py"", line 486, in __getattr__
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    raise AttributeError(name)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mAttributeError: add_tunnel_port"
1618,1353949,nova,d96a8f0e6b524c10e563418c2770069030455c25,0,0,Feature “this exception is not handled in V3 api.”,nova.exception.ExternalNetworkAttachForbidden is n...,"When creating an instance and assigning a public network to it without admin authority, ExternalNetworkAttachForbidden
will be raised. But this exception is not handled in V3 api.
2014-08-07 19:40:55.032 ERROR nova.api.openstack.extensions [req-a3a824a2-d477-4720-98c7-d3161de268ba demo demo] Unexpected exception in API method
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 473, in wrapped
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/validation/__init__.py"", line 39, in wrapper
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     return func(*args, **kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/servers.py"", line 507, in create
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     **create_kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/hooks.py"", line 131, in inner
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     rv = f(*args, **kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 1351, in create
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     legacy_bdm=legacy_bdm)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 967, in _create_instance
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     max_count)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 734, in _validate_and_build_base_options
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     requested_networks, max_count)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 447, in _check_requested_networks
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     max_count)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 709, in validate_networks
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     neutron=neutron)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 169, in _get_available_networks
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     network_uuid=net['id'])
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions ExternalNetworkAttachForbidden: It is not allowed to create an interface on external network 447d82c5-bf58-4f39-ac2f-a30227a464e2"
1619,1354072,neutron,b4eaa0520990f30519aaa073f352541f17b0577b,1,1,"“Since change:
https://review.openstack.org/#/c/73234/
“",Transaction is not rollbacked properly in bind_rou...,"This stracktrace has been observed:
http://logs.openstack.org/29/112229/5/gate/gate-tempest-dsvm-neutron-pg/db8ad58/logs/screen-q-svc.txt.gz?level=TRACE#_2014-08-07_15_49_00_061
Since change:
https://review.openstack.org/#/c/73234/
A unique constraint has been introduced. This prevents the race condition to cause multiple entries in the table, however concurrent changes can still occur, hence the DBDuplicateEntry can still occur. That  said, it looks like the transaction is not cleaned up properly."
1620,1354218,neutron,661e3ec795be075eb8f66ff15de1c8ad3603a682,1,1,,heal script is not idempotent,"tested with mysql
1) upgrade head
2) downgrade havana
3) upgrade head
4) BOOM -> http://paste.openstack.org/show/91769/
5) This is easy [1]
6) Try 1-3 again
7) BOOM -> http://paste.openstack.org/show/91770/
8) This is easy as well [2]
9) Repeat again steps 1-3
10) BOOM -> http://paste.openstack.org/show/91771/
I'm clueless so far about the last failure.
[1]
--- a/neutron/db/migration/alembic_migrations/heal_script.py
+++ b/neutron/db/migration/alembic_migrations/heal_script.py
@@ -103,12 +103,12 @@ def parse_modify_command(command):
     #              autoincrement=None, existing_type=None,
     #              existing_server_default=False, existing_nullable=None,
     #              existing_autoincrement=None, schema=None, **kw)
+    bind = op.get_bind()
     for modified, schema, table, column, existing, old, new in command:
         if modified.endswith('type'):
             modified = 'type_'
         elif modified.endswith('nullable'):
             modified = 'nullable'
-            bind = op.get_bind()
             insp = sqlalchemy.engine.reflection.Inspector.from_engine(bind)
             if column in insp.get_primary_keys(table) and new:
                 return
[2]
--- a/neutron/db/migration/alembic_migrations/heal_script.py
+++ b/neutron/db/migration/alembic_migrations/heal_script.py
@@ -103,12 +103,12 @@ def parse_modify_command(command):
     #              autoincrement=None, existing_type=None,
     #              existing_server_default=False, existing_nullable=None,
     #              existing_autoincrement=None, schema=None, **kw)
+    bind = op.get_bind()
     for modified, schema, table, column, existing, old, new in command:
         if modified.endswith('type'):
             modified = 'type_'
         elif modified.endswith('nullable'):
             modified = 'nullable'
-            bind = op.get_bind()
             insp = sqlalchemy.engine.reflection.Inspector.from_engine(bind)
             if column in insp.get_primary_keys(table) and new:
                 return
@@ -123,7 +123,7 @@ def parse_modify_command(command):
                 existing['existing_server_default'] = default.arg
             else:
                 existing['existing_server_default'] = default.arg.compile(
-                    dialect=bind.engine.name)
+                    dialect=bind.dialect)
         kwargs.update(existing)
         op.alter_column(table, column, **kwargs)"
1621,1354272,cinder,86fc539a23b0b2cdd6e0803aa3a3cb10fc5758a6,1,1,Minor issues in comments,Bug #1354272 “EMC,"There were some minor issues in initial commit of VNX Direct Driver
Juno Update (https://review.openstack.org/#/c/104413/), such as typo,
unclear config option help message and missing period.
This bug is to track these issues."
1622,1354285,neutron,6c1eec67d2e7def7561bde906df9a50006a86b10,1,1,,L3-agent using MetaInterfaceDriver failed,"MetaInterfaceDriver communicates with neutron-server using REST API.
If a user intend to use internalurl for neutron-server endpoint, MataInterfaceDriver fails.
This is because MetaInterfaceDriver does not specify endpoint_type. Thus it assumes using publicurl.
---
class MetaInterfaceDriver(LinuxInterfaceDriver):
    def __init__(self, conf):
        super(MetaInterfaceDriver, self).__init__(conf)
        from neutronclient.v2_0 import client
        self.neutron = client.Client(
            username=self.conf.admin_user,
            password=self.conf.admin_password,
            tenant_name=self.conf.admin_tenant_name,
            auth_url=self.conf.auth_url,
            auth_strategy=self.conf.auth_strategy,
            region_name=self.conf.auth_region
        )
---
Note that MetaInterfaceDriver is used with Metaplugin only."
1623,1354448,nova,3a5919fd4af6c3b772397a5e7d90eebdf9b371af,1,0,“The Hyper-V driver does not support resize down and is currently”,The Hyper-V driver should raise a InstanceFaultRol...,"The Hyper-V driver does not support resize down and is currently rising an exception if the user attempts to do that, causing the instance to go in ERROR state.
The driver should use the recently introduced instance faults ""exception.InstanceFaultRollback"" instead, which will leave the instance in ACTIVE state as expected."
1624,1354499,nova,c55736d9fc941ae3f00a29e945b8881be7813e52,1,0,“In c5402ef4fc509047d513a715a1c14e9b4ba9674f we added support for the new cinder V2 API.”,boot from volume fails when upgrading using cinder...,"In c5402ef4fc509047d513a715a1c14e9b4ba9674f we added support for the new cinder V2 API.
When a user who was previously using the Cinder v1 API (which would have been required) updates to the new code the immediate defaults cause the cinder v2 API to be chosen. This is because we now default cinder_catalog_info to 'volumev2:cinder:publicURL'. So if a user was using the previous default value of 'volumev2:cinder:publicURL' their configuration would now be broken.
Given the new deprecation code hasn't been released yet I think we need to wait at least one release before we can make this change to our cinder_catalog_info default value."
1625,1354664,nova,23340b49b1adee5cb9592b8e6a8471969b9341c7,1,0,"""This is due to the change to objects.”,”Commit 1023e703bd41c2a42b1159af0d9e907e94440b34 added support
    for objects.”",Bug #1354664 “Image cache aging,"An extra entry of invalid data is passes to the image cache aging:
USED: {'': (2, 0, ['instance-0000000a', 'instance-0000000a']), '7ee53435-b6d5-4c15-bce4-2f3dfac96ffd': (1, 0, ['instance-0000000a'])}
This is due to the change to objects."
1626,1354801,cinder,a00f41a02b90ca93e21e9a98236af21b454872b3,1,0,"""The Swift backup driver for Cinder supports only Auth 1.0,”",Cinder backup for Swift does not work with Auth 2....,"The Swift backup driver for Cinder supports only Auth 1.0, for Swift clusters that work with Auth 2.0, the backup driver cannot authenticate with Swift."
1627,1354912,neutron,03920290d00148437fe14696621bc7446bfc7e46,1,1,,Exception in l3_rbc_base,"The traceback observed in many of tempest jobs:
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 63, in sync_routers
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     self._ensure_host_set_on_ports(context, plugin, host, routers)
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 87, in _ensure_host_set_on_ports
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     interface, router['id'])
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 100, in _ensure_host_set_on_port
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     {'port': {portbindings.HOST_ID: host}})
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/plugins/ml2/plugin.py"", line 891, in update_port
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     return bound_port._port
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher AttributeError: 'dict' object has no attribute '_port'
Logstash query:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIkF0dHJpYnV0ZUVycm9yOiAnZGljdCcgb2JqZWN0IGhhcyBubyBhdHRyaWJ1dGUgJ19wb3J0J1wiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA3Njk1MjI1MzM1fQ=="
1628,1355087,neutron,11ca12dd8752a7d8fc13027c21ee572233becb74,1,1,,when a interface is added after router gateway set...,"1.create n/w,subnet
2.create a dvr and attach the subnet
3/create external network and attach the router gateway
4.now boot a vm in that subnet
5.ping to external network -successful
6.create a new network,subnet attach it to router created in step 2.
7.boot a vm and ping to external network -fails
8.try to ping to external network using vm created in step 4 -fails
Reason:
=======
when new subnet is added ,all the sg ports inside snat namespace are updated with default gateway of subnet added
say i had subnet 4.4.4.0/24 already attached to router its sg port had ip 4.4.4.2,now when i add new subnet say 5.5.5.0/24 this router
sg port of 4.4.4.0/24 becomes 5.5.5.1 also sg ip of 5.5.5.0/24 also becomes 5.5.5.1 (even though 5.5.5.1 has device owner =network:router_interface_distributed and 5.5.5.2 has device owner as network:router_centralized_snat)"
1629,1355251,neutron,3082a074f819f0479fc941b30dc7cfdbb8aab394,0,0,Bug in test,Bug #1355251 “db plugin sort testing function should not use sor... ,"tests/unit/test_db_plugin.py line 585
(https://github.com/openstack/neutron/blob/master/neutron/tests/unit/test_db_plugin.py#L585)
sorted() function is used within assertEqual() function.
This breaks tests objective which is to test sorting
Every unit test using this function (_test_list_with_sort) for sorting tests will always succeed.
sorted() should not be used for proper test results"
1630,1355297,cinder,186c3e99cf04b22ddbb1046cc9e42d1f7473e8cf,1,1,,manage.py is unusable,"cinder/db/sqlalchemy/migrate_repo/manage.py is unusable
It's not possible to use manage.py in its current state due to an exception thrown by oslo.config:
  oslo.config.cfg.ArgsAlreadyParsedError:
    arguments already parsed: cannot register CLI option
This exception is a side-effect of including cinder.openstack.common.logging in most of the migration scripts."
1631,1355348,nova,4c4dc3a6d331426e472e2dd1e9b0513da7cb7450,1,1,,Terminating an instance while attaching a volume l...,"This is happening with the xenapi driver, but it's possible that this can happen with others.  The sequence of events I'm witnessing is:
An attach_volume request is made and shortly after a terminate_instance request is made.
From the attach_volume request the block device mapping has been updated, the volume has been connected to the hypervisor, but has not been attached to the instance.  The terminate request begins processing before the volume connection is attached to the instance so when it detaches volumes and their connections it misses the latest one that's still attaching.  This leads to a failure when asking Cinder to clean up the volume, such as:
2014-08-06 20:30:14.324 30737 TRACE nova.compute.manager [instance: <uuid>] ClientException: DELETE on http://127.0.0.1/volumes/<uuid>/export?force=False returned '409' with 'Volume '<uuid>' is currently attached to '127.0.0.1'' (HTTP 409) (Request-ID: req-)
And in turn, when the attach_volume tries to attach the volume to the instance it finds that the instance no longer exists due to the terminate request.  This leaves the instance undeletable and the volume stuck.
Having attach_volume share the instance lock with terminate_instance should resolve this.  Virt drivers may also want to try to cope with this internally and not rely on a lock."
1632,1355409,neutron,aee5344db7972e2e12ed056c2e6467f7952aec3f,1,1,,Key error in l3_dvr_db,"The following stack trace observed in the gate:
ERROR oslo.messaging.rpc.dispatcher [req-110db567-3322-4922-95c8-a54d166c8ead ] Exception during message handling: u'b132e9da-3ee2-47ac-9ed5-f04ca73c01d1'
TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
TRACE oslo.messaging.rpc.dispatcher     incoming.message))
TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 57, in sync_routers
TRACE oslo.messaging.rpc.dispatcher     context, host, router_ids)
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_agentschedulers_db.py"", line 191, in list_active_sync_routers_on_active_l3_agent
TRACE oslo.messaging.rpc.dispatcher     active=True)
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_dvr_db.py"", line 306, in get_sync_data
TRACE oslo.messaging.rpc.dispatcher     DEVICE_OWNER_DVR_INTERFACE])
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_db.py"", line 1016, in _get_router_info_list
TRACE oslo.messaging.rpc.dispatcher     active=active)
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_db.py"", line 919, in _get_sync_routers
TRACE oslo.messaging.rpc.dispatcher     return self._build_routers_list(context, router_dicts, gw_ports)
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_dvr_db.py"", line 241, in _build_routers_list
TRACE oslo.messaging.rpc.dispatcher     rtr['gw_port'] = gw_ports[gw_port_id]
TRACE oslo.messaging.rpc.dispatcher KeyError: u'b132e9da-3ee2-47ac-9ed5-f04ca73c01d1'a
Link: http://logs.openstack.org/48/112948/3/check/check-tempest-dsvm-neutron-pg/503d619/logs/screen-q-svc.txt.gz?#_2014-08-11_07_19_33_893"
1633,1355502,neutron,0ac15ad5d7902ef30d495acf737c90ba9377566a,0,0,"“This patch clarifies that in order to use NSX distributed
    routers,”",NSX - add note in configuration files regarding di...,"In order to leverage distributed routing with the NSX plugin - the replication_mode parameter should be set to 'service'.
Otherwise the backend wil throw 409 errors resulting in 500 NSX errors.
This should be noted in the configuration files."
1634,1355565,neutron,5defcd0aa61b96933369a545040e57d9665add7a,0,0,"cleanup. “As a minor cleanup, we should remove the argument in the interests”",config argument to config.setup_logging is unused,"In neutron/common/config.py, the argument to setup_logging is unused:
  def setup_logging(conf):
     product_name = ""neutron""
     logging.setup(product_name)
     LOG.info(_(""Logging enabled!""))
As a minor cleanup, we should remove the argument in the interests of simpler code and avoiding confusion."
1635,1355622,cinder,49b54af36a2aacd3d27330636f61d6f996016d65,1,1,,solidfire api commands not encoded,"Lines 132 & 133 of cinder/cinder/volume/drivers/solidfire.py are currently the following:
payload = json.dumps(command, ensure_ascii=False)
payload.encode('utf-8')
The string.encode() method used on line 133 returns an encoded string, but does not modify the payload string itself. Because of this, payload is never actually encoded to UTF-8. A suggested fix might be to modify the two lines to be the following:
payload = json.dumps(command, ensure_ascii=False).encode('utf-8')"
1636,1355759,neutron,0125cf7815acb677b619905e2d9d258a9bae1c48,1,1,,L2populationRpcCallBackTunnelMixin get_agent_ports...,"L2populationRpcCallBackTunnelMixin get_agent_ports yields (None, {}) for unknown networks.
it's useless for consumers."
1637,1355777,nova,3d4e421204145bec16117ef3b9d5053ae27e836b,0,0,“Current git version of nova does not fully support ipv6 nameservers despite being able to set them during subnet creation.”,support for ipv6 nameservers,"Current git version of nova does not fully support ipv6 nameservers despite being able to set them during subnet creation.
This patch adds this support in nova (git) and its interfaces.template. It is currently deployed and used in our infrastructure based on icehouse (Nova 2.17.0)."
1638,1355882,nova,d449b5d8556e67be08e016c94f9b1c523a69ce7e,1,0,API change “Commit e00bdd7aa8c1ac9f1ae5057eb2f774f34a631845 change get_floating_ip_pools in a way that it now return a list of names”,get_floating_ip_pools for neutron v2 API inconsist...,"Commit e00bdd7aa8c1ac9f1ae5057eb2f774f34a631845 change get_floating_ip_pools in a way that it now return a list of names rather than a list whose elements are in the form {'name': 'pool_name'}.
The implementation of this method in nova.network.neutron_v2.api has not been adjusted thus causing tempest.api.compute.floating_ips.test_list_floating_ips.FloatingIPDetailsTestJSON to always fail with neutron
The fix is straightforward."
1639,1355922,nova,b21d3b6402fe10616fa07df3c0fa6154f46f4b21,1,1,,instance fault not created when boot process fails...,"If the build process makes it to build_and_run_instance in the compute manager no instance faults are recorded for failures after that point.  The instance will be set to an ERROR state appropriately, but no information is stored to return to the user."
1640,1355929,nova,02fa15b4caaa414930b6ddf6a3a9fe8019751c26,0,0,Bug in test,test_postgresql_opportunistically fails in stable/...,"http://logs.openstack.org/22/112422/1/check/gate-nova-python26/621e0ae/console.html
This is probably a latent bug in the nova unit tests for postgresql in stable/havana, or it's due to slow nodes for the py26 jobs.
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRVJST1I6ICBzb3VyY2UgZGF0YWJhc2UgXFxcInRlbXBsYXRlMVxcXCIgaXMgYmVpbmcgYWNjZXNzZWQgYnkgb3RoZXIgdXNlcnNcIiBBTkQgdGFnczpcImNvbnNvbGVcIiBBTkQgYnVpbGRfYnJhbmNoOlwic3RhYmxlL2hhdmFuYVwiIEFORCBidWlsZF9uYW1lOlwiZ2F0ZS1ub3ZhLXB5dGhvbjI2XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDc4NjA5ODg1MjMsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0=
3 hits in 7 days, check queue only but multiple changes and all failures."
1641,1356051,nova,6c085e1d6652df013ec176d46ffb234065373193,1,1,,Cannot load 'instance' in the base class - problem...,"I tried the following on VMware using the VMwareVCDriver with nova-network:
1. Create an instance
2. Create a floating IP: $ nova floating-ip-create
3. Associate a floating IP with the instance: $ nova floating-ip-associate test1 10.131.254.249
4. Attempt a list of the floating IPs:
$ nova floating-ip-list
ERROR (ClientException): The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-dcb17077-c670-4e2a-8a34-715a8afc5f33)
It failed and printed out the following messages in n-api logs:
2014-08-12 13:54:29.578 ERROR nova.api.openstack [req-86d8f466-cfae-42ac-8340-9eac36d6fc71 demo demo] Caught error: Cannot load 'instance' in the base class
2014-08-12 13:54:29.578 TRACE nova.api.openstack Traceback (most recent call last):
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/__init__.py"", line 124, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return req.get_response(self.application)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-08-12 13:54:29.578 TRACE nova.api.openstack     application, catch_exc_info=False)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-08-12 13:54:29.578 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/keystonemiddleware/auth_token.py"", line 565, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return self._app(env, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     response = self.app(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 908, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     content_type, body, accept)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 974, in _process_stack
2014-08-12 13:54:29.578 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 1058, in dispatch
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return method(req=request, **action_args)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips.py"", line 146, in index
2014-08-12 13:54:29.578 TRACE nova.api.openstack     self._normalize_ip(floating_ip)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips.py"", line 117, in _normalize_ip
2014-08-12 13:54:29.578 TRACE nova.api.openstack     floating_ip['instance'] = fixed_ip['instance']
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/objects/base.py"", line 447, in __getitem__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return getattr(self, name)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/objects/base.py"", line 67, in getter
2014-08-12 13:54:29.578 TRACE nova.api.openstack     self.obj_load_attr(name)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/objects/base.py"", line 375, in obj_load_attr
2014-08-12 13:54:29.578 TRACE nova.api.openstack     _(""Cannot load '%s' in the base class"") % attrname)
2014-08-12 13:54:29.578 TRACE nova.api.openstack NotImplementedError: Cannot load 'instance' in the base class
2014-08-12 13:54:29.579 INFO nova.api.openstack [req-86d8f466-cfae-42ac-8340-9eac36d6fc71 demo demo] http://10.131.179.211:8774/v2/875c3f62ef75400487e4a68679f8e239/os-floating-ips returned with HTTP 500
2014-08-12 13:54:29.580 DEBUG nova.api.openstack.wsgi [req-86d8f466-cfae-42ac-8340-9eac36d6fc71 demo demo] Returning 500 to user: The server has either erred or is incapable of performing the requested operation. from (pid=12246) __call__ /opt/stack/nova/nova/api/openstack/wsgi.py:1200"
1642,1356120,neutron,b025ccff2c7320caecfc005cfcbc3f4dfadfa505,1,1,,PortNotFound in update_device_up for DVR,"An example of a failure has been observed here:
http://logs.openstack.org/80/113580/3/experimental/check-tempest-dsvm-neutron-dvr/a0e0c32/logs/screen-q-svc.txt.gz?level=TRACE#_2014-08-13_00_13_00_674
More triaging needed but I suspect this is caused by interleaved create/delete requests of router resources."
1643,1356227,neutron,65de01dcb1c49271ad1e5067d8dfd206788086d4,1,1,,TestLoadBalancerBasic fails and leads to consequen...,"Logstash query:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOnRlbXBlc3Quc2NlbmFyaW8udGVzdF9sb2FkX2JhbGFuY2VyX2Jhc2ljLlRlc3RMb2FkQmFsYW5jZXJCYXNpYy50ZXN0X2xvYWRfYmFsYW5jZXJfYmFzaWMgQU5EIG1lc3NhZ2U6RkFJTEVEIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA3OTE1MTA0MTQwfQ=="
1645,1356464,neutron,1deb787c15a0f24c6c079c5e5fe122dc54188cdf,1,1,,"Assigning a FIP to a VIP port does not work, if th...","With DVR routers for FIP to work, there has to be a FIP Namespace that should be created and an IR Namespace.
But when a LBaaS VIP port is created on a Subnet that is part of the DVR router, the IR Namespace is not created for the VIP port since that port is not a Compute Port.
There are some corner cases here, when there are VMs in a node then the IR's for that subnet is already created and so the FIP namespace also will be created.
The issue will only be seen if the compute and the VIP ports are separated apart."
1646,1356552,nova,671aa9f8b7ca5274696f83bde0d4822ee431b837,1,1,,Bug #1356552 “Live migration,"When live-migrating an instance that has a Cinder volume (stored on NFS) attached, the operation fails if the volume size is bigger than the space left on the destination node. This should not happen, since this volume does not have to be migrated. Here is how to reproduce the bug on a cluster with one control node and two compute nodes, using the NFS backend of Cinder.
$ nova boot --flavor m1.tiny --image 173241e-babb-45c7-a35f-b9b62e8ced78 test_vm
...
$ nova volume-create --display-name test_volume 100
...
| id                  | 6b9e1d03-3f53-4454-add9-a8c32d82c7e6 |
...
$ nova volume-attach test_vm  6b9e1d03-3f53-4454-add9-a8c32d82c7e6 auto
...
$ nova show test_vm | grep OS-EXT-SRV-ATTR:host
| OS-EXT-SRV-ATTR:host                 | t1-cpunode0                                                |
$ nova service-list | grep nova-compute
| nova-compute     | t1-cpunode0 | nova     | enabled | up    | 2014-08-13T19:14:40.000000 | -               |
| nova-compute     | t1-cpunode1 | nova     | enabled | up    | 2014-08-13T19:14:41.000000 | -               |
Now, let's say I want to live-migrate test_vm to t1-cpunode1:
$ nova live-migration --block-migrate test_vm t1-cpunode1
ERROR: Migration pre-check error: Unable to migrate a0d9c991-7931-4710-8684-282b1df4cca6: Disk of instance is too large(available on destination host:46170898432 < need:108447924224) (HTTP 400) (Request-ID: req-b4f00867-df51-44be-8f97-577be385d536)
In nova/virt/libvirt/driver.py, _assert_dest_node_has_enough_disk() calls get_instance_disk_info(), which in turn, calls _get_instance_disk_info(). In this method, we see that volume devices are not taken into account when computing the amount of space needed to migrate an instance:
...
            if disk_type != 'file':
                LOG.debug('skipping %s since it looks like volume', path)
                continue
            if target in volume_devices:
                LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
                          'volume', {'path': path, 'target': target})
                continue
...
But for some reason, we never get into these conditions.
If we ssh the compute where the instance currently lies, we can get more information about it:
$ virsh dumpxml 11
...
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/nova/mnt/84751739e625d0ea9609a65dd9c0a6f1/volume-6b9e1d03-3f53-4454-add9-a8c32d82c7e6'/>
      <target dev='vdb' bus='virtio'/>
      <serial>6b9e1d03-3f53-4454-add9-a8c32d82c7e6</serial>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>
...
The disk type is ""file"", which might explain why this volume is not skipped in the code snippet shown above. When we use the default Cinder backend, we get something such as:
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/dev/disk/by-path/ip-192.168.200.250:3260-iscsi-iqn.2010-10.org.openstack:volume-47ecc6a6-8af9-4011-a53f-14a71d14f50b-lun-1'/>
      <target dev='vdb' bus='virtio'/>
      <serial>47ecc6a6-8af9-4011-a53f-14a71d14f50b</serial>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'
function='0x0'/>
    </disk>
I think that the code in LibvirtNFSVolumeDriver.connect_volume() might be wrong: conf.source_type should be set to something else than ""file"" (and some other changes might be needed), but I must admit I'm not a libvirt expert.
Any thoughts ?"
1647,1356665,neutron,6d7b2e007cf477a7f212312bad4ce0cbe8032d1a,0,0,Bug in test,WSGI unittest fails if HTTP_PROXY if set,"urllib2.urlopen uses $http_proxy/$HTTP_PROXY environment variables by
default.  If set (and pointing to a remote host), then the WSGI tests
that spin up a local server and connect to it using
http://127.0.0.1:$port/ instead connect to $port *on the proxy*,
and (hopefully) fail.
We shouldn't follow HTTP_PROXY when trying to connect to test servers."
1648,1356679,neutron,0cc7444f7577e53f14068a2cb35431713d6d21a0,1,1,,Neutron is checking stricter policies than an oper...,"I'm trying to set a custom policy.json for Neutron based on new roles I have defined.
In this task, I changed the ""default"" policy from ""rule: admin_or_owner"" to ""rule:admin_only"". After that, a bunch of operations stopped working, including, for instance, a regular user deleting a network or a router of his/her own project. Even with the policy for ""delete_network"" unchanged -- rule:admin_or_owner --, only the admin could delete a network.
I put a print statement in neutron.openstack.common.policy.check method to investigate what was happening. On the following lines you can compare the debug message in the logs with the actual content of the ""rule"" parameter passed to ""check"".
- - -
DEBUG neutron.policy [...] Failed policy check for 'delete_network'
(((rule:delete_network and rule:delete_network:provider:physical_network) and rule:delete_network:provider:network_type) and rule:delete_network:provider:segmentation_id)
- - -
DEBUG neutron.policy [...] Failed policy check for 'delete_port'
(((((((rule:delete_port and rule:delete_port:binding:host_id) and rule:delete_port:allowed_address_pairs) and rule:delete_port:binding:vif_details) and rule:delete_port:binding:vif_ty
pe) and rule:delete_port:mac_address) and rule:delete_port:binding:profile) and rule:delete_port:fixed_ips)
- - -
DEBUG neutron.policy [...] Failed policy check for 'delete_router'
(rule:delete_router and rule:delete_router:distributed)
- - -
DEBUG neutron.policy [...] Failed policy check for 'update_subnet'
(rule:update_subnet and rule:update_subnet:shared)
- in this case, there is no ""update_subnet:shared"" rule, but there is a ""subnets:shared:write"" rule (which doesn't seem to be used).
- - -
These are the tests I've implemented that got broken after changing the default rule. The update tests simply try to rename the resource.
test_delete_network_of_own_project
test_delete_port_own_project
test_add_router_interface_to_router_of_own_project*
test_delete_router_of_own_project
test_remove_router_interface_from_router_of_own_project*
test_update_router_of_own_project
test_update_shared_subnet_of_own_project
* these tests got broken because of this bug: https://bugs.launchpad.net/neutron/+bug/1356678."
1649,1356687,nova,42d017c0d498aa4034032104f9cdd56300c866e0,1,0,"“commit 243879f5c51fc45f03491bcb78765945ddf76be8”, “The jsonutils rule was added in:
    I86ed6cd3316dd4da5e1b10b36a3ddba3739316d3”",hacking check for jsonutils produces pep8 tracebac...,"the new jsonutils hacking check produces a pep8 traceback because it returns a set (column offset and error text) instead of an iterable (as logical line checks, like this check, should).
commit 243879f5c51fc45f03491bcb78765945ddf76be8
Change-Id: I86ed6cd3316dd4da5e1b10b36a3ddba3739316d3
===== 8< ===== TEST CASE ===== 8< =====
$ echo 'foo = json.dumps(bar)' >nova/foobar.py
$ flake8 -vv nova/foobar.py
local configuration: in /home/dev/Desktop/nova-test
  ignore = E121,E122,E123,E124,E125,E126,E127,E128,E129,E131,E251,H405,H803,H904
  exclude = .venv,.git,.tox,dist,doc,*openstack/common*,*lib/python*,*egg,build,tools
checking nova/foobar.py
foo = json.dumps(bar)
Traceback (most recent call last):
  File ""/home/dev/Desktop/nova-test/.venv/bin/flake8"", line 9, in <module>
    load_entry_point('flake8==2.1.0', 'console_scripts', 'flake8')()
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/flake8/main.py"", line 32, in main
    report = flake8_style.check_files()
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/pep8.py"", line 1672, in check_files
    runner(path)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/flake8/engine.py"", line 73, in input_file
    return fchecker.check_all(expected=expected, line_offset=line_offset)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/pep8.py"", line 1436, in check_all
    self.check_logical()
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/pep8.py"", line 1338, in check_logical
    for offset, text in self.run_check(check, argument_names) or ():
TypeError: 'int' object is not iterable
===== 8< ===== TEST CASE ===== 8< =====
diff --git a/nova/hacking/checks.py b/nova/hacking/checks.py
index a1dd614..7fe7412 100644
--- a/nova/hacking/checks.py
+++ b/nova/hacking/checks.py
@@ -300,7 +300,7 @@ def use_jsonutils(logical_line, filename):
         for f in json_funcs:
             pos = logical_line.find('json.%s' % f)
             if pos != -1:
-                return (pos, msg % {'fun': f})
+                yield (pos, msg % {'fun': f})
 def factory(register):
===== 8< ===== PATCH ===== 8< =====
it's late, so tomorrow, if there hasn't been any activity on this, then i'll submit a patch for review."
1650,1356734,neutron,e74b85f854e248919299fd875066903eba0a37c9,1,1,“The recently added external_gateway method does not check the agent mode or host binding before adding an external gateway to a particular node/agent.”,SNAT name space is being hosted on all nodes thoug...,"SNAT name space is being hosted on all nodes though agent_mode is set as dvr
Here are the steps followed.
1. one service node with agent_mode=snat
2. Two compute nodes with agent_mode=dvr
3. Create an external network. Subnet within it.
4.  Create a private network & subnet(sub1) within it.
5 .Create a Distributed Router(DVR)
6. Set the  external network as gateway to the DVR.
7. Add the ptivate subnet  (sub1) as interface to the DVR.
8. Since the service node has the agent_mode as DVR-snat, the snat namespace will sit on this service node.  But the snat namespace is hosted on all 3 nodes(service node with agent_mode=dvr_snat & Compute nodes with agent_mode=dvr)
Expected behavior:
snat name space should sit only on the node with agent_mode=dvr_snat."
1651,1356794,cinder,f668b347ca181229fab4b89fda25f1aadfbfdef1,1,1,,Bug #1356794 “VMware,"Set vmware_volume_folder=/cinder-volumes in cinder.conf
> cinder create 1
fails with following error:
Unable to find suitable datastore for volume of size: 1 GB under host: (obj){
   value = ""host-361""
   _type = ""HostSystem""
 }. More details: Server raised fault: 'The name '/cinder-volumes' already exists"
1652,1356815,nova,a0a6017f9b58941a4f8e67300a5dc57e34aada35,1,1,“commit 243879f5c51fc45f03491bcb78765945ddf76be8 was bad”,Nova hacking check for jsonutils used invalid numb...,"It should have been 324 and not 324
commit 243879f5c51fc45f03491bcb78765945ddf76be8 was bad"
1653,1357048,neutron,9fc8b6c4b4d43df50b9bc15f4fdf274ad6dd711c,0,0,"Refactoring ""does not have anymore any restriction on transformation of centralized routers in distributed.”",Bug #1357048 “NSX,"The NSX backend from version 4.1 does not have anymore any restriction on transformation of centralized routers in distributed.
Version 3.x instead could not transform distributed routers into centralized, which is anyway consistent with the current DVR extension.
The current restriction specific for the NSX plugin must therefore be lifted."
1654,1357084,neutron,0d8911115e1b722da2f1e92f444e53b22223ee32,1,1,,IPv6 slaac is broken when subnet is less than /64,"SLAAC and DHCPv6 stateless work only with subnets with mask /64 and more (/63, /62) because EUI-64 calculated IP takes 8 octets.
If subnet mask is /65, /66, .., /128 SLAAC/DHCP stateless should be disabled.
API call for creating subnet with SLAAC/DHCP stateless and mask more than /64 should fail.
Example:
let's create net and subnet with mask /96:
$ neutron net-create 14
$ neutron subnet-create 14 --ipv6-ra-mode=slaac --ipv6-address-mode=slaac --ip-version=6 2003::/96
Created a new subnet:
...
| allocation_pools  | {""start"": ""2003::2"", ""end"": ""2003::ffff:fffe""} |
| cidr              | 2003::/96                                      |
....                                   |
| gateway_ip        | 2003::1                                        |
...
| ipv6_address_mode | slaac                                          |
| ipv6_ra_mode      | slaac                                          |
...
Let's create port in this network:
$  neutron port-create 14 --mac-address=11:22:33:44:55:66
Created a new port:
...
| fixed_ips             | {""subnet_id"": ""1bfe4522-3b71-4e74-bb80-44c853ff868d"", ""ip_address"": ""2003::1322:33ff:fe44:5566""} |
...
| mac_address           | 11:22:33:44:55:66                                                                                |
...
As you see port gets IP 2003::1322:33ff:fe44:5566 which is not from original network 2003::/96."
1655,1357102,neutron,b09c9950649c4c9fffa0b096eb21152cd84fe56f,1,0,"“However,
there is no longer a point to this protection because the DB lock is gone and”",Bug #1357102 “Big Switch,"The Big Switch consistency DB throws an exception if read_for_update() is called multiple times without closing the transaction in between. This was originally because there was a DB lock in place and a single thread could deadlock if it tried twice. However,
there is no longer a point to this protection because the DB lock is gone and certain response failures result in the DB being read twice (the second time for a retry).
2014-08-14 21:56:41.496 12939 ERROR neutron.plugins.ml2.managers [req-ee311173-b38a-481e-8900-d963c676b05f None] Mechanism driver 'bigswitch' failed in update_port_postcommit
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers Traceback (most recent call last):
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/managers.py"", line 168, in _call_on_drivers
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     getattr(driver.obj, method_name)(context)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/mech_bigswitch/driver.py"", line 91, in update_port_postcommit
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     port[""network""][""id""], port)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/servermanager.py"", line 555, in rest_update_port
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     self.rest_create_port(tenant_id, net_id, port)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/servermanager.py"", line 545, in rest_create_port
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     self.rest_action('PUT', resource, data, errstr)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/servermanager.py"", line 476, in rest_action
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     timeout)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     return f(*args, **kwargs)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/servermanager.py"", line 423, in rest_call
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     hash_handler=hash_handler)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/servermanager.py"", line 139, in rest_call
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     headers[HASH_MATCH_HEADER] = hash_handler.read_for_update()
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/db/consistency_db.py"", line 56, in read_for_update
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     raise MultipleReadForUpdateCalls()
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers MultipleReadForUpdateCalls: Only one read_for_update call may be made at a time.
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers"
1656,1357152,nova,21aeb80a83e157dbc2ba27f16a83c98a37b7067d,1,1,"""commit 374e9766c20c9f83dbd8139aa9d95a66b5da7295”",nova.utils.TIME_UNITS['Day'] is only 84400 sec,"Full day is 86400 sec
diff --git a/nova/utils.py b/nova/utils.py
index 65d99aa..0b9afe4 100644
--- a/nova/utils.py
+++ b/nova/utils.py
@@ -90,7 +90,7 @@ TIME_UNITS = {
     'SECOND': 1,
     'MINUTE': 60,
     'HOUR': 3600,
-    'DAY': 84400
+    'DAY': 86400
 }
Based on code from master branch 2014-08-14 HEAD commit 374e9766c20c9f83dbd8139aa9d95a66b5da7295"
1657,1357198,neutron,dd5c73450da908d95724e20eb3be09bc936cb551,1,1,“l2pop can send us entry removal without the corresponding addition.”,Bug #1357198 “ofagent,"del_arp_table_entry  crashes when it gets unknown ip.
l2pop sometimes send us fdb removal without the corresponding fdb add."
1658,1357236,neutron,185f09b7c46edcbb1d39d3b79993fa8c07dafc2c,0,0,Refactoring “Also removed custom dispatch”,Neutron creates oslo.messaging.Server object direc...,oslo.messaging provides a factory method to create a Server object. It should be used to create it. Additionaly Neutron uses custom RPCDispatcher to log incoming messages. This duplicates existing functionality in oslo.messaging.
1659,1357239,cinder,7ac4fc76f2d777fd4e0ec4245eaf989f99917be2,1,1,,Bug #1357239 “VMware,"Uploading a 2Gi volume to image service resulted in volume being stuck in uploading state for a long time.
As per the logs, the image service raised an internal server error. In this case, the volume should return back to available state.
Cinder volume logs:
2014-08-14 17:56:27.846 DEBUG glanceclient.common.http [-]
HTTP/1.1 500 Internal Server Error
date: Thu, 14 Aug 2014 12:26:27 GMT
content-length: 91
content-type: text/plain; charset=UTF-8
x-openstack-request-id: req-f741d884-6cdd-4304-b809-63469b12aaa8
500 Internal Server Error
Failed to upload image 3cc9f1f9-8749-4556-81f5-8a0c2bfa4f1f
 from (pid=35027) log_http_response /opt/stack/python-glanceclient/glanceclient/common/http.py:167
2014-08-14 17:56:27.846 DEBUG glanceclient.common.http [-] Request returned failure status: 500 from (pid=35027) _http_request /opt/stack/python-glanceclient/glanceclien
t/common/http.py:263
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/io_util.py"", line 112, in _inner
    data=self.input_file)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 320, in update
    _reraise_translated_image_exception(image_id)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 318, in update
    **image_meta)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 167, in call
    return getattr(client.images, method)(*args, **kwargs)
  File ""/opt/stack/python-glanceclient/glanceclient/v1/images.py"", line 332, in update
    'PUT', url, headers=hdrs, body=image_data)
  File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 328, in raw_request
    return self._http_request(url, method, **kwargs)
  File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 264, in _http_request
    raise exc.from_response(resp, body_str)
HTTPInternalServerError: 500 Internal Server Error
Failed to upload image 3cc9f1f9-8749-4556-81f5-8a0c2bfa4f1f
    (HTTP 500)
Removing descriptor: 10"
1660,1357261,cinder,7a72303aff86ba2b06c41e73b35989cddd884652,1,1,,Bug #1357261 “SolidFire,"For some reason, when Solidfire driver tried to accept_transfer(), the backend failed to find the volume (the volume is there, but that is another issue to dig into), then the driver puked:
2014-08-14 14:54:48.350 36969 ERROR cinder.volume.drivers.solidfire [req-cc45c0a8-4760-4890-bd2a-93498fadad7b 6a08f3e43965436fb028eda1005ec77b b14c5ae7dd504e6c8ab1f8f2e568fb46] Volume cbb95e17-f66e-4081-b3aa-521889ed436d, not found on SF Cluster.
2014-08-14 14:54:48.351 36969 ERROR cinder.openstack.common.rpc.amqp [req-cc45c0a8-4760-4890-bd2a-93498fadad7b 6a08f3e43965436fb028eda1005ec77b b14c5ae7dd504e6c8ab1f8f2e568fb46] Exception during message handling
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/volume/manager.py"", line 731, in accept_transfer
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp     new_project)
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/volume/drivers/solidfire.py"", line 750, in accept_transfer
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp     'volumeID': sf_vol['volumeID'],
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp TypeError: 'NoneType' object has no attribute '__getitem__'
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp
accept_transfer() should handle Volume Not Found like attach/detach_volume() do."
1661,1357372,nova,948ff4f3d0a159f1aed9fab65d205ede845b3eb9,1,1,“CVE-2014-8750“,[oss-security] [OSSA 2014-035] Nova VMware driver ...,"When spawning some instances,  nova VMware driver could have a race condition in VNC port allocation. Although the get_vnc_port function has a lock it not guarantee that the whole vnc port allocation process is locked, so another instance could receive the same port if it requests the VNC port before nova has finished the vnc port allocation to another VM.
If the instances with the same VNC port are allocated in same host it could lead to a improper access to the instance console.
Reproduce the problem: Launch  two or more instances at same time. In some cases one instance could execute the get_vnc_port and pick a port but before this instance has finished the _set_vnc_config another instance could execute get_vnc_port and pick the same port.
How often this occurs: unpredictable."
1662,1357379,neutron,74d10939903984d5f06c1749a8707fa3257e44ff,1,1,“CVE-2014-6414“,[OSSA 2014-031] policy admin_only rules not enforc...,"If a non-admin user tries to update an attribute, which should be updated only by admin, from a non-default value to default,  the update is successfully performed and PolicyNotAuthorized exception is not raised.
The reason is that when a rule to match for a given action is built there is a verification that each attribute in a body of the resource is present and has a non-default value. Thus, if we try to change some attribute's value to default, it is not considered to be explicitly set and a corresponding rule is not enforced."
1663,1357432,cinder,7e95b05b937e632ae8aee82cfa8f6f31835e6316,1,1,,Accept Transfer race condition,"In accept_transfer() workflow (https://github.com/openstack/cinder/blob/master/cinder/transfer/api.py#L193)
         try:
             # Transfer ownership of the volume now, must use an elevated
             # context.
             self.volume_api.accept_transfer(context,
                                             vol_ref,
                                             context.user_id,
                                             context.project_id)
             self.db.transfer_accept(context.elevated(),
                                     transfer_id,
                                     context.user_id,
                                     context.project_id)
   self.volume_api.accept_transfer() sends out a RPC request (cast) to volume manager and volume driver may do something in backend (e.g. modify account for the volume), but since it's a unblocking RPC cast, this call returns pretty fast and the volume record in DB will be updated.   There can be cases where DB update finishes even before volume manager / driver does their job.  Unfortunately some driver(s) relies on original DB record to do their stuff (SolidFire for example).  Such situation will turn volume into inconsistent state (between backend and cinder DB) and become unusable.
   We may either change accept_transfer() in volume RPC API from CAST to blocking CALL, or we can pass enough original volume state to volume manger so that they don't have to rely on unreliable DB state."
1664,1357462,glance,4502759243c0f363545f4236b41abaf8bcf9a37b,1,0,“Since the `_upload` method loads the store based on the scheme”,glance cannot find store for scheme vmware_datasto...,"I have python-glance-2014.1.2-1.el7ost.noarch
when configuring
default_store=vmware_datastore
known_stores = glance.store.vmware_datastore.Store
vmware_server_host = 10.34.69.76
vmware_server_username=root
vmware_server_password=qum5net
vmware_datacenter_path=""New Datacenter""
vmware_datastore_name=shared
glance-api doesn't seem to come up at all.
glance image-list
Error communicating with http://172.16.40.9:9292 [Errno 111] Connection refused
there seems to be nothing interesing in the logs. After changing to the
  default_store=file
  glance image-create --disk-format vmdk --container-format bare     --copy-from 'http://str-02.rhev/OpenStack/cirros-0.3.1-x86_64-disk.vmdk'     --name cirros-0.3.1-x86_64-disk.vmdk --is-public true     --property vmware_disktype=""sparse""     --property vmware_adaptertype=""ide""     --property vmware_ostype=""ubuntu64Guest"" --name prdel --store vmware_datastore
or
  glance image-create --disk-format vmdk --container-format bare     --file 'cirros-0.3.1-x86_64-disk.vmdk'     --name cirros-0.3.1-x86_64-disk.vmdk --is-public true     --property vmware_disktype=""sparse""     --property vmware_adaptertype=""ide""     --property vmware_ostype=""ubuntu64Guest"" --name prdel --store vmware_datastore
the image remains in queued state
I can see log lines
2014-08-15 12:38:55.885 24732 DEBUG glance.store [-] Registering store <class 'glance.store.vmware_datastore.Store'> with schemes ('vsphere',) create_stores /usr/lib/python2.7/site-packages/glance/store/__init__.py:208
2014-08-15 12:39:54.119 24764 DEBUG glance.api.v1.images [-] Store for scheme vmware_datastore not found get_store_or_400 /usr/lib/python2.7/site-packages/glance/api/v1/images.py:1057
2014-08-15 12:43:31.408 24764 DEBUG glance.api.v1.images [eac2ff8d-d55a-4e2c-8006-95beef8a0d7b caffabe3f56e4e5cb5cbeb040224fe69 77e18ad8a31e4de2ab26f52fb15b3cc1 - - -] Store for scheme vmware_datastore not found get_store_or_400 /usr/lib/python2.7/site-packages/glance/api/v1/images.py:1057
so it looks like there is inconsistency on the scheme that should be used. After hardcoding
  STORE_SCHEME = 'vmware_datastore'
in the
  /usr/lib/python2.7/site-packages/glance/store/vmware_datastore.py
the behaviour changed, but did not improve very much:
  glance image-create --disk-format vmdk --container-format bare     --file 'cirros-0.3.1-x86_64-disk.vmdk'     --name cirros-0.3.1-x86_64-disk.vmdk --is-public true     --property vmware_disktype=""sparse""     --property vmware_adaptertype=""ide""     --property vmware_ostype=""ubuntu64Guest"" --name prdel --store vmware_datastore
400 Bad Request
Store for image_id not found: 7edc22ae-f229-4f21-8f7d-fa19a03410be
    (HTTP 400)"
1665,1357972,nova,ea19fb10c5e09ff5df383607654ab9dc2791ec21,0,0,Bug in test,boot from volume fails on Hyper-V if boot device i...,"The Tempest test ""tempest.scenario.test_volume_boot_pattern.TestVolumeBootPatternV2.test_volume_boot_pattern"" fails on Hyper-V.
The cause is related to the fact that the root device is ""sda"" and not ""vda""."
1666,1358196,neutron,eca9069be3d730c788e433edb291123eede07839,1,1,,IpNetnsCommand will refuse to work without root_he...,"The network namespace is not mandatory, but that makes
root_wrap non mandatory too, because you could
want to start non-privileged processes outside a
namespace through the same API, covering both
the namespace & non-namespace needs."
1667,1358206,neutron,44eff5daad81966f6530d84ca3683b2f9c58debf,1,1,,ovsdb_monitor.SimpleInterfaceMonitor throws eventl...,"This is found during functional testing, when .start() is called with
block=True during sightly high load.
This suggest the default timeout needs to be rised to make this module
work in all situations.
https://review.openstack.org/#/c/112798/14/neutron/agent/linux/ovsdb_monitor.py (I will extract patch from here)"
1668,1358380,nova,f31ac7711bacb1408b17759426f3407937ba4d79,1,0,“This goes way back to grizzly and commit 718a3f057cee0b1163c40fbcbedda29bd2ef9dfe”,rebuild API doesn't handle OnsetFile*LimitExceeded...,"I noticed this while reviewing https://review.openstack.org/#/c/102103/ for bug 1298131, the 3 OnsetFile*LimitExceeded exceptions from nova.compute.api.API._check_injected_file_quota are not handled in the os-compute rebuild APIs (v2 or v3), and I'm not even seeing specific unit testing for those exceptions in the _check_injected_file_quota method."
1670,1358552,nova,ce2031afb2e42c756c065f423126ffd1a61b820a,1,1,,Fail to remove logical volume,"The logical volume can not be removed when delete VM error. I look at the code, found that  parameter is a list in the libvirt's lvm, but in imagebackend, parameters passed is a string.
in the Libvirt's LVM
def remove_volumes(paths):  ## #the path is list
    """"""Remove one or more logical volume.""""""
    errors = []
    for path in paths:
        clear_volume(path)
        lvremove = ('lvremove', '-f', path)
        try:
            utils.execute(*lvremove, attempts=3, run_as_root=True)
        except processutils.ProcessExecutionError as exp:
            errors.append(str(exp))
    if errors:
        raise exception.VolumesNotRemoved(reason=(', ').join(errors))
in the imagebackend's LVM
 @contextlib.contextmanager
    def remove_volume_on_error(self, path):
        try:
            yield
        except Exception:
            with excutils.save_and_reraise_exception():
                lvm.remove_volumes(path)  ### the path is string"
1671,1358554,neutron,78059968e212d537c813f398da7f76714380bf4e,0,0,Bug in test,test_volume_boot_pattern fails with DVR routers,"This happens on master and appears to be a regression of https://review.openstack.org/#/c/112465/
Steps to reproduce:
- Deploy devstack with DVR *ON*
- Run tempest test test_volume_boot_pattern (more details available in [1])
- Watch the test fail
Once change 112465 is reverted, the test will be successful.
This does not imply that the change is at fault, instead, that another failure mode has been unveiled.
In fact, prior to change 112465, call [2] was made with the wrong agent id, which caused the delete_port operation to abort altogether; this is wrong and addressed in 112465. However, the boot from volume test, and its sequence of operations revealed that the clean-up logic is not working as it should.
[1] - https://github.com/openstack/tempest/blob/master/tempest/scenario/test_volume_boot_pattern.py#L31
[2] - https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L1029"
1672,1358636,neutron,89e76a8afae19c5f66d532538607aedaac59722a,0,0,Bug in test,test_add_list_remove_router_on_l3_agent fails with...,"Test tempest.api.network.admin.test_l3_agent_scheduler.L3AgentSchedulerTestXML.test_add_list_remove_router_on_l3_agent fails for job gate-tempest-dsvm-neutron-full with traceback:
ft335.1: tempest.api.network.admin.test_l3_agent_scheduler.L3AgentSchedulerTestXML.test_add_list_remove_router_on_l3_agent[gate,smoke]_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
2014-08-18 15:31:39,865 535 INFO     [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:test_add_list_remove_router_on_l3_agent): 200 POST http://127.0.0.1:5000/v2.0/tokens
2014-08-18 15:31:40,148 535 INFO     [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:test_add_list_remove_router_on_l3_agent): 201 POST http://127.0.0.1:9696/v2.0/routers 0.282s
2014-08-18 15:31:40,259 535 INFO     [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:test_add_list_remove_router_on_l3_agent): 409 POST http://127.0.0.1:9696/v2.0/agents/47dd83c6-f92d-40d9-8601-5a38b6b9eda0/l3-routers 0.109s
2014-08-18 15:31:40,714 535 INFO     [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:_run_cleanups): 204 DELETE http://127.0.0.1:9696/v2.0/routers/fd7d082c-71db-4fa0-bd0c-0b31acef9b1a 0.450s
}}}
Traceback (most recent call last):
  File ""tempest/api/network/admin/test_l3_agent_scheduler.py"", line 66, in test_add_list_remove_router_on_l3_agent
    self.agent['id'], router['router']['id'])
  File ""tempest/services/network/xml/network_client.py"", line 218, in add_router_to_l3_agent
    resp, body = self.post(uri, str(common.Document(router)))
  File ""tempest/services/network/network_client_base.py"", line 73, in post
    return self.rest_client.post(uri, body, headers)
  File ""tempest/common/rest_client.py"", line 219, in post
    return self.request('POST', url, extra_headers, headers, body)
  File ""tempest/common/rest_client.py"", line 431, in request
    resp, resp_body)
  File ""tempest/common/rest_client.py"", line 485, in _error_checker
    raise exceptions.Conflict(resp_body)
Conflict: An object with that identifier already exists
Details: {'message': 'The router fd7d082c-71db-4fa0-bd0c-0b31acef9b1a has been already hosted by the L3 Agent 47dd83c6-f92d-40d9-8601-5a38b6b9eda0.', 'type': 'RouterHostedByL3Agent', 'detail': {}}
There is log of test results:  http://logs.openstack.org/43/97543/10/gate/gate-tempest-dsvm-neutron-full/8f09c74/logs/testr_results.html.gz"
1673,1358668,neutron,295efa00081c770a57a5541652d9d3ffd5c1074d,1,1,,Bug #1358668 “Big Switch,"If get_ports is called in the Big Switch plugin without 'id' being one of the included fields, _extend_port_dict_binding will fail with the following error.
Traceback (most recent call last):
  File ""neutron/tests/unit/bigswitch/test_restproxy_plugin.py"", line 87, in test_get_ports_no_id
    context.get_admin_context(), fields=['name'])
  File ""neutron/plugins/bigswitch/plugin.py"", line 715, in get_ports
    self._extend_port_dict_binding(context, port)
  File ""neutron/plugins/bigswitch/plugin.py"", line 361, in _extend_port_dict_binding
    hostid = porttracker_db.get_port_hostid(context, port['id'])
KeyError: 'id'"
1674,1358702,nova,d88b4cf31dbff942cf529e63ee5b09356970ed50,0,0,Bug in test,Hyper-V unit test fails on Windows due to path sep...,"The following test fails due to mismatching in the path separator.
FAIL: nova.tests.virt.hyperv.test_pathutils.PathUtilsTestCase.test_lookup_config_drive_path
----------------------------------------------------------------------
_StringException: Empty attachments:
  pythonlogging:''
Traceback (most recent call last):
  File ""C:\OpenStack\nova\nova\tests\virt\hyperv\test_pathutils.py"", line 48, i
 test_lookup_configdrive_path
    format_ext)
  File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\
ite-packages\testtools\testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\
ite-packages\testtools\testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = 'C:/fake_instance_dir\\configdrive.vhd'
actual    = 'C:/fake_instance_dir/configdrive.vhd'"
1675,1358818,nova,e9a5463bf71cb2f2e7185dea3ed8ee8b973a3208,1,0,"“The API change guidelines”, “In icehouse, we were able to store ints, floats in extra_specs, with I195bd5d45a896e9b26dd81dab1e49c9f939b4805 we forced the value(s) to be just strings.”",extra_specs string check breaks backward compatibi...,"We've found that while with Icehouse we were able to specify extra_specs values as ints or floats, in Juno the command fails unless we make these values strings by quoting them. This breaks backward compatibility.
compare Icehouse:
curl -k -i -X POST http://127.0.0.1:8774/v2/982607a6a1134514abac252fc25384ad/flavors/1/os-extra_specs -H ""X-Auth-Token: *****"" -H ""Accept: application/json"" -H ""Content-Type: application/json"" -d '{""extra_specs"":{""powervm:proc_units"":""0.2"",""powervm:processor_compatibility"":""default"",""powervm:min_proc_units"":""0.1"",""powervm:max_proc_units"":""0.5"",""powervm:min_vcpu"":1,""powervm:max_vcpu"":5,""powervm:min_mem"":1024,""powervm:max_mem"":4096,""powervm:availability_priority"":127,""powervm:dedicated_proc"":""false"",""powervm:uncapped"":""true"",""powervm:shared_weight"":128}}'; echo
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 385
X-Compute-Request-Id: req-9132922d-c703-4573-9822-9ca7a6bf7b0d
Date: Thu, 14 Aug 2014 18:25:02 GMT
{""extra_specs"": {""powervm:processor_compatibility"": ""default"", ""powervm:max_proc_units"": ""0.5"", ""powervm:shared_weight"": 128, ""powervm:min_mem"": 1024, ""powervm:max_mem"": 4096, ""powervm:uncapped"": ""true"", ""powervm:proc_units"": ""0.2"", ""powervm:dedicated_proc"": ""false"", ""powervm:max_vcpu"": 5, ""powervm:availability_priority"": 127, ""powervm:min_proc_units"": ""0.1"", ""powervm:min_vcpu"": 1}}
to Juno:
curl -k -i -X POST http://127.0.0.1:8774/v2/be2ffade1e0b4bed83619e00482317d1/flavors/1/os-extra_specs -H ""X-Auth-Token: *****"" -H ""Accept: application/json"" -H ""Content-Type: application/json"" -d '{""extra_specs"":{""powervm:proc_units"":""0.2"",""powervm:processor_compatibility"":""default"",""powervm:min_proc_units"":""0.1"",""powervm:max_proc_units"":""0.5"",""powervm:min_vcpu"":1,""powervm:max_vcpu"":5,""powervm:min_mem"":1024,""powervm:max_mem"":4096,""powervm:availability_priority"":127,""powervm:dedicated_proc"":""false"",""powervm:uncapped"":""true"",""powervm:shared_weight"":128}}'; echo
HTTP/1.1 400 Bad Request
Content-Length: 88
Content-Type: application/json; charset=UTF-8
Date: Thu, 14 Aug 2014 18:25:46 GMT
{""badRequest"": {""message"": ""extra_specs value is not a string or unicode"", ""code"": 400}}
if I modify the data sent so that everything is a string, it will work for Juno:
curl -k -i -X POST http://127.0.0.1:8774/v2/be2ffade1e0b4bed83619e00482317d1/flavors/1/os-extra_specs -H ""X-Auth-Token: *****"" -H ""Accept: application/json"" -H ""Content-Type: application/json"" -d '{""extra_specs"":{""powervm:proc_units"":""0.2"",""powervm:processor_compatibility"":""default"",""powervm:min_proc_units"":""0.1"",""powervm:max_proc_units"":""0.5"",""powervm:min_vcpu"":""1"",""powervm:max_vcpu"":""5"",""powervm:min_mem"":""1024"",""powervm:max_mem"":""4096"",""powervm:availability_priority"":""127"",""powervm:dedicated_proc"":""false"",""powervm:uncapped"":""true"",""powervm:shared_weight"":""128""}}'; echo
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 397
Date: Thu, 14 Aug 2014 18:26:27 GMT
{""extra_specs"": {""powervm:processor_compatibility"": ""default"", ""powervm:max_proc_units"": ""0.5"", ""powervm:shared_weight"": ""128"", ""powervm:min_mem"": ""1024"", ""powervm:max_mem"": ""4096"", ""powervm:uncapped"": ""true"", ""powervm:proc_units"": ""0.2"", ""powervm:dedicated_proc"": ""false"", ""powervm:max_vcpu"": ""5"", ""powervm:availability_priority"": ""127"", ""powervm:min_proc_units"": ""0.1"", ""powervm:min_vcpu"": ""1""}}
The API change guidelines (https://wiki.openstack.org/wiki/APIChangeGuidelines) describe as ""generally not acceptable"": ""A change such that a request which was successful before now results in an error response (unless the success reported previously was hiding an existing error condition)"". That is exactly what this is."
1676,1358881,nova,630a7f369a76eaf7f942d8989a30f5dc7b09327e,0,0,Bug in test,jjsonschema 2.3.0 -> 2.4.0 upgrade breaking nova.t...,"the following two failures appeared after upgrading jsonschema to 2.4.0.  downgrading to 2.3.0 returned the tests to passing.
======================================================================
FAIL: nova.tests.test_api_validation.TcpUdpPortTestCase.test_validate_tcp_udp_port_fails
----------------------------------------------------------------------
Traceback (most recent call last):
_StringException: Empty attachments:
  pythonlogging:''
  stderr
  stdout
Traceback (most recent call last):
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 602, in test_validate_tcp_udp_port_fails
    expected_detail=detail)
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 31, in check_validation_error
    self.assertEqual(ex.kwargs, expected_kwargs)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = {'code': 400,
 'detail': u'Invalid input for field/attribute foo. Value: 65536. 65536 is greater than the maximum of 65535'}
actual    = {'code': 400,
 'detail': 'Invalid input for field/attribute foo. Value: 65536. 65536.0 is greater than the maximum of 65535'}
======================================================================
FAIL: nova.tests.test_api_validation.IntegerRangeTestCase.test_validate_integer_range_fails
----------------------------------------------------------------------
Traceback (most recent call last):
_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
INFO [migrate.versioning.api] 215 -> 216...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 216 -> 217...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 217 -> 218...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 218 -> 219...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 219 -> 220...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 220 -> 221...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 221 -> 222...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 222 -> 223...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 223 -> 224...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 224 -> 225...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 225 -> 226...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 226 -> 227...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 227 -> 228...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 228 -> 229...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 229 -> 230...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 230 -> 231...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 231 -> 232...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 232 -> 233...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 233 -> 234...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 234 -> 235...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 235 -> 236...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 236 -> 237...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 237 -> 238...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 238 -> 239...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 239 -> 240...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 240 -> 241...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 241 -> 242...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 242 -> 243...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 243 -> 244...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 244 -> 245...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 245 -> 246...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 246 -> 247...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 247 -> 248...
INFO [248_add_expire_reservations_index] Skipped adding reservations_deleted_expire_idx because an equivalent index already exists.
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 248 -> 249...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 249 -> 250...
INFO [migrate.versioning.api] done
}}}
Traceback (most recent call last):
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 361, in test_validate_integer_range_fails
    expected_detail=detail)
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 31, in check_validation_error
    self.assertEqual(ex.kwargs, expected_kwargs)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = {'code': 400,
 'detail': u'Invalid input for field/attribute foo. Value: 0. 0 is less than the minimum of 1'}
actual    = {'code': 400,
 'detail': 'Invalid input for field/attribute foo. Value: 0. 0.0 is less than the minimum of 1'}"
1677,1359002,nova,eaf9522c842614dc83d3fd541bfdc1159416d438,1,1,,comments misspelled in type_filter,"the comments of nova.scheduler.filters.type_filter.py:
class TypeAffinityFilter(filters.BaseHostFilter):
...
def host_passes(self, host_state, filter_properties):
        """"""Dynamically limits hosts to one instance type
        Return False if host has any instance types other then the requested
        type. Return True if all instance types match or if host is empty.
        """"""
...
'other then' in the next-to-last line should be 'other than'"
1678,1359113,neutron,65b16f50bdd6544244d80c0f07653f4f478d3244,0,0,Typos,Typos in l3_router_plugin's comments,"In the services.l3_router.L3RouterPlugin.create_floatingip(), we have some comments (docstring) to help us to know this function, but there are some nits inside them:
  :param floatingip: data fo the floating IP being created
  :returns: A floating IP object on success
  AS the l3 router plugin aysnchrounously creates floating IPs
  leveraging tehe l3 agent, the initial status fro the floating
  IP object will be DOWN.
data fo the floating IP --> data of the floating IP
aysnchrounously creates --> asynchronously creates
status fro the floating IP --> status for the floating IP"
1679,1359138,nova,e1f8664c9fa83f77f5bb763ffcc3157905ed954c,1,1,,Bug #1359138 “[OSSA 2014-037] vmware,"For vmware vcenter driver,  resize a VM, during resizing , at the same time, delete the vm, the VM-orig  can not be deleted in ESXi host. So makes VM leaks."
1680,1359427,nova,9f1f9a8b6fadbba2eb6733e0f13b727f14a52a3f,1,1,,Potential codec errors during rescheduling operati...,"In the compute manager's 'retry' logic (i.e., handling RescheduledException) in _build_and_run_instance, it has a catch-all case for all exceptions and sets reason=str(e).  This works well in most cases, but for cases in which lower level code may generate locale-specific messages, it's possible to see the ""UnicodeEncodeError: 'ascii' codec can't encode character..."" error, which ultimately masks the message in the compute logs, etc.
It also means the instance won't be rescheduled as it fails with an error similar to...
WARNING nova.compute.manager [req-7fe662b1-c947-4303-b43f-114fbdafe875 None] [instance: 333474eb-dd07-4e05-b920-787da0fd5b32] Unexpected build failure, not rescheduling build.
(i.e., as a result of the UnicodeEncodeError previously mentioned)
The simple solution is to use six.text_type(e) instead of str(e)."
1681,1360022,nova,acf881a69b672936fec83900fc803c65c669f2a9,1,1,,min_ram and min_disk is ignored when boot from vol...,"When boot from volume and the volume is created from a image,  the original image's min_ram, min_disk attributes are ignored, this is not good.
The reason of this failure is because the _check_requested_image() in compute/api.py ignore if the source if a volume."
1682,1360104,neutron,fa0abeb043c2f66c537e43a8ea7181f648c71d33,1,1,,bind_router fails to log proper error message,"auto_schedule_routers can call bind_router within a transaction.  If a DBDuplicateEntry exception happens in bind_router in such a case, the access to chosen_agent.id causes another exception as the outer transaction is already aborted, resulting in a useless debug log."
1683,1360113,nova,cad9e77091dfa896ad59233341e8947eca13f66b,0,0,Bug in test,V2 hypervisors Unit Test tests hypervisors API as ...,"hypervisors API are Admin API and unit tests should test those accordingly. But All the V2 hypervisors Unit tests (https://github.com/openstack/nova/blob/master/nova/tests/api/openstack/compute/contrib/test_hypervisors.py) tests those as a  non Admin API.
Issue is in fake_policy.py where V2 hypervisors API is not marked as Admin role unlike V3. https://github.com/openstack/nova/blob/master/nova/tests/fake_policy.py#L221"
1684,1360119,nova,2418a9dce86280fbf269f68f7d7c9b739f662ef1,1,0,"“FYI, Nova has always been broken in this regard, but libvirt was buggy and did not enforce uuid,name stability for nwfilters until release 1.2.7”",Nova tries to re-define an existing nwfilter with ...,"Hello,
    I have successfully compiled libvirt 1.2.7 and qemu 2.1.0 but had some troubles with nova-compute. It appears like libvirt is throwing back an error if a nwfilter is already present.
Here is my debug log:
2014-08-22 08:22:25.032 15354 DEBUG nova.virt.libvirt.firewall [req-0959ec86-3939-4e38-9505-48494b44a9fa f1d21892f9a0413c9437b6771e4290ce 9cad53a0432d4164837b8c0b35d91307] nwfilterDefineXML may have failed with (operation failed: filter 'nova-nodhcp' already exists with uuid 59970732-ca52-4521-ba0c-d001049d8460)! _define_filter /usr/lib/python2.6/site-packages/nova/virt/libvirt/firewall.py:239
2014-08-22 08:22:25.033 15354 DEBUG nova.virt.libvirt.firewall [req-0959ec86-3939-4e38-9505-48494b44a9fa f1d21892f9a0413c9437b6771e4290ce 9cad53a0432d4164837b8c0b35d91307] nwfilterDefineXML may have failed with (operation failed: filter 'nova-base' already exists with uuid b5aa80ad-ea4a-4633-84ac-442c9270a143)! _define_filter /usr/lib/python2.6/site-packages/nova/virt/libvirt/firewall.py:239
2014-08-22 08:22:25.034 15354 DEBUG nova.virt.libvirt.firewall [req-0959ec86-3939-4e38-9505-48494b44a9fa f1d21892f9a0413c9437b6771e4290ce 9cad53a0432d4164837b8c0b35d91307] nwfilterDefineXML may have failed with (operation failed: filter 'nova-vpn' already exists with uuid b61eb708-a9a5-4a16-8787-cdc58310babc)! _define_filter /usr/lib/python2.6/site-packages/nova/virt/libvirt/firewall.py:239
Here is the original function:
    def _define_filter(self, xml):
        if callable(xml):
            xml = xml()
        self._conn.nwfilterDefineXML(xml)
And here is the ""patched"" function"":
    def _define_filter(self, xml):
        if callable(xml):
            xml = xml()
        try:
            self._conn.nwfilterDefineXML(xml)
        except Exception, e:
            LOG.debug(_('nwfilterDefineXML may have failed with (%s)!'), e)
I'm not a python expert but I think that patch could be adapted to raise an error ONLY if the nwfilter rule doesn't already exist.
Dave"
1685,1360395,neutron,0985ef78d43ae7d910669c3ca968fd977f2ea2ae,1,1,,FIP namespace should not track connections for DVR...,"With DVR, connections to external network using floating ips flow through the router namespace into the fip namespace and then to the external network. These connections are tracked in router namespace for filter and nating purpose. Currently these connections are also being tracked in the fip namespace because tracking is turned on by default. To avoid unnecessary consumption of resources tracking of such connections should be turned off in fip namespace."
1686,1360656,nova,b32912f8b47f50f72a59d97387fc7f28e3e096a1,1,0,"“Since this change https://review.openstack.org/#/c/98607/, if the conductor sends back “",Objects remotable decorator fails to properly hand...,"Since this change https://review.openstack.org/#/c/98607/, if the conductor sends back  a field of type ListOfObjects field in the updates dictionary after a remotable decorator has called the object_action RPC method, restoring them into objects will fail since they will already be 'hydrated' but the field's from_primitive logic won't know hot to deal with that."
1687,1360719,nova,7090595658c2f931008fda62d612ea702509e930,0,0,Bug in test,unit test test_killed_worker_recover taking 160 se...,"nova patch https://review.openstack.org/#/c/104099/ caused the following unit tests to take 160 seconds:
nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITest.test_killed_worker_recover
nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITestV3.test_killed_worker_recover
This is because Server.wait() now waits for all workers to finish, but test_killed_worker_recover doesn't attempt to kill the workers like some of the other tests in MultiprocessWSGITest"
1688,1361097,nova,88d91252eca1f8734e316a6e4b9ebc3a977bee44,1,1,,Compute exception text never present when max sche...,"When scheduling VMs and the retry logic kicks in, the failed compute exception text is saved to be displayed for triaging purposes in the conductor/scheduler logs.  When the conductor tries to display the exception text when the maximum scheduling attempts have been reached, the exception always shows 'None' for the exception text.
Snippet from scheduler_utils.py...
 msg = (_('Exceeded max scheduling attempts %(max_attempts)d '
'for instance %(instance_uuid)s. '
'Last exception: %(exc)s.')
% {'max_attempts': max_attempts,
'instance_uuid': instance_uuid,
'exc': exc})
That is, 'exc' is erroneously ALWAYS None in this case."
1689,1361176,neutron,25f117bacf1cb110260d96e764ae8fac9440c14e,1,0,“After commit commit 466e89970f11918a809aafe8a048d138d4664299 migrations should not anymore explicitly specify the engine used for mysql.”,Bug #1361176 “DB,"After commit commit 466e89970f11918a809aafe8a048d138d4664299 migrations should not anymore explicitly specify the engine used for mysql.
There are still some migrations which do that, and they should be amended."
1690,1361366,neutron,9294de441e684a81f6e802ba0564083f1ad319d6,0,0,Bug in test,all one convergence IPv6 tests have to be skipped ...,"The One Convergence plugin doesn't currently support IPv6 so every new IPv6 test has to be explicitly skipped in the plugin's tests. This is a burden to IPv6 developers. As an interim until v6 support is added, a way to skip IPv6 tests by default should be added."
1691,1361487,nova,e0552af51ce1246e9c938720009c89ccfa008f2f,0,0,Feature. “Allow backup operation in paused and suspend state”,backup operation cannot be done in pause and suspe...,"jichen@cloudcontroller:~$ nova backup jitest3 jiback1 daily 2
ERROR (Conflict): Cannot 'createBackup' while instance is in vm_state paused (HTTP 409) (Request-ID: req-7554dea8-92aa-480c-a1f4-e3d7e479c6b3)
jichen@cloudcontroller:~$ nova list
+--------------------------------------+---------+--------+------------+-------------+--------------------+
| ID                                   | Name    | Status | Task State | Power State | Networks           |
+--------------------------------------+---------+--------+------------+-------------+--------------------+
| cb7c6742-7b7a-44de-ad5a-8570ee520f9e | jitest1 | ACTIVE | -          | Running     | private=10.0.0.2   |
| 702d1d2b-f72d-4759-8f13-9ffbcc0ca934 | jitest3 | PAUSED | -          | Paused      | private=10.0.0.200 |
+--------------------------------------+---------+--------+------------+-------------+--------------------+
jichen@cloudcontroller:~$ nova image-create  --show jitest3 test3image1
+-------------------------------------+--------------------------------------+
| Property                            | Value                                |
+-------------------------------------+--------------------------------------+
| OS-EXT-IMG-SIZE:size                | 0                                    |
| created                             | 2014-08-26T04:06:41Z                 |
| id                                  | 96a5284c-5feb-4231-8b01-9a522a7c5aab |
| metadata base_image_ref             | 94e061fb-e628-4deb-901c-9d44c059ecd9 |
| metadata clean_attempts             | 2                                    |
| metadata image_type                 | snapshot                             |
| metadata instance_type_ephemeral_gb | 0                                    |
| metadata instance_type_flavorid     | 1                                    |
| metadata instance_type_id           | 2                                    |
| metadata instance_type_memory_mb    | 512                                  |
| metadata instance_type_name         | m1.tiny                              |
| metadata instance_type_root_gb      | 1                                    |
| metadata instance_type_rxtx_factor  | 1.0                                  |
| metadata instance_type_swap         | 0                                    |
| metadata instance_type_vcpus        | 1                                    |
| metadata instance_uuid              | 702d1d2b-f72d-4759-8f13-9ffbcc0ca934 |
| metadata kernel_id                  | 20be8b63-5a84-4440-a0bd-8f69898d5965 |
| metadata ramdisk_id                 | 07f6f85f-c1dc-4790-98b5-14ab86f21b59 |
| metadata user_id                    | 256dc6db4b5c45ae90fee8132cbaad7c     |
| minDisk                             | 1                                    |
| minRam                              | 0                                    |
| name                                | test3image1                          |
| progress                            | 25                                   |
| server                              | 702d1d2b-f72d-4759-8f13-9ffbcc0ca934 |
| status                              | SAVING                               |
| updated                             | 2014-08-26T04:06:41Z                 |
+-------------------------------------+--------------------------------------+"
1692,1361517,nova,83ab44f6ffb8533dd53d5eb385f99f62d23a6056,1,1,,Nova prompt wrong message when boot from a error s...,"1. create a cinder from an existed image
cinder create 2 --display-name hbvolume-newone --image-id 9769cbfe-2d1a-4f60-9806-16810c666d7f
2. set the created volume with error status
cinder reset-state --state error 76f5e521-d45f-4675-851e-48f8e3a3f039
3. boot a vm from the created volume
nova boot --flavor 2 --block-device-mapping vda=76f5e521-d45f-4675-851e-48f8e3a3f039:::0 device-mapping-test2 --nic net-id=231eb787-e5bf-4e65-a822-25d37a84eab8
# cinder list
+--------------------------------------+-----------+-----------------+------+-------------+----------+-------------+
|                  ID                  |   Status  |   Display Name  | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-----------------+------+-------------+----------+-------------+
| 21c50923-7341-49ba-af48-f4a7e2099bfd | available |       None      |  1   |     None    |  false   |             |
| 76f5e521-d45f-4675-851e-48f8e3a3f039 |   error   |    hbvolume-2   |  2   |     None    |   true   |             |
| 92de3c7f-9c56-447a-b06a-a5c3bdfca683 | available | hbvolume-newone |  2   |     None    |   true   |             |
+--------------------------------------+-----------+-----------------+------+-------------+----------+-------------+
#RESULTS
it reports ""failed to get the volume""
ERROR (BadRequest): Block Device Mapping is Invalid: failed to get volume 76f5e521-d45f-4675-851e-48f8e3a3f039. (HTTP 400)
#Expected Message:
report the status of the volume is not correct to boot a VM"
1693,1361545,neutron,9569b2fe58d0e836071992f545886ca985d5ace8,1,1,"This is tricky! The code works with one configuration, but when there is another one it requires changes. The bug is solved by generalizing the code. If the requirements at the time of writing the original code were just to use that configuration, then there is no BIC. My understanding is that when writing the code, the coder should have been aware, so I am more inclined towards a BIC! (42061fc9cb26558baef52541eb4a02d66575f1c1)",Bug #1361545 “dhcp agent shouldn't spawn metadata-proxy for non-... ,"The ""enable_isolated_metadata = True"" options tells DHCP agents that for each network under its care, a neutron-ns-metadata-proxy process should be spawned, regardless if it's isolated or not.
This is fine for isolated networks (networks with no routers and no default gateways), but for networks which are connected to a router (for which the L3 agent spawns a separate neutron-ns-metadata-proxy which is attached to the router's namespace), 2 different metadata proxies are spawned. For these networks, the static routes which are pushed to each instance, letting it know where to search for the metadata-proxy, is not pushed and the proxy spawned from the DHCP agent is left unused.
The DHCP agent should know if the network it handles is isolated or not, and for non-isolated networks, no neutron-ns-metadata-proxy processes should spawn."
1694,1361611,nova,0c3a47e57317f6efd573388c2c400a6a88745fd8,0,0,"Feature “Adds typed console objects used by virt driver API to
    return information about consoles.”",console/virt stop returning arbitrary dicts in dri...,We have a general desire though to stop returning / passing arbitrary dicts in the virt driver API - On this report we would like to create typed objects for consoles that will be used by drivers to return values on the compute manager.
1695,1361738,cinder,ddcad011db507cde66b6b1e655d5ffc91ab8880f,1,1,"“And in turn to this Cinder patch that introduced the regression:
https://review.openstack.org/#/c/76471/7/cinder/volume/manager.py""",Live migration disconnects attached volumes on tar...,"Live migration disconnects attached volumes on target. The action itself does not fail and the instance appears online on the target host, but the underlying iSCSi connection results closed on the destination host, resulting in failures in the guest when accessing the volume(s).
The issue can be traced to this call to the following self.volume_api.terminate_connection(...) in the manager:
https://github.com/openstack/nova/blob/c594340359e3d148b6c349c388f10327ac7b8bfa/nova/compute/manager.py#L4868
And in turn to this Cinder patch that introduced the regression:
https://review.openstack.org/#/c/76471/7/cinder/volume/manager.py
Verified on Hyper-V using Devstack with both LVM and Windows Cinder drivers."
1696,1361840,nova,0b01e846d40f3b343da9ebe1dae89cca8bc2ac66,1,1,“if I revert ecce888c469c62374a3cc43e3cede11d8aa1e799 everything works fine.”,nova boot fails with rbd backend,"Booting a VM in a plain devstack setup with ceph enabled, I get an error like:
libvirtError: internal error: process exited while connecting to monitor: qemu-system-x86_64: -drive file=rbd:vmz/27dcd57f-948f-410c-830f-48d8fda0d968_disk.config:id=cindy:key=AQA00PxTiFa0MBAAQ9Uq9IVtBwl/pD8Fd9MWZw==:auth_supported=cephx\;none:mon_host=192.168.122.76\:6789,if=none,id=drive-ide0-1-1,readonly=on,format=raw,cache=writeback: error reading header from 27dcd57f-948f-410c-830f-48d8fda0d968_disk.config
even though config_drive is set to false.
This seems to be related to https://review.openstack.org/#/c/112014/, if I revert ecce888c469c62374a3cc43e3cede11d8aa1e799 everything works fine."
1697,1361963,glance,819f28a0b8863bd18f8a14491b5966c8b2723432,1,0,"""glance should do same change as well.”",No default control_exchange configuration prompt i...,"In current default glance-api.conf, messaging configurations as below, but actually 'rabbit_notification_exchange = glance' and 'qpid_notification_exchange = glance' do not impact topic consumer_queue creation. because Oslo .messaging uses 'control_exchange' as queue name, default value is 'openstack'.  other component such as cinder has written ''control_exchange=cinder'' into cinder conf.  glance should do same change as well.
# Messaging driver used for 'messaging' notifications driver
# rpc_backend = 'rabbit'
# Configuration options if sending notifications via rabbitmq (these are
# the defaults)
rabbit_host = localhost
rabbit_port = 5672
rabbit_use_ssl = false
rabbit_userid = guest
rabbit_password = guest
rabbit_virtual_host = /
rabbit_notification_exchange = glance
rabbit_notification_topic = notifications
rabbit_durable_queues = False
# Configuration options if sending notifications via Qpid (these are
# the defaults)
qpid_notification_exchange = glance
qpid_notification_topic = notifications
qpid_hostname = localhost
qpid_port = 5672
qpid_username =
qpid_password =
qpid_sasl_mechanisms =
qpid_reconnect_timeout = 0
qpid_reconnect_limit = 0
qpid_reconnect_interval_min = 0
qpid_reconnect_interval_max = 0
qpid_reconnect_interval = 0"
1698,1362129,nova,2e90d62727c093efbf60a7d86b1ee5262b041bbb,1,1,,"For rbd image backend, disk IO rate limiting isn't...","when using rbd as disk backend.   images_type=rbd in nova.conf
disk IO tunning doesn't work as described https://wiki.openstack.org/wiki/InstanceResourceQuota"
1699,1362221,nova,e9eb1d69bf75c22b0ad4b50e4892a5644c78cf68,1,1,,VMs fail to start when Ceph is used as a backend f...,"VMs' drives placement in Ceph option has been chosen (libvirt.images_types == 'rbd').
When user creates a flavor and specifies:
   - root drive size >0
   - ephemeral drive size >0 (important)
and tries to boot a VM, he gets ""no valid host was found"" in the scheduler log:
Error from last host: node-3.int.host.com (node node-3.int.host.com): [u'Traceback (most recent call last):\n', u'
 File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1305, in _build_instance\n set_access_ip=set_access_ip)\n', u' File ""/usr/l
ib/python2.6/site-packages/nova/compute/manager.py"", line 393, in decorated_function\n return function(self, context, *args, **kwargs)\n', u' File
 ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1717, in _spawn\n LOG.exception(_(\'Instance failed to spawn\'), instance=instanc
e)\n', u' File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\n six.reraise(self.type_, self.value, se
lf.tb)\n', u' File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1714, in _spawn\n block_device_info)\n', u' File ""/usr/lib/py
thon2.6/site-packages/nova/virt/libvirt/driver.py"", line 2259, in spawn\n admin_pass=admin_password)\n', u' File ""/usr/lib/python2.6/site-packages
/nova/virt/libvirt/driver.py"", line 2648, in _create_image\n ephemeral_size=ephemeral_gb)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/
libvirt/imagebackend.py"", line 186, in cache\n *args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/imagebackend.py"",
line 587, in create_image\n prepare_template(target=base, max_size=size, *args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/opens
tack/common/lockutils.py"", line 249, in inner\n return f(*args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/imagebac
kend.py"", line 176, in fetch_func_sync\n fetch_func(target=target, *args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/libvir
t/driver.py"", line 2458, in _create_ephemeral\n disk.mkfs(os_type, fs_label, target, run_as_root=is_block_dev)\n', u' File ""/usr/lib/python2.6/sit
e-packages/nova/virt/disk/api.py"", line 117, in mkfs\n utils.mkfs(default_fs, target, fs_label, run_as_root=run_as_root)\n', u' File ""/usr/lib/pyt
hon2.6/site-packages/nova/utils.py"", line 856, in mkfs\n execute(*args, run_as_root=run_as_root)\n', u' File ""/usr/lib/python2.6/site-packages/nov
a/utils.py"", line 165, in execute\n return processutils.execute(*cmd, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/openstack/commo
n/processutils.py"", line 193, in execute\n cmd=\' \'.join(cmd))\n', u""ProcessExecutionError: Unexpected error while running command.\nCommand: sudo
 nova-rootwrap /etc/nova/rootwrap.conf mkfs -t ext3 -F -L ephemeral0 /var/lib/nova/instances/_base/ephemeral_1_default\nExit code: 1\nStdout: ''\nStde
rr: 'mke2fs 1.41.12 (17-May-2010)\\nmkfs.ext3: No such file or directory while trying to determine filesystem size\\n'\n""]"
1700,1362233,nova,dbc46b99cc85402c087bb214c120bce2c65dfea9,1,1,,instance_create() DB API method implicitly creates...,"In DB API code we have a notion of 'public' and 'private' methods. The former are conceptually executed within a *single* DB transaction and the latter can either create a new transaction or participate in the existing one. The whole point is to be able to roll back the results of DB API methods easily and be able to retry method calls on connection failures. We had a bp (https://blueprints.launchpad.net/nova/+spec/db-session-cleanup) in which all DB API have been re-factored to maintain these properties.
instance_create() is one of the methods that currently violates the rules of 'public' DB API methods and creates a concurrent transaction implicitly."
1701,1362308,neutron,271a280d0e8f86b96d24970f588184b1cfc34ef8,1,0,"This is strange! It looks like there is no BIC, but a change in the requirements. One of the fixing commits is huge and changes many, many files",Use dict_extend_functions mechanism to populate pr...,Use dict_extend_functions mechanism to handle populating additional provider network attributes into Network model.
1702,1362405,nova,bf5d003af694ea1eb95fceea8c36e961092cb516,1,1,"“Commit 77b4012a02c6b4827a7a6b112f35deb9ce94c395 added further
    validation on the quota update controller that isn't aware of the
    'force' parameter. This”",'Force' option broken for quota update,"This change broke the ability to force quotas below the current in-use value by adding new validation checks:
https://review.openstack.org/#/c/28232/
$ nova quota-update --force --cores 0 132
ERROR (BadRequest): Quota limit must be greater than 1. (HTTP 400) (Request-ID: req-ff0751a9-9e87-443e-9965-a30768f91d9f)"
1703,1362416,neutron,648063881ebfbb54a362ef32d58263fc0f20981c,1,1,,Bug #1362416 “midonet,"midonet ""delete_dhcp"" function doesn't check subnet length when searching for the DHCP entries to delete.  This means it can delete the wrong subnet entry if two are nested (ie: have the same prefix, but different lengths).
Disclaimer: I don't have midonet equipment and don't know if it is even capable of supporting nested DHCP subnets - I just found this while wondering why 'net_len' variable was unused."
1704,1362466,neutron,4e80524a61a655fe9732ff42dc8f035810214670,1,1,,iptables metering removes wrong labels on update,"If a router is removed from the list passed to update_routers(), the iptables_driver removes the labels for the last(?) router passed, not the one removed."
1705,1362480,neutron,ace3a01ebf980c166b4dfe6d1216b17bf0aec2ed,1,1,,Datacenter moid should be a value not a tuple,"In edge_appliance_driver.py, there is a comma added when setting the datacenter moid, so the result is the value datacenter moid is changed to the tuple type, that is wrong.
 if datacenter_moid:
edge['datacenterMoid'] = datacenter_moid,  ===> Should remove the ','
return edge"
1706,1362595,nova,bfdae32efbeffcb74e7b2a0c48cb89cbf4c11329,1,1,,move_vhds_into_sr - invalid cookie,"When moving VHDs on the filesystem a coalesce may be in progress.  The result of this is that the VHD file is not valid when it is copied as it is being actively changed - and the VHD cookie is invalid.
Seen in XenServer CI: http://dd6b71949550285df7dc-dda4e480e005aaa13ec303551d2d8155.r49.cf1.rackcdn.com/36/109836/4/23874/run_tests.log
2014-08-28 12:26:37.538 |     Traceback (most recent call last):
2014-08-28 12:26:37.543 |       File ""tempest/api/compute/servers/test_server_actions.py"", line 251, in test_resize_server_revert
2014-08-28 12:26:37.550 |         self.client.wait_for_server_status(self.server_id, 'VERIFY_RESIZE')
2014-08-28 12:26:37.556 |       File ""tempest/services/compute/json/servers_client.py"", line 179, in wait_for_server_status
2014-08-28 12:26:37.563 |         raise_on_error=raise_on_error)
2014-08-28 12:26:37.570 |       File ""tempest/common/waiters.py"", line 77, in wait_for_server_status
2014-08-28 12:26:37.577 |         server_id=server_id)
2014-08-28 12:26:37.583 |     BuildErrorException: Server e58677ac-dd72-4f10-9615-cb6763f34f50 failed to build and is in ERROR status
2014-08-28 12:26:37.589 |     Details: {u'message': u'[\'XENAPI_PLUGIN_FAILURE\', \'move_vhds_into_sr\', \'Exception\', ""VDI \'/var/run/sr-mount/16f5c980-eeb6-0fd3-e9b1-dec616309984/os-images/instancee58677ac-dd72-4f10-9615-cb6763f34f50/535cd7f2-80a5-463a-935c-9c4f52ba0ecf.vhd\' has an invalid footer: \' invalid cook', u'code': 500, u'created': u'2014-08-28T11:57:01Z'}"
1707,1363014,nova,148fd1bb124fa80f70a8983242dc979a34acceab,1,1,,NoopQuotasDriver.get_settable_quotas() method alwa...,"NoopQuotasDriver.get_settable_quotas() tries to call update() on non-existing dictionary entry. While NoopQuotasDriver is not really useful, we still want it to be working."
1708,1363016,cinder,5c5f3c0684caada0fb7efe33d59bc7f1d66c72d8,1,1,,Volume initialisation with image might fail due to...,"When using the volume initialisation with an image, not all of the image might be written back on the volume before it gets unmapped due to not flushing/avoiding disk caching on write. The problem seems to occur mostly with volumes that are provided via multipath/Fibre Channel, e.g. where the LUNs disappear without the cinder-volume guest node having full control over it (and the kernel there being able to trigger a flush in time)."
1709,1363058,neutron,dcd3fb9da5ba64b5ae56a97c147f125bb7e33c4c,0,0,Bug in test,cfg.CONF.state_path set to wrong value in tests,"cfg.CONF.state_path is set to a random temporary directory in neutron.tests.base:BaseTestCase.setUp. This value was then over written in neutron.tests.unit.__init__. Tests that need to read or otherwise use cfg.CONF.state_path were getting the directory from which the tests were running and not the temporary directory specially created for the current test run.
Note that the usage of state_path to set lock_path, dhcp state path and the likes was working as expected, and was not affected by this bug."
1710,1363324,nova,f6d6c632f620411816979f04b7b3dda28681db18,1,1,,a bug in quota check,"\nova\db\sqlalchemy\api.py   quota_reserve()
when decided whether to refresh the user_usages[resource], one rule is that if the last refresh was too long time ago, we need refresh user_usages[resource].
 elif max_age and (user_usages[resource].updated_at -
                              timeutils.utcnow()).seconds >= max_age:
using last update time minus current time result in a overflow ,so that the refresh action always be executed,in consideration of the max_age won't be a max number."
1711,1363326,nova,4ba7ee4c1bb89b2fa48c2091df1d7f0bcf10ee02,1,1,,Error retries in _allocate_network,"In /nova/compute/manager.py/def _allocate_network_async,line 1559.
attempts = retries > 1 and retries + 1 or 1
retry_time = 1
for attempt in range(1, attempts + 1):
Variable attempts wants to determine the retry times of allocate network,but it made a small mistake.
See the Simulation results below:
retries=0,attempts=1
retries=1,attempts=1
retries=2,attempts=3
When retries=1, attempts=1 ,It actually does not retry."
1713,1364232,cinder,6d59790b9cd42309417ae1866e40a06dcb557abe,1,1,,EMC VMAX driver does not handle newlines and white...,"This is my first openstack bug report. I believe this should go to Xing Yang.
The EMC VMAX driver parses an XML config file (default is /etc/cinder/cinder_emc_config.xml) to retrieve different pieces of config information. The parsing helper methods are not consistent in their expectation of the format of this file.
For example, I provide a file with newlines and white-space like the following:
<?xml version=""1.0"" encoding=""UTF-8""?>
<EMC>
  <EcomServerPort>5988
  </EcomServerPort>
  <Array>000198700498</Array>
  <EcomServerIp>5.55.55.5
  </EcomServerIp>
  <Pool>PowerVC_thin</Pool>
<EcomUserName>admin</EcomUserName><PortGroups><PortGroup>PORTGROUP1</PortGroup><PortGroup>PORTGROUP2</PortGroup></PortGroups></EMC>
This shows newlines and indenting for some properties, and no newlines or indenting for others.
Then, EMCVMAXUtils.get_ecom_server() will return:
    (u'5.55.55.5\n  ', u'5988\n  ')
Where the newlines and whitespace are included in the address and port strings. This causes the driver to fail.
If I take out the whitespace and newlines, then the IP address and port are parsed correctly.  However, the portgroups are not parsed correctly if I remove whitespace and newlines as is shown in the last line of the file.  In this case EMCVMAXUtils.parse_file_to_get_port_group_name() returns:
    u'PORTGROUP1PORTGROUP2'
Where it should return one of the port groups at random. Instead it munges the two together. Can the EMCVMAXUtils class be fixed to handle hand tolerate normal XML?"
1714,1364279,cinder,80b14a54710dad659fde3e319f8b0cf0e673acdd,1,0,“The new cinder pools submission breaks any cinder operation that involves <host> as input parameter”,"Cinder manage now requires a pool name, which admi...","The new cinder pools submission breaks any cinder operation that involves <host> as input parameter. I tested cinder manage/unmanage feature taking the latest code. cinder manage breaks if host not given in the form host@backend#pool. For eg the following breaks:
cinder manage --source-name vol1 openstack1@iscsi
but this succeeds
cinder manage --source-name vol1 openstack1@iscsi#pool1
The knowledge of pool is only limited to the backend and hence any operation that involves <host> as input should not be burdened to also include pool name with it. This should be looked into at high priority."
1715,1364344,nova,3e11db0e3b832703feafe8317c0c12fb0a149e53,1,1,,Nova client produces a wrong exception when user t...,"Description of problem:
=======================
pyhton-novaclient produces a wrong exception when user tries to boot an instance without specific network UUID.
The issue will only reproduce when an external network is shared with the tenant, but not created from within it (I created it in admin tenant).
Version-Release:
================
python-novaclient-2.17.0-2
How reproducible:
=================
Always
Steps to Reproduce:
===================
1. Have 2 tenants (admin + additional tenant would do).
2. In tenant A (admin), Create a network and mark it as both shared and external.
3. In tenant B, Create a network which is not shared or external.
4. Boot an instance within tenant B (I tested this via CLI), do not use the --nic option.
Actual results:
===============
DEBUG (shell:783) It is not allowed to create an interface on external network 49d0cb8a-2631-4308-89c4-cac502ef0bad (HTTP 403) (Request-ID: req-caacfa72-82f8-492a-8ce2-9476be8f3e0c)
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 780, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 716, in main
    args.func(self.cs, args)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/shell.py"", line 433, in do_boot
    server = cs.servers.create(*boot_args, **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 871, in create
    **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 534, in _boot
    return_raw=return_raw, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/base.py"", line 152, in _create
    _resp, body = self.api.client.post(url, body=body)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 312, in post
    return self._cs_request(url, 'POST', **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 286, in _cs_request
    **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 268, in _time_request
    resp, body = self.request(url, method, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 262, in request
    raise exceptions.from_response(resp, body, url, method)
Forbidden: It is not allowed to create an interface on external network 49d0cb8a-2631-4308-89c4-cac502ef0bad (HTTP 403) (Request-ID: req-afce2569-6902-4b25-a9b8-9ebf1a6ce1b9)
ERROR: It is not allowed to create an interface on external network 49d0cb8a-2631-4308-89c4-cac502ef0bad (HTTP 403) (Request-ID: req-afce2569-6902-4b25-a9b8-9ebf1a6ce1b9)
Expected results:
=================
This is what happens if:
1. The shared network is no longer marked as external.
2. The tenant itself has two networks.
(+ no network UUID is speficied in the 'nova boot' command)
DEBUG (shell:783) Multiple possible networks found, use a Network ID to be more specific. (HTTP 400) (Request-ID: req-a4e90abd-2ad7-4342-aa3c-1a9aa9f5e2a0)
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 780, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 716, in main
    args.func(self.cs, args)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/shell.py"", line 433, in do_boot
    server = cs.servers.create(*boot_args, **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 871, in create
    **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 534, in _boot
    return_raw=return_raw, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/base.py"", line 152, in _create
    _resp, body = self.api.client.post(url, body=body)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 312, in post
    return self._cs_request(url, 'POST', **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 286, in _cs_request
    **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 268, in _time_request
    resp, body = self.request(url, method, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 262, in request
    raise exceptions.from_response(resp, body, url, method)
BadRequest: Multiple possible networks found, use a Network ID to be more specific. (HTTP 400) (Request-ID: req-a4e90abd-2ad7-4342-aa3c-1a9aa9f5e2a0)
ERROR: Multiple possible networks found, use a Network ID to be more specific. (HTTP 400) (Request-ID: req-a4e90abd-2ad7-4342-aa3c-1a9aa9f5e2a0)"
1716,1364392,neutron,3146837b7aea1a6c767131ad9aebaac2acb765df,1,1,"“Missing constructor when
“",Missing constructor when raising NoNetworkAvailabl...,"code in type_tunnel.py type_vlan.py incorrectly throws exception class instead of exception class instance.
Some other places need to be fixed as well."
1717,1364692,nova,a5cf28a68ab400fc59ae77bb11542a84b97b0d56,1,1,,Error msg says “No valid host found for cold migra...,"Because resize and cold migrate share common code, some of the migrate code shows through in the error handling code in that when a valid host can't be found when performing a resize operation, the user sees:
""No valid host found for cold migrate""
This can be confusing, especially when a number of different actions can be going in parallel."
1718,1364696,neutron,50c65d16bee8e6e46840f232519e92d9ba9989b4,0,0,Feature request,Bug #1364696 “Big Switch,The request context that comes into Neutron is not included in the request to the backend. This makes it difficult to correlate events in the debug logs on the backend such as what incoming Neutron request resulted in particular REST calls to the backend and if admin privileges were used.
1719,1364839,neutron,9cd96fe116ac1d990410cb1135473bd15c1f7dce,1,1,,DVR namespaces not deleted on LBaaS VIP Port remov...,The removal of LBaaS VIP Port (and other DVR Serviced ports except compute port) does not delete the DVR namespace from the service nodes.
1720,1364849,nova,c4d0e55098952ad3180f74357a9256da43bf7d15,1,1,The BIC is not the pc”Change I8f6a857b88659ee30b4aa1a25ac52d7e01156a68”,VMware driver doesn't return typed console,"Change I8f6a857b88659ee30b4aa1a25ac52d7e01156a68 added typed consoles, and updated drivers to use them. However, when it touched the VMware driver, it modified get_vnc_console in VMwareVMOps, but not in VMwareVCVMOps, which is the one which is actually used.
Incidentally, VMwareVMOps has now been removed, so this type of confusion should not happen again."
1721,1364986,nova,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,1,0,“A recent commit to oslo.db changed the way the 'raw' DB exceptions are wrapped”,oslo.db now wraps all DB exceptions,"tl;dr
In a few versions of oslo.db (maybe when we release 1.0.0?), every project using oslo.db should inspect their code and remove usages of 'raw' DB exceptions like IntegrityError/OperationalError/etc from except clauses and replace them with the corresponding custom exceptions from oslo.db (at least a base one - DBError).
Full version
A recent commit to oslo.db changed the way the 'raw' DB exceptions are wrapped (e.g. IntegrityError, OperationalError, etc). Previously, we used decorators on Session methods and wrapped those exceptions with oslo.db custom ones. This is mostly useful for handling them later (e.g. to retry DB API methods on deadlocks).
The problem with Session decorators was that it wasn't possible to catch and wrap all possible exceptions. E.g. SA Core exceptions and exceptions raised in Query.all() calls were ignored. Now we are using a low level SQLAlchemy event to catch all possible DB exceptions. This means that if consuming projects had workarounds for those cases and expected 'raw' exceptions instead of oslo.db ones, they would be broken. That's why we *temporarily* added both 'raw' exceptions and new ones to expect clauses in consuming projects code when they were ported to using of oslo.db to make the transition smooth and allow them to work with different oslo.db versions.
On the positive side, we now have a solution for problems like https://bugs.launchpad.net/nova/+bug/1283987 when exceptions in Query methods calls weren't handled properly.
In a few releases of oslo.db we can safely remove 'raw' DB exceptions like IntegrityError/OperationalError/etc from projects code and except only oslo.db specific ones like DBDuplicateError/DBReferenceError/DBDeadLockError/etc (at least, we wrap all the DB exceptions with our base exception DBError, if we haven't found a better match).
oslo.db exceptions and their description: https://github.com/openstack/oslo.db/blob/master/oslo/db/exception.py"
1722,1365065,cinder,90bbed483618505620346255d0f1f3eeb539fea0,1,1,,Restore to new volume ends up on the wrong host,"cinder/backup/api.py in the case of a restore to a new volume, creates the volume and then casts the restore request to the backup service. Unfortunately it casts it to the backup host name, not the volume host name, and since the lvm driver uses localpath in the restore code, this means the volume isn't found and the restore fails (lvm, multi-node case only)
i.e. the existing:
        self.backup_rpcapi.restore_backup(context,
                                          backup['host'],
                                          backup['id'],
                                          volume_id)
needs to be:
        self.backup_rpcapi.restore_backup(context,
                                          extract_host_from_volume(volume['host']),
                                          backup['id'],
                                          volume_id)"
1723,1365352,neutron,3c503adcf703db7b772208389b43ee09cee66185,1,1,,metadata agent does not cache auth info,"metadata agent had tried to cache auth info by the means of ""self.auth_info = qclient.get_auth_info()"" in _get_instance_and_tenant_id(), however this qclient is not the exact one which would be used in inner methods,  In short, metadata agent does not implement auth info caching correctly but still retrieves new token from keystone every time."
1724,1365392,cinder,3783c50dce3917257060dd977f69785238f58078,1,1,,c-vol.log reports “Error checking replication stat...,"Test Steps:
1. Run Cinder CI with ibm storwize storage, it was found much more ERRORs than before:
10:11:45 *** Not Whitelisted *** 2014-09-04 14:49:54.821 29428 ERROR cinder.volume.manager [-] Error checking replication status for volume 438e620d-8b04-49c0-9968-820c2bf8590f
10:11:45 *** Not Whitelisted *** 2014-09-04 14:50:54.769 29428 ERROR cinder.volume.manager [-] Error checking replication status for volume 3c9e9a38-e606-47f7-bbea-c3e84205bd79
10:11:45 *** Not Whitelisted *** 2014-09-04 14:52:54.799 29428 ERROR cinder.volume.manager [-] Error checking replication status for volume fedb9af6-d7dc-4f0d-ad7b-eb28127ac507
2. Manually Create volumes without replication  on IBM storwize v7000u, the volume creation successfully.
3. Check c-vol.log, it was found the below logs every 1 minutes:
2014-09-04 17:07:11.515 ERROR cinder.volume.manager [-] Error checking replication status for volume d2ffa93d-4140-4f1e-bcb7-c70af51781a3
2014-09-04 17:07:11.515 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:07:11.515 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1597, in _update_replication_relationship_status
2014-09-04 17:07:11.515 TRACE cinder.volume.manager     ctxt, vol)
2014-09-04 17:07:11.515 TRACE cinder.volume.manager   File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:07:11.515 TRACE cinder.volume.manager     return f(*args, **kwargs)
2014-09-04 17:07:11.515 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 756, in get_replication_status
2014-09-04 17:07:11.515 TRACE cinder.volume.manager     return self.replication.get_replication_status(volume)
2014-09-04 17:07:11.515 TRACE cinder.volume.manager AttributeError: 'NoneType' object has no attribute 'get_replication_status'
2014-09-04 17:07:11.515 TRACE cinder.volume.manager
2014-09-04 17:07:11.516 ERROR cinder.volume.manager [-] Error checking replication status for volume d9791267-bb7a-448c-938e-c98dff38bcf5
2014-09-04 17:07:11.516 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1597, in _update_replication_relationship_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     ctxt, vol)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     return f(*args, **kwargs)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 756, in get_replication_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     return self.replication.get_replication_status(volume)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager AttributeError: 'NoneType' object has no attribute 'get_replication_status'
2014-09-04 17:07:11.516 TRACE cinder.volume.manager
2014-09-04 17:07:11.516 ERROR cinder.volume.manager [-] Error checking replication status for volume fedd3b21-d60e-497e-b309-b125d81b1bcd
2014-09-04 17:07:11.516 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1597, in _update_replication_relationship_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     ctxt, vol)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     return f(*args, **kwargs)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 756, in get_replication_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     return self.replication.get_replication_status(volume)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager AttributeError: 'NoneType' object has no attribute 'get_replication_status'
2014-09-04 17:07:11.516 TRACE cinder.volume.manager
3. Expected result:
There is no replication status update Error for those volumes without replication."
1725,1365429,neutron,0bd4472ef7bdb9d94f988669f34f7eaa53ca0a89,1,1,,All HA routers become active on the same agent,"How to reproduce:
On a setup with two L3 agents, create ten HA routers, the scheduler will place them on both agents, but the same agent will host the active instance of all ten routers. This defeats the idea of load sharing traffic across all L3 agents.
Solutions:
This can be solved in one of two ways:
1) Enable preemptive elections for HA routers. Keepalived enables a configuration value that causes VRRP pre-emptive elections. This way we can set a random VRRP priority for each router instance, and the elections process will guarantee a random distribution of active routers on the available agents. Preemptive elections have a major downside - If an agent that's hosting a master instance drops, the backup router will come in to play, but when the node is fixed the old master will re-assume its role. This second state transition is costly and redundant.
2) With non-preemptive elections the first router instance to come up will become the master. We can exploit this and send the notification from the server to the agents in a random order."
1726,1365474,cinder,ce7c8dfba800874fec7411393e6799d81064f939,1,1,,gpfs driver not setting pool information properly,"In the current code, the configuration option 'gpfs_storage_pool' is assigned with default value of 'None'
And in the do_setup(),  it line self._storage_pool = pool or system, it get assigned with ""None"" if the user has not specified any value.
Following is seen in the c-vol log.
2014-09-04 17:43:44.039 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:43:44.039 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 250, in init_host
2014-09-04 17:43:44.039 TRACE cinder.volume.manager     self.driver.do_setup(ctxt)
2014-09-04 17:43:44.039 TRACE cinder.volume.manager   File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:43:44.039 TRACE cinder.volume.manager     return f(*args, **kwargs)
2014-09-04 17:43:44.039 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/gpfs.py"", line 341, in do_setup
2014-09-04 17:43:44.039 TRACE cinder.volume.manager     raise exception.VolumeBackendAPIException(data=msg)
2014-09-04 17:43:44.039 TRACE cinder.volume.manager VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Invalid storage pool None specificed.
""gpfs_storage_pool"" option needs to assigned by default with ""system"" instead of ""None"""
1727,1365487,cinder,0e429c71f34c5a7603854171c6843a67c61b9127,1,0,“Qemu-img still uses the legacy name 'vpc' for vhd files which causes this confusion”,Cinder fails to upload volume to vhd image,"The method copy_volume_to_image from windows.py specifies the wrong volume format, namely 'vpc'. In this case, the upload_volume method from image_utils will attempt to convert the volume to vhd which will result in an error from qemu as it does not recognize the format. Qemu-img still uses the legacy name 'vpc' for vhd files which causes this confusion.
The solution is to explicitly using vhd as a format when uploading volumes."
1728,1365516,cinder,75564b4af164786f43f00a12bd16f26f4b0b9197,0,0,"this is basicly ""dead code"" so it should be removed",Revert parent commit for iSCSI refactor that didn'...,"During Juno we had plans to clean up the iSCSI and target code.  Sadly the work rotted in gerrit without reviews and didn't make it into this release.  In the process there was a commit that merged that layed some foundation work:
Change-Id: Iaa55e31e3dadc7dcb58112302c3807a8f92bcada
Without the follow up work however this is basicly ""dead code"" so it should be removed from the Juno release and added back in with Kilo when we're ready to go again."
1731,1365806,neutron,524981cce05a9b365036c0a1e9810036936d3d5b,1,1,“Add missing methods to NoopFirewallDriver”,Noopfirewall driver or security group disabled sho...,"With openvswitch neutron agent, during the daemon loop, the phase for setup_port_filters will try to grab/call method 'security_group_rules_for_devices'  to Neutron Server.
And this operation will be very time consuming and have big performance bottleneck as it include ports query,  rules query, network query as well as reconstruct the huge Security groups Dict Message.  This message size is very large and for processing it, it will occupy a lot of CPU of Neutron Server. In cases like VM/perhost arrive to 700, the Neutron server will be busy doing the message and couldn't to do other thing and this could lead to message queue connection timeout and make queue disconnect the consumers. As a result the Neutron server is crashed and not function either for deployments or for API calls.
For the Noopfirewall or security group disabled situation, this operation should be avoided. Because eventually these reply message would not be used by Noopfirewall driver.  (There methods are pass).
 with self.firewall.defer_apply():
            for device in devices.values():
                LOG.debug(_(""Update port filter for %s""), device['device'])
                self.firewall.update_port_filter(device)"
1732,1365829,neutron,74a292ceba0be6aaa7ea1911cac2f102eb3db90e,0,0,Bug in test,ipv6 tests leave ipv6 flag disabled,"The test_disabled test for the IPv6 utilities leaves a module level flag set to disabled, which will break any following tests that depend on IPv6  being enabled (e.g. the iptables tests).
http://logs.openstack.org/49/113749/8/check/gate-neutron-python27/8787946/console.html
To reproduce, you can run these two tests serially.
neutron.tests.unit.test_ipv6.TestIsEnabled.test_disabled
neutron.tests.unit.test_security_groups_rpc.TestSecurityGroupAgentEnhancedRpcWithIptables.test_prepare_remove_port"
1733,1365881,cinder,e6ea26ea1272f9407879b822b3913b4308e3f2f8,0,0,Feature.” This patch optimizes the data path”,E-Series driver does not determine preferred path/...,"The E-Series driver does not determine the preferred data path to access a newly created lun.
This can cause problems with data access and the iscsi path to the device that is handed to a vm or that is used when copying an image to the device should always use the preferred path. E-Series arrays do something called an implicit transfer when a host accesses data from a non-preferred path for more than 5 minutes. At 5 minutes the array will transfer ownership of the volume to the path over which IO is currently flowing. If this issue is not resolved the current behavior of the driver is to establish every IO path on the first iSCSI target that is returned. In this scenario the controller A will quickly become heavily loaded and the controller B will have no load because of the implicit transfers from B --> A To get better throughput this issued needs to be fixed."
1734,1365884,cinder,a120ede9ae3f18756db07d1d6696b9ac773b84bf,1,1,,E-Series driver creates incorrect host type,"Various performance related issues with block IO to the mapped devices on linux hosts it became clear that the choices that were made in how the driver does host creation are not in line with best practices for E-Series arrays. The driver creates a host that is of type 'linux' as it appears from querying the proxy and type ""MPP/RDAC"" when viewing the host within Santricity itself. At first glance ""linux"" would appear reasonable, but in fact the correct default choice should have been LnxALUA (from the proxy) or Linux (DM-MP)."
1735,1365901,cinder,92ae5ac0df07a68b9c7c8f58740e2d7b2e2959d0,1,0,“inder-api ran into hang loop in python2.6”,cinder-api ran into hang loop in python2.6,"cinder-api ran into hang loop in python2.6
#cinder-api
...
...
snip...
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
...
...
snip..."
1736,1366011,cinder,c9e5bb4fe8b23743f4e2a064395584c87d1232ba,1,1,,Some tcp options are ignored,"The values for tcp_keepalive, tcp_keepidle, tcp_keepalive_count and tcp_keepalive_interval in cinder.conf are ignored."
1737,1366083,cinder,9cc9cf111ef8a0265df5323fecaed12b0dabe312,1,1,,Race condition in remoteFS _ensure_shares_mounted(...,"There is a race condition in the remoteFS _ensure_shares_mounted() method:
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/remotefs.py#L141
The list of known mounted shares is cleared before being rebuilt.  This can cause a race where an operation attempts to run and finds an empty list, simply because it is in the process of being refreshed."
1738,1366105,neutron,11c8f8cbca0fc3feed0469346d682225f9301251,1,1,"""misses format for parameter 
“",Debug string in l3 agent misses format for paramet...,"neutron/agent/l3_agent.py:
 LOG.debug(""not hosting snat for router: %"", ri.router['id'])"
1740,1366371,cinder,9082273305b0b9c117eb677a0e34c2ffde4e66f0,1,1,0,Volume type needs to be provided when creating a c...,"When creating a consistency group, the scheduler will find a backend that
supports all input volume types. If volume types are not provided, the
default_volume_type in cinder.conf will be used, however, this could cause
inconsistent behavior in a user environment where the default_volume_type
is defined in some places but not in others. This fix removed the use of
default_volume_type for CG creation, added a check to verify that
volume types are provided when creating a CG.
When creating a volume and adding it to a CG, we need to make sure a
volume type is provided as well."
1741,1366506,nova,f4fec08e9850dae163f447e72cd1c7f638b2bb10,1,1,,Bug #1366506 “VMware,An administrator can delete VM's running omthe VC without knowing that they belong to OpenStack
1742,1366548,nova,bc40e85c0278afcca74892fc818da1e005f13fc7,0,0,Refactoring “the instance is updated so there is no need for the save”,Bug #1366548 “libvirt,instance save is used by the driver when it does not need to be. each instance save will invoke a db access. after the spawn method is called the instance is updated so there is no need for the save
1743,1366859,nova,5fd0500e7e262c602dc7bbbff456326598da41bc,1,0,"“This was recently changed as part of the Ironic -> Nova driver patch series. We still want to canonicalize supported_instances though…""",Bug #1366859 “Ironic,"Using the latest Nova Ironic compute drivers (either from Ironic or Nova) I'm hitting scheduling ERRORS:
Sep 08 15:26:45 localhost nova-scheduler[29761]: 2014-09-08 15:26:45.620 29761 DEBUG nova.scheduler.filters.compute_capabilities_filter [req-9e34510e-268c-40de-8433-d7b41017b54e None] extra_spec requirement 'amd64' does not match 'x86_64' _satisfies_extra_specs /opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/filters/compute_capabilities_filter.py:70
I've gone ahead and patched in https://review.openstack.org/#/c/117555/.
The issue seems to be that ComputeCapabilitiesFilter does not itself canonicalize instance_types when comparing them which will breaks existing TripleO baremetal clouds using x86_64 (amd64)."
1744,1366982,nova,35008c0d16e41fdae6e868626477f361392aa923,1,1,,Exception NoMoreFixedIps doesn't show which networ...,"The exception NoMoreFixedIps in nova/exception.py has a very generic error message:
""Zero fixed ips available.""
When performing a deploy with multiple networks, it can become difficult to determine which network has been exhausted.  Slight modification to this error message will help simplify the debug process for operators."
1745,1367172,glance,2df48336d0e0381522e9ec100cb56579367cfa1f,1,1,,glance-manage db_load_metadefs should use 'with op...,Currently script which loads metadeta definitions to database uses try except block with open(file) inside. There is better solution in Python for file streams - use 'with open(file) as...'. With this approach we will be sure that every resource is cleaned up when the code finishes running.
1746,1367238,cinder,3fff55681739872ab125aa7f0677f7d836c10615,1,0,"""IBM NAS cinder driver sets 'rw' permissions to all during volume creat” external artefact",IBM NAS cinder driver sets 'rw' permissions to all...,"IBM NAS cinder driver sets 'rw' permissions to all during volume create operation from a volume snapshot or from an existing volume (volume clone operation).
This is not required as 'rw' permissions to the user only should be sufficient.
This also helps resolve the security issue setting 'rw' permissions to all."
1747,1367342,nova,efe1002459145c43577b53ba03a228ecf9a78434,1,1,,call to _set_instance_error_state is incorrect in ...,"nova/compute/manager.py  in do_build_and_run_instance
Under  except exception.RescheduledException as e:
...
self._set_instance_error_state(context, instance.uuid)
This should be passing instance only not instance.uuid"
1748,1367344,nova,79bfb1bf343484e98aa36dcc663a5370baf4cab7,1,1,“Commit f0ff4d51057080e769407e873e5ed212f15b773d caused the problem.”,Libvirt Watchdog support is broken when ComputeCap...,"The doc (http://docs.openstack.org/admin-guide-cloud/content/customize-flavors.html , section ""Watchdog behavior"") suggests to use the flavor extra specs property called ""hw_watchdog_action"" to configure a watchdog device for libvirt guests. Unfortunately, this is broken due to ComputeCapabilitiesFilter trying to use this property to filter compute hosts, so that scheduling of a new instance always fails with NoValidHostFound error."
1749,1367363,nova,5f93c841a80663b68da2fb04df78d5acd0754d68,1,1,,Libvirt-lxc will leak nbd devices on instance shut...,"Shutting down a libvirt-lxc based instance will leak the nbd device. This happens because _teardown_container will only be called when libvirt domain's are running. During a shutdown, the domain is not running at the time of the destroy. Thus, _teardown_container is never called and the nbd device is never disconnected.
Steps to reproduce:
1) Create devstack using local.conf: https://gist.github.com/ramielrowe/6ae233dc2c2cd479498a
2) Create an instance
3) Perform ps ax |grep nbd on devstack host. Observe connected nbd device
4) Shutdown instance
5) Perform ps ax |grep nbd on devstack host. Observe connected nbd device
6) Delete instance
7) Perform ps ax |grep nbd on devstack host. Observe connected nbd device
Nova has now leaked the nbd device."
1750,1367454,cinder,3d301346a7100e52fe0c319c1c1b5b81a3bc1660,1,0,Bug in API  “There was a bug in WSGIService “,osapi_volume_workers warning when unspecified,"Seeing the following warning in cinder logs (juno):
cinder.service [-] Value of config option osapi_volume_workers must be integer greater than 1.  Input value ignored.
We're not specifying this in cinder.conf, so the following OpenStack code from cinder/service.py attempts to get the default:
        self.workers = getattr(CONF, '%s_workers' % name,
                               processutils.get_worker_count())
        setup_profiler(name, self.host)
        if self.workers < 1:
            LOG.warn(_(""Value of config option %(name)s_workers must be ""
                       ""integer greater than 1.  Input value ignored."") %
                     {'name': name})
            # Reset workers to default
            self.workers = processutils.get_worker_count()
processutils.get_worker_count() looks like this:
    try:
        return multiprocessing.cpu_count()
    except NotImplementedError:
        return 1
and multiprocessing.cpu_count() returns this:
>>> import multiprocessing
>>> multiprocessing.cpu_count()
8
It looks to me like getattr is reading a value for osapi_volume_workers from the conf file even though there is no default set and we didn't specify this in the conf, preventing the default processutils.get_worker_count() from being returned by getattr when it should have been. It still works because processutils.get_worker_count() gets called again, but that shouldn't have been necessary, and neither should the warning (in this case)."
1752,1367540,nova,688be19e8a2170276451926401af8257c4faece5,1,1,,Bug #1367540 “VMware,"If you create a volume from an image and launch an instance from it, the instance fails to be created.
To recreate this bug:
1. Create a volume from an image
2. Launch an instance from the volume
The following error is thrown in n-cpu.log:
2014-09-09 22:33:47.037 ERROR nova.compute.manager [req-e17654a6-a58b-4760-a383-643dd054c691 demo demo] [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] Instance failed to spawn
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] Traceback (most recent call last):
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/compute/manager.py"", line 2171, in _build_resources
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     yield resources
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/compute/manager.py"", line 2050, in _build_and_run_instance
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     block_device_info=block_device_info)
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 446, in spawn
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     admin_password, network_info, block_device_info)
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 298, in spawn
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     vi = self._get_vm_config_info(instance, image_info, instance_name)
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 276, in _get_vm_config_info
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     image_info.file_size_in_gb > instance.root_gb):
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/virt/vmwareapi/vmware_images.py"", line 92, in file_size_in_gb
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     return self.file_size / units.Gi
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] TypeError: unsupported operand type(s) for /: 'unicode' and 'int'
It appears that a simple conversion of the file_size to an int in vmware_images.py should fix this."
1753,1367552,neutron,eccf5e819ef918d566b7efd1e91719550d0fbef9,0,0,Bug in test,Cisco test cases are using _do_side_effect() which...,"In https://review.openstack.org/78880  NeutronDbPluginV2TestCase._do_side_effect()  was renamed to  _fail_second_call().
But some cisco nexus test cases still use the old name."
1755,1367575,nova,a5c9340130f86d1f2a54a808add32d9d7d7355cd,1,1,server actions were missed for V2.1 API.,"Bug #1367575 """"os-start/os-stop” server actions does not work fo... ","""os-start/os-stop"" server action does not work for V2.1 API.
Those needs to be converted to V2.1 from V3 base code.
This needs to be fixed to make V2.1 backward compatible with V2 APIs"
1757,1367642,nova,61b2824a1ff9c57066e3277b2f66f6b93a5d5f08,1,1,“backward compatible with V2 APIs”,"Bug #1367642 """"revertResize/confirmResize” server actions does n... ","""revertResize/confirmResize"" server actions does not work for v2.1 API
Those needs to be converted to V2.1 from V3 base code.
This needs to be fixed to make V2.1 backward compatible with V2 APIs"
1758,1367771,glance,89c04904416270d3c306d430f443a7127c5fc206,1,1,,glance-manage db load_metadefs will fail if DB is ...,"To insert data into DB 'glance-manage db load_metadefs' uses IDs for namespaces which are generated by built-in function in Python - enumerate:
for namespace_id, json_schema_file in enumerate(json_schema_files, start=1):
For empty database it works fine, but this causes problems when there are already metadata namespaces in database. The problem is that when there are already metadata definitions in DB then every invoke of glance-manage db load_metadefs leads to IntegrityErrors because of duplicated IDs.
There are two approaches to fix this:
1. Ask for a namespace just after inserting it. Unfortunately in current implementation we need to do one more query.
2. When this go live - https://review.openstack.org/#/c/120414/ - then we won't need to do another query, because ID is available just after inserting a namespace to DB (namespace.save(session=session))."
1759,1367864,neutron,1d4e13574574472e299296892d7c9e8f706aea67,1,1,,User and group association not deleted after route...,"In nuage plugin when a router delete operation is performed, the user and group association is not deleted. This is a bug which is caused by a check for a nuage zone attached to the router even after the router is deleted.
This commit associated with this bug will fix the above issue."
1760,1367881,neutron,c5ae5ad2637a561ccbc7484045e99d8140822b8d,1,1,,l2pop RPC code throwing an exception in fdb_chg_ip...,"I'm seeing an error in the l2pop code where it's failing to add a flow for the ARP entry responder.
This is sometimes leading to DHCP failures for VMs, although a soft reboot typically fixes that problem.
Here is the trace:
2014-09-10 15:10:36.954 9351 ERROR neutron.agent.linux.ovs_lib [req-de0c2985-1fac-46a8-a42b-f0bad5a43805 None] OVS flows could not be applied on bridge br-tun
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib Traceback (most recent call last):
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 407, in _fdb_chg_ip
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib     self.local_ip, self.local_vlan_map)
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib     return method(*args, **kwargs)
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 250, in fdb_chg_ip_tun
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib     for mac, ip in after:
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib TypeError: 'NoneType' object is not iterable
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib
2014-09-10 15:10:36.955 9351 ERROR oslo.messaging.rpc.dispatcher [req-de0c2985-1fac-46a8-a42b-f0bad5a43805 ] Exception during message handling: 'NoneType' object is not iterable
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     return method(*args, **kwargs)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 55, in update_fdb_entries
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     self.fdb_update(context, fdb_entries)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     return method(*args, **kwargs)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 212, in fdb_update
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     getattr(self, method)(context, values)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 407, in _fdb_chg_ip
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     self.local_ip, self.local_vlan_map)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     return method(*args, **kwargs)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 250, in fdb_chg_ip_tun
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     for mac, ip in after:
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher
2014-09-10 15:10:36.957 9351 ERROR oslo.messaging._drivers.common [req-de0c2985-1fac-46a8-a42b-f0bad5a43805 ] Returning exception 'NoneType' object is not iterable to caller
I don't know this code well enough to suggest a fix - whether it's checking the return from agent_ports.items() better, or that there is a bug elsewhere, so any help would be appreciated."
1761,1367918,nova,ba49c5eb387d841d40f81f57d5cefbb00c835bff,1,1,,Xenapi attached volume with no VM leaves instance ...,"As shown by the stack trace below, when a volume is attached but the VM is not present the volume can't be cleaned up by Cinder and will raise an Exception which puts the instance into an error state.  The volume attachment isn't removed because an if statement is hit in the xenapi destroy method which logs ""VM is not present, skipping destroy..."" and then moves on to trying to cleanup the volume in Cinder.  This is because most operations in xen rely on finding the vm_ref and then cleaning up resources that are attached there.  But if the volume is attached to an SR but not associated with an instance it ends up being orphaned.
014-08-29 15:54:02.836 8766 DEBUG nova.volume.cinder [req-341cd17d-0f2f-4d64-929f-a94f8c0fa295 None] Cinderclient connection created using URL: https://localhost/v1/<tenant>
cinderclient /opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/volume/cinder.py:108
2014-08-29 15:54:03.251 8766 ERROR nova.compute.manager [req-341cd17d-0f2f-4d64-929f-a94f8c0fa295 None] [instance: <uuid>] Setting instance vm_state to ERROR
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] Traceback (most recent call last):
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2443, in do_terminate_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] self._delete_instance(context, instance, bdms, quotas)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/hooks.py"", line 131, in inner
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] rv = f(*args, **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2412, in delete_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] quotas.rollback()
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 82, in exit
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] six.reraise(self.type, self.value, self.tb)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2390, in _delete_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] self._shutdown_instance(context, instance, bdms)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2335, in _shutdown_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] connector)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/volume/cinder.py"", line 189, in wrapper
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] res = method(self, ctx, volume_id, *args, **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/volume/cinder.py"", line 309, in terminate_connection
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] connector)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/v1/volumes.py"", line 331, in terminate_connection
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] {'connector': connector})
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/v1/volumes.py"", line 250, in _action
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] return self.api.client.post(url, body=body)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/client.py"", line 223, in post
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] return self._cs_request(url, 'POST', **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/client.py"", line 187, in _cs_request
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/client.py"", line 170, in request
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] raise exceptions.from_response(resp, body)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] ClientException: DELETE on http://localhost:8081/volumes/<volume_uuid>/export?force=False returned '409' with 'Volume '<volume_uuid>' is currently attached to '<ip>'' (HTTP 409) (Request-ID: req-d8a81cfc-5ba2-4bfb-b519-c92a2"
1762,1368006,neutron,60816b637d391aa5e5aef9b4b3a432b5ce552cc8,1,1,,Bug #1368006 “ofagent,"ofagent has code for agent-on-DomU support inherited from OVS agent.
However, it's incomplete and broken.  Because ofagent uses a direct
OpenFlow channel instead of ovs-ofctl command to program a switch,
the method to use the special rootwrap can not work."
1763,1368033,neutron,485c174fc7e52e697d1d73a96cd924c2e3a04414,0,0,"Feature ""So the CRD configuration options are to be moved to a separate file to be used with other plugin/drivers”",Separate Configuration from Freescale SDN ML2 mech...,"In the current implementation, CRD configuration is existing within the code of ML2 mechanism driver.
When any other plugin/driver need to use this configuration needs to duplicate the complete configuration.
So the CRD configuration options are to be moved to a separate file to be used with other plugin/drivers."
1764,1368035,nova,f7ef2b09a195f853cecaa28db710d643a8e30f78,0,0,Feature “V3 resource name needs to be changed to work that for V2.1.”,'os-interface' resource name is wrong for Nova V2....,"'os-interface' resource name needs to be fixed in V3 code base to work for V2.1 API.
V2 and V3 diff for this resource name is below-
V2 - '/servers/os-interface'
V3 - '/servers/os-attach-interfaces'
V3 resource name needs to be changed to work that for V2.1.
This needs to be fixed to make V2.1 backward compatible with V2 APIs"
1765,1368055,neutron,ed499b6af1ee8cc3003a991eea2ab4d0fdd461c3,1,1,,NoneType not iterable with None in bulk body,"2014-09-11 01:38:57.752 22667 ERROR neutron.api.v2.base [req-91733c92-6d15-4fd1-97b6-edcd704280d3 None] Request body: {u'subnets': None}
2014-09-11 01:38:57.752 22667 ERROR neutron.api.v2.resource [req-91733c92-6d15-4fd1-97b6-edcd704280d3 None] create failed
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/site-packages/neutron/api/v2/resource.py"", line 87, in resource
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/site-packages/neutron/api/v2/base.py"", line 357, in create
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource     allow_bulk=self._allow_bulk)
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/site-packages/neutron/api/v2/base.py"", line 573, in prepare_request_body
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource     bulk_body = [prep_req_body(item) for item in body[collection]]
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource TypeError: 'NoneType' object is not iterable"
1766,1368152,neutron,e7f0b56d74fbfbb08a3b7a0d2da4cefb6fe2aa67,1,1,,l3 agent dies after MessagingTimeout error,"l3 agent dies after MessagingTimeout error, attached is the trace.
I am not sure about the root cause of the MessagingTimeout either but that seems a different problem, I think agent should survive the timeout and retry maybe."
1767,1368202,nova,a957488450e6d31dfc34eade38e03b5535c1d017,0,0,Refactoring “was deprecated”,Nova console VMRCConsole will break with ESX drive...,VMware ESX driver was deprecated in J.This console class will break.
1768,1368234,neutron,3b0ac61dea7178e858c6dede44a13818c5162283,0,0,Bug in test,faulty way of checking if mock wasn't called,Some UT use <mock>.assert_has_calls([]) as a way to check if mock wasn't called - this doesn't work because assert_has_calls only checks if passed calls are present in mock_calls and hence it is always true regardless of whether mock was called or not. This can lead to falsely passed tests.
1769,1368251,neutron,b030300b1687b853a95b6bd2e706045f549d1e50,1,0,"Environment. “was defined to be a boolean type and in postgresql this type is enforced,
while in mysql this just maps to tinyin”",migrate_to_ml2 accessing boolean as int fails on p...,"The ""allocated"" variable used in migrate_to_ml2 was defined to be a boolean type and in postgresql this type is enforced,
while in mysql this just maps to tinyint and accepts both numbers and bools.
Thus the migrate_to_ml2 script breaks on postgresql"
1770,1368381,cinder,804f5de841b410c3e6a2ff30756546f2a086b112,1,1,,Export target objects hang in Datera backend,"For every export in the Datera driver, there is an export object associated with it.
Problem: When a detach operation happens, the export object hangs around.
To Reproduce: Attach a Datera volume to a virtual machine. Then detach it and notice in the backend the object is still there.
Expected Behavior: If the lun that exists in the export object is removed disassociated, the export object should be destroyed."
1771,1368404,nova,c860280a92c2d0e32897d71aa4c6f083ee13345c,1,1,,Bug #1368404 “Uncaught 'libvirtError,"Some uncaught libvirt errors may result in instances being set to ERROR state and is causing sporadic gate failures. This can happen for any of the code paths that use _destroy().  Here is a recent example of a failed resize:
[req-06dd4908-382e-455e-854e-e4d42a4bf62b TestServerAdvancedOps-724416891 TestServerAdvancedOps-711228572] [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] Setting instance vm_state to ERROR
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] Traceback (most recent call last):
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 5902, in _error_out_instance_on_exception
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     yield
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 3658, in resize_instance
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     timeout, retry_interval)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 5468, in migrate_disk_and_power_off
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     self.power_off(instance, timeout, retry_interval)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 2400, in power_off
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     self._destroy(instance)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 998, in _destroy
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     timer.start(interval=0.5).wait()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 121, in wait
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     return hubs.get_hub().switch()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 293, in switch
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     return self.greenlet.switch()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/openstack/common/loopingcall.py"", line 81, in _inner
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     self.f(*self.args, **self.kw)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 971, in _wait_for_destroy
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     dom_info = self.get_info(instance)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 3922, in get_info
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     dom_info = virt_dom.info()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 183, in doit
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     result = proxy_call(self._autowrap, f, *args, **kwargs)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 141, in proxy_call
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     rv = execute(f, *args, **kwargs)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 122, in execute
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     six.reraise(c, e, tb)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 80, in tworker
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     rv = meth(*args, **kwargs)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 1068, in info
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     if ret is None: raise libvirtError ('virDomainGetInfo() failed', dom=self)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] libvirtError: Domain not found: no domain with matching uuid '525f4f95-f631-4fbb-a884-20c37711fb0d' (instance-00000097)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]"
1772,1368495,nova,a5147669f4d584fdbee10b4a2c77b93b31ef3108,1,0,“Reverting those attribute same as V2 to work with V2.1”,'type'/'mac_adrr' attribute of server's address fi...,"For 'extended_ips'/'extended_ips_mac' extension, there are difference between V2 and V3 server show/index & server address index API response listed below-
'address' field of V2->V3 server API response-
""OS-EXT-IPS:type"" -> ""type""
""OS-EXT-IPS-MAC:mac_addr"" -> ""mac_addr""
Above attribute needs to be fixed in V2.1 to make it backward compatible with V2."
1773,1368595,cinder,26de1b1d829849665dae921b8be739194b84515d,1,1,,Bug #1368595 “IBM Storwize,"QoS should be changed after retyping the volume.
So far for storwzie driver, if the new type does not have QoS configurations, the old QoS configurations remain in the volume."
1774,1368597,nova,21baedfc3185111589535cdc24fff83603a5e3fc,1,1,,Wrong status code in @wsgi.response decorator in s...,"In server`s action `confirmResize` status code in @wsgi.response decorator is set as 202 but this is overridden/ignored by return statement (return exc.HTTPNoContent()) which return 204 status code  - https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/servers.py#L1080
This is very confusing and we should have expected status code in @wsgi.response decorator as consistence with other APIs.
There is no change required in API return status code but in code it should be fixed to avoid the confusion."
1775,1368900,neutron,a2ef082cb0b25d14595a8810adb16df3f9661e43,1,1,,Exception masked due to log exception,"In neutron/plugins/cisco/cfg_agent/device_drivers/driver_mgr.py, if an exception occurs loading the Cfg Agent, a log message is created and it causes a second exception due to incorrect key used to access template name."
1776,1368989,nova,197bb467c0fa33700e5397c934fa10d8c16f1fbc,1,1,Bug It is reporting some bug or a consecutive list of actions that should happen and they can end in a bug (78dbed87b53ad3e60dc00f6c077a23506d228b6c),service_update() should not set an RPC timeout lon...,"nova.servicegroup.drivers.db.DbDriver._report_state() is called every service.report_interval seconds from a timer in order to periodically report the service state.  It calls self.conductor_api.service_update().
If this ends up calling nova.conductor.rpcapi.ConductorAPI.service_update(), it will do an RPC call() to nova-conductor.
If anything happens to the RPC server (failover, switchover, etc.) by default the RPC code will wait 60 seconds for a response (blocking the timer-based calling of _report_state() in the meantime).  This is long enough to cause the status in the database to get old enough that other services consider this service to be ""down"".
Arguably, since we're going to call service_update( ) again in service.report_interval seconds there's no reason to wait the full 60 seconds.  Instead, it would make sense to set the RPC timeout for the service_update() call to to something slightly less than service.report_interval seconds.
I've also submitted a related bug report (https://bugs.launchpad.net/bugs/1368917) to improve RPC loss of connection in general, but I expect that'll take a while to deal with while this particular case can be handled much more easily."
1777,1369136,cinder,b04a1dd063d9678747194e416ed157109a17fdc7,1,1,,Timeout triggers failures running tempest for ZFSS...,"Running tempest  in a long distance link between the cinder controller and ZFS Storage Appliance, ZFSSA iSCSI Cinder driver timeouts, resulting in tests failures.
e.g
...
{1} tempest.api.volume.test_volumes_get.VolumesV2GetTest.test_volume_create_get_update_delete_from_image [355.316953s] ... FAILED
{1} tempest.api.volume.test_volumes_get.VolumesV2GetTestXML.test_volume_create_get_update_delete_as_clone [389.165116s] ... FAILED
...
Or
{1} tempest.api.volume.test_volumes_get.VolumesV1GetTest.test_volume_create_get_update_delete [507.216176s] ... FAILED
Some of these errors are the results of a volume not been reported as created, or an error uploading an image to a volume.
e.g (from cinder volume service)
2014-09-12 11:15:43.940 ERROR oslo.messaging.rpc.dispatcher [req-1879fa63-44c7-4f22-92fd-b139cce4bae7 85f8e9058af848aa9e10d2615e2e47dc 43
ab44ae2c7d4606ae30bc3b551cfe25] Exception during message handling: timed out
2014-09-12 11:15:44.643 ERROR cinder.volume.flows.manager.create_volume [req-da06124c-366b-48b7-95ca-96720967438c 85f8e9058af848aa9e10d26
15e2e47dc 43ab44ae2c7d4606ae30bc3b551cfe25] Volume c74b1a80-0528-41dd-b3d5-275410fa557a: create failed
2014-09-12 11:15:44.648 ERROR oslo.messaging.rpc.dispatcher [req-da06124c-366b-48b7-95ca-96720967438c 85f8e9058af848aa9e10d2615e2e47dc 43
ab44ae2c7d4606ae30bc3b551cfe25] Exception during message handling: timed out"
1778,1369151,nova,e65bf1febb66cb2771834f4bbb11c2e61579ad7d,1,0,"Chnage requirements, specifications. API change. “This patch updates the relevate baremetal/ironic host state
    classes to do likewise.”",Bug #1369151 “TypeError,"As of today we are seeing the following scheduler errors when trying to schedule Ironic instances:
Sep 13 16:42:48 ubuntu nova-scheduler: a/local/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 147, in select_destinations\n    filter_properties)\n', '  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 300, in _schedule\n    chosen_host.obj.consume_from_instance(context, instance_properties)\n', 'TypeError: consume_from_instance() takes exactly 2 arguments (3 given)\n']"
1779,1369355,cinder,debbf0ee49dcb78a92afaed24a60d81ba441430f,1,1,,Getting iscsi_ip_address from config file,"In the current version (2.0) of the VMAX driver, we retrieve iSCSI IP addresses dynamically from SMI-S.  However, we ran into situations where we can't get them reliably during testing.  The fix is to get this information from cinder.conf, just like in version 1.0."
1780,1369418,cinder,47a26f1ac41100c40b7997bc4df802fa867fbd42,1,1,"“This issue is introduced in patch https://review.openstack.org/#/c/106377/""",Set socket options in correct way,"Currently socket options, socket.SO_REUSEADDR and socket.SO_KEEPALIVE
are set only if SSL is enabled.
Ref: https://github.com/openstack/cinder/blob/master/cinder/wsgi.py#L209
The above socket options should be set no matter SSL is enabled or not.
This issue is introduced in patch https://review.openstack.org/#/c/106377/"
1781,1369431,neutron,bf4a0199a73374d786e3a5bda770fd8545ebc4e9,1,1,,Don't create ipset chain if corresponding security...,"when a security group has bellow rule, it should not create ipset chain:
security group id is: fake_sgid, it has rule bellow:
{'direction': 'ingress', 'remote_group_id': 'fake_sgid2'}
but the security group:fake_sgid2 has no member, so when the port in security group:fake_sgid should not create corresponding ipset chain
root@devstack:/opt/stack/neutron# ipset list
Name: IPv409040f9f-cb86-4f72-a
Type: hash:ip
Revision: 2
Header: family inet hashsize 1024 maxelem 65536
Size in memory: 16520
References: 1
Members:
20.20.20.11
Name: IPv609040f9f-cb86-4f72-a
Type: hash:ip
Revision: 2
Header: family inet6 hashsize 1024 maxelem 65536
Size in memory: 16504
References: 1
Members:
because the security group:09040f9f-cb86-4f72-af74-4de4f2b86442 has no ipv6 member, so it should't create ipset chain:IPv609040f9f-cb86-4f72-a"
1782,1369502,nova,1d91921459e0d67eb668838253dc76371dbdf553,1,1,,NUMA topology _get_constraints_auto assumes flavor...,Resulting in AttributeError: 'dict' object has no attribute 'vcpus' if we try to start with a flavor that will result in Nova trying to decide on an automatic topology (for example providing only number of nodes with hw:numa_nodes extra_spec)
1783,1369508,nova,12b8b56807af7ce3bbe330d73864abc87cdadbf1,1,1,,Instance with NUMA topology causes exception in th...,"This was reported by Michael Turek as he was testing this while the patches were still in flight See: https://review.openstack.org/#/c/114938/26/nova/virt/hardware.py
As described on there - the code there makes a bad assumption about the format in which it will get the data in the scheduler, which results in:
2014-09-15 10:45:44.906 ERROR oslo.messaging.rpc.dispatcher [req-f29a469e-268d-49bf-abfa-0ccb228d768c admin admin] Exception during message handling: An object of type InstanceNUMACell is required here
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     return func(*args, **kwargs)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/scheduler/manager.py"", line 175, in select_destinations
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     filter_properties)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 147, in select_destinations
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     filter_properties)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 300, in _schedule
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     chosen_host.obj.consume_from_instance(context, instance_properties)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/scheduler/host_manager.py"", line 252, in consume_from_instance
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     self, instance)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/hardware.py"", line 978, in get_host_numa_usage_from_instance
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     instance_numa_topology = instance_topology_from_instance(instance)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/hardware.py"", line 949, in instance_topology_from_instance
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     cells=cells)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/base.py"", line 242, in __init__
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     self[key] = kwargs[key]
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/base.py"", line 474, in __setitem__
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     setattr(self, name, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/base.py"", line 75, in setter
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     field_value = field.coerce(self, name, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/fields.py"", line 189, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     return self._type.coerce(obj, attr, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/fields.py"", line 388, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     obj, '%s[%i]' % (attr, index), element)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/fields.py"", line 189, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     return self._type.coerce(obj, attr, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/fields.py"", line 474, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     self._obj_name)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher ValueError: An object of type InstanceNUMACell is required here"
1786,1369605,nova,2192102d85d6d6742832b4961556ad6a2ac7c089,0,0,Feature,nova.db.sqlalchemy.api.quota_reserve is not very u...,"The method is huge and has lots of conditional blocks.
We should break the big conditional blocks out into private methods so the top-level quota_reserve logic can be unit tested on it's own.
This became an issue in this review for a separate bug fix in the logic:
https://review.openstack.org/#/c/121259/"
1787,1369750,cinder,f94d671e627dd7b5143422ffe739418fcfb51a70,1,1,,ZFSSA iSCSI  driver cannot add multple initiators ...,"The zfssa driver is configured by giving a list of initiators and an initiator group
zfssa_initiator = iqn1,iqn2
zfssa_initiator_group = group1
The driver creates the initiator group on the zfssa storage appliance and adds the initiators to the group.
But currently the driver creates the initiators but will only add one initiator to the group. So the others will not be able to
do IO with the volume.
The problem seems to be in the zfssarest.py file add_to_initiatorgroup() function which replaces the existing initiators in the group with the new one instead of adding."
1788,1369815,cinder,ea32e07f36781c2c0c7a0aa28817cc2df23df32d,1,1,,Retype a “None,"Test Step:
1. Create a type-1,  backend IBM storwize SVC, without replication .
2. Create a type-2, backend IBM storwize SVC, with replication = TRUE.
3. Create a volume without type information:
command :　cinder create 1
check the volume status, the information of the volume :
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ffd8f99a-9e17-4943-ac56-eee2e060a86a | available | None |  1   |     None    |  false   |                                      |
The volume type is none.
4. Do Retype to enable replication for the volume with command :
cinder retype ffd8f99a-9e17-4943-ac56-eee2e060a86a type-2
It will always be failed, check the c-vol.log, it was found :
2014-09-16 09:16:01.664 ^[[01;31mERROR cinder.volume.manager [^[[01;36mreq-97d04d55-3d12-4317-bce1-ba61b0edb8d7 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e^[[01;31m] ^[[01;35m^[[01;31mVolume ffd8f99a-9e17-4943-ac56-eee2e060a86a: driver error when trying to retype, falling back to generic mechanism.^[[00m
2014-09-16 09:16:01.665 ^[[01;31mERROR cinder.volume.manager [^[[01;36mreq-97d04d55-3d12-4317-bce1-ba61b0edb8d7 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e^[[01;31m] ^[[01;35m^[[01;31mInvalid volume type: id cannot be None^[[00m
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1410, in retype
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m    host)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m    return f(*args, **kwargs)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 926, in retype
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m    model_update = self.replication.create_replica(ctxt, volume)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/replication.py"", line 73, in create_replica
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m    vol_type = volume_types.get_volume_type(ctxt, vol_type)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/volume_types.py"", line 103, in get_volume_type
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m    raise exception.InvalidVolumeType(reason=msg)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00mInvalidVolumeType: Invalid volume type: id cannot be None
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m
2014-09-16 09:16:01.737 ^[[01;31mERROR oslo.messaging.rpc.dispatcher [^[[01;36mreq-97d04d55-3d12-4317-bce1-ba61b0edb8d7 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e^[[01;31m] ^[[01;35m^[[01;31mException during message handling: Volume migration failed: Retype requires migration but is not allowed.^[[00m
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    incoming.message))
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return self._do_dispatch(endpoint, method, ctxt, args)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    result = getattr(endpoint, method)(ctxt, **new_args)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return f(*args, **kwargs)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1434, in retype
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    raise exception.VolumeMigrationFailed(reason=msg)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mVolumeMigrationFailed: Volume migration failed: Retype requires migration but is not allowed.
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m
It looks the volume type can't be ""none"" if retype to a volume with replication=TRUE.
5. Do retype to try if all the retype are disabled for the ""none"" type volume with command :
cinder retype ffd8f99a-9e17-4943-ac56-eee2e060a86a type-1
Since type-1 replication =false, the operation was successful.
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ffd8f99a-9e17-4943-ac56-eee2e060a86a | available | None |  1   |    type-1   |  false   |                                      |
6. Do retype to enable replication again with command:
 cinder retype ffd8f99a-9e17-4943-ac56-eee2e060a86a type-2
It will be success since the current operation is from type-1 to type-2.
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ffd8f99a-9e17-4943-ac56-eee2e060a86a | available | None |  1   |    type-2   |  false   |                                      |
From the front  test, the retype to enable replication with ""none"" volume type have issues"
1789,1369973,nova,e3fbdfb3aa2251bb57e53b856e6c5c2d89a23150,0,0,Bug in test,libvirt test cases fail due to lockutils error whe...,"When running libvirt tests via testtools.run, there are a number of cases which fail due to lockutils setup
$ .venv/bin/python -m testtools.run nova.tests.virt.libvirt.test_driver
..snip...
======================================================================
ERROR: nova.tests.virt.libvirt.test_driver.IptablesFirewallTestCase.test_multinic_iptables
----------------------------------------------------------------------
pythonlogging:'': {{{INFO [nova.network.driver] Loading network driver 'nova.network.linux_net'}}}
Traceback (most recent call last):
  File ""nova/tests/virt/libvirt/test_driver.py"", line 10182, in test_multinic_iptables
    self.fw.prepare_instance_filter(instance_ref, network_info)
  File ""nova/virt/firewall.py"", line 184, in prepare_instance_filter
    self.refresh_provider_fw_rules()
  File ""nova/virt/firewall.py"", line 474, in refresh_provider_fw_rules
    self._do_refresh_provider_fw_rules()
  File ""nova/openstack/common/lockutils.py"", line 267, in inner
    with lock(name, lock_file_prefix, external, lock_path):
  File ""/usr/lib64/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""nova/openstack/common/lockutils.py"", line 231, in lock
    ext_lock = external_lock(name, lock_file_prefix, lock_path)
  File ""nova/openstack/common/lockutils.py"", line 180, in external_lock
    lock_file_path = _get_lock_path(name, lock_file_prefix, lock_path)
  File ""nova/openstack/common/lockutils.py"", line 171, in _get_lock_path
    raise cfg.RequiredOptError('lock_path')
RequiredOptError: value required for option: lock_path
The tox.ini / run_tests.sh work around this problem by using ""-m nova.openstack.common.lockutils"" but this is somewhat tedious to remember to add. A simple mock addition to the tests in question can avoid the issue in the first place."
1790,1369984,nova,903ebc2b71bbfafc29fb42eebbd87ce7e04605f4,1,1,,NUMA topology checking will not check if instance ...,"When testing weather the instance can fit into the host topology will currently not take into account the number of cells hte instance has, and will only claim matching cells and pass an instance if the matching cells fit.
So for example a 4 NUMA cell isntance would pass the claims test on a 2 NUMA cell host, as long as the first 2 cells fit, without considering that the whole instance will not actually fit."
1791,1370019,nova,27a5296f4d7f045abf9534155081025b214ce088,1,1,,unshelve and resize instance unnecessarily logs ‘i...,"unshelve and resize instance (created by bootable volume) unnecessarily logs ‘image not found’ error/warning messages
In both the cases, it logs following misleading error/warning messages in the compute.log when image_id_or_uri is passed as None to nova/compute/utils->get_image_metadata method.
14-09-05 03:41:54.834 ERROR glanceclient.common.http [req-80c9db2e-cc3d-481c-a5a3-babd917a3698 admin admin] Request returned failure status 404.
14-09-05 03:41:54.834 WARNING nova.compute.utils [req-80c9db2e-cc3d-481c-a5a3-babd917a3698 admin admin]  [instance: d5b137ab-19a1-484a-a828-6a229ec66950] Can't access image : Image  could not be found."
1792,1370068,nova,e517c884eff3c2bb1255dc2bf8e52fd655b1ea1a,1,1,,Numa filter fails to get instance_properties,NUMATopologyFilter tries to get  instance_properties from filter_properties. But in fact instance_properties are in another dictionary (request_spec)  that is embedded in  filter_properties.
1793,1370112,neutron,59d4c264bdcf056eb44d9d137336fde7c00f3a1b,1,1,,NSX plugin should set VNIC_TYPE,"This nova commit: http://git.openstack.org/cgit/openstack/nova/commit/?id=a8a5d44c8aca218f00649232c2b8a46aee59b77e
made VNIC_TYPE a compulsory port bindings attribute.
This broke the NSX plugin which is now not able to boot VMs anymore. Probably other plugins are affected.
Whether VNIC_TYPE is really a required attribute questionable; the fact that port bindings is such a messy interface that can cause this kind of breakages is at least annoying.
Regardless, all plugins must now adapt.
This will also be fixed once a general fix for bug 1370077 is introduced - nevertheless, the NSX plugin can't risk staying broken for more time, and also its 3rd party integration tests are disabled because of this. For this reason we're opening a bug specific for this plugin to fast-track a fix for it."
1794,1370184,nova,8fb62fe8274ef4a8b7e2a811d4c55999246cdb5f,0,0,"Refactoring “is out-of-date, and was’",Ironic driver states file out-of-date,"The current ironic states file, nova/virt/ironic/ironic_states.py, is out-of-date, and was recently updated in ironic with this change:
https://review.openstack.org/118467
Ideally, we should keep these in sync to prevent confusion."
1795,1370265,nova,1dea1cd710d54d4a2a584590e4ccf59dd3a41faa,1,1,,Crash on describing EC2 volume backed image with m...,"EC2 describe images crashes on volume backed instance snapshot which has several volumes:
$ euca-describe-images
euca-describe-images: error (KeyError): Unknown error occurred.
Steps to reproduce
1 Create bootable volume
$ cinder create --image <image-id> <size>
2 Boot instance from volume
$ nova boot --flavor m1.nano --block-device-mapping /dev/vda=<volume_id>:::1 inst
3 Create empty volume
$ cinder create 1
4 Attach the volume to the instance
$ nova volume-attach inst <empty-volume-id> /dev/vdd
5 Create volume backed snapshot
$ nova image-create inst sn-in
6 Describe EC2 images
$ euca-describe-images"
1796,1370308,cinder,ee6299620ec1ce5c5ff22e0888e00a70b840a708,0,0,Bug in test,wrong test for test_create_missing_specs_name in t...,"Since the test is `test_create_missing_specs_name` [1], we should define the body like `body = {'qos_specs': {'a': 'b'}}`. If we use body={'foo': {'a': 'b'}}, the code even do not reach [2], and it is actually raised by a not valid body [3].
[1]: https://github.com/openstack/cinder/blob/master/cinder/tests/api/contrib/test_qos_specs_manage.py#L326
[2]: https://github.com/openstack/cinder/blob/master/cinder/api/contrib/qos_specs_manage.py#L131
[3]: https://github.com/openstack/cinder/blob/master/cinder/api/contrib/qos_specs_manage.py#L127"
1797,1370578,nova,1a17d1719d163a360dfd02f9b124f8dc55a5fd0a,1,1,,Ironic Hostmanager does not pass hypervisor_type t...,"The Ironic Hostmager does not include the compute node hypervisor values such as type, version,  hostname.
Including these values, which are included by the normal HostManager, is needed to allow the capabilities filter to work in a combined Ironic / virt Nova"
1798,1370680,swift,38ba5790fb527967c2fcbaf094e76a73f4b94d38,1,1,,swift ring builder shows nasty stacktrace if build...,"swift-ring-builder command shows nasty stacktraces if the builder file is empty or invalid.
If, its empty file, throws EOF error. if a file is corrupted, throws unpickling error.
Also, during these cases throws an error code of 1, which is wrong since the chosen values for swift-ring-builder is
Exit codes: 0 = operation successful
            1 = operation completed with warnings
            2 = error
-- empty file--
[keshava@Kbook tmp]$ >object.builder
[keshava@Kbook tmp]$ swift-ring-builder object.builder
Traceback (most recent call last):
  File ""/usr/local/bin/swift-ring-builder"", line 6, in <module>
    exec(compile(open(__file__).read(), __file__, 'exec'))
  File ""/opt/stack/swift/bin/swift-ring-builder"", line 24, in <module>
    sys.exit(main())
  File ""/opt/stack/swift/swift/cli/ringbuilder.py"", line 834, in main
    builder = RingBuilder.load(builder_file)
  File ""/opt/stack/swift/swift/common/ring/builder.py"", line 991, in load
    builder = pickle.load(open(builder_file, 'rb'))
EOFError
[keshava@Kbook tmp]$ echo $?
1
-- a corrupted file --
[keshava@Kbook tmp]$ swift-ring-builder object.builder
Traceback (most recent call last):
  File ""/usr/local/bin/swift-ring-builder"", line 6, in <module>
    exec(compile(open(__file__).read(), __file__, 'exec'))
  File ""/opt/stack/swift/bin/swift-ring-builder"", line 24, in <module>
    sys.exit(main())
  File ""/opt/stack/swift/swift/cli/ringbuilder.py"", line 834, in main
    builder = RingBuilder.load(builder_file)
  File ""/opt/stack/swift/swift/common/ring/builder.py"", line 991, in load
    builder = pickle.load(open(builder_file, 'rb'))
cPickle.UnpicklingError: invalid load key, 'n'.
[keshava@Kbook tmp]$ echo $?
1
The error codes needs to be corrected, and also a meaningful user information should be provided if a file is empty or corrupted"
1799,1370769,glance,1cdc555d21c532d7e4963ed1784a488769abdd1b,1,1,,Ensure all metadata definition code uses six.itert...,"Similar to https://review.openstack.org/#/c/95467/
According to https://wiki.openstack.org/wiki/Python3 dict.iteritems()
should be replaced with six.iteritems(dict).
All metadata definition code added should ensure that six.iteritems is used."
1800,1370885,nova,027a913a304f8e7000fa3e8ede7776e250a029fb,1,1,,The log info is error in the method '_sync_instanc...,"In the method '_sync_instance_power_state', the log info is wrong.
if self.host != db_instance.host:
            # on the sending end of nova-compute _sync_power_state
            # may have yielded to the greenthread performing a live
            # migration; this in turn has changed the resident-host
            # for the VM; However, the instance is still active, it
            # is just in the process of migrating to another host.
            # This implies that the compute source must relinquish
            # control to the compute destination.
            LOG.info(_(""During the sync_power process the ""
                       ""instance has moved from ""
                       ""host %(src)s to host %(dst)s"") %
                       {'src': self.host,
                        'dst': db_instance.host},
                     instance=db_instance)
            return
The 'src' value should be 'db_instance.host'and the 'dst' value should be the 'self.host'.  The method '_post_live_migration' should be invoked after the live migration completes and it is used to update the database.
In the situation, the instance has been migrated to another host successfully and the database has not been updated. The '_sync_instance_power_state' method is executed. Nova can list it in the dst host with the driver and the data in the database should be the source host."
1801,1370914,neutron,e90d6d6c833db51964037afd1c4aa1f521ee380a,1,1,,Bug #1370914 “When two ovs ports contain same external_ids,"As the title says, if there are 2 different ovs ports with the same external_ids:iface-id field (which is the port_id), when at least one of them is managed by the ovs agent, it might fail finding the correct one if they are not connected to the same bridge.
Steps to reproduce:
1. Create a router with an internal port to some Neutron network
2. Find the port in 'ovs-vsctl show'
3. Use the following command to find the port_id in ovs: sudo ovs-vsctl  --columns=external_ids list Interface <port_name>
4. Use the following commands to create a new port with the same field in a new bridge:
 sudo ovs-vsctl add-br a
 sudo ip link add dummy12312312 type dummy
 sudo ovs-vsctl add-port br-a dummy12312312
 sudo ovs-vsctl set Interface dummy12312312 external_ids:iface-id=""<port_id>"" # port_id was obtained in point 3.
5. Restart the ovs agent.
At this point the ovs agent's log should show ""Port: dummy12312312 is on br-a, not on br-int"".
Expected result: ovs agent should know to iterate though the options and find the correct port in the correct bridge."
1802,1370999,nova,d65ca49990f1241a5a8afdc3ad0e1a99c878275a,1,0,"Platform. “The windows nova-agent now can trigger a gust reboot during resetnetwork, so the hostname is correctly updated.
“",Bug #1370999 “xenapi,"The windows nova-agent now can trigger a gust reboot during resetnetwork, so the hostname is correctly updated.
Also there was always a reboot during the first stages of polling for the agent version that can cause the need to wait for a call to timeout, rather than detecting a reboot.
Either way, we need to take more care to detect reboots while talking to the agent."
1803,1371022,cinder,08bfa77aeccb8ca589e3fb5cf9771879818f59de,1,1,"(21eb376df58d48ac5b7d57224484f6db1bba6114) There is a bug but I don’t know whether there is a BIC or not “Add a parameter to take advantage of the new(ish) eventlet socket timeout behaviour”. Bug! I would say that there is an omission in the original code, that is solved by adding a parameter",Idle client connections can persist indefinitely,"Idle client socket connections can persist forever, eg:
$ nc localhost 8776
[never returns]"
1804,1371072,nova,3f4a0e40af7705b037db1a2ccc63de94547f6dd7,0,0,cleanup. “it would be good to clean up older snapshots”,Bug #1371072 “xenapi,"When nova-compute gets forcably restarted, or fails, we get left over snapshots.
We have some clean up code for after nova-compute comes back up, but it would be good to clean up older snapshots, and generally try to minimize the size of the snapshot that goes to glance."
1805,1371160,neutron,f92d1300e1aca68b39b927282af038acc68c5013,1,1,,HTTP 500 while retrieving metadata by non-existent...,"HTTP 500 error occurs when one tries to get metadata by path constructed from folder name with appended value.
Steps to repro:
1. Launch VM and access its terminal
2. curl http://169.254.169.254/latest/meta-data/instance-id  -- this returns some string, i.e. i-00000001
3. curl http://169.254.169.254/latest/meta-data/instance-id/i-00000001  -- this returns HTTP 500
It's expected that the last call returns meaningful message and not produce trace backs in logs.
Errors:
----------
In VM terminal:
$ curl http://169.254.169.254/latest/meta-data/instance-id/i-00000001
<html>
 <head>
  <title>500 Internal Server Error</title>
 </head>
 <body>
  <h1>500 Internal Server Error</h1>
  Remote metadata server experienced an internal server error.<br /><br />
 </body>
</html>$
In Neutron metadata agent:
2014-09-18 14:44:37.563 ERROR neutron.agent.metadata.agent [-] Unexpected error.
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent Traceback (most recent call last):
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent   File ""/opt/stack/neutron/neutron/agent/metadata/agent.py"", line 130, in __call__
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent     return webob.exc.HTTPNotFound()
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent   File ""/opt/stack/neutron/neutron/agent/metadata/agent.py"", line 248, in _proxy_request
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent     def _sign_instance_id(self, instance_id):
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent Exception: Unexpected response code: 400
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent
2014-09-18 14:44:37.566 INFO eventlet.wsgi.server [-] 10.0.0.2,<local> - - [18/Sep/2014 14:44:37] ""GET /latest/meta-data/instance-id/i-00000001 HTTP/1.1"" 500 229 0.348877
In Nova API service:
2014-09-18 14:31:19.030 ERROR nova.api.ec2 [req-5c84e0ae-7d18-4113-a08b-ed068e5333ed None None] FaultWrapper: string indices must be integers, not unicode
2014-09-18 14:31:19.030 TRACE nova.api.ec2 Traceback (most recent call last):
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/ec2/__init__.py"", line 87, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2     return req.get_response(self.application)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-09-18 14:31:19.030 TRACE nova.api.ec2     application, catch_exc_info=False)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-09-18 14:31:19.030 TRACE nova.api.ec2     app_iter = application(self.environ, start_response)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2     resp = self.call_func(req, *args, **self.kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-09-18 14:31:19.030 TRACE nova.api.ec2     return self.func(req, *args, **kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/ec2/__init__.py"", line 99, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2     rv = req.get_response(self.application)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-09-18 14:31:19.030 TRACE nova.api.ec2     application, catch_exc_info=False)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-09-18 14:31:19.030 TRACE nova.api.ec2     app_iter = application(self.environ, start_response)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2     resp = self.call_func(req, *args, **self.kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-09-18 14:31:19.030 TRACE nova.api.ec2     return self.func(req, *args, **kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/metadata/handler.py"", line 128, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2     data = meta_data.lookup(req.path_info)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/metadata/base.py"", line 418, in lookup
2014-09-18 14:31:19.030 TRACE nova.api.ec2     data = self.get_ec2_item(path_tokens[1:])
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/metadata/base.py"", line 300, in get_ec2_item
2014-09-18 14:31:19.030 TRACE nova.api.ec2     return find_path_in_tree(data, path_tokens[1:])
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/metadata/base.py"", line 565, in find_path_in_tree
2014-09-18 14:31:19.030 TRACE nova.api.ec2     data = data[path_tokens[i]]
2014-09-18 14:31:19.030 TRACE nova.api.ec2 TypeError: string indices must be integers, not unicode
2014-09-18 14:31:19.030 TRACE nova.api.ec2
2014-09-18 14:31:19.032 INFO nova.metadata.wsgi.server [req-5c84e0ae-7d18-4113-a08b-ed068e5333ed None None] 10.0.0.2,172.18.76.77 ""GET /latest/meta-data/placement/availability-zone/nova HTTP/1.1"" status: 400 len: 246 time: 0.5495760"
1806,1371696,neutron,7ca696c7355ba93dd89e69a76162c6d9a3429a2b,1,1,,Bug #1371696 “Cannot add or update a child row,"We've hit this foreign key constraint error.  This is due to a sync message coming in from the L3 agent over RPC.  The message contains n update for  a port that has just been deleted.  This is just log noise because it all gets worked out quickly following the error.  The full trace is here [1].
2014-09-18 21:22:39.735 29984 TRACE oslo.messaging.rpc.dispatcher DBReferenceError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`neutron`.`ml2_dvr_port_bindings`, CONSTRAINT `ml2_dvr_port_bindings_ibfk_1` FOREIGN KEY (`port_id`) REFERENCES `ports` (`id`) ON DELETE CASCADE)') 'INSERT INTO ml2_dvr_port_bindings (port_id, host, router_id, vif_type, vif_details, vnic_type, profile, cap_port_filter, driver, segment, status) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)' ('0fe7b532-343e-4ba0-83d9-c51b1c55f533', 'devstack-trusty-hpcloud-b4-2246632', '45248fa2-4372-4ad9-8e60-afabe39c6f6a', 'unbound', '', 'normal', '', 0, None, None, 'DOWN')
[1] http://paste.openstack.org/show/113360/"
1807,1371732,neutron,411836b5411411a6046043e0264aaa7b6f5760f0,1,1,,create/update_port failure resulting in Lock wait ...,"create_port can fail with a Lock wait timeout error.
The transaction performed in create_port/update_port makes a call to _process_port_bindings which then calls dvr_update_router_addvm.  The notification made within dvr_update_router_addvm can hang.  The transaction lock is held through the entire hang preventing other create_port  threads from getting the lock and then timing out with a ""Lock wait timeout.""
2014-09-19 04:16:53.332 3391 TRACE neutron.api.v2.resource OperationalError: (OperationalError) (1205, 'Lock wait timeout exceeded; try restarting transaction') 'SELECT ipavailabilityranges.allocation_pool_id AS ipavailabilityranges_allocation_pool_id, ipavailabilityranges.first_ip AS ipavailabilityranges_first_ip, ipavailabilityranges.last_ip AS ipavailabilityranges_last_ip \nFROM ipavailabilityranges INNER JOIN ipallocationpools ON ipallocationpools.id = ipavailabilityranges.allocation_pool_id \nWHERE ipallocationpools.subnet_id = %s \n LIMIT %s FOR UPDATE' ('550ee3a3-6c4d-4a4a-b173-b7603e43356a', 1)"
1808,1372076,neutron,086496bfc45e01cd2905a074d526a7d513bf4ec2,0,0,Bug in test,OVS Agent unit tests waste time waiting for timeou...,"The OVS Agent unit tests do not prevent it from trying to report its state which eats up >10 seconds per unit test.
Traceback (most recent call last):
  File ""neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 260, in _report_state
    self.use_call)
  File ""neutron/agent/rpc.py"", line 70, in report_state
    return self.call(context, msg)
  File ""neutron/common/log.py"", line 34, in wrapper
    return method(*args, **kwargs)
  File ""neutron/common/rpc.py"", line 161, in call
    context, msg, rpc_method='call', **kwargs)
  File ""neutron/common/rpc.py"", line 187, in __call_rpc_method
    return func(context, msg['method'], **msg['args'])
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/client.py"", line 389, in call
    return self.prepare().call(ctxt, method, **kwargs)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/client.py"", line 152, in call
    retry=self.retry)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/transport.py"", line 90, in _send
    timeout=timeout, retry=retry)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/_drivers/impl_fake.py"", line 194, in send
    return self._send(target, ctxt, message, wait_for_reply, timeout)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/_drivers/impl_fake.py"", line 186, in _send
    'No reply on topic %s' % target.topic)
MessagingTimeout: No reply on topic q-plugin"
1809,1372196,neutron,d1f1722e0edb63c73b60c80abafa63749349cd8e,1,1,0,Bug #1372196 “ofagent,"ofagent's arp responder has some LOG.info which can be triggered by tenant's OSes.
they allow bad tenants flood host logs."
1810,1372337,neutron,55f6a8ac5d234f004ef06add87d16284e9f048d3,1,1,“The patch fixes a regression introduced with secgroup rpc refactor by”,KeyError in neutron server when L2 agent requests ...,"if L2 agent uses enhanced-security-group-rpc, in bellow case there will be a KeyError in neutron server:
1. Create security group with IPv6 ingress rule but no IPv4 ingress rule.
  (or delete IPv4 ingress rule from default security group)
2. Launch a VM on an IPv4 subnet, making it member of sec group created earlier
Instance will not get its network info. Neutron server starts reporting following errors and sends them to agent on each request for devices info:
2014-09-24 02:01:51.353 ERROR oslo.messaging.rpc.dispatcher [req-9b631b65-a753-4292-8442-98936a31db74 None None] Exception during message handling: 'IPv4'
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/neutron/neutron/api/rpc/handlers/securitygroups_rpc.py"", line 75, in security_group_info_for_devices
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     return self.plugin.security_group_info_for_ports(context, ports)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/neutron/neutron/db/securitygroups_rpc_base.py"", line 201, in security_group_info_for_ports
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     return self._get_security_group_member_ips(context, sg_info)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/neutron/neutron/db/securitygroups_rpc_base.py"", line 209, in _get_security_group_member_ips
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     if ip not in sg_info['sg_member_ips'][sg_id][ethertype]:
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher KeyError: 'IPv4'
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher
2014-09-24 02:01:51.354 ERROR oslo.messaging._drivers.common [req-9b631b65-a753-4292-8442-98936a31db74 None None] Returning exception 'IPv4' to caller"
1811,1372438,neutron,3cd2163d5105faad389bee5175ef446f0bb90289,1,1,,Race condition in l2pop drops tunnels,"The issue was originally raised by a Red Hat performance engineer (Joe Talerico)  here: https://bugzilla.redhat.com/show_bug.cgi?id=1136969 (see starting from comment 4).
Joe created a Fedora instance in his OS cloud based on RHEL7-OSP5 (Icehouse), where he installed Rally client to run benchmarks against that cloud itself. He assigned a floating IP to that instance to be able to access API endpoints from inside the Rally machine. Then he ran a scenario which basically started up 100+ new instances in parallel, tried to access each of them via ssh, and once it succeeded, clean up each created instance (with its ports). Once in a while, his Rally instance lost connection to outside world. This was because VXLAN tunnel to the compute node hosting the Rally machine was dropped on networker node where DHCP, L3, Metadata agents were running. Once we restarted OVS agent, the tunnel was recreated properly.
The scenario failed only if L2POP mechanism was enabled.
I've looked thru the OVS agent logs and found out that the tunnel was dropped due to a legitimate fdb entry removal request coming from neutron-server side. So the fault is probably on neutron-server side, in l2pop mechanism driver.
I've then looked thru the patches in Juno to see whether there is something related to the issue already merged, and found the patch that gets rid of _precommit step when cleaning up fdb entries. Once we've applied the patch on the neutron-server node, we stopped to experience those connectivity failures.
After discussion with Vivekanandan Narasimhan, we came up with the following race condition that may result in tunnels being dropped while legitimate resources are still using them:
(quoting Vivek below)
'''
- - port1 delete request comes in;
- - port1 delete request acquires lock
- - port2 create/update request comes in;
- - port2 create/update waits on due to unavailability of lock
- - precommit phase for port1 determines that the port is the last one, so we should drop the FLOODING_ENTRY;
- - port1 delete applied to db;
- - port1 transaction releases the lock
- - port2 create/update acquires the lock
- - precommit phase for port2 determines that the port is the first one, so request FLOODING_ENTRY + MAC-specific flow creation;
- - port2 create/update request applied to db;
- - port2 transaction releases the lock
Now at this point postcommit of either of them could happen, because code-pieces operate outside the
locked zone.
If it happens, this way, tunnel would retain:
- - postcommit phase for port1 requests FLOODING_ENTRY deletion due to port1 deletion
- - postcommit phase requests FLOODING_ENTRY + MAC-specific flow creation for port2;
If it happens the below way, tunnel would break:
- - postcommit phase for create por2 requests FLOODING_ENTRY + MAC-specific flow
- - postcommit phase for delete port1 requests FLOODING_ENTRY deletion
'''
We considered the patch to get rid of precommit for backport to Icehouse [1] that seems to eliminate the race, but we're concerned that we reverted that to previous behaviour in Juno as part of DVR work [2], though we haven't done any testing to check whether the issue is present in Juno (though brief analysis of the code shows that it should fail there too).
Ideally, the fix for Juno should be easily backportable because the issue is currently present in Icehouse, and we would like to have the same fix for both branches (Icehouse and Juno) instead of backporting patch [1] to Icehouse and implementing another patch for Juno.
[1]: https://review.openstack.org/#/c/95165/
[2]: https://review.openstack.org/#/c/102398/"
1812,1372454,cinder,dafe048f2f36d221242036c159271d7544ad39e3,0,0,Feature “we need to to check the value of the configuration item”,check the value of the configuration item eqlx_cli...,"we need to to check the value of the configuration item glance_num_retries in the code in order to ensure the ""eqlx_cli_max_retries "" equal or bigger than 0"
1813,1372570,neutron,32ea2e349decd750e25cb00a8c907b8f73f795f3,1,1,,Booting multiple instances causes race with port s...,"On a fresh installation, without spinning up any instance, if one boots a number of instances via nova there are a number of default security groups created for the admin user:
mysql> select * from securitygroups;
+----------------------------------+--------------------------------------+---------+-------------+
| tenant_id                        | id                                   | name    | description |
+----------------------------------+--------------------------------------+---------+-------------+
| 67e87d2ac4944c3eb93283637b6e4000 | 0fe7f583-7d9e-4be0-9ade-6a4ee9a07daa | default | default     |
| 67e87d2ac4944c3eb93283637b6e4000 | 1a237a3c-39c3-4a92-bd7b-55a3bc5ef7e9 | default | default     |
| 67e87d2ac4944c3eb93283637b6e4000 | 51c83ff8-97f9-426d-836e-10bf631219e4 | default | default     |
| 67e87d2ac4944c3eb93283637b6e4000 | 8882d943-4309-410c-bd56-981da8d5d095 | default | default     |
| 67e87d2ac4944c3eb93283637b6e4000 | 893b2462-800a-4936-9b35-fc47822237e9 | default | default     |
| d639ee58de4f4db7b29f30a7b142a047 | baf7ac95-efaf-427d-8fd9-f8210d23cab1 | default | default     |
+----------------------------------+--------------------------------------+---------+-------------+
This should never happen as the admin user is not even spinning up the vm's...."
1814,1372571,neutron,c756d3c3ca9e90fab5e1d3ff37af33972747ba70,0,0,Bug in test,Neutron cannot possibly pass unit tests under Pyth...,"On neutron/tests/unit/test_api_v2.py:148, _get_collection_kwargs() uses collections.OrderedDict, which does not exist in Python 2.6.  As the ordering here is clearly unimportant, this should be converted to be just a plain dict."
1815,1372672,nova,aafe39cd94c7311ae6d02a6b98cfd09d70ae0b56,1,1,,Bug #1372672 “VMware,"There are a couple of places in the driver where we use the keys() method without checking for None.
I have seen several times the following exception:
2014-09-22 11:45:07.312 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: 'NoneType' object has no attribute 'keys'
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 198, in run_periodic_tasks
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task     task(self, context)
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/compute/manager.py"", line 5909, in update_available_resource
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task     nodenames = set(self.driver.get_available_nodes())
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 426, in get_available_nodes
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task     self._update_resources()
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 306, in _update_resources
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task     added_nodes = set(self.dict_mors.keys()) - set(self._resource_keys)
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task AttributeError: 'NoneType' object has no attribute 'keys'"
1817,1373100,neutron,45a523681f2136f8fefb6c3da44540decd6a0fda,1,1,“1373100” revert,New race condition exposed when cleaning up floati...,"The patch that cleans up floating ips on router deletion [1] has triggered a race condition that causes spurious failures in the dvr job in the check queue.  Reverting this patch [2] has shown to stabilize it.
[1] https://review.openstack.org/#/c/120885/
[2] https://review.openstack.org/#/c/121729/"
1818,1373159,nova,6a374f21495c12568e4754800574e6703a0e626f,1,1,Requirements were not fulfill,"NUMA Topology cell memory sent to xml in MiB, but ...","Currently when specifying NUMA cell memory via flavor extra_specs or image properties, MiB units are used. According to the libvirt xml domain format documentation (http://libvirt.org/formatdomain.html) , cell memory should be specified in KiB.
In this example, we use the following extra_specs:
""hw:numa_policy"": ""strict"", ""hw:numa_mem.1"": ""2048"", ""hw:numa_mem.0"": ""6144"", ""hw:numa_nodes"": ""2"", ""hw:numa_cpus.0"": ""0,1,2"", ""hw:numa_cpus.1"": ""3""
The flavor has 8192 MB of ram and 4 vcpus.
When using qemu 2.1.0, the following will be seen in the n-cpu logs when booting a machine with NUMA specs.
""libvirtError: internal error: process exited while connecting to monitor: qemu-system-x86_64: total memory for NUMA nodes (8388608) should equal RAM size (200000000)""
Please note that the 200000000 is 8388608 KiB in bytes and hex (simply an issue with the qemu error message). The error shows that 8192 KiB is being requested rather than 8192 MiB. Because the RAM size does not equal the total memory size, the machine fails to boot.
When using versions of qemu lower than 2.1.0 the issue is not obvious, as machines with  NUMA specs boot, but only because of a bug (that has since been resolved) in qemu. This is because the check to ensure that RAM size equals the NUMA node total memory does not happen in versions lower than 2.1.0
In short, we should be using KiB units for NUMA cell memory, or at least be converting from MiB to KiB before creating the xml. Otherwise, NUMA placement will not behave as intended.
To be fair, I haven't had the chance to look at the memory placement in a guest booted using qemu 2.0.0 or lower, though I suspect the memory placement would be incorrect.. If anyone has the chance to look, it would be greatly appreciated.
I am currently investigating the appropriate fix for this alongside Tiago Mello. We made a quick fix in /nova/virt/libvirt/config.py on line 495:
                cell.set(""memory"", str(self.memory * 1024))
Mutiplying by 1024 allowed the machine to properly boot, but it is probably a bit too quick and dirty. Just thought it would be worth mentioning.
Sys-info:
x86_64 machine
Virt-info:
qemu version 2.1.0
libvirt version 1.2.2
Kenerl-info:
3.13.0-35-generic #62-Ubuntu SMP Fri Aug 15 01:58:42 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
OS-info:
Distributor ID:	Ubuntu
Description:	Ubuntu 14.04.1 LTS
Release:	14.04
Codename:	trusty"
1819,1373230,nova,42c7c7cfc96045930820c37b45f54ba717da117e,1,1,,start/stop instance in EC2 API shouldn't return ac...,"Always see this error in the gate:
http://logs.openstack.org/73/122873/1/gate/gate-tempest-dsvm-neutron-full/e5a2bf6/logs/screen-n-cpu.txt.gz?level=ERROR#_2014-09-21_05_18_23_709
014-09-21 05:18:23.709 ERROR oslo.messaging.rpc.dispatcher [req-52e7fee5-65ee-4c4d-abcc-099b29352846 InstanceRunTest-2053569555 InstanceRunTest-179702724] Exception during message handling: Unexpected task state: expecting [u'powering-off'] but the actual state is deleting
Checking the EC2 API test in tempest,
    def test_run_stop_terminate_instance(self):
        # EC2 run, stop and terminate instance
        image_ami = self.ec2_client.get_image(self.images[""ami""]
                                              [""image_id""])
        reservation = image_ami.run(kernel_id=self.images[""aki""][""image_id""],
                                    ramdisk_id=self.images[""ari""][""image_id""],
                                    instance_type=self.instance_type)
        rcuk = self.addResourceCleanUp(self.destroy_reservation, reservation)
        for instance in reservation.instances:
            LOG.info(""state: %s"", instance.state)
            if instance.state != ""running"":
                self.assertInstanceStateWait(instance, ""running"")
        for instance in reservation.instances:
            instance.stop()
            LOG.info(""state: %s"", instance.state)
            if instance.state != ""stopped"":
                self.assertInstanceStateWait(instance, ""stopped"")
        self._terminate_reservation(reservation, rcuk)
The test is wait for instance become to stopped. But check the ec2 api code
https://github.com/openstack/nova/blob/master/nova/api/ec2/cloud.py#L1075
it always return stopped status immediately. Actually start/stop action is async call."
1820,1373333,glance,4b2304d6bb1d77db1e411f375a00a93aaabca262,1,1,,glance-replicator livecopy cannot work due to Bad ...,"I would like to use ""glance-replicator livecopy"" command to replicate the images from one openstack instance to anthor, it goes to error while running the livecopy command:
2014-09-24 17:32:09.082 24953 CRITICAL glance [-] ServerErrorException: 400 Bad Request
The server could not comply with the request since it is either malformed or otherwise incorrect.
 Bad header: x-image-meta-virtual-size
2014-09-24 17:32:09.082 24953 TRACE glance Traceback (most recent call last):
2014-09-24 17:32:09.082 24953 TRACE glance   File ""/usr/bin/glance-replicator"", line 10, in <module>
2014-09-24 17:32:09.082 24953 TRACE glance     sys.exit(main())
2014-09-24 17:32:09.082 24953 TRACE glance   File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 739, in main
2014-09-24 17:32:09.082 24953 TRACE glance     command(options, args)
2014-09-24 17:32:09.082 24953 TRACE glance   File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 516, in replication_livecopy
2014-09-24 17:32:09.082 24953 TRACE glance     image_response)
2014-09-24 17:32:09.082 24953 TRACE glance   File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 238, in add_image
2014-09-24 17:32:09.082 24953 TRACE glance     response = self._http_request('POST', url, headers, image_data)
2014-09-24 17:32:09.082 24953 TRACE glance   File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 125, in _http_request
2014-09-24 17:32:09.082 24953 TRACE glance     raise ServerErrorException(response.read())
2014-09-24 17:32:09.082 24953 TRACE glance ServerErrorException: 400 Bad Request
2014-09-24 17:32:09.082 24953 TRACE glance
2014-09-24 17:32:09.082 24953 TRACE glance The server could not comply with the request since it is either malformed or otherwise incorrect.
2014-09-24 17:32:09.082 24953 TRACE glance
2014-09-24 17:32:09.082 24953 TRACE glance  Bad header: x-image-meta-virtual-size
2014-09-24 17:32:09.082 24953 TRACE glance"
1821,1373524,neutron,5be55ac161adfec9085121fe59ea3bf59daa92cf,1,1,,dvr snat delete binding changed,"When router_gateway_clear happens, the schedule_router calls the unbind_snat_servicenode in the plugin.  This will clear the agent binding from the binding table.  But the l3-agent was expecting the ex_gw_port binding to be present. The agent needs to check its cache of the router['gw_host_port'] value now.  SNAT namespaces will not be deleted in all cases without this fix."
1822,1373535,nova,279b6e98bc72c906ac2c3f8665a1acdc99b30833,1,1,"“Commit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make the obj_make_compatible calls consistent,”",obj_make_compatible is wrong,"Commit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make the obj_make_compatible calls consistent, but it actually changed them the wrong way.
Change https://review.openstack.org/#/c/121663/ addresses the bug but is sitting on top of a change that might be too risky at this point for juno-rc1, so this should be a separate fix."
1823,1373851,neutron,04df85b6e5a098f8f55bb82f04d9769763beb487,0,0,Optimization “ptimize a query in _get_lla_gateway_ip_for_subnet”,security groups db queries load excessive data,The security groups db queries are loading extra data from the ports table that is unnecessarily hindering performance.
1824,1373950,nova,48e94bf75ce2be50d323e8b883cf3322c4d06c4e,1,1,,Serial proxy service  and API broken by design,"As part of the blueprint https://blueprints.launchpad.net/nova/+spec/serial-ports we introduced an API extension and a websocket proxy binary. The problem with the 2 is that a lot of the stuff was copied verbatim from the novnc-proxy API and service which relies heavily on the internal implementation details of NoVNC and python-websockify libraries.
We should not ship a service that will proxy websocket traffic if we do not acutally serve a web-based client for it (in the NoVNC case, it has it's own HTML5 VNC implementation that works over ws://). No similar thing was part of the proposed (and accepted) implementation. The websocket proxy based on websockify that we currently have actually assumes it will serve static content (which we don't do for serial console case) which will then when excuted in the browser initiate a websocket connection that sends the security token in the cookie: field of the request. All of this is specific to the NoVNC implementation (see: https://github.com/kanaka/noVNC/blob/e4e9a9b97fec107b25573b29d2e72a6abf8f0a46/vnc_auto.html#L18) and does not make any sense for serial console functionality.
The proxy service was introduced in https://review.openstack.org/#/c/113963/
In a similar manner - the API that was proposed and implemented (in https://review.openstack.org/#/c/113966/) that gives us back the URL with the security token makes no sense for the same reasons outlined above.
We should revert at least these 2 patches before the final Juno release as we do not want to ship a useless service and commit to a useles API method.
We could then look into providing similar functionality through possibly something like https://github.com/chjj/term.js which will require us to write a different proxy service."
1825,1373993,nova,30871e8702737edbbfbcbbb5f21858873b37685c,1,0,Library “This should be changed to use the requests lib.”,Trusted Filter uses unsafe SSL connection,"HTTPSClientAuthConnection uses httplib.HTTPSConnection objects. In Python 2.x those do not perform CA checks so client connections are vulnerable to MiM attacks.
This should be changed to use the requests lib."
1826,1374044,neutron,acfcb523b15fbd9ccc509e4366e4a141a66d4783,1,1,,_make_subnet_dict lazy loads not required attibute...,"The get_active_networks_info rpc call causes high number of sql query.
For example the following query
SELECT subnetroutes.destination AS subnetroutes_destination, subnetroutes.nexthop AS subnetroutes_nexthop, subnetroutes.subnet_id AS subnetroutes_subnet_id
FROM subnetroutes
WHERE %s = subnetroutes.subnet_id
was used on this trace:
  File ""/usr/lib/python2.7/site-packages/eventlet/greenpool.py"", line 82, in _spawn_n_impl
    func(*args, **kwargs)
  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 462, in _process_data
    **args)
  File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
    neutron_ctxt, version, method, namespace, **kwargs)
  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/new/neutron/neutron/db/dhcp_rpc_base.py"", line 92, in get_active_networks_info
    networks = self._get_active_networks(context, **kwargs)
  File ""/opt/stack/new/neutron/neutron/db/dhcp_rpc_base.py"", line 42, in _get_active_networks
    plugin.auto_schedule_networks(context, host)
  File ""/opt/stack/new/neutron/neutron/db/agentschedulers_db.py"", line 211, in auto_schedule_networks
    self.network_scheduler.auto_schedule_networks(self, context, host)
  File ""/opt/stack/new/neutron/neutron/scheduler/dhcp_agent_scheduler.py"", line 114, in auto_schedule_networks
    subnets = plugin.get_subnets(context, fields=fields)
  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 1333, in get_subnets
    page_reverse=page_reverse)
  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 209, in _get_collection
    items = [dict_func(c, fields) for c in query]
  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 931, in _make_subnet_dict
    for route in subnet['routes']],
  File ""/opt/stack/new/neutron/neutron/openstack/common/db/sqlalchemy/models.py"", line 57, in __getitem__
    return getattr(self, key)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/attributes.py"", line 237, in __get__
    return self.impl.get(instance_state(instance), dict_)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/attributes.py"", line 590, in get
    value = self.callable_(state, passive)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/strategies.py"", line 529, in _load_for_state
    return self._emit_lazyload(session, state, ident_key, passive)
  File ""<string>"", line 1, in <lambda>
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/strategies.py"", line 598, in _emit_lazyload
    result = q.all()
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2363, in all
    return list(self)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2480, in __iter__
    return self._execute_and_instances(context)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2495, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/engine/base.py"", line 730, in execute
    return meth(self, multiparams, params)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/sql/elements.py"", line 322, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/engine/base.py"", line 827, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/engine/base.py"", line 913, in _execute_context
    tb = str(traceback.format_stack())
1. dhcp_agent_scheduler.py explicitly specifies he is interested only in two fields ['network_id', 'enable_dhcp']
https://github.com/openstack/neutron/blob/ee4f94e32c8422eb5785f0bb1eb39b94b4e9b064/neutron/scheduler/dhcp_agent_scheduler.py#L103
2. _get_collection makes lazy query (not fetching every related data)
3.  make_subnet_dict  forces the ORM to post load not required filed
https://github.com/openstack/neutron/blob/ee4f94e32c8422eb5785f0bb1eb39b94b4e9b064/neutron/db/db_base_plugin_v2.py#L830
4. The self._fields  drops the lazy fetched fields.
So the Db api provides the interface for efficient selective DB load, but at the and it does one of the most inefficient thing is doable on listing. Issues new SQL SELECt statements  per row/object.
Looks like all function does similar thing which uses the self._field method.
The make dict methods MUST NOT try to fetch the not requested fields from the data object at all.
In this case the post _filter method also a not needed."
1827,1374140,nova,1af3ca69c250f8cd2f8e60ed596ad2df30353713,1,1,,Need to log the orignial libvirtError when Interfa...,"This is not really useful:
http://logs.openstack.org/17/123917/2/check/check-tempest-dsvm-neutron/4bc2052/logs/screen-n-cpu.txt.gz?level=TRACE#_2014-09-25_17_35_11_635
2014-09-25 17:35:11.635 ERROR nova.virt.libvirt.driver [req-50afcbfb-203e-454d-a7eb-1549691caf77 TestNetworkBasicOps-985093118 TestNetworkBasicOps-1055683132] [instance: 960ee0b1-9c96-4d5b-b5f5-be76ae19a536] detaching network adapter failed.
2014-09-25 17:35:11.635 27689 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: <nova.objects.instance.Instance object at 0x422fe90>
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 393, in decorated_function
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 4411, in detach_interface
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     self.driver.detach_interface(instance, condemned)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 1448, in detach_interface
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     raise exception.InterfaceDetachFailed(instance)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher InterfaceDetachFailed: <nova.objects.instance.Instance object at 0x422fe90>
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher
The code is logging that there was an error, but not the error itself:
        try:
            self.vif_driver.unplug(instance, vif)
            flags = libvirt.VIR_DOMAIN_AFFECT_CONFIG
            state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
            if state == power_state.RUNNING or state == power_state.PAUSED:
                flags |= libvirt.VIR_DOMAIN_AFFECT_LIVE
            virt_dom.detachDeviceFlags(cfg.to_xml(), flags)
        except libvirt.libvirtError as ex:
            error_code = ex.get_error_code()
            if error_code == libvirt.VIR_ERR_NO_DOMAIN:
                LOG.warn(_LW(""During detach_interface, ""
                             ""instance disappeared.""),
                         instance=instance)
            else:
                LOG.error(_LE('detaching network adapter failed.'),
                         instance=instance)
                raise exception.InterfaceDetachFailed(
                        instance_uuid=instance['uuid'])
We should log the original libvirt error."
1828,1374158,nova,480a932173219bda0f7ead61fd406fe4a366832c,1,1,“Typo in call to LibvirtConfigObject's parse_dom() method”,Typo in call to LibvirtConfigObject's parse_dom() ...,"In Juno in nova/virt/libvirt/config.py:
LibvirtConfigGuestPUNUMA.parse_dom() calls super with a capital 'D' in parse_dom().
        super(LibvirtConfigGuestCPUNUMA, self).parse_Dom(xmldoc)
LibvirtConfigObject does not have a 'parse_Dom()' method. It has a 'parse_dom()' method. This causes the following exception to be raised.
...
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack   File ""/usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py"", line 1733, in parse_dom
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack     obj.parse_dom(c)
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack   File ""/usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py"", line 542, in parse_dom
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack     numa.parse_dom(child)
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack   File ""/usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py"", line 509, in parse_dom
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack     super(LibvirtConfigGuestCPUNUMA, self).parse_Dom(xmldoc)
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstackAttributeError: 'super' object has no attribute 'parse_Dom'
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack
2014-09-25 15:35"
1829,1374458,nova,eef97cdf4bb7f426d7feb394ef54510db8b1656b,1,1,“There is an error in the n-cpu log”,test_encrypted_cinder_volumes_luks fails to detach...,"http://logs.openstack.org/98/124198/3/check/check-grenade-dsvm-icehouse/c89f18f/console.html#_2014-09-26_03_38_56_940
2014-09-26 03:38:57.259 |     Traceback (most recent call last):
2014-09-26 03:38:57.259 |       File ""tempest/scenario/manager.py"", line 142, in delete_wrapper
2014-09-26 03:38:57.259 |         delete_thing(*args, **kwargs)
2014-09-26 03:38:57.259 |       File ""tempest/services/volume/json/volumes_client.py"", line 108, in delete_volume
2014-09-26 03:38:57.259 |         resp, body = self.delete(""volumes/%s"" % str(volume_id))
2014-09-26 03:38:57.259 |       File ""tempest/common/rest_client.py"", line 225, in delete
2014-09-26 03:38:57.259 |         return self.request('DELETE', url, extra_headers, headers, body)
2014-09-26 03:38:57.259 |       File ""tempest/common/rest_client.py"", line 435, in request
2014-09-26 03:38:57.259 |         resp, resp_body)
2014-09-26 03:38:57.259 |       File ""tempest/common/rest_client.py"", line 484, in _error_checker
2014-09-26 03:38:57.259 |         raise exceptions.BadRequest(resp_body)
2014-09-26 03:38:57.259 |     BadRequest: Bad request
2014-09-26 03:38:57.260 |     Details: {u'message': u'Invalid volume: Volume status must be available or error, but current status is: in-use', u'code': 400}
2014-09-26 03:38:57.260 |     }}}
2014-09-26 03:38:57.260 |
2014-09-26 03:38:57.260 |     traceback-2: {{{
2014-09-26 03:38:57.260 |     Traceback (most recent call last):
2014-09-26 03:38:57.260 |       File ""tempest/common/rest_client.py"", line 561, in wait_for_resource_deletion
2014-09-26 03:38:57.260 |         raise exceptions.TimeoutException(message)
2014-09-26 03:38:57.260 |     TimeoutException: Request timed out
2014-09-26 03:38:57.260 |     Details: (TestEncryptedCinderVolumes:_run_cleanups) Failed to delete resource 704461b6-3421-4959-8113-a011e6410ede within the required time (196 s).
2014-09-26 03:38:57.260 |     }}}
2014-09-26 03:38:57.260 |
2014-09-26 03:38:57.261 |     traceback-3: {{{
2014-09-26 03:38:57.261 |     Traceback (most recent call last):
2014-09-26 03:38:57.261 |       File ""tempest/services/volume/json/admin/volume_types_client.py"", line 97, in delete_volume_type
2014-09-26 03:38:57.261 |         resp, body = self.delete(""types/%s"" % str(volume_id))
2014-09-26 03:38:57.261 |       File ""tempest/common/rest_client.py"", line 225, in delete
2014-09-26 03:38:57.261 |         return self.request('DELETE', url, extra_headers, headers, body)
2014-09-26 03:38:57.261 |       File ""tempest/common/rest_client.py"", line 435, in request
2014-09-26 03:38:57.261 |         resp, resp_body)
2014-09-26 03:38:57.261 |       File ""tempest/common/rest_client.py"", line 484, in _error_checker
2014-09-26 03:38:57.261 |         raise exceptions.BadRequest(resp_body)
2014-09-26 03:38:57.261 |     BadRequest: Bad request
2014-09-26 03:38:57.261 |     Details: {u'message': u'Target volume type is still in use.', u'code': 400}
2014-09-26 03:38:57.262 |     }}}
2014-09-26 03:38:57.262 |
2014-09-26 03:38:57.262 |     Traceback (most recent call last):
2014-09-26 03:38:57.262 |       File ""tempest/test.py"", line 142, in wrapper
2014-09-26 03:38:57.262 |         return f(self, *func_args, **func_kwargs)
2014-09-26 03:38:57.262 |       File ""tempest/scenario/test_encrypted_cinder_volumes.py"", line 56, in test_encrypted_cinder_volumes_luks
2014-09-26 03:38:57.262 |         self.attach_detach_volume()
2014-09-26 03:38:57.262 |       File ""tempest/scenario/test_encrypted_cinder_volumes.py"", line 49, in attach_detach_volume
2014-09-26 03:38:57.262 |         self.nova_volume_detach()
2014-09-26 03:38:57.262 |       File ""tempest/scenario/manager.py"", line 439, in nova_volume_detach
2014-09-26 03:38:57.262 |         'available')
2014-09-26 03:38:57.262 |       File ""tempest/services/volume/json/volumes_client.py"", line 181, in wait_for_volume_status
2014-09-26 03:38:57.263 |         raise exceptions.TimeoutException(message)
2014-09-26 03:38:57.263 |     TimeoutException: Request timed out
2014-09-26 03:38:57.263 |     Details: Volume 704461b6-3421-4959-8113-a011e6410ede failed to reach available status within the required time (196 s).
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRGV0YWlsczogKFRlc3RFbmNyeXB0ZWRDaW5kZXJWb2x1bWVzOl9ydW5fY2xlYW51cHMpIEZhaWxlZCB0byBkZWxldGUgcmVzb3VyY2VcIiBBTkQgbWVzc2FnZTpcIndpdGhpbiB0aGUgcmVxdWlyZWQgdGltZVwiIEFORCB0YWdzOlwiY29uc29sZVwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDExNzM4OTc0MTMwfQ==
130 hits in 7 days, check and gate, all failures."
1830,1374461,neutron,ccd650732729451aa8e5ce3401f9570c70c4f066,1,1,,potential lock wait timeout issue when creating a ...,"Currently the failures during the creation of resources related to the creation of a HA router are handled my a try/except to avoid a potential lock wait timeout. This has been done in order to keep the RPC calls outside the transactions.
All the related resources are created in the _create_router_db but this method is called inside a transaction which is started is the create_router method. Moreover the try/except mechanism used to rollback the router creation will not work since we are in a already opened transaction."
1831,1374573,neutron,36e8cbb34e78ff367cb501b8c494d9a02228251d,1,1,"""This looks like a regression caused by commit: b1677dcb80ce8b83aadb2180efad3527a96bd3bc”",Server hang on external network deletion with FIPs...,"This happens on master:
Follow these steps:
1) neutron net-create test --router:external=True
2) neutron subnet-create test 200.0.0.0/22 --name test
3) neutron floatingip-create test
4) neutron net-delete test
Watch command 4) hang (the server never comes back). Expected behavior would be for the command to succeed and delete the network successfully.
This looks like a regression caused by commit: b1677dcb80ce8b83aadb2180efad3527a96bd3bc (https://review.openstack.org/#/c/82945/)"
1833,1374902,nova,6b640163ab8133085e7bb970d3ac08a86e339ddb,1,1,“missing vcpupin”,missing vcpupin elements in cputune for numa case,"Boot instance with flavor as below:
os@os2:~$ nova flavor-show 100
+----------------------------+------------------------+
| Property                   | Value                  |
+----------------------------+------------------------+
| OS-FLV-DISABLED:disabled   | False                  |
| OS-FLV-EXT-DATA:ephemeral  | 0                      |
| disk                       | 0                      |
| extra_specs                | {""hw:numa_nodes"": ""2""} |
| id                         | 100                    |
| name                       | numa.nano              |
| os-flavor-access:is_public | True                   |
| ram                        | 512                    |
| rxtx_factor                | 1.0                    |
| swap                       |                        |
| vcpus                      | 8                      |
+----------------------------+------------------------+
The result is
  <cputune>
    <vcpupin vcpu='3' cpuset='0-7,16-23'/>
    <vcpupin vcpu='7' cpuset='8-15,24-31'/>
  </cputune>
The cputune should be:
  <cputune>
    <vcpupin vcpu='0' cpuset='0-7,16-23'/>
    <vcpupin vcpu='1' cpuset='0-7,16-23'/>
    <vcpupin vcpu='2' cpuset='0-7,16-23'/>
    <vcpupin vcpu='3' cpuset='0-7,16-23'/>
    <vcpupin vcpu='4' cpuset='8-15,24-31'/>
    <vcpupin vcpu='5' cpuset='8-15,24-31'/>
    <vcpupin vcpu='6' cpuset='8-15,24-31'/>
    <vcpupin vcpu='7' cpuset='8-15,24-31'/>
  </cputune>"
1834,1375379,nova,4f0547d4978172e29eb328bceb404335da1b9e0a,1,1,,Bug #1375379 “console,"When trying to connect to a console with internal_access_path if the server does not respond by 200 we should raise an exception but the current code does not insure this case.
https://github.com/openstack/nova/blob/master/nova/console/websocketproxy.py#L68
The method 'find' return -1 on failure not False or 0"
1835,1375382,cinder,5d22ec17c7548f3de85ba9e3ad54ce5799dc5fff,1,1,,Race condition in iscsi disconnect_volume,"I am seeing an error occur due to a race condition in cinder/brick/connector.py ISCSIConnector's disconnect_volume method. The scenario being that when we do the scsi delete it is expected that the symlink in /dev/disk/by-path is removed if it is no longer in use, and that when we call self.driver.get_all_block_devices() we can check to see if anyone is still using it by assuming if the symlink is there it is in use. Then only if it is no longer in the list we disconnect the iscsi portal.
The issue I am seeing is that there is some delay between calling the scsi delete and the symlink being removed which causes us to see the broken symlink when calling os.listdir(). My understanding is that the rules for udev to clean up the symlinks is done asynchronously, which means we are not guaranteed it will be cleaned up before we call self.driver.get_all_block_devices(). The end result of this causes the iscsi portal to be left open indefinitely when it should have been closed."
1836,1375478,nova,2ee0d651b18a5840033f9338f3b605b5a64769f1,1,1,,image metadata not copied when bdm v2 source=snaps...,"If an instance is booted using the block device mapping v2 API and source=snapshot is used, no image metadata will be copied into the instance system_metadata which can cause issues further in the boot process.  Since properties like os_type are missed which may be used by a virt driver."
1837,1375597,neutron,79f1e8a9c1f308586077483d849e66dcdc83144f,1,1,,exception will kill router rescheduling call,An encountered exception in the router rescheduling loop will cause the loop to terminate and not be tried again. Retries should continue at the normal interval.
1838,1376128,neutron,20dbbab61c1159a3dc6ef838ec65b04f7e9cf560,1,1,"""Changes in commit 7f8ae630b87392193974dd9cb198c1165cdec93b moved
    pid files”",Neutron agents won't pick up neutron-ns-metadata-p...,"The pid file path changes from %s.pid to %s/pid during juno, due to this change:
https://github.com/openstack/neutron/commit/7f8ae630b87392193974dd9cb198c1165cdec93b#diff-448060f24c6b560b2cbac833da6a143dL68
That means the l3-agent and dhcp-agent (when isolated metadata is enabled)
will respawn a second neutron-ns-metadata proxy on each namespace/resource
after upgrade (I->J) and agent restart due the inability to find the old PID
file and external process PID."
1839,1376194,cinder,7ee80f76674bf4ee37b42a1223eb570138425ddd,1,0,"Requirements. “Currently the display name and description of the target volume
    is changed to”","After restore operation , target  volume name gett...","while restoring backup to a volume only data should get updated however target volume name getting changed with  name of backuped  volume source.
ssatya@devstack:~/devstack$ cinder list
+--------------------------------------+-----------+------------+------+-------------+----------+-------------+
|                  ID                  |   Status  |    Name    | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------------+------+-------------+----------+-------------+
| 9f673616-3742-49b3-8861-b85706faf763 | available | image_vol  |  1   |     None    |   true   |             |
| a0f016b7-6e6a-4d6f-a17b-1f18b0408158 | available | silver_vol |  1   |    silver   |  false   |             |
+--------------------------------------+-----------+------------+------+-------------+----------+-------------+
ssatya@devstack:~/devstack$  cinder backup-restore a8b2f82c-a6a9-4fd1-b087-18798970ff4d  --volume  a0f016b7-6e6a-4d6f-a17b-1f18b0408158
ssatya@devstack:~/devstack$ cinder list
+--------------------------------------+-----------+-----------+------+-------------+----------+-------------+
|                  ID                  |   Status  |    Name   | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-----------+------+-------------+----------+-------------+
| 9f673616-3742-49b3-8861-b85706faf763 | available | image_vol |  1   |     None    |   true   |             |
| a0f016b7-6e6a-4d6f-a17b-1f18b0408158 | available | image_vol |  1   |    silver   |   true   |             |
+--------------------------------------+-----------+-----------+------+-------------+----------+-------------+"
1840,1376199,cinder,82716b4ac836024b968c76db75be8a92ede0e226,1,1,,cinder backup service should change backup & volum...,"stopped one of swift service [s-object]
Take back up of any existing volume
observed exception in c-bak service logs however status of backup remains in 'creating' only
commands :
stop s-object service
ssatya@autojuno:~/juno/devstack$ cinder backup-create b7d8b000-d69c-4411-ab00-a3911c64ecdc  --name test_backup2
+-----------+--------------------------------------+
|  Property |                Value                 |
+-----------+--------------------------------------+
|     id    | c19ecea3-a57c-49b4-94ed-22854a031f53 |
|    name   |             test_backup2             |
| volume_id | b7d8b000-d69c-4411-ab00-a3911c64ecdc |
+-----------+--------------------------------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     |      None     |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder list
+--------------------------------------+------------+------+------+-------------+----------+-------------+
|                  ID                  |   Status   | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+------------+------+------+-------------+----------+-------------+
| 74c5a098-b2a8-4e80-b82d-18441396280e | available  | test |  1   |     None    |  false   |             |
| b7d8b000-d69c-4411-ab00-a3911c64ecdc | backing-up | test |  1   |     None    |  false   |             |
+--------------------------------------+------------+------+------+-------------+----------+-------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder list
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
| 74c5a098-b2a8-4e80-b82d-18441396280e | available | test |  1   |     None    |  false   |             |
| b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test |  1   |     None    |  false   |             |
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ date
Wed Oct  1 13:50:27 IST 2014
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ date
Wed Oct  1 13:55:27 IST 2014
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+"
1841,1376307,nova,0251b53966eaa9e724377a300ea247367fd778c7,1,1,"“Seems like the commit https://github.com/openstack/nova/commit/6a374f21495c12568e4754800574e6703a0e626f
is the cause.”",Bug #1376307 “nova compute is crashing with the error TypeError,"nova compute is crashing with the below error when nova compute is started
2014-10-01 14:50:26.854 ^[[00;32mDEBUG nova.virt.libvirt.driver [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mUpdating host stats^[[00m ^[[00;33mfrom (pid=9945) update_status /opt/stack/nova/nova/virt/libvirt/driver.py:6361^[[00m
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 449, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 58, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 167, in _do_send
    waiter.switch(result)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 207, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/service.py"", line 490, in run_service
    service.start()
  File ""/opt/stack/nova/nova/service.py"", line 181, in start
    self.manager.pre_start_hook()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1152, in pre_start_hook
    self.update_available_resource(nova.context.get_admin_context())
  File ""/opt/stack/nova/nova/compute/manager.py"", line 5946, in update_available_resource
    nodenames = set(self.driver.get_available_nodes())
  File ""/opt/stack/nova/nova/virt/driver.py"", line 1237, in get_available_nodes
    stats = self.get_host_stats(refresh=refresh)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 5771, in get_host_stats
    return self.host_state.get_host_stats(refresh=refresh)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 470, in host_state
    self._host_state = HostState(self)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 6331, in __init__
    self.update_status()
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 6387, in update_status
    numa_topology = self.driver._get_host_numa_topology()
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4828, in _get_host_numa_topology
    for cell in topology.cells])
TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'
2014-10-01 14:50:26.989 ^[[01;31mERROR nova.openstack.common.threadgroup [^[[00;36m-^[[01;31m] ^[[01;35m^[[01;31munsupported operand type(s) for /: 'NoneType' and 'int'^[[00m
Seems like the commit https://github.com/openstack/nova/commit/6a374f21495c12568e4754800574e6703a0e626f
is the cause."
1842,1376349,cinder,933a7c01ffa78810b283aa3c3b32d75e4c4504bb,0,0,Feature “Warn at driver startup if the configuration option should be set to True”,Bug #1376349 “NetApp ESeries driver,"ESeries filers require appropriate multipath/DMMP configuration on the host running cinder volume process for volume attach and image transfers to work reliably and efficiently, and the cinder.conf option 'use_multipath_for_image_xfer' should be set to True.
Here we will commit a fix to log a warning on driver startup if Eseries backend is being used and 'use_multipath_for_image_xfer' is not set to True."
1843,1376368,nova,3f9003270efd9ac036f3c229b36baa0bb05203bf,1,1,"“Cert revocation was broken by
    32b0adb591f80ad2c5c19519b4ffc2b55dbea672.”",nova.crypto.revoke_cert always raises ProjectNotFo...,"(Marked this as a security issue for now, since cert revocation not working is pretty serious)
https://github.com/openstack/nova/blob/master/nova/crypto.py#L277-L278
os.chdir *always* returns None, which means that path is always taken and the cert is never revoked"
1844,1376492,nova,5065aeca1b4acad513c07e3832ec0e12de2e6568,1,1,“Patch I7598afbf0dc3c527471af34224003d28e64daaff introduces a tempest failure with Minesweeper”,Bug #1376492 “Minesweeper failure,"Patch I7598afbf0dc3c527471af34224003d28e64daaff introduces a tempest failure with Minesweeper due to the fact that the destroy operation can be triggered by both the user and the revert resize operation. In case of a revert resize operation, we do not want to delete the original VM."
1845,1376651,nova,c20f50b419ca95c7df35a2b2a546a8f36f311d98,1,0,“versions of libvirt that are older than the minimum supported version”,Enforce the minimum required libvirt in nova,"In a number of cases now users are reporting bugs for features that don't work in Nova with versions of libvirt that are older than the minimum supported version. Most recently in Juno libvirt 0.9.11 is required but user reported bugs about a problem in 0.9.8
https://bugs.launchpad.net/nova/+bug/1376307
People clearly aren't seeing the log error message about their unsupported libvirt version, so we should turn this into a fatal exception that blocks nova-compute startup."
1846,1376945,nova,0aeffa12a62604ee3238323d969345e41937b642,1,1,,os-networks extension displays cidr incorrectly,"The nova-networks extension is improperly converting cidr values to strings:
$ nova network-list
shows a list of ips for cidr:
[u'192.168.50.0', u'192.168.50.1', u'192.168.50.2', u'192.168.50.3',...]
This is possibly due to the extension being updated to use objects, but I don't recall seeing it previously, so it is possible something changed the way an ipnetwork is converted to json so that it now iterates through the object isntead of printing it as a string."
1847,1377241,neutron,4c2b42e21744be56cbf32aeac6f4b4f1c87de24e,1,1,,Lock wait timeout on delete port for DVR,"We run a script to configure networks, VMs, Routers and assigin floatingIP to the VM.
After it is created, then we run a script to clean all ports, networks, routers and gateway and FIP.
The issue is seen when there is a back to back call to router-interface-delete and router-gateway-clear.
There are three calls to router-interface-delete and the fourth call to router-gateway-clear.
At this time there is a db lock obtained for port delete and when the other delete comes in, it timeout.
2014-10-03 09:28:39.587 DEBUG neutron.openstack.common.lockutils [req-a89ee05c-d8b2-438a-a707-699f450d3c41 admin d3bb4e1791814b809672385bc8252688] Got semaphore ""db-access"" from (pid=25888) lock /opt/stack/neutron/neutron/openstack/common/lockutils.py:168
2014-10-03 09:29:30.777 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54899)
2014-10-03 09:29:30.778 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54900)
2014-10-03 09:29:30.778 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54901)
2014-10-03 09:29:30.778 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54902)
2014-10-03 09:29:30.780 ERROR neutron.api.v2.resource [req-a89ee05c-d8b2-438a-a707-699f450d3c41 admin d3bb4e1791814b809672385bc8252688] remove_router_interface failed
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 87, in resource
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 200, in _handle_action
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     return getattr(self._plugin, name)(*arg_list, **kwargs)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/l3_dvr_db.py"", line 247, in remove_router_interface
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     context.elevated(), router, subnet_id=subnet_id)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/l3_dvr_db.py"", line 557, in delete_csnat_router_interface_ports
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     l3_port_check=False)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/ml2/plugin.py"", line 983, in delete_port
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     port_db, binding = db.get_locked_port_and_binding(session, id)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/ml2/db.py"", line 135, in get_locked_port_and_binding
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     with_lockmode('update').
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2310, in one
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     ret = list(self)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2353, in __iter__
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     return self._execute_and_instances(context)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2368, in _execute_and_instances
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     result = conn.execute(querycontext.statement, self._params)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 662, in execute
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     params)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 761, in _execute_clauseelement
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     compiled_sql, distilled_params
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 874, in _execute_context
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     context)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/compat/handle_error.py"", line 125, in _handle_dbapi_exception
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     six.reraise(type(newraise), newraise, sys.exc_info()[2])
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/compat/handle_error.py"", line 102, in _handle_dbapi_exception
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     per_fn = fn(ctx)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/exc_filters.py"", line 323, in handler
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     context.is_disconnect)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/exc_filters.py"", line 254, in _raise_operational_errors_directly_filter
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     raise operational_error
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource OperationalError: (OperationalError) (1205, 'Lock wait timeout exceeded; try restarting transaction') 'SELECT ports.tenant_id AS ports_tenant_id, ports.id AS ports_id, ports.name AS ports_name, ports.network_id AS ports_network_id, ports.mac_address AS ports_mac_address, ports.admin_state_up AS ports_admin_state_up, ports.status AS ports_status, ports.device_id AS ports_device_id, ports.device_owner AS ports_device_owner \nFROM ports \nWHERE ports.id = %s FOR UPDATE' ('bec69266-227d-4482-a346-ef47dd3a7a78',)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource"
1848,1377307,neutron,29250949012e9c0a60b0ddb56ddbf18d7b68106b,1,0,"“When DVR is enabled and enable_isolated_metadata=True,
the DHCP agent should only inject a metadata host route” change in the requirements",Metadata host route added when DVR and isolated me...,"When DVR is enabled and enable_isolated_metadata=True in dhcp_agent.ini, the agent should only inject a metadata host route when there is no gateway on the subnet.  But it does it all the time:
$ ip r
default via 10.0.0.1 dev eth0
10.0.0.0/24 dev eth0  src 10.0.0.5
169.254.169.254 via 10.0.0.4 dev eth0
The ""opts"" file for dnsmasq confirms it was the Neutron code that configured this.
The code in neutron/agent/linux/dhcp.py:get_isolated_subnets() is only looking at ports where the device_owner field is DEVICE_OWNER_ROUTER_INTF, it also needs to look for DEVICE_OWNER_DVR_INTERFACE.  Simlar changes have been made in other code.
Making that simple change fixes the problem:
$ ip r
default via 10.0.0.1 dev eth0
10.0.0.0/24 dev eth0  src 10.0.0.5
I have a patch I'll get out for this."
1849,1377346,neutron,db5e370b0d68c3e71626c99941fe487059b3cf88,0,0,“ML2: Invalid unit test case”,Bug #1377346 “ML2,"In test_create_network_multiprovider() , an invalid comparison is used ...
tz = network['network'][mpnet.SEGMENTS]
for tz in data['network'][mpnet.SEGMENTS]:  <=== tz from previous statement is lost
   for field in [pnet.NETWORK_TYPE, pnet.PHYSICAL_NETWORK,
                         pnet.SEGMENTATION_ID]:
        self.assertEqual(tz.get(field), tz.get(field))  <===== this is always true"
1850,1377447,nova,74145b625a2544eb0d72c811a20c64c9987bedf0,1,0,"API Change - Change requirements, “a8a5d44c8aca218f00649232c2b8a46aee59b77e that make each tuple have four items Old code only accept three items for each tuple:”",pci_request_id break the upgrade from icehouse to ...,"The rpc api build_and_run_instance should be back-compatible with icehouse, as the code https://github.com/openstack/nova/blob/master/nova/compute/rpcapi.py#L887
It turn the request_network object into tuple that can be understand by icehouse code.
But the commit a8a5d44c8aca218f00649232c2b8a46aee59b77e change the request_network parameter. It add pci_request_id for each item. When request_network object turn it to tuple, it will add pci_request_id into
tuple also, that make each tuple have four items. https://github.com/openstack/nova/blob/master/nova/objects/network_request.py#L37
Old code only accept three items for each tuple: https://github.com/openstack/nova/blob/2014.1/nova/network/neutronv2/api.py#L237
Then the rpc api back-compatiblity is broken.
Then juno controller boot instance to icehouse compute node, will get error as below:
2014-10-04 21:08:17.455 ERROR nova.compute.manager [req-58b84295-ce36-479f-b50e-dfe4f86cc1d9 admin demo] [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4] Insta
nce failed to spawn
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4] Traceback (most recent call last):
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/compute/manager.py"", line 2014
, in _build_resources
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     yield resources
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1917
, in _build_and_run_instance
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     block_device_info=block_device_info)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line
2246, in spawn
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     admin_pass=admin_password)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line
2677, in _create_image
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     content=files, extra_md=extra_md, network_info=network_
info)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/api/metadata/base.py"", line 16
5, in __init__
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     ec2utils.get_ip_info_for_instance_from_nw_info(network_info)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/api/ec2/ec2utils.py"", line 147, in get_ip_info_for_instance_from_nw_info
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     fixed_ips = nw_info.fixed_ips()
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/network/model.py"", line 407, in _sync_wrapper
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     self.wait()
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/network/model.py"", line 439, in wait
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     self[:] = self._gt.wait()
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     return self._exit_event.wait()
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 124, in wait
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     current.throw(*self._exc)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 207, in main
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     result = function(*args, **kwargs)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1510, in _allocate_network_async
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     dhcp_options=dhcp_options)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 238, in allocate_for_instance
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     for network_id, fixed_ip, port_id in requested_networks:
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4] ValueError: too many values to unpack"
1851,1377981,cinder,5e4e1f7ea71f9b4c7bd15809c58bc7a1838ed567,1,1,“cve-2014-7230)”,[OSSA 2014-036] Missing fix for ssh_execute (Excep...,"Former bugs:
  https://bugs.launchpad.net/ossa/+bug/1343604
  https://bugs.launchpad.net/ossa/+bug/1345233
The ssh_execute method is still affected in Cinder and Nova Icehouse release.
It is prone to password leak if:
- passwords are used on the command line
- execution fail
- calling code catch and log the exception
The missing fix from oslo-incubator to be merged is: 6a60f84258c2be3391541dbe02e30b8e836f6c22"
1852,1378389,nova,8299e80ad437d86925484366c29a01fa30c344ed,1,1,,Bug #1378389 “os-interface,"The os-interface:show method in the v2/v3 compute API is catching a NotFound(NovaException):
http://git.openstack.org/cgit/openstack/nova/tree/nova/api/openstack/compute/contrib/attach_interfaces.py?id=2014.2.rc1#n67
But when using the neutronv2 API, if you get a port not found it's going to raise up a PortNotFoundClient(NeutronClientException), which won't be handled by the NotFound(NovaException) in the compute API since it's not the same type of exception.
http://git.openstack.org/cgit/openstack/nova/tree/nova/network/neutronv2/api.py?id=2014.2.rc1#n584
This bug has two parts:
1. The neutronv2 API show_port method needs to return nova exceptions, not neutron client exceptions.
2. The os-interfaces:show v2/v3 APIs need to handle the exceptions (404 is handled, but neutron can also raise Forbidden/Unauthorized which the compute API isn't handling)."
1853,1378398,neutron,5e9305a6f934549408a9c18480fc1c000126621e,0,0,Refactoring “Remove legacy weight from l3 agent”,Remove legacy weight from l3 agent _process_router...,"Some work in Juno around adding a new router processing queue to the l3_agent.py obsoleted much of the logic in the _process_routers method.  The following can be simplified.
1. No loop is necessary since the list passed always has exactly one router in it.
2. No thread pool is necessary because there is only one thread active and the method waits for it to complete at the end.
3. The set logic is no longer needed."
1854,1378461,nova,cf3c1d10b10067cab226bf9a82e89017a4b27fec,1,0,"“The VERSION of the nova.objects.NetworkRequest object was incorrectly
    still at 1.0, even though a 1.1 version had been incremented previously”",nova.objects.network_request.NetworkRequest's vers...,"from nova/objects/network_request.py:
class NetworkRequest(obj_base.NovaObject):
    # Version 1.0: Initial version
    # Version 1.1: Added pci_request_id
    VERSION = '1.0'
VERSION should be 1.1, per the comment above it."
1855,1378508,neutron,7ea605df3ac71dc568194bcd5eaf1c115008e1ee,1,1,,KeyError in DHPC RPC when port_update happens.- th...,"When there is a delete_port event occassionally we are seeing a TRACE in dhcp_rpc.py file.
2014-10-07 12:31:39.803 DEBUG neutron.api.rpc.handlers.dhcp_rpc [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Update dhcp port {u'port': {u'network_id': u'12548499-8387-480e-b29c-625dbf320ecf', u'fixed_ips': [{u'subnet_id': u'88031ffe-9149-4e96-a022-65468f6bcc0e'}]}} from ubuntu. from (pid=4414) update_dhcp_port /opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py:290
2014-10-07 12:31:39.803 DEBUG neutron.openstack.common.lockutils [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Got semaphore ""db-access"" from (pid=4414) lock /opt/stack/neutron/neutron/openstack/common/lockutils.py:168
2014-10-07 12:31:39.832 ERROR oslo.messaging.rpc.dispatcher [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Exception during message handling: 'network_id'
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 294, in update_dhcp_port
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher     'update_port')
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 81, in _port_action
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher     net_id = port['port']['network_id']
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher KeyError: 'network_id'
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher
2014-10-07 12:31:39.833 ERROR oslo.messaging._drivers.common [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Returning exception 'network_id' to caller
2014-10-07 12:31:39.833 ERROR oslo.messaging._drivers.common [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] ['Traceback (most recent call last):\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n    incoming.message))\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', '  File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 294, in update_dhcp_port\n    \'update_port\')\n', '  File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 81, in _port_action\n    net_id = port[\'port\'][\'network_id\']\n', ""KeyError: 'network_id'\n""]
2014-10-07 12:31:39.839 DEBUG neutron.context [req-7d40234b-6e11-4645-9bab-8f9958df5064 None None] Arguments dropped when creating context: {u'project_name': None, u'tenant': None} from (pid=4414) __init__ /opt/stack/neutron/neutron/context.py:83"
1856,1378525,neutron,abe30e973442271b0093eaa61c59d171a68a2028,0,0,"No bug, it does not have a BFC. “This is deceiving and confusing     and should be blocked until the migration itself is fixed     in a future patch.”",Broken L3 HA migration should be blocked,"While the HA property is update-able, and resulting router-get
invocations suggest that the router is HA, the migration
itself fails on the agent. This is deceiving and confusing
and should be blocked until the migration itself is fixed
in a future patch."
1857,1378756,neutron,a1fac106479f9c3c5559f8b2cfbc01fe12d3a575,1,1,,set_context in L3NatTestCaseMixin.floatingip_with_...,"We have following code in neutron.test.unit.L3NatTestCaseMixin.floatingip_with_assoc get ""set_context"" from external but not use it.
    @contextlib.contextmanager
    def floatingip_with_assoc(self, port_id=None, fmt=None, fixed_ip=None,
    ######################################
                              set_context=False):                                                 # <---- We get set_context here
    ######################################
        with self.subnet(cidr='11.0.0.0/24') as public_sub:
            self._set_net_external(public_sub['subnet']['network_id'])
            private_port = None
            if port_id:
                private_port = self._show('ports', port_id)
            with test_db_plugin.optional_ctx(private_port,
                                             self.port) as private_port:
                with self.router() as r:
                    sid = private_port['port']['fixed_ips'][0]['subnet_id']
                    private_sub = {'subnet': {'id': sid}}
                    floatingip = None
                    self._add_external_gateway_to_router(
                        r['router']['id'],
                        public_sub['subnet']['network_id'])
                    self._router_interface_action(
                        'add', r['router']['id'],
                        private_sub['subnet']['id'], None)
                    floatingip = self._make_floatingip(
                        fmt or self.fmt,
                        public_sub['subnet']['network_id'],
                        port_id=private_port['port']['id'],
                        fixed_ip=fixed_ip,
    ######################################
                        set_context=False)                                                    ### <---- But we don't really use it
    ######################################
                    yield floatingip"
1858,1378786,nova,6ed57972093835f449ad645b3783bbb8b3c4245e,0,0,“Update rpc version aliases for juno”,Update rpc version aliases for juno,"Update all of the rpc client API classes to include a version alias
for the latest version implemented in Juno.  This alias is needed when
doing rolling upgrades from Juno to Kilo.  With this in place, you can
ensure all services only send messages that both Juno and Kilo will
understand."
1859,1378866,neutron,93012915a3445a8ac8a0b30b702df30febbbb728,1,1,,leftover router ports,"During testing, we've found some instances of leftover router ports.  The ports are not properly cleaned up when a router is removed.  The user is able to manually remove the ports to work around the issue."
1860,1378952,neutron,8a08a3cb47d0dd69d4aa2e8fa661d04054fe95ae,0,0,It’s a proposed feature,Updating ipv6 modes is problematic,"See
http://lists.openstack.org/pipermail/openstack-dev/2014-October/047978.html"
1861,1378964,cinder,9710357f246e670bd3cacb153cae245941362c54,1,1,,GPFS should snap the glance image file only the im...,"GPFS 'copy_on_write' mode works only when creating a volume from an image, with image format being 'raw'.
When the image format is other than 'raw' like for example 'qcow2', the GPFS driver copies the image file to the volume by converting it to 'raw' format.
But, currently during this operation as well, GPFS driver is snap'ing the glance image, making it a clone parent, with no child associated.
Though this does not break any functionality, this is a unnecessary operation and needs to be avoided."
1862,1379510,neutron,24e4110eb284078775496501ff81630eb1619c11,0,0,“If the topology sync fails”,Bug #1379510 “Big Switch,"If the topology sync fails, no other sync attempts will be made because the server manager clears the hash from the DB before the sync operation. It shouldn't do this because the backend ignores the hash on a sync anyway."
1863,1379609,neutron,c97069dc9a73344ebdc7b686133269850a81b3b2,1,0,Change requirements.:  “it does not delete the older profile bindings and maintains them”,Bug #1379609 “Cisco N1kv,"During cisco-network-profile-update, if a tenant id is being added to the network profile, the current behavior is to remove all the tenant-network profile bindings and add the new list of tenants. This works well with horizon since all the existing tenant UUIDs, along with the new tenant id, are passed during update network profile.
If you try to update a network profile and add new tenant to the network profile via CLI, this will replace the existing tenant-network profile bindings and add only the new one.
Expected behavior is to not delete the existing tenant bindings and instead only add new tenants to the list."
1864,1379654,cinder,afcbfe053c9165ab84af30c8e663dfaee2a79e81,1,1,,Storwize_SVC Create volume with replication type a...,"Test step :
1. Create a qos spec :
cinder qos-create qos-spec qos:IOThrottling=30000
2. Create type-1 with Pool-1 without replication.
3. Create type-2 with Primary Pool-1 and Secondary Pool-2, replication =TRUE.
4. qos-spec associate to type-1 and type-2.
cinder qos-associate qos-spec type-1
cinder qos-associate qos-spec type-2
5. cinder create --volume-type type-1 1 ,
The volume creation will be successed.
6. cinder create --volume-type type-2 1.
The volume creation will be failed.
Check the cinder log , it was found:
screen-c-sch.2014-09-11-151843.log:2014-10-10 14:01:41.497 ERROR cinder.scheduler.filter_scheduler [[[01;36mreq-9d20b04b-94fc-4936-a9c8-12a41690e313 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e] ^[[01;35mError scheduling None from last vol-service: ubuntu247@driver4#driver4 : [u'Traceback (most recent call last):\n', u' File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 35, in execute_task\n result = task.execute(**arguments)\n', u' File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 624, in execute\n **volume_spec)\n', u' File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 598, in _create_raw_volume\n return self.driver.create_volume(volume_ref)\n', u' File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper\n return f(*args, **kwargs)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/init.py"", line 571, in create_volume\n model_update = self.replication.create_replica(ctxt, volume)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/replication.py"", line 78, in create_replica\n self.driver.add_vdisk_copy(volume[\'name\'], dest_pool, vol_type)\n', u' File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper\n return f(*args, **kwargs)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/init_.py"", line 661, in add_vdisk_copy\n self.configuration)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 858, in add_vdisk_copy\n volume_type=volume_type)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 561, in get_vdisk_params\n kvs = qos_specs.get_qos_specs(ctxt, qos_specs_id)[\'specs\']\n', u""UnboundLocalError: local variable 'ctxt' referenced before assignment\n""]"
1865,1379811,neutron,2c5f99a4b1376a46b8c1e1f4e82a015c2d29aa1f,0,0,"“he VPN logging code should use the new marker functions for info, warning, error”",Bug #1379811 “VPN,"The VPN logging code should use the new marker functions for info, warning, error and critical log levels to separate log messages into different catalogs for translation priority. We also need to ensure that the new i18n guidelines are followed."
1866,1379830,cinder,48cb82971e0418f9a629e2b39d0433dc2c0e6919,1,1,“This is a regression caused by commit 4be8913520f5e9fe4109ade101da9509e4a83360”,unable to re-attach a  volume to instance in VMWar...,"Attach a volume to instance fails with following exception  :
014-10-10 19:29:54.112 [00;32mDEBUG cinder.volume.drivers.vmware.vmdk [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[00;32m] [01;35m[00;32mThe instance: (Property){
   value = ""vm-2360""
   _type = ""VirtualMachine""
 } for which initialize connection is called, exists.[00m [00;33mfrom (pid=10484) _initialize_connection /opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py:656[00m
2014-10-10 19:29:55.114 [00;32mDEBUG cinder.volume.drivers.vmware.vmdk [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[00;32m] [01;35m[00;32mBacking exists[00m [00;33mfrom (pid=10484) _initialize_connection /opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py:666[00m
2014-10-10 19:29:57.068 [00;32mDEBUG cinder.volume.drivers.vmware.vmdk [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[00;32m] [01;35m[00;32mDatastore: (ManagedObjectReference){
   value = ""datastore-34""
   _type = ""Datastore""
 }, profile: None[00m [00;33mfrom (pid=10484) _relocate_backing /opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py:1934[00m
2014-10-10 19:29:59.015 [01;31mERROR cinder.volume.manager [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31mUnable to fetch connection information from backend: 'Text' object has no attribute 'value'[00m
2014-10-10 19:29:59.017 [01;31mERROR oslo.messaging.rpc.dispatcher [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31mException during message handling: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value'[00m
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00mTraceback (most recent call last):
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m    incoming.message))
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m    return self._do_dispatch(endpoint, method, ctxt, args)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m    result = getattr(endpoint, method)(ctxt, **new_args)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m  File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m    return f(*args, **kwargs)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 901, in initialize_connection
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m    raise exception.VolumeBackendAPIException(data=err_msg)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00mVolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value'
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m
2014-10-10 19:29:59.021 [01;31mERROR oslo.messaging._drivers.common [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31mReturning exception Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value' to caller[00m
2014-10-10 19:29:59.022 [01;31mERROR oslo.messaging._drivers.common [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31m['Traceback (most recent call last):\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n    incoming.message))\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', '  File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper\n    return f(*args, **kwargs)\n', '  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 901, in initialize_connection\n    raise exception.VolumeBackendAPIException(data=err_msg)\n', ""VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value'\n""][00m
2014-10-10 19:30:02.547 [00;32mDEBUG cinder.openstack.common.periodic_task [[00;36m-[00;32m] [01;35m[00;32mRunning periodic task VolumeManager._publish_service_capabilities[00m [00;33mfrom (pid=10484) run_periodic_tasks /opt/stack/cinder/cinder/openstack/common/periodic_task.py:193[00m
2014-10-10 19:30:02.547 [00;32mDEBUG cinder.manager [[00;36m-[00;32m] [01;35m[00;32mNotifying Schedulers of capabilities ...[00m [00;33mfrom (pid=10484) _publish_service_capabilities /opt/stack/cinder/cinder/manager.py:128[00m
2014-10-10 19:30:02.552 [00;32mDEBUG cinder.openstack.common.periodic_task [[00;36m-[00;32m] [01;35m[00;32mRunning periodic task VolumeManager._report_driver_status[00m [00;33mfrom (pid=10484) run_periodic_tasks /opt/stack/cinder/cinder/openstack/common/periodic_task.py:193[00m
2014-10-10 19:30:02.553 [00;36mINFO cinder.volume.manager [[00;36m-[00;36m] [01;35m[00;36mUpdating volume status[00m"
1867,1380456,nova,5e3f8db96fd0390c6f66754d0e1105ba127e1aa8,1,1,,Use 400 instead of 422 for security_groups v2 API,"For any invalid request format we should return 400, not 422, we should fix this case for v2 security_groups API. That is also good for sharing the unittest between v2.1 and v2.
To change the API that follow the rule: https://wiki.openstack.org/wiki/APIChangeGuidelines"
1868,1380552,glance,675a39a74faad5f7fe4eb94e1dcf7f4359ed2285,1,0,API change. “but the glance project code was not updated properly for the required changes.”,Fix for adopt glance.store library in Glance,"The store module is removed from glance project and new glance_store module is created, but the glance project code was not updated properly for the required changes.
There are few cases in v1 api which needs to be addressed:
1. _get_from_store method raises store.NotFound exception from glance_store but it is still trying to catch exception.NotFound exception in glance and the actual exception has not been caught which causes 500 HTTPInternalServerError.
2. _get_size method raises store.NotFound exception from glance_store but it is still trying to catch exception.NotFound exception in glance and the actual exception has not been caught which causes 500 HTTPInternalServerError."
1869,1380624,nova,f274816f073cfe9d0071a99159309fee643dbdb8,1,1,,Bug #1380624 “VMware,When booting from a volume the config driver will not be mounted (if configured)
1870,1380675,cinder,6ac6225e72bde92f66da8e92c563c140471b949b,1,0,"Change requirements. “This results in failures of all storage policy
    related APIs invoked using datastore selector”",Bug #1380675 “VMware,"Retype fails with
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1410, in retype
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    host)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    return f(*args, **kwargs)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py"", line 1434, in retype
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    new_profile)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/datastore.py"", line 263, in is_datastore_compliant
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    profile_id = self.get_profile_id(profile_name)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/datastore.py"", line 59, in get_profile_id
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    profile_id = self._vops.retrieve_profile_id(profile_name)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/volumeops.py"", line 1361, in retrieve_profile_id
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    for profile in self.get_all_profiles():
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/volumeops.py"", line 1339, in get_all_profiles
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    profile_manager = pbm.service_content.profileManager
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00mAttributeError: 'NoneType' object has no attribute 'service_content'"
1871,1381094,neutron,fd37ce7d943ab1c2dbc1cf3b6f0187c227f658ad,1,1,,NSX plugin request retry failure with “[Errno 104]...,"Http connections in connection pool to NSX controller can be reset due to reasons such as when LB is in the middle, and connection idle timeout.
In such case, while recreating the connection, NSX plugin would retry the request with ""next"" connection, which could also be reset already because of idle timeout. This would leads to continuous retry failure and the request fails finally, with a misleading error log: ""Request timeout ..."".
Error log example:
2014-10-12 11:07:18,163 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Acquired connection https://os-nvp.vip.ppp01.corp.com:443. 14 connection(s) available.
2014-10-12 11:07:18,163 43793136    DEBUG [neutron.plugins.nicira.api_client.request] [13168] Issuing - request GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,163 43793136    DEBUG [neutron.plugins.nicira.api_client.request] Setting X-Nvp-Wait-For-Config-Generation request header: '377462645'
2014-10-12 11:07:18,164 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Exception issuing request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,164 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Failed request 'GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus': '[Errno 104] Connection reset by peer' (0.00 seconds)
2014-10-12 11:07:18,164 43793136  WARNING [neutron.plugins.nicira.api_client.client] [13168] Connection returned in bad state, reconnecting to https://os-nvp.vip.ppp01.corp.com:443
2014-10-12 11:07:18,164 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Released connection https://os-nvp.vip.ppp01.corp.com:443. 15 connection(s) available.
2014-10-12 11:07:18,165 43793136     INFO [neutron.plugins.nicira.api_client.request_eventlet] [13168] Error while handling request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,165 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Acquired connection https://os-nvp.vip.ppp01.corp.com:443. 14 connection(s) available.
2014-10-12 11:07:18,165 43793136    DEBUG [neutron.plugins.nicira.api_client.request] [13168] Issuing - request GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,165 43793136    DEBUG [neutron.plugins.nicira.api_client.request] Setting X-Nvp-Wait-For-Config-Generation request header: '377462645'
2014-10-12 11:07:18,166 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Exception issuing request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,166 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Failed request 'GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus': '[Errno 104] Connection reset by peer' (0.00 seconds)
2014-10-12 11:07:18,166 43793136  WARNING [neutron.plugins.nicira.api_client.client] [13168] Connection returned in bad state, reconnecting to https://os-nvp.vip.ppp01.corp.com:443
2014-10-12 11:07:18,166 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Released connection https://os-nvp.vip.ppp01.corp.com:443. 15 connection(s) available.
2014-10-12 11:07:18,166 43793136     INFO [neutron.plugins.nicira.api_client.request_eventlet] [13168] Error while handling request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,167 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Acquired connection https://os-nvp.vip.ppp01.corp.com:443. 14 connection(s) available.
2014-10-12 11:07:18,167 43793136    DEBUG [neutron.plugins.nicira.api_client.request] [13168] Issuing - request GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,167 43793136    DEBUG [neutron.plugins.nicira.api_client.request] Setting X-Nvp-Wait-For-Config-Generation request header: '377462645'
2014-10-12 11:07:18,167 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Exception issuing request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,167 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Failed request 'GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus': '[Errno 104] Connection reset by peer' (0.00 seconds)
2014-10-12 11:07:18,168 43793136  WARNING [neutron.plugins.nicira.api_client.client] [13168] Connection returned in bad state, reconnecting to https://os-nvp.vip.ppp01.corp.com:443
2014-10-12 11:07:18,168 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Released connection https://os-nvp.vip.ppp01.corp.com:443. 15 connection(s) available.
2014-10-12 11:07:18,168 43793136     INFO [neutron.plugins.nicira.api_client.request_eventlet] [13168] Error while handling request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,168 133917104    ERROR [NVPApiHelper] Request timed out: GET to /ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,169 133917104    ERROR [NeutronPlugin] An exception occured while selecting logical switch for the port
...
Suggestion to the fix issue by using the newly created connection when retry instead of using the ""next"" connection."
1872,1381221,neutron,06fc675928408e462f01178e0158a72f8518188a,0,0,Refactoring “cleanup’,VPNaaS Cisco cleanup test code,"There is unused arguments in a mock method, and duplication of constants, in the Cisco VPNaaS unit test files."
1873,1381238,neutron,9902400039018d77aa3034147cfb24ca4b2353f6,1,1,,Race condition on processing DVR floating IPs,"A race condition can sometimes occur in l-3 agent when a dvr based floatingip is being deleted from one router and another dvr based floatingip is being configured on another router in the same node. Especially if the floatingip being deleted was the last floatingip on the node.  Although fix for Bug # 1373100 [1] eliminated frequent observation of this behavior in upstream tests, it still shows up. Couple of recent examples:
http://logs.openstack.org/88/128288/1/check/check-tempest-dsvm-neutron-dvr/8fdd1de/
http://logs.openstack.org/03/123403/7/check/check-tempest-dsvm-neutron-dvr/859534a/
Relevant log messages:
2014-10-14 16:06:15.803 22303 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'fip-82fb2751-30ba-4015-a5da-6c8563064db9', 'ip', 'link', 'del', 'fpr-7ed86ca6-b'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:46
2014-10-14 16:06:15.838 22303 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-7ed86ca6-b42d-4ba9-8899-447ff0509174', 'ip', 'addr', 'show', 'rfp-7ed86ca6-b']
Exit code: 0
Stdout: '2: rfp-7ed86ca6-b: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether c6:88:ee:71:a7:51 brd ff:ff:ff:ff:ff:ff\n    inet 169.254.30.212/31 scope global rfp-7ed86ca6-b\n       valid_lft forever preferred_lft forever\n    inet6 fe80::c488:eeff:fe71:a751/64 scope link \n       valid_lft forever preferred_lft forever\n'
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:81
2014-10-14 16:06:15.839 22303 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-7ed86ca6-b42d-4ba9-8899-447ff0509174', 'ip', '-4', 'addr', 'add', '172.24.4.91/32', 'brd', '172.24.4.91', 'scope', 'global', 'dev', 'rfp-7ed86ca6-b'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:46
2014-10-14 16:06:16.221 22303 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'fip-82fb2751-30ba-4015-a5da-6c8563064db9', 'ip', 'link', 'del', 'fpr-7ed86ca6-b']
Exit code: 0
Stdout: ''
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:81
2014-10-14 16:06:16.222 22303 DEBUG neutron.agent.l3_agent [-] DVR: unplug: fg-f04e25ef-e3 _destroy_fip_namespace /opt/stack/new/neutron/neutron/agent/l3_agent.py:679
2014-10-14 16:06:16.222 22303 DEBUG neutron.agent.linux.utils [-] Running command: ['ip', '-o', 'link', 'show', 'br-ex'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:46
2014-10-14 16:06:16.251 22303 ERROR neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-7ed86ca6-b42d-4ba9-8899-447ff0509174', 'ip', '-4', 'addr', 'add', '172.24.4.91/32', 'brd', '172.24.4.91', 'scope', 'global', 'dev', 'rfp-7ed86ca6-b']
Exit code: 1
Stdout: ''
Stderr: 'Cannot find device ""rfp-7ed86ca6-b""\n'
[1] https://bugs.launchpad.net/neutron/+bug/1373100"
1874,1381277,neutron,5e4b600528a2b58cbb38d6b8d55b316602d4d015,1,1,,Fix docstring on send_delete-port_request in N1kv ...,The docstring for _send_delete_port_request wasn't properly updated as part of bug/1373547 to reflect that the last port check and subsequent call to delete_vm_network were removed. Need to update this docstring to reflect that change.
1875,1381414,nova,f91d4ebeac9181ff279158fe89a8d50b34184a89,0,0,Bug in test,Bug #1381414 “Unit test failure “AssertionError,"This looks to be due to tests test_get_port_vnic_info_2 and 3 sharing some code and is easily reproduced by running these two tests alone with no concurrency.
./run_tests.sh --concurrency 1 test_get_port_vnic_info_2 test_get_port_vnic_info_3
The above always results in:
Traceback (most recent call last):
  File ""/home/hans/nova/nova/tests/network/test_neutronv2.py"", line 2615, in test_get_port_vnic_info_3
    self._test_get_port_vnic_info()
  File ""/home/hans/nova/.venv/local/lib/python2.7/site-packages/mock.py"", line 1201, in patched
    return func(*args, **keywargs)
  File ""/home/hans/nova/nova/tests/network/test_neutronv2.py"", line 2607, in _test_get_port_vnic_info
    fields=['binding:vnic_type', 'network_id'])
  File ""/home/hans/nova/.venv/local/lib/python2.7/site-packages/mock.py"", line 845, in assert_called_once_with
    raise AssertionError(msg)
AssertionError: Expected to be called once. Called 2 times."
1876,1381464,cinder,fbd04f71ed28efc85e51465527e91c21bd0d7328,1,1,,Incorrectly formatted debug message in Prophetstor...,"correct the message string: %(volumeid) -->%(volumeid)s in cinder/volume/drivers/prophetstor/dpl_fc.py
msg = _('Volume %(volumeid) failed to send assign command, '
                    'ret: %(status)s output: %(output)s') % \
                {'volumeid': volumeid, 'status': ret, 'output': output}"
1877,1381900,neutron,ffcb30c4fbee334f9903d2719418bc6aa9d7721c,1,1,,l3 agent context name conflict,context module imported in l3_agent conflicts with argument name.
1878,1382318,nova,73fcf4628089dd784889062e916b80d3fc9988a2,1,0,"Bug in ExtL. “The libvirt config code shouldn't be casting values to str(), it should be using six.text_type.” The bug was fix in 2014 and the pc was done in 2012config code shouldn't be casting values to str(), it should be using six.text_type.”",NoValidHost failure when trying to spawn instance ...,"Using the libvirt driver on Juno RC2 code, trying to create an instance with unicode name:
""\uff21\uff22\uff23\u4e00\u4e01\u4e03\u00c7\u00e0\u00e2\uff71\uff72\uff73\u0414\u0444\u044d\u0628\u062a\u062b\u0905\u0907\u0909\u20ac\u00a5\u5642\u30bd\u5341\u8c79\u7af9\u6577""
Blows up:
http://paste.openstack.org/show/121560/
The libvirt config code shouldn't be casting values to str(), it should be using six.text_type."
1879,1383757,nova,7f7cfc2dc6baa53adbea431aba840cd88eea8040,1,1,Revert,oslo-incubator copy of request_id needed for grena...,"http://logs.openstack.org/74/128974/1/check/check-grenade-dsvm/6b7d6f3/logs/new/screen-n-api.txt.gz#_2014-10-21_07_12_30_741
https://review.openstack.org/#/c/127057/ caused the issue."
883320,883320,nova,32b0adb591f80ad2c5c19519b4ffc2b55dbea672,1,1,“This is a bad pattern” Code does not fulfil policy,nova.crypto. revoke_certs_by_user should handle Pro...,"revoke_cert may throw ProcessExecutionError.
https://github.com/openstack/nova/blob/master/nova/crypto.py#L164
This is bad pattern.
(See Policy of Exception Handling
https://blueprints.launchpad.net/openstack-qa/+spec/nova-exception-policy)"
948179,948179,Swift,fdc775d6d52150c8314ec43ef5ab14a5b751e2c7,0,0,"""db_replicator.py needs more/better tests""",db_replicator.py needs more/better tests,"the db_replicator.py is probably one of the more complex pieces of swift code, and one of the least tested.  We need more tests, and better tests for this functionality."
1065531,1065531,neutron,eece55ceb687c425de1066851c9601221f1ef2b7,1,1, ,lockutils - remove lock dir creation and cleanup,"See https://review.openstack.org/14139
This:
                    if not local_lock_path:
                        cleanup_dir = True
                        local_lock_path = tempfile.mkdtemp()
                    if not os.path.exists(local_lock_path):
                        cleanup_dir = True
                        ensure_tree(local_lock_path)
                    ...
                    finally:
                        # NOTE(vish): This removes the tempdir if we needed
                        #             to create one. This is used to cleanup
                        #             the locks left behind by unit tests.
                        if cleanup_dir:
                            shutil.rmtree(local_lock_path)
Why are we deleting the lock dir here? Does that even work? i.e. what if someone concurrently tries to take the lock, re-creates the dir and lock a new file?"
1095346,1095346,neutron,ed1071ec1981821275fedf35c5c90ab0a5080c58,1,1, “recent change to oslo allows the configuration of the interval that ProcessLauncher waits between checks of child exit.” The change didn’t check the current code and caused the bug because the neutron service consumed unnecessary cpu,Excessive CPU usage in ProcessLauncher()'s wait lo...,"See https://review.openstack.org/18689 for some background
We can't use os.wait() to block until a child exited so, instead, we're busy-looping
We should be able to come up with another way of doing this - e.g. using pipes provided to each child to give us a selectable handle we can block on"
1097999,1097999,nova,5161d6c0023151d39fb56a85f739063205e676f4,1,1, “iptables setup multiple times”,multi-process metadata server runs iptables  setup...,"Using Devstack (latest master 1/9/2012)
If I enable multi process metadata service (metadata_workers=5 in nova.conf)
the iptables are modified multiple times:
2013-01-10 00:25:16.0 25535 INFO nova.wsgi [-] metadata listening on 0.0.0.0:8775
2013-01-10 00:25:16.1 25535 INFO nova.service [-] Starting 5 workers
2013-01-10 00:25:16.3 25535 INFO nova.service [-] Started child 255442013-01-10 00:25:16.6 25535 INFO nova.service [-] Started child 255452013-01-10 00:25:16.8 25535 INFO nova.service [-] Started child 25546
2013-01-10 00:25:16.9 25544 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.10 25544 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189
2013-01-10 00:25:16.11 25544 DEBUG nova.openstack.common.lockutils [-] Got file lock ""iptables"" at /opt/stack/data/nova/nova-iptables for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:219
2013-01-10 00:25:16.11 25535 INFO nova.service [-] Started child 25547
2013-01-10 00:25:16.18 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t filter execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:16.20 25535 INFO nova.service [-] Started child 25548
2013-01-10 00:25:16.16 25546 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.22 25546 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189
2013-01-10 00:25:16.27 25548 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.17 25545 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.33 25545 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189
2013-01-10 00:25:16.32 25547 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.33 25547 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189
2013-01-10 00:25:16.42 25548 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189
2013-01-10 00:25:16.214 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:16.216 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:16.368 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:16.370 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t mangle execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:16.535 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:16.536 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:16.703 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:16.704 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t nat execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:16.855 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:16.856 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.18 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.19 25544 DEBUG nova.network.linux_net [-] IPTablesManager.apply completed with success _apply /opt/stack/nova/nova/network/linux_net.py:385
2013-01-10 00:25:17.20 25544 INFO nova.metadata.wsgi.server [-] (25544) wsgi starting up on http://0.0.0.0:8775/
2013-01-10 00:25:17.25 25548 DEBUG nova.openstack.common.lockutils [-] Got file lock ""iptables"" at /opt/stack/data/nova/nova-iptables for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:219
2013-01-10 00:25:17.26 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t filter execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.180 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.182 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.340 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.342 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t mangle execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.510 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.511 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.666 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.667 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t nat execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.800 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.801 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.952 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.953 25548 DEBUG nova.network.linux_net [-] IPTablesManager.apply completed with success _apply /opt/stack/nova/nova/network/linux_net.py:385
2013-01-10 00:25:17.955 25548 INFO nova.metadata.wsgi.server [-] (25548) wsgi starting up on http://0.0.0.0:8775/
2013-01-10 00:25:17.962 25546 DEBUG nova.openstack.common.lockutils [-] Got file lock ""iptables"" at /opt/stack/data/nova/nova-iptables for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:219
2013-01-10 00:25:17.964 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t filter execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.120 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.121 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.278 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.279 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t mangle execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.439 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.441 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.596 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.598 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t nat execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.750 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.751 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.932 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.932 25546 DEBUG nova.network.linux_net [-] IPTablesManager.apply completed with success _apply /opt/stack/nova/nova/network/linux_net.py:385
2013-01-10 00:25:18.934 25546 INFO nova.metadata.wsgi.server [-] (25546) wsgi starting up on http://0.0.0.0:8775/
2013-01-10 00:25:18.941 25545 DEBUG nova.openstack.common.lockutils [-] Got file lock ""iptables"" at /opt/stack/data/nova/nova-iptables for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:219
2013-01-10 00:25:18.942 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t filter execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.94 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.95 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.210 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.210 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t mangle execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.328 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.330 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.470 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.471 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t nat execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.617 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.619 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.735 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.736 25545 DEBUG nova.network.linux_net [-] IPTablesManager.apply completed with success _apply /opt/stack/nova/nova/network/linux_net.py:385
2013-01-10 00:25:19.737 25547 DEBUG nova.openstack.common.lockutils [-] Got file lock ""iptables"" at /opt/stack/data/nova/nova-iptables for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:219
2013-01-10 00:25:19.738 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t filter execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.739 25545 INFO nova.metadata.wsgi.server [-] (25545) wsgi starting up on http://0.0.0.0:8775/
2013-01-10 00:25:19.854 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.855 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.971 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.972 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t mangle execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:20.88 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:20.89 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:20.205 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:20.206 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t nat execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:20.327 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:20.328 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:20.447 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:20.447 25547 DEBUG nova.network.linux_net [-] IPTablesManager.apply completed with success _apply /opt/stack/nova/nova/network/linux_net.py:385
2013-01-10 00:25:20.449 25547 INFO nova.metadata.wsgi.server [-] (25547) wsgi starting up on http://0.0.0.0:8775/"
1100697,1100697,nova,dbb7ef845f5684673b320d0914d4f9cea12fef65,1,0,“Nova doesn't enable pae setting for Xen or KVM guest in its libvirt driver. Windows(Win7 in my enviroment)” A change in the Environment,libvirt should enable pae setting for Xen or KVM g...,"Currently, nova doesn't enable pae setting for Xen or KVM guest in its libvirt driver. Windows(Win7 in my enviroment) guests would not boot successful in such case. This patch adds pae setting in libvirt driver for Xen or KVM guest, which would fix this problem."
1106423,1106423,nova,1df9b26b51ff3a153b93d79e25bef454e08a8e38,0,0,"“If https://review.openstack.org/#/c/18042/ is merged, a new configuration option/feature will be added to nova.” Hypothetical bugs, we do not consider them as bugs",Bug #1106423 “grizzly,"If https://review.openstack.org/#/c/18042/ is merged, a new configuration option/feature will be added to nova. The manuals should be updated to reflect this - configuration references and sample files
In past versions of Nova it was possible to explicitly configure
the cache mode of disks via the libvirt XML template. The curent approach
makes this a derived setting of either “none” or “writethrough” based
on the support of O_DIRECT. Whilst this provides a good set of default
settings it removes the ability of the cloud provider to use other
modes such as “writeback” and “unsafe” which are valuable in certain
configurations.
This change allows the cache mode to be specified on a per-disk type
basis. Leaving the specify_cachemode option set to False retains the
current behaviour.
 189	    cfg.ListOpt('disk_cachemodes',
   190	                 default=[],
   191	                help='Specific cachemodes to use for different disk types '
   192	                     'e.g: [""file=directsync"",""block=none""]'),
 189	    ]	193	    ]
   317	        self.valid_cachemodes = [""default"",
   318	                                 ""none"",
   319	                                 ""writethrough"",
   320	                                 ""writeback"",
   321	                                 ""directsync"",
   322	                                 ""writethrough"",
   323	                                 ""unsafe"",
   324	                                ]"
1131395,1131395,nova,6132f991bdc8515aa665db16fef260ff71a618e6,1,1,“This settings is ignored and all instances go to a stop state rather than termination.”,EC2 API does not support InstanceInstantiatedShutd...,"The EC2 API allows the setting of the InstanceInstantiatedShutdownBehavior to either ""stop"" or ""terminate"".
This settings is ignored and all instances go to a stop state rather than termination.
From the comments, it looks like this was removed in I91845a64 -- maybe Vish can comment before I dive in?"
1136252,1136252,neutron,7a67a5676fb82ddb717e6b606cc702c31523b9fa,1,1,I’m not sure at all. I would say it has a BIC since the beginning the meta plugin had wrong logic,[Metaplugin] fails with VIF extension,"Current metaplugin didn't implements get_ports or get_port to proxy request for actual plugins.
so extension such as VIF extension will not be added the result of port."
1152748,1152748,nova,7eedeb2e5db4cfcd8505cc74d85ec625b7069302,0,0,“needs to be refactored”,libvirt imagebackend cache code encapsulation need...,"libvirt.imagebackend.cache needs to be refactored to eliminate the callback to call_if_not_exists().  The only way it could meet all the requirements while remaining a callback is if the os.path.exists() check were removed from the function, which would then require path checks in each of the backend classes where prepare_template() is called.  Ultimately, this is a lot more code than simply adding a backend-specific call to each class.
There's too much risk associated with such a refactor at such a late stage for Grizzly, so we'll look to tackle this for Havana."
1154303,1154303,nova,67179bf58f35d54bee12e6e8eaf084e2f70ea6a2,0,0,“Improved logs for add/remove security group rules.”,Security group rule AUDIT message could be more us...,"Hi! This is mostly just a wishlist request related to operational usability.
Recently I was investigating a report of changes to security group rules for a tenant and found that the AUDIT log messages captured during security group rule changes was, well, less than useful :)
Example:
2013-03-12 17:25:31 AUDIT nova.compute.api [req-ea8ad999-2154-4631-8d80-e33eeeb5f9b6 a8f944429f2b43758079dfda3a123222 8a25888b704146ab95c1e3e8928253f6] Authorize security group ingress default
What would be more useful to know in this particular AUDIT log message would be something like this:
2013-03-12 17:25:31 AUDIT nova.compute.api [req-ea8ad999-2154-4631-8d80-e33eeeb5f9b6 a8f944429f2b43758079dfda3a123222 8a25888b704146ab95c1e3e8928253f6] Security group default added TCP ingress (22:22)
or:
2013-03-12 17:25:31 AUDIT nova.compute.api [req-ea8ad999-2154-4631-8d80-e33eeeb5f9b6 a8f944429f2b43758079dfda3a123222 8a25888b704146ab95c1e3e8928253f6] Security group default removed ICMP ingress (-1:-1)
Best,
-jay"
1157922,1157922,nova,6a0e6209cab2ced467b63ce9ce69a41daae669fe,1,0,Wrong logic,Instance state is not set to ERROR when guestfs fi...,"Instances have been observed to remain stuck forever in ""BUILD"" state, with no errors surfaced to `nova show` after nova boot fails with the following guestfs error.
2013-03-20 18:49:08,590.590 ERROR nova.compute.manager [req-f85ccdcd-74f1-4f50-98eb-b68fb8dc8e1a dba071d520c9438ab9fb91077b6f3248 1ba6328ea66c4041bfab7cfcbc2305cf] [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] Instance failed to spawn
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] Traceback (most recent call last):
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1055, in _spawn
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     block_device_info)
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1517, in spawn
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     admin_pass=admin_password)
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1913, in _create_image
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     instance=instance)
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     self.gen.next()
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1908, in _create_image
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     mandatory=('files',))
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/disk/api.py"", line 304, in inject_data
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     fs.setup()
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/disk/vfs/guestfs.py"", line 114, in setup
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     {'imgfile': self.imgfile, 'e': e})
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] NovaException: Error mounting /var/lib/nova/instances/5f3fe8ba-a148-48e5-8e19-d2f65968b2db/disk with libguestfs (cannot find any suitable libguestfs supermin, fixed or old-style appliance on LIBGUESTFS_PATH (search path: /usr/lib/guestfs))"
1168318,1168318,nova,2ed3e28442e7dd8ded485cdf96a2053386441051,1,1,"Wrong logic, “should not return empty QemuImgInfo objects”",qemu_image_info should not return empty QemuImgInf...,"In nova/virt/images.py, if qemu-img command fails or the image is missing, instead of returning en empty QemuImgInfo it should probably throw some exception to inform the caller of the situation instead of hiding the problem like it does today."
1175695,1175695,neutron,011d99f300ea5d5f4ce48023bd04a795a4872287,1,1, ,L3 agent restart causes network outage,"When L3 agent is restarted, it destroys all existing namespaces and then recreates them.  This causes a network outage for the affected routers and floating IPs, even if those routers/floating IPs are still valid.  We should be able to preserve existing, valid namespaces across an agent restart and avoid the network outage."
1175940,1175940,glance,1fbf09814481229e722873474d0be56bb7d53803,0,0,Test files,Do not use swift / s3 to test copy file functional...,"Currently, test_copy_to_file test requires swift and / or s3 to test this functionality.
Swift / S3 tests belong to their stores and test_copy_to_file should use a HTTP instance or whatever, to test this functionality.
https://github.com/openstack/glance/blob/master/glance/tests/functional/v1/test_copy_to_file.py"
1177973,1177973,neutron,cb0df591a9508e863ad5d5d71190eca349dc551f,1,1, ,OVS L2 agent polling is too cpu intensive,"On a Devstack-deployed, single-node install, the ovs l2 agent is using an order of magnitude more cpu than any other service.  On a nested-virt VM running on a 2.5GHz host, with no VM's provisioned:
 - WIth L2 agent running, 10-100% cpu usage recorded, averaging ~40%
 - With L2 agent stopped, 0-10% cpu usage recorded
Casual inspection with something like top or htop will show the excessive cpu usage, but won't give a good indication of the culprit.  Enabling the CUTIME and CSTIME stats is necessary to highlight the problem, as all the time taken by the agent is in subprocess-invoked polling whose execution time is not directly included in the parent's cpu usage."
1178375,1178375,cinder,a1fe496e1113737d0b133a64078bc45c485dd3b2,1,1,"“Sync the following fix from oslo-incubator:
    76972e2 Support a new qpid topology
    This includes one other commit, so that the above fix could be brought”",Orphan exchanges in Qpid and lack of option for ma...,"Start qpid, nova-api, nova-scheduler, and nova-conductor, and nova-compute.
There are orphan direct exchanges in qpid. Checked using qpid-config exchanges. The exchanges continue to grow, presumably, whenever nova-compute does a periodic update over AMQP.
Moreover, the direct and topic exchanges are by default durable which is a problem. We want the ability to turn on/off the durable option just like Rabbit options."
1179007,1179007,Swift,4332bff3f500f062e7e06edccf0da5a9d9379d79,0,0,"""Migrate to pbr for build"",""pbr updates are also part of the upcoming automation around ensuring
global requirements stay in sync.""",Migrate build system to pbr,"-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
openstack.common.setup and openstack.common.version are now in the
standalone library pbr. Migrating involves moving build config to
setup.cfg, copying in a stub setup.py file, adding pbr and d2to1 to the
build depends, removing openstack.common.(setup|version) from the
filesystem and from openstack-common.conf and making sure *.egg is in
.gitignore.
 affects ceilometer
 affects cinder
 affects git-review
 affects heat-cfntools
 affects heat
 affects keystone
 affects openstack-ci
 affects oslo
 affects python-ceilometerclient
 affects python-cinderclient
 affects python-gear
 affects python-glanceclient
 affects python-heatclient
 affects python-keystoneclient
 affects python-novaclient
 affects python-openstackclient
 affects python-quantumclient
 affects python-swiftclient
 affects reddwarf
 affects swift
 affects zuul
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.12 (GNU/Linux)
Comment: Using GnuPG with undefined - http://www.enigmail.net/
iEYEARECAAYFAlGObdUACgkQ2Jv7/VK1RgFlkACgzycOW0/rPvnLaXXX9/oqYA7q
kGEAoMaEzGbFEAnsQA6+cEsKIUSMWAPD
=W8F0
-----END PGP SIGNATURE-----"
1180044,1180044,nova,a25b2ac5f440f7ace4678b21ada6ebf5ce5dff3c,1,1,"I’m not sure, maybe when the commit was done the specifications were only one datacenter…",nova failures when vCenter has multiple datacenter...,"The method at vmops.py _get_datacenter_ref_and_name does not calculate datacenter properly.
    def _get_datacenter_ref_and_name(self):
        """"""Get the datacenter name and the reference.""""""
        dc_obj = self._session._call_method(vim_util, ""get_objects"",
                ""Datacenter"", [""name""])
        vm_util._cancel_retrieve_if_necessary(self._session, dc_obj)
        return dc_obj.objects[0].obj, dc_obj.objects[0].propSet[0].val
This will not be correct on systems with more than one datacenter.
Stack trace from logs:
ERROR nova.compute.manager [req-9395fe41-cf04-4434-bd77-663e93de1d4a foo bar] [instance: 484a42a2-642e-4594-93fe-4f72ddad361f] Error: ['Traceback (most recent call last):\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 942, in _build_instance\n    set_access_ip=set_access_ip)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 1204, in _spawn\n    LOG.exception(_(\'Instance failed to spawn\'), instance=instance)\n', '  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__\n    self.gen.next()\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 1200, in _spawn\n    block_device_info)\n', '  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 176, in spawn\n    block_device_info)\n', '  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 208, in spawn\n    _execute_create_vm()\n', '  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 204, in _execute_create_vm\n    self._session._wait_for_task(instance[\'uuid\'], vm_create_task)\n', '  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 559, in _wait_for_task\n    ret_val = done.wait()\n', '  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait\n    return hubs.get_hub().switch()\n', '  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch\n    return self.greenlet.switch()\n', 'NovaException: A specified parameter was not correct. \nspec.location.folder\n']
vCenter error is:
""A specified parameter was not correct. spec.location.folder""
Work around:
use only one datacenter, use only one cluster, turn on DRS
Additional failures:
2013-07-18 10:59:12.788 DEBUG nova.virt.vmwareapi.vmware_images [req-e8306ffe-c6c7-4d0f-a466-fb532375cbd3 7799f10ca7da47f3b2660feb363b370b 0e1771f8db984a3599596fae62609d9a] [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] Got image size of 687865856 for the image cde14862-60b8-4360-a145-06585b06577c get_vmdk_size_and_properties /usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmware_images.py:156
2013-07-18 10:59:12.963 WARNING nova.virt.vmwareapi.network_util [req-e8306ffe-c6c7-4d0f-a466-fb532375cbd3 7799f10ca7da47f3b2660feb363b370b 0e1771f8db984a3599596fae62609d9a] [(ManagedObjectReference){
   value = ""network-1501""
   _type = ""Network""
 }, (ManagedObjectReference){
   value = ""network-1458""
   _type = ""Network""
 }, (ManagedObjectReference){
   value = ""network-2085""
   _type = ""Network""
 }, (ManagedObjectReference){
   value = ""network-1143""
   _type = ""Network""
 }]
2013-07-18 10:59:13.326 DEBUG nova.virt.vmwareapi.vmops [req-e8306ffe-c6c7-4d0f-a466-fb532375cbd3 7799f10ca7da47f3b2660feb363b370b 0e1771f8db984a3599596fae62609d9a] [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] Creating VM on the ESX host _execute_create_vm /usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmops.py:207
2013-07-18 10:59:14.258 3145 DEBUG nova.openstack.common.rpc.amqp [-] Making synchronous call on conductor ... multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:583
2013-07-18 10:59:14.259 3145 DEBUG nova.openstack.common.rpc.amqp [-] MSG_ID is 8ef36d061a9341a09d3a5451df798673 multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:586
2013-07-18 10:59:14.259 3145 DEBUG nova.openstack.common.rpc.amqp [-] UNIQUE_ID is 680b790574c64a9783fd2138c43f5f6d. _add_unique_id /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:337
2013-07-18 10:59:18.757 3145 WARNING nova.virt.vmwareapi.driver [-] Task [CreateVM_Task] (returnval){
   value = ""task-33558""
   _type = ""Task""
 } status: error The input arguments had entities that did not belong to the same datacenter.
2013-07-18 10:59:18.758 ERROR nova.compute.manager [req-e8306ffe-c6c7-4d0f-a466-fb532375cbd3 7799f10ca7da47f3b2660feb363b370b 0e1771f8db984a3599596fae62609d9a] [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] Instance failed to spawn
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] Traceback (most recent call last):
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1103, in _spawn
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] block_device_info)
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/driver.py"", line 177, in spawn
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] block_device_info)
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmops.py"", line 217, in spawn
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] _execute_create_vm()
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmops.py"", line 213, in _execute_create_vm
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] self._session._wait_for_task(instance['uuid'], vm_create_task)
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/driver.py"", line 554, in _wait_for_task
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] ret_val = done.wait()
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] return hubs.get_hub().switch()
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] return self.greenlet.switch()
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] NovaException: The input arguments had entities that did not belong to the same datacenter.
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539]
2013-07-18 10:59:20.029 ERROR nova.compute.manager [req-e8306ffe-c6c7-4d0f-a466-fb532375cbd3 7799f10ca7da47f3b2660feb363b370b 0e1771f8db984a3599596fae62609d9a] [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] Error: ['Traceback (most recent call last):\n', ' File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 848, in _run_instance\n set_access_ip=set_access_ip)\n', ' File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1107, in _spawn\n LOG.exception(_(\'Instance failed to spawn\'), instance=instance)\n', ' File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__\n self.gen.next()\n', ' File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1103, in _spawn\n block_device_info)\n', ' File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/driver.py"", line 177, in spawn\n block_device_info)\n', ' File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmops.py"", line 217, in spawn\n _execute_create_vm()\n', ' File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmops.py"", line 213, in _execute_create_vm\n self._session._wait_for_task(instance[\'uuid\'], vm_create_task)\n', ' File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/driver.py"", line 554, in _wait_for_task\n ret_val = done.wait()\n', ' File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait\n return hubs.get_hub().switch()\n', ' File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch\n return self.greenlet.switch()\n', 'NovaException: The input arguments had entities that did not belong to the same datacenter.\n']
2013-07-18 10:59:23.831 3145 WARNING nova.virt.vmwareapi.driver [-] Task [CreateVM_Task] (returnval){
   value = ""task-33558""
   _type = ""Task""
 } status: error The input arguments had entities that did not belong to the same datacenter.
2013-07-18 10:59:23.832 3145 WARNING nova.virt.vmwareapi.driver [-] In vmwareapi:_poll_task, Got this error Trying to re-send() an already-triggered event.
2013-07-18 10:59:23.833 3145 ERROR nova.utils [-] in fixed duration looping call
2013-07-18 10:59:23.833 3145 TRACE nova.utils Traceback (most recent call last):
2013-07-18 10:59:23.833 3145 TRACE nova.utils File ""/usr/lib/python2.7/dist-packages/nova/utils.py"", line 594, in _inner
2013-07-18 10:59:23.833 3145 TRACE nova.utils self.f(*self.args, **self.kw)
2013-07-18 10:59:23.833 3145 TRACE nova.utils File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/driver.py"", line 580, in _poll_task
2013-07-18 10:59:23.833 3145 TRACE nova.utils done.send_exception(excep)
2013-07-18 10:59:23.833 3145 TRACE nova.utils File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 208, in send_exception
2013-07-18 10:59:23.833 3145 TRACE nova.utils return self.send(None, args)
2013-07-18 10:59:23.833 3145 TRACE nova.utils File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 150, in send
2013-07-18 10:59:23.833 3145 TRACE nova.utils assert self._result is NOT_USED, 'Trying to re-send() an already-triggered event.'
2013-07-18 10:59:23.833 3145 TRACE nova.utils AssertionError: Trying to re-send() an already-triggered event.
2013-07-18 10:59:23.833 3145 TRACE nova.utils"
1182131,1182131,nova,72dd81343e73baf838bc58d413413f0d57018f15,1,1,“Expected result: no stackstrace to be thrown”,Bug #1182131 “nova-compute,"Hi,
Steps to reproduce:
1) create a security group that is referencing itself, for example
euca-create-group test2
euca-authorize test2 -P tcp -p 22 -s 0.0.0.0/0
euca-authorize test2 -P tcp -p 6666 -o test2
2) create any instance in this security group
euca-run-instance .. -g test2 ..
Expected result:
no stackstrace to be thrown
Actual result:
stacktrace with KeyError appears in the log. The iptable rules are created correctly and instance ends up in running state.
File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 390, in refresh_instance_security_rules
  return self.driver.refresh_instance_security_rules(instance)
File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 2269, in refresh_instance_security_rules
  self.firewall_driver.refresh_instance_security_rules(instance)
File ""/usr/lib/python2.6/site-packages/nova/virt/firewall.py"", line 440, in refresh_instance_security_rules
  self.do_refresh_instance_rules(instance)
File ""/usr/lib/python2.6/site-packages/nova/virt/firewall.py"", line 457, in do_refresh_instance_rules
  network_info = self.network_infos[instance['id']]
KeyError: 4168
It seems that self.network_infos is accessed in wrong order for the security group that is referencing itself. The stacktrace is from 'do_refresh_instance_rules' which expects network info to be already present for the instance that is being created. Reported KeyError is the id of newly created instance. The dictionary entry is added few seconds later processing the same request.
Fortunately, this issue does not appear to have any negative impact aside the stacktrace in the log.
Openstack version: Folsom 2012.2.4
Attaching verbose log from nova-compute.
Regards,
Brano Zarnovican"
1183152,1183152,swift,1bc4fe891a359c626ebc8049403d4d72a75b6be7,1,1, ,statsd timing unhandled exception operation not pe...,"-container-5.2..com: May 22 22:49:25 container-server ERROR __ll__ error with DELETE /d84/58775/AUTH_benchmark/bench_000078/tiny_020610 :
    Traceback (most recent ll last):
      File ""/opt//lib/python2.7/site-packages/swift/container/server.py"", line 474, in __ll__
        res = method(req)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 1458, in wrapped
        return func(*a, **kw)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 480, in _timing_stats
        ctrl.logger.timing_since(method + '.timing', start_time)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 610, in wrapped
        return func(self.logger.statsd_client, *a, **kw)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 464, in timing_since
        sample_rate)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 460, in timing
        return self._send(metric, timing_ms, 'ms', sample_rate)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 445, in _send
        return sock.sendto('|'.join(parts), self._target)
      File ""/opt//lib/python2.7/site-packages/eventlet/greenio.py"", line 290, in sendto
        return self.fd.sendto(*args)
    error: [Errno 1] Operation not permitted (txn: tx60ec62fa962d4d96ba5dace5d81eede2)
Just need to audit the error handling, I think this happened under high load and the network was saturated maybe unable to send new udp packets or something..."
1187244,1187244,nova,322cc9336fe6f6fe9b3f0da33c6b26a3e5ea9b0c,1,1,“there is a NetworkDuplicated erro”,"quantum NetworkDuplicated error ,when boot a vm wi...","in /var/log/nova/nova-api.log
```
013-06-04 12:01:17.998 ERROR nova.api.openstack [req-ea58ab37-aea6-40a4-b85f-2ab19285101e 5d45600dae844064b514f1549fe0dab9 b082fcb819db4104bb6d3dc18bcc4f17] Caught error: Network 5332f0f7-3156-4961-aa67-0b8507265fa5 is duplicated.
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack Traceback (most recent call last):
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/__init__.py"", line 81, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return req.get_response(self.application)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     application, catch_exc_info=False)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return resp(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/keystoneclient/middleware/auth_token.py"", line 450, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return self.app(env, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return resp(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return resp(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return resp(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     response = self.app(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return resp(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 890, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     content_type, body, accept)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 942, in _process_stack
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 1022, in dispatch
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return method(req=request, **action_args)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/servers.py"", line 898, in create
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     scheduler_hints=scheduler_hints)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/hooks.py"", line 85, in inner
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     rv = f(*args, **kwargs)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 962, in create
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     scheduler_hints=scheduler_hints)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 676, in _create_instance
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     reservation_id, scheduler_hints)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 634, in _validate_and_provision_instance
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     QUOTAS.rollback(context, quota_reservations)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     self.gen.next()
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 522, in _validate_and_provision_instance
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     self._check_requested_networks(context, requested_networks)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 358, in _check_requested_networks
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     self.network_api.validate_networks(context, requested_networks)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/network/quantumv2/api.py"", line 454, in validate_networks
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     raise exception.NetworkDuplicated(network_id=net_id)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack NetworkDuplicated: Network 5332f0f7-3156-4961-aa67-0b8507265fa5 is duplicated.
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack
2013-06-04 12:01:18.126 INFO nova.api.openstack [req-ea58ab37-aea6-40a4-b85f-2ab19285101e 5d45600dae844064b514f1549fe0dab9 b082fcb819db4104bb6d3dc18bcc4f17] http://172.16.136.111:8774/v2/b082fcb819db4104bb6d3dc18bcc4f17/os-volumes_boot returned with HTTP 500
2013-06-04 12:01:18.129 INFO nova.osapi_compute.wsgi.server [req-ea58ab37-aea6-40a4-b85f-2ab19285101e 5d45600dae844064b514f1549fe0dab9 b082fcb819db4104bb6d3dc18bcc4f17] 172.16.136.111 ""POST /v2/b082fcb819db4104bb6d3dc18bcc4f17/os-volumes_boot HTTP/1.1"" status: 500 len: 335 time: 0.4138958
```
how to reproduce this error
we need two subnets which belongs to one network
in my environment,
network id is 5332f0f7-3156-4961-aa67-0b8507265fa5
subnet1 id is e8a9be74-2f39-4d7e-9287-c5b85b573cca
subnet2 id is dca45033-e506-42e4-bf05-aaccd0591c55
1. create ports  from subnet1 and subnet2
quantum port-create --fixed-ip subnet_id=e8a9be74-2f39-4d7e-9287-c5b85b573cca  5332f0f7-3156-4961-aa67-0b8507265fa5
port id is :  dca1f741-a1fc-444c-b764-7aa74ed29d1f
quantum port-create --fixed-ip subnet_id=dca45033-e506-42e4-bf05-aaccd0591c55  5332f0f7-3156-4961-aa67-0b8507265fa5
port id is : 72acaba1-6e84-468b-b3f7-52eb89972eb7
2. boot with two port id
nova boot --flavor 1 --nic port-id=dca1f741-a1fc-444c-b764-7aa74ed29d1f  --nic port-id=72acaba1-6e84-468b-b3f7-52eb89972eb7 --block_device_mapping vda=1cf2a0b0-8c7f-4de1-8dba-91bae1f1856c:::0 m1
ERROR: The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-ea58ab37-aea6-40a4-b85f-2ab19285101e)
it returns an error , and there is a NetworkDuplicated error  in /var/log/nova/nova-api.log
i refered this bug https://bugs.launchpad.net/nova/+bug/1165088 , but it no helps ."
1189671,1189671,neutron,de15e0b9c51cf9124de41258c1e3d774de215213,0,0,“Can we change the default to quantum.db.quota_db.DbQuotaDriver please3”,default quota driver not suitable for production,"The default quota driver is conf file driven, which isn't particularly useful in non-trivial clouds.
Can we change the default to  quantum.db.quota_db.DbQuotaDriver please?"
1190615,1190615,neutron,872d1982716acf426d21dcac838102e9479408f7,0,0,Test files,Improve unit test coverage for Cisco plugin common...,"Improve unit test coverage for ...
quantum/plugins/cisco/common/cisco_constants	77	0	0	0	0	100%
quantum/plugins/cisco/common/cisco_credentials_v2	30	6	0	4	0	82%
quantum/plugins/cisco/common/cisco_exceptions	37	0	0	2	0	100%
quantum/plugins/cisco/common/cisco_faults	34	34	0	0	0	0%
quantum/plugins/cisco/common/cisco_utils	15	15	0	0	0	0%
quantum/plugins/cisco/common/config	23	2	0	8	1	84%"
1190620,1190620,neutron,b6a8aea4d1fe8be6073af57fad2ab6863d8f359c,0,0,Test files,Improve unit test coverage for Cisco plugin model ...,"Improve unit test coverage for ...
quantum/plugins/cisco/models/virt_phy_sw_v2	193	34	0	31	5	78%"
1190621,1190621,neutron,3cb9168822a0805a9b8904ce54577f3e3f05d9eb,0,0,Test files,Improve unit test coverage for Cisco plugin base c...,"Improve unit test coverage for ...
quantum/plugins/cisco/network_plugin	199	131	0	17	2	34%"
1190622,1190622,neutron,26e9fad61f86bedd16179ede9736b56c4a9a5752,0,0,Test files,Improve unit test coverage for Cisco plugin nexus ...,"Improve unit test coverage for ...
quantum/plugins/cisco/nexus/cisco_nexus_network_driver_v2	100	24	0	26	1	72%
quantum/plugins/cisco/nexus/cisco_nexus_plugin_v2	117	23	0	12	2	79%"
1191812,1191812,cinder,f702fe7e30e4021895dac8e7ab243e5192f8182d,1,1, ,volume cloning is failing with an I/O error happen...,"cloning a volume using the --source-volid argument is failing with the following in the volume.log file:
""""""
Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf dd if=/dev/zero of=/dev/mapper/cinder--volumes-clone--snap--727c84cc--69ee--4b7d--bdb0--832b5086f1bb count=1024 bs=1M conv=fdatasync
Exit code: 1
Stdout: ''
Stderr: ""/bin/dd: fdatasync failed for `/dev/mapper/cinder--volumes-clone--snap--727c84cc--69ee--4b7d--bdb0--832b5086f1bb': Input/output error\n1024+0 records in\n1024+0 records out\n1073741824 bytes (1.1 GB) copied, 19.7612 s, 54.3 MB/s\n""
""""""
NOTE: the actual dd from the origin snapshot into the destination volume is working; it is the cleanup of the origin snapshot which fails"
1195139,1195139,nova,e2efa57ab2fb03b92f80abbf612a257c28475140,1,0,Change in the environment “This fails is some DBs like postgresql. Needs a fix in the VMware driver”,Bug #1195139 “vmware,"Hi,
I am trying to use VMware hypervisor ESXi 5.0.0 as a compute resource in OpenStack 2013.1.1. The driver being used is ""•vmwareapi.VMwareVCDriver"". The vCenter is contains a single cluster (two nodes) running ESXi version 5.0.
In the compute node, I am seeing the below error
2013-06-26 17:45:27.532 10253 AUDIT nova.compute.resource_tracker [-] Free ram (MB): 146933
2013-06-26 17:45:27.532 10253 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 55808
2013-06-26 17:45:27.533 10253 AUDIT nova.compute.resource_tracker [-] Free VCPUS: 24
2013-06-26 17:45:27.533 10253 DEBUG nova.openstack.common.rpc.amqp [-] Making synchronous call on conductor ... multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:583
2013-06-26 17:45:27.534 10253 DEBUG nova.openstack.common.rpc.amqp [-] MSG_ID is ece63342e4254910be13ee92b948ace6 multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:586
2013-06-26 17:45:27.534 10253 DEBUG nova.openstack.common.rpc.amqp [-] UNIQUE_ID is c85fb461be22468b846368a6b8608ac2. _add_unique_id /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:337
2013-06-26 17:45:27.566 10253 DEBUG nova.openstack.common.rpc.amqp [-] Making synchronous call on conductor ... multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:583
2013-06-26 17:45:27.567 10253 DEBUG nova.openstack.common.rpc.amqp [-] MSG_ID is 3f5d065d1c8a4989b4d36415d45abe8b multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:586
2013-06-26 17:45:27.567 10253 DEBUG nova.openstack.common.rpc.amqp [-] UNIQUE_ID is 1421bebfd9744271be63dcef74551c93. _add_unique_id /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:337
2013-06-26 17:45:27.597 10253 CRITICAL nova [-] Remote error: DBError (DataError) invalid input syntax for integer: ""5.0.0""
LINE 1: ..., 7, 24, 147445, 55808, 0, 512, 0, 'VMware ESXi', '5.0.0', '...
                                                             ^
 'INSERT INTO compute_nodes (created_at, updated_at, deleted_at, deleted, service_id, vcpus, memory_mb, local_gb, vcpus_used, memory_mb_used, local_gb_used, hypervisor_type, hypervisor_version, hypervisor_hostname, free_ram_mb, free_disk_gb, current_workload, running_vms, cpu_info, disk_available_least) VALUES (%(created_at)s, %(updated_at)s, %(deleted_at)s, %(deleted)s, %(service_id)s, %(vcpus)s, %(memory_mb)s, %(local_gb)s, %(vcpus_used)s, %(memory_mb_used)s, %(local_gb_used)s, %(hypervisor_type)s, %(hypervisor_version)s, %(hypervisor_hostname)s, %(free_ram_mb)s, %(free_disk_gb)s, %(current_workload)s, %(running_vms)s, %(cpu_info)s, %(disk_available_least)s) RETURNING compute_nodes.id' {'local_gb': 55808, 'vcpus_used': 0, 'deleted': 0, 'hypervisor_type': u'VMware ESXi', 'created_at': datetime.datetime(2013, 6, 26, 12, 15, 34, 804731), 'local_gb_used': 0, 'updated_at': None, 'hypervisor_hostname': u'10.100.10.42', 'memory_mb': 147445, 'current_workload': 0, 'vcpus': 24, 'free_ram_mb': 146933, 'running_vms': 0, 'free_disk_gb': 55808, 'service_id': 7, 'hypervisor_version': u'5.0.0', 'disk_available_least': None, 'deleted_at': None, 'cpu_info': u'{""model"": ""Intel(R) Xeon(R) CPU           L5640  @ 2.27GHz"", ""vendor"": ""HP"", ""topology"": {""cores"": 12, ""threads"": 24, ""sockets"": 2}}', 'memory_mb_used': 512}
[u'Traceback (most recent call last):\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 430, in _process_data\n    rval = self.proxy.dispatch(ctxt, version, method, **args)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 133, in dispatch\n    return getattr(proxyobj, method)(ctxt, **kwargs)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/conductor/manager.py"", line 350, in compute_node_create\n    result = self.db.compute_node_create(context, values)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/api.py"", line 192, in compute_node_create\n    return IMPL.compute_node_create(context, values)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py"", line 96, in wrapper\n    return f(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py"", line 498, in compute_node_create\n    compute_node_ref.save()\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/models.py"", line 54, in save\n    session.flush()\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/session.py"", line 437, in _wrap\n    raise exception.DBError(e)\n', u'DBError: (DataError) invalid input syntax for integer: ""5.0.0""\nLINE 1: ..., 7, 24, 147445, 55808, 0, 512, 0, \'VMware ESXi\', \'5.0.0\', \'...\n                                                             ^\n \'INSERT INTO compute_nodes (created_at, updated_at, deleted_at, deleted, service_id, vcpus, memory_mb, local_gb, vcpus_used, memory_mb_used, local_gb_used, hypervisor_type, hypervisor_version, hypervisor_hostname, free_ram_mb, free_disk_gb, current_workload, running_vms, cpu_info, disk_available_least) VALUES (%(created_at)s, %(updated_at)s, %(deleted_at)s, %(deleted)s, %(service_id)s, %(vcpus)s, %(memory_mb)s, %(local_gb)s, %(vcpus_used)s, %(memory_mb_used)s, %(local_gb_used)s, %(hypervisor_type)s, %(hypervisor_version)s, %(hypervisor_hostname)s, %(free_ram_mb)s, %(free_disk_gb)s, %(current_workload)s, %(running_vms)s, %(cpu_info)s, %(disk_available_least)s) RETURNING compute_nodes.id\' {\'local_gb\': 55808, \'vcpus_used\': 0, \'deleted\': 0, \'hypervisor_type\': u\'VMware ESXi\', \'created_at\': datetime.datetime(2013, 6, 26, 12, 15, 34, 804731), \'local_gb_used\': 0, \'updated_at\': None, \'hypervisor_hostname\': u\'10.100.10.42\', \'memory_mb\': 147445, \'current_workload\': 0, \'vcpus\': 24, \'free_ram_mb\': 146933, \'running_vms\': 0, \'free_disk_gb\': 55808, \'service_id\': 7, \'hypervisor_version\': u\'5.0.0\', \'disk_available_least\': None, \'deleted_at\': None, \'cpu_info\': u\'{""model"": ""Intel(R) Xeon(R) CPU           L5640  @ 2.27GHz"", ""vendor"": ""HP"", ""topology"": {""cores"": 12, ""threads"": 24, ""sockets"": 2}}\', \'memory_mb_used\': 512}\n'].
2013-06-26 17:45:27.597 10253 TRACE nova Traceback (most recent call last):
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/bin/nova-compute"", line 85, in <module>
2013-06-26 17:45:27.597 10253 TRACE nova     service.wait()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 689, in wait
2013-06-26 17:45:27.597 10253 TRACE nova     _launcher.wait()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 209, in wait
2013-06-26 17:45:27.597 10253 TRACE nova     super(ServiceLauncher, self).wait()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 179, in wait
2013-06-26 17:45:27.597 10253 TRACE nova     service.wait()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2013-06-26 17:45:27.597 10253 TRACE nova     return self._exit_event.wait()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-06-26 17:45:27.597 10253 TRACE nova     return hubs.get_hub().switch()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-06-26 17:45:27.597 10253 TRACE nova     return self.greenlet.switch()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2013-06-26 17:45:27.597 10253 TRACE nova     result = function(*args, **kwargs)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 147, in run_server
2013-06-26 17:45:27.597 10253 TRACE nova     server.start()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 446, in start
2013-06-26 17:45:27.597 10253 TRACE nova     self.manager.pre_start_hook(rpc_connection=self.conn)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 612, in pre_start_hook
2013-06-26 17:45:27.597 10253 TRACE nova     self.update_available_resource(nova.context.get_admin_context())
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 3877, in update_available_resource
2013-06-26 17:45:27.597 10253 TRACE nova     rt.update_available_resource(context)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 242, in inner
2013-06-26 17:45:27.597 10253 TRACE nova     retval = f(*args, **kwargs)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 272, in update_available_resource
2013-06-26 17:45:27.597 10253 TRACE nova     self._sync_compute_node(context, resources)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 293, in _sync_compute_node
2013-06-26 17:45:27.597 10253 TRACE nova     self._create(context, resources)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 293, in _sync_compute_node
2013-06-26 17:45:27.597 10253 TRACE nova     self._create(context, resources)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 307, in _create
2013-06-26 17:45:27.597 10253 TRACE nova     values)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/conductor/api.py"", line 617, in compute_node_create
2013-06-26 17:45:27.597 10253 TRACE nova     return self.conductor_rpcapi.compute_node_create(context, values)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py"", line 349, in compute_node_create
2013-06-26 17:45:27.597 10253 TRACE nova     return self.call(context, msg, version='1.33')
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/proxy.py"", line 80, in call
2013-06-26 17:45:27.597 10253 TRACE nova     return rpc.call(context, self._get_topic(topic), msg, timeout)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/__init__.py"", line 140, in call
2013-06-26 17:45:27.597 10253 TRACE nova     return _get_impl().call(CONF, context, topic, msg, timeout)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py"", line 798, in call
2013-06-26 17:45:27.597 10253 TRACE nova     rpc_amqp.get_connection_pool(conf, Connection))
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 612, in call
2013-06-26 17:45:27.597 10253 TRACE nova     rv = list(rv)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 561, in __iter__
2013-06-26 17:45:27.597 10253 TRACE nova     raise result
2013-06-26 17:45:27.597 10253 TRACE nova RemoteError: Remote error: DBError (DataError) invalid input syntax for integer: ""5.0.0""
2013-06-26 17:45:27.597 10253 TRACE nova LINE 1: ..., 7, 24, 147445, 55808, 0, 512, 0, 'VMware ESXi', '5.0.0', '...
2013-06-26 17:45:27.597 10253 TRACE nova                                                              ^
2013-06-26 17:45:27.597 10253 TRACE nova  'INSERT INTO compute_nodes (created_at, updated_at, deleted_at, deleted, service_id, vcpus, memory_mb, local_gb, vcpus_used, memory_mb_used, local_gb_used, hypervisor_type, hypervisor_version, hypervisor_hostname, free_ram_mb, free_disk_gb, current_workload, running_vms, cpu_info, disk_available_least) VALUES (%(created_at)s, %(updated_at)s, %(deleted_at)s, %(deleted)s, %(service_id)s, %(vcpus)s, %(memory_mb)s, %(local_gb)s, %(vcpus_used)s, %(memory_mb_used)s, %(local_gb_used)s, %(hypervisor_type)s, %(hypervisor_version)s, %(hypervisor_hostname)s, %(free_ram_mb)s, %(free_disk_gb)s, %(current_workload)s, %(running_vms)s, %(cpu_info)s, %(disk_available_least)s) RETURNING compute_nodes.id' {'local_gb': 55808, 'vcpus_used': 0, 'deleted': 0, 'hypervisor_type': u'VMware ESXi', 'created_at': datetime.datetime(2013, 6, 26, 12, 15, 34, 804731), 'local_gb_used': 0, 'updated_at': None, 'hypervisor_hostname': u'10.100.10.42', 'memory_mb': 147445, 'current_workload': 0, 'vcpus': 24, 'free_ram_mb': 146933, 'running_vms': 0, 'free_disk_gb': 55808, 'service_id': 7, 'hypervisor_version': u'5.0.0', 'disk_available_least': None, 'deleted_at': None, 'cpu_info': u'{""model"": ""Intel(R) Xeon(R) CPU           L5640  @ 2.27GHz"", ""vendor"": ""HP"", ""topology"": {""cores"": 12, ""threads"": 24, ""sockets"": 2}}', 'memory_mb_used': 512}
2013-06-26 17:45:27.597 10253 TRACE nova [u'Traceback (most recent call last):\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 430, in _process_data\n    rval = self.proxy.dispatch(ctxt, version, method, **args)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 133, in dispatch\n    return getattr(proxyobj, method)(ctxt, **kwargs)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/conductor/manager.py"", line 350, in compute_node_create\n    result = self.db.compute_node_create(context, values)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/api.py"", line 192, in compute_node_create\n    return IMPL.compute_node_create(context, values)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py"", line 96, in wrapper\n    return f(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py"", line 498, in compute_node_create\n    compute_node_ref.save()\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/models.py"", line 54, in save\n    session.flush()\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/session.py"", line 437, in _wrap\n    raise exception.DBError(e)\n', u'DBError: (DataError) invalid input syntax for integer: ""5.0.0""\nLINE 1: ..., 7, 24, 147445, 55808, 0, 512, 0, \'VMware ESXi\', \'5.0.0\', \'...\n                                                             ^\n \'INSERT INTO compute_nodes (created_at, updated_at, deleted_at, deleted, service_id, vcpus, memory_mb, local_gb, vcpus_used, memory_mb_used, local_gb_used, hypervisor_type, hypervisor_version, hypervisor_hostname, free_ram_mb, free_disk_gb, current_workload, running_vms, cpu_info, disk_available_least) VALUES (%(created_at)s, %(updated_at)s, %(deleted_at)s, %(deleted)s, %(service_id)s, %(vcpus)s, %(memory_mb)s, %(local_gb)s, %(vcpus_used)s, %(memory_mb_used)s, %(local_gb_used)s, %(hypervisor_type)s, %(hypervisor_version)s, %(hypervisor_hostname)s, %(free_ram_mb)s, %(free_disk_gb)s, %(current_workload)s, %(running_vms)s, %(cpu_info)s, %(disk_available_least)s) RETURNING compute_nodes.id\' {\'local_gb\': 55808, \'vcpus_used\': 0, \'deleted\': 0, \'hypervisor_type\': u\'VMware ESXi\', \'created_at\': datetime.datetime(2013, 6, 26, 12, 15, 34, 804731), \'local_gb_used\': 0, \'updated_at\': None, \'hypervisor_hostname\': u\'10.100.10.42\', \'memory_mb\': 147445, \'current_workload\': 0, \'vcpus\': 24, \'free_ram_mb\': 146933, \'running_vms\': 0, \'free_disk_gb\': 55808, \'service_id\': 7, \'hypervisor_version\': u\'5.0.0\', \'disk_available_least\': None, \'deleted_at\': None, \'cpu_info\': u\'{""model"": ""Intel(R) Xeon(R) CPU           L5640  @ 2.27GHz"", ""vendor"": ""HP"", ""topology"": {""cores"": 12, ""threads"": 24, ""sockets"": 2}}\', \'memory_mb_used\': 512}\n'].
2013-06-26 17:45:27.597 10253 TRACE nova
Is this a bug with OpenStack?
Thank you,
Pragadees"
1195947,1195947,nova,8f932311da19ea9de7ba1b344484ccdb748f5786,1,1, ,VM re-scheduler mechanism will cause BDM-volumes c...,"Due to re-scheduler mechanism, when a user tries to
 create (in error) an instance using a volume
 which is already in use by another instance,
the error is correctly detected, but the recovery code
 will incorrectly affect the original instance.
Need to raise exception directly when the situation above occurred.
------------------------
------------------------
We can create VM1 with BDM-volumes (for example, one volume we called it “Vol-1”).
But when the attached-volume (Vol-1..) involved in BDM parameters to create a new VM2, due to VM re-scheduler mechanism, the volume will change to attach on the new VM2 in Nova & Cinder, instead of raise an “InvalidVolume” exception of “Vol-1 is already attached on VM1”.
In actually, Vol-1 both attached on VM1 and VM2 on hypervisor. But when you operate Vol-1 on VM1, you can’t see any corresponding changes on VM2…
I reproduced it and wrote in the doc. Please check the attachment for details~
-------------------------
I checked on the Nova codes, the problem is caused by VM re-scheduler mechanism:
Now Nova will check the state of BDM-volumes from Cinder now [def _setup_block_device_mapping() in manager.py]. If any state is “in-use”, this request will fail, and trigger VM re-scheduler.
According to existing processes in Nova, before VM re-scheduler, it will shutdown VM and detach all BDM-volumes in Cinder for rollback [def _shutdown_instance() in manager.py]. As the result, the state of Vol-1 will change from “in-use” to “available” in Cinder. But, there’re nothing detach-operations on the Nova side…
Therefore, after re-scheduler, it will pass the BDM-volumes checking in creating VM2 on the second time, and all VM1’s BDM-volumes (Vol-1) will be possessed by VM2 and are recorded in Nova & Cinder DB. But Vol-1 is still attached on VM1 on hypervisor, and will also attach on VM2 after VM creation success…
---------------
Moreover, the problem mentioned-above will occur when “delete_on_termination” of BDMs is “False”. If the flag is “True”, all BDM-volumes will be deleted in Cinder because the states are already changed from “in-use” to “available” before [def _cleanup_volumes() in manager.py].
(P.S. Success depends on the specific implementation of Cinder Driver)
Thanks~"
1196924,1196924,nova,b32d01d44ca5711c96d192df51bf7acd34f52556,0,0,Feature should give guest a chance to shutdown,Stop and Delete operations should give the Guest a...,"This feature will cause an ACPI event to be sent to the system while shutting down, and the acpid running inside the system can catch the event, thus giving the system a chance to shutdown cleanly.
[Impact]
 * VMs being shutdown with any signal/notification from the The hypervisor level, services running inside VMs have no chance to perform a clean shutoff
[Test Case]
 * 1. stop a VM
   2. the VM is shutdown without any notification
The can be easily seen by ssh into the system before shutting down. With the patch in place, the ssh session will be close during shutdown, because the sshd has the chance to close the connection before being brought down. Without the patch, the ssh session will just hang there for a while until timeout, because the connection is not promptly closed.
To leverage the clean shutdown feature, one can create a file named /etc/acpi/events/power that contains the following:
              event=button/power
              action=/etc/acpi/power.sh ""%e""
Then   create   a  file  named  /etc/acpi/power.sh  that  contains  whatever required to gracefully shutdown a particular server (VM).
With the apicd running, shutdown of the VM will cause  the rule in /etc/acpi/events/power to trigger the script in /etc/acpi/power.sh, thus cleanly shutdown the system.
[Regression Potential]
 * none
Currently in libvirt stop and delete operations simply destroy the underlying VM.     Some GuestOS's do not react well to this type of power failure, and it would be better if these operations followed the same approach a a soft_reboot and give the guest a chance to shutdown gracefully.   Even where VM is being deleted, it may be booted from a volume which will be reused on another server."
1196963,1196963,Neutron,a369f9e39691c01a4e4f7f8668cb37fc17ba03b3,0,0,"""As part of the changes to support multiple tunnel_types [1] and deprecate enable_tunneling [2]""",Update the OVS agent code to program tunnels using...,"As part of the changes to support multiple tunnel_types [1] and deprecate enable_tunneling [2] in the agent configuration, it was decided during the review to program tunnels using port endpoints instead of using tunnel IDs. Once the OVS agent can concurrently support both GRE and VXLAN tunnel types in the context of ML2, this becomes relevant and will prevent tunnels sharing the same tunnel ID from polluting their BUM traffic between each other.
[1] https://review.openstack.org/#/c/33107/16
[2] https://review.openstack.org/#/c/34779/"
1198796,1198796,nova,f10f67dcc82dc7fda5aaf546b145dad34db82f7b,1,1, “The method nova.api.openstack.compute.contrib.cells_filter_keys() uses the 'Query' object as a dict”,update cell error,"I updated the cell info and the response is as follows:
{
    ""computeFault"": {
        ""message"": ""The server has either erred or is incapable of performing the requested operation."",
        ""code"": 500
    }
}
I found error in nova-cells.log:
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack Traceback (most recent call last):
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/__init__.py"", line 81, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return req.get_response(self.application)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     application, catch_exc_info=False)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return resp(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/keystoneclient/middleware/auth_token.py"", line 450, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return self.app(env, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return resp(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return resp(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return resp(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     response = self.app(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return resp(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 890, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     content_type, body, accept)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 942, in _process_stack
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 1022, in dispatch
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return method(req=request, **action_args)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/contrib/cells.py"", line 257, in update
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return dict(cell=_scrub_cell(cell))
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/contrib/cells.py"", line 118, in _scrub_cell
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     cell_info = _filter_keys(cell, keys)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/contrib/cells.py"", line 110, in _filter_keys
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return dict((k, v) for k, v in item.iteritems() if k in keys)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack AttributeError: 'Query' object has no attribute 'iteritems'
I viewed the code and found that:
@require_admin_context
def cell_update(context, cell_name, values):
    session = get_session()
    with session.begin():
        cell = _cell_get_by_name_query(context, cell_name, session=session)
        cell.update(values)
    return cell
The method nova.db.sqlalchemy.api.cell_update() returns a 'Query' object.
def _filter_keys(item, keys):
    """"""
    Filters all model attributes except for keys
    item is a dict
    """"""
    return dict((k, v) for k, v in item.iteritems() if k in keys)
The method nova.api.openstack.compute.contrib.cells_filter_keys() uses the 'Query' object as a dict.
This bring on the error."
1199308,1199308,nova,d6e6c35ff653565aa65e049ed1de371235b261de,1,1,Ambiguity33 “The exception class InvalidInstanceIDMalformed in exception.py could be changed to something like InvalidEC2IDMalformed.”,Ambiguous exception class for validate_ec2_id,"The validate_ec2_id() method is used to validate both the Instance ID as well as Volume ID for valid EC2 ID format.
However the exception class raised in both cases, if the respective ID were invalid is ""InvalidInstanceIDMalformed"".
This is ambiguous and needs to be fixed such that a clearer exception is seen in the stack trace.
The exception class InvalidInstanceIDMalformed in exception.py could be changed to something like InvalidEC2IDMalformed."
1199954,1199954,nova,c586d635387e9baa3c0857afb56d05137fcddd7c,1,1, ,Bug #1199954 “VCDriver,"Steps to reproduce:
nova resize <UUID> 2
Error:
 ERROR nova.openstack.common.rpc.amqp [req-762f3a87-7642-4bd3-a531-2bcc095ec4a5 demo demo] Exception during message handling
  Traceback (most recent call last):
    File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 421, in _process_data
      **args)
    File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
      result = getattr(proxyobj, method)(ctxt, **kwargs)
    File ""/opt/stack/nova/nova/exception.py"", line 99, in wrapped
      temp_level, payload)
    File ""/opt/stack/nova/nova/exception.py"", line 76, in wrapped
      return f(self, context, *args, **kw)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 218, in decorated_function
      pass
    File ""/opt/stack/nova/nova/compute/manager.py"", line 204, in decorated_function
      return function(self, context, *args, **kwargs)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 269, in decorated_function
      function(self, context, *args, **kwargs)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 246, in decorated_function
      e, sys.exc_info())
    File ""/opt/stack/nova/nova/compute/manager.py"", line 233, in decorated_function
      return function(self, context, *args, **kwargs)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 2633, in resize_instance
      block_device_info)
    File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 410, in migrate_disk_and_power_off
      dest, instance_type)
    File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 893, in migrate_disk_and_power_off
      raise exception.HostNotFound(host=dest)
  HostNotFound:"
1201873,1201873,nova,1e3d2fbcb13e01bee0a8f90bd2078b1f5063b4d5,1,0, “This bug describes and interaction between Openstack and Ubuntu”,"Bug #1201873 “dnsmasq does not use -h, so /etc/hosts sends folks... ","from dnsmasq(8):
      -h, --no-hosts
              Don't read the hostnames in /etc/hosts.
I reliably get bit by this during certain kinds of deployments, where my nova-network/dns host has an entry in /etc/hosts such as:
127.0.1.1    hostname.example.com hostname
I keep having to edit /etc/hosts on that machine to use a real IP, because juju gets really confused when it looks up certain openstack hostnames and gets sent to its own instance!"
1201875,1201875,swift,56440eb95da79506cc27d92e07f0f5969cc683ce,1,1, ,POST &  X-Copy-From can result in over quota with ...,"Bug  #1200271 reported a bug when using X-Copy-From together with account_quotas resulting in an over-quota.
This also affects container_quotas; additionally container_quotas doesn't check POST operations, for example from formpost middleware."
1202177,1202177,Nova,3e1e2448bf162bd2750416efe5d7c1010b51b52a,1,1,"""After catching an exception, it is required to save off the exception details""",Missing exception data for exceptions raised in fl...,"When I tried to assign one of my successfully-allocated floating IPs to one of my instances, my nova-network.log showed this rather unhelpful traceback: http://paste.openstack.org/show/40674/
I'm running nova-network 1:2013.1.2-0ubuntu1~cloud0 from the cloud archive, on Ubuntu 12.04 LTS.
My searches through bugs while trying to find the *real* error turned up bug#1119817 which seems to be of a type with this one (although apparently more thoroughly researched)."
1202687,1202687,neutron,08112f087ffb221513db2872b6db0726d0c30702,0,0,Clean up,"Cisco plugin db clean up, part II","The cisco_vlan_ids table is no longer used.
Remove __init__ and __repr__ methods from models since they are provided by model_base."
1203152,1203152,cinder,7aa4f65a8c17aa037deff0f5b534ed694c17e62a,1,0,,long flashcopy operations  in the storwize_scv dri...,"There is a loop inside cinder/volume/drivers/storwize_svc.py _delete_vdisk() function that will
wait on flashcopy to finish before the vdisk can be deleted. If trying to delete a cinder volume
that is created from snapshot or another volume before the flashcopy finishes, the volume
service process will loop and wait for the flashcopy to be done. Since the code is blocked
in the _delete_vdisk code, volume service is blocked and won't respond to REST API
or update status. The service will be marked offline.
I am waiting for the person who found this bug to test a change that puts the while loop into an
inline function that I then run with FixedIntervalLoopingCall.
I hope to have a patch to post here later today once we have been able to test the code I wrote."
1204169,1204169,nova,3267d3ad92274303b6339e8a8d237ad8a3bb7bb9,1,1,"“The solution would be a refactor”,”Whatever refactor we do come up with”",compute instance.update messages sometimes have th...,"Compute instance.update messages that are not triggered by a state change (e.g. setting the host in the resource tracker) have default (None) values for task_state, old_vm_state and old_ task_state.
This can make the instance state sequence look wrong to anything consuming the messages (e.g stacktach)
 compute.instance.update  None(None) -> Building(none)
 scheduler.run_instance.scheduled
 compute.instance.update  building(None) ->  building(scheduling)
 compute.instance.create.start
 compute.instance.update  building(None) ->  building(None)
 compute.instance.update  building(None) ->  building(networking)
 compute.instance.update  building(networking) -> building(block_device_mapping)"
1204173,1204173,neutron,39c04fdc2c94b8ee48600f3148c5f850645b7c4e,1,1,,subnet CIDR validation broken for non-root CIDRs a...,"Earlier this month a fix was released to allow non-root CIDRs to be entered.  The fix was for bug 1188845.  It was approved and merged.  Now, however, the functionality has been reversed by commit 53a66b299f18a7184972502b43441a5ad7b050fd (bug 1195974) which checks the input earlier in the path and rejects anything but the root CIDR.
I am unable to find documentation that says you cannot use a non-root (I.E. X.X.X.254) IP to specify a subnet.  With an IP and subnet mask it is possible to create a network and determine the root for the subnet."
1204424,1204424,nova,474e48bf1169a24123e92381f45650b29990d3bf,1,1,,Unable to create VM with ephemeral block device in...,"Hey,
I am running Xubuntu 12.04.2 and I installed OpenStack Grizzly through Ubuntu Cloud Archive.
I configured nova-compute to use LVM as instance storage backend and I put these two lines in nova.conf.
libvirt_images_type=lvm
libvirt_images_volume_group=nova
When I am trying to boot a VM with a self-created flavor like:
nova flavor-create --ephemeral 10 m1.myWithEphSmall 7 4096 20 2
(with ephemeral block device != 0) the VM doesn't boot.
All VM with pre-defined flavors works fine.
here the /var/log/nova/nova-compute.log:
http://paste.openstack.org/show/41529/
some other infos:
http://paste.openstack.org/show/41533/
nova.conf:
http://paste.openstack.org/show/41534/
many thanks..."
1205089,1205089,nova,05e0ceb242f16a236f3c793305ed24e1f9a43efb,0,0,"Feature. “powervm driver needs to support”, “Implements the OS reboot API”",powervm driver needs to support reboot operation,"The reboot operation is part of the core nova API but the powervm driver doesn't support it:
https://github.com/openstack/nova/blob/master/nova/virt/powervm/driver.py#L126
https://wiki.openstack.org/wiki/HypervisorSupportMatrix
This is a pretty basic operation that should be supported if the driver is to be considered valid.
Looking here:
http://pic.dhe.ibm.com/infocenter/powersys/v3r1m5/topic/iphcg/chsysstate.htm
It should be a pretty straight-forward operation:
""To perform an immediate restart of a partition (operator panel function 3):
chsysstate -r lpar -o shutdown --immed --restart { -n Name | --id PartitionID } [ -m ManagedSystem ]"""
1206032,1206032,nova,045e1b11fd058441539e5e6888da7bd873d58c89,0,0,Refactoring “Rename instance_actions v3”,instance actions should be called server actions i...,"instance is not really a user concept, the user concept, in the api, is servers.
Maybe in the v3 api we should rename instance actions to ""server actions""."
1206650,1206650,cinder,783e3243f98a90550a8b7ec8ce279709ab8db372,0,0,“This fixes the sample configuration to contain proper config groups alongway.”,"Bug #1206650 “Cinder does not use Oslo for config generator "" ","We should update Cinder for using Oslo when we generate configuration files.
Here are the steps :
- Add generator.py from Oslo
- Add generator bash script used by other projects to generate configuration
files
- Update openstack-common.conf file
- Generate configuration file"
1206884,1206884,nova,c883862751d3081bbc05eb4b40fc15abd4fa5b1b,0,0,Test files,Enable v3 test_create_multiple_servers integrated ...,"Need to enable the integrated test_create_multiple_servers v3 version of the test
when the V3 multiple create extension merges"
1207402,1207402,neutron,9bf0e6654480f98f2315a43687b267263a82a823,1,0,“According to neutron/db/migration/README this means that I have to stamp it to manually.”,neutron should automatically stamp the database ve...,"When neutron automatically deploys the database schema it does not ""stamp"" the version.  According to neutron/db/migration/README this means that I have to stamp it to manually.
Neutron should automatically stamp the version to head when deploys the schema the first time.  That way I don't have to determine what version I'm running so that I can do a schema migration."
1207914,1207914,nova,2d20b87aef5a7d00cb36826b8907032912fb17fc,1,1, ,nova stacktraces when neutron throws a quota error...,"From nova-compute log when exceeding a quota.
/json; charset=UTF-8'} {""NeutronError"": ""Quota exceeded for resources: ['port']""}
 http_log_resp /opt/stack/python-neutronclient/neutronclient/common/utils.py:179
2013-08-02 21:30:38.115 30306 DEBUG neutronclient.v2_0.client [-] Error message: {""NeutronError"": ""Quota exceeded for resources: ['port']""} _handle_fault_response /opt/stack/python-neutronclient/neutronclient/v2_0/client.py:756
2013-08-02 21:30:38.115 30306 ERROR nova.compute.manager [-] Instance failed network setup after 1 attempt(s)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager Traceback (most recent call last):
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/nova/nova/compute/manager.py"", line 1280, in _allocate_network_async
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     security_groups=security_groups)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/nova/nova/network/api.py"", line 49, in wrapper
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     res = f(self, context, *args, **kwargs)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 310, in allocate_for_instance
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     LOG.exception(msg, port_id)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 287, in allocate_for_instance
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     port_client.create_port(port_req_body)['port']['id'])
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 108, in with_params
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     ret = self.function(instance, *args, **kwargs)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 276, in create_port
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     return self.post(self.ports_path, body=body)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 872, in post
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     headers=headers, params=params)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 795, in do_request
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     self._handle_fault_response(status_code, replybody)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 765, in _handle_fault_response
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     exception_handler_v20(status_code, des_error_body)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 81, in exception_handler_v20
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     message=error_dict)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager NeutronClientException: Quota exceeded for resources: ['port']
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
Furthermore nova doesn't show its a quota issue but instead says:
| fault                                | {u'message': u'NoValidHost', u'code': 500, u'created': u'2013-08-02T21:31:04Z'} |"
1208396,1208396,Swift,53345da70e3c969168e7192c3098540ba642ea09,0,0,“/opt/stack/swift/swift/common/utils.py:docstring of swift.common.utils.parse_content_type:7: ERROR: Unexpected indentation.”,error in swift.comm.utils when building doc,/opt/stack/swift/swift/common/utils.py:docstring of swift.common.utils.parse_content_type:7: ERROR: Unexpected indentation.
1208707,1208707,nova,fbe3c734a8675ed36d7e021584612d1413444667,1,0,“Previously baremetal volume driver used DB API to get fixed ips and DBNotAllowed was raised.”,Cannot attach a volume to a bare-metal instance,"virt/baremetal/volume_driver.py calls db api directly then fails.
2013-08-06 15:59:47.239 ERROR nova.compute [req-117555a1-bb81-43ec-9eee-ce976729093b demo demo] No db access allowed in nova-compute:   File ""/usr/lib/python2.7/dist-packages/eventlet/greenpool.py"", line 80, in _spawn_n_impl
    func(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 421, in _process_data
    **args)
  File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/nova/nova/exception.py"", line 77, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 216, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 245, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3424, in attach_volume
    mountpoint, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3456, in _attach_volume
    mountpoint)
  File ""/opt/stack/nova/nova/virt/baremetal/driver.py"", line 357, in attach_volume
    instance, mountpoint)
  File ""/opt/stack/nova/nova/virt/baremetal/volume_driver.py"", line 225, in attach_volume
    fixed_ips = nova_db_api.fixed_ip_get_by_instance(ctx, instance['uuid'])
  File ""/opt/stack/nova/nova/db/api.py"", line 496, in fixed_ip_get_by_instance
    return IMPL.fixed_ip_get_by_instance(context, instance_uuid)
  File ""/opt/stack/nova/nova/cmd/compute.py"", line 47, in __call__
    stacktrace = """".join(traceback.format_stack())"
1208734,1208734,cinder,061938563097241983b70fc6b85e1c196dfebfeb,0,0,Refactoring “deprecated”,Drop openstack.common.exception,The library openstack.common.exceptions is deprecated in Oslo and should be removed.
1209410,1209410,nova,5fc059f13d4f0b2a5fef63095ea3fc710d46b5b3,1,0,“That provokes the next error in Windows:” change in the environment,processutlis.execute not usable in windows,"In Havana time, in the execute method of processutils the condition (os.geteuid() != 0) was added in line 130
    if run_as_root and os.geteuid() != 0:
        if not root_helper:
            raise NoRootWrapSpecified(
                message=('Command requested root, but did not specify a root '
                         'helper.'))
        cmd = shlex.split(root_helper) + list(cmd)
That provokes the next error in Windows:
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp     (stdout, stderr) = processutils.execute(*cmd, **kwargs)
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp   File ""C:\Users\Pedro\dev\cinder\cinder\openstack\common\processutils.py"", line 130, in execute
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp     if run_as_root and os.geteuid() != 0:
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp AttributeError: 'module' object has no attribute 'geteuid'
This should be removed in order to this code be used in nova and cinder windows-based drivers"
1210276,1210276,neutron,fefad06eabc7fb23ccb3ca1a6093d3460065adc7,0,0, “The neutron.common.exceptions.AlreadyAttached exception is not used.” Refactorings,exceptions.AlreadyAttached is not used,"The neutron.common.exceptions.AlreadyAttached exception is not used.  It looks like it's replaced by this exception in the nicira plugin:
https://github.com/openstack/neutron/blob/master/neutron/plugins/nicira/common/exceptions.py#L51
Either common.exceptions.AlreadyAttached should be removed or NvpPortAlreadyAttached should extend the common AlreadyAttached exception class."
1210877,1210877,neutron,3e1116eb0f1d94530707cd6ef4b37f17e9a13918,1,1, ,"Bug #1210877 “Sync router fails with db exception "" ","While investigating https://bugs.launchpad.net/neutron/+bug/1210664, salvatore-orlando discovered that a db exception was being raised during router syncing:
Traceback (most recent call last):
  File ""/opt/stack/neutron/neutron/openstack/common/rpc/amqp.py"", line 424, in _process_data
    **args)
  File ""/opt/stack/neutron/neutron/common/rpc.py"", line 44, in dispatch
    neutron_ctxt, version, method, namespace, **kwargs)
  File ""/opt/stack/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/neutron/neutron/db/l3_rpc_base.py"", line 47, in sync_routers
    plugin.auto_schedule_routers(context, host, router_ids)
  File ""/opt/stack/neutron/neutron/db/agentschedulers_db.py"", line 303, in auto_schedule_routers
    self, context, host, router_ids)
  File ""/opt/stack/neutron/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers
    context.session.add(binding)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
    self.commit()
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
    self._prepare_impl()
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
    self.session.flush()
  File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap
    raise exception.DBError(e)
DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_1` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('2df68c3d-f3c9-43d2-bf45-e2e57e84b054', 'c4502f1f-a093-4c7c-b161-929b6342509b', '85d6b60f-f3ff-4437-8f5a-af165087f3ea')
This can be reproduced by running the quantum smoke test in tempest (nosetests tempest/scenario/test_network_basic_ops.py).  The smoke test passes - the exception only occurs at test cleanup.  It may be that the router syncing code is working with stale state after router deletion."
1211307,1211307,nova,767fb98b885327d1ad1cd380682ae4745aa78387,1,1,"“is executed successfully on MySQL and SQLite (though WHERE clause never evaluates to True, so related entities aren't actually deleted), but fails on PostgreSQL,”",instance_group_delete() filters related entities b...,"Table 'instance_groups' has two columns to unambiguously identify the specific row:
1) id (Integer, primary_key=True, autoincrement=True)
2) uuid  (String)
The former is used internally to bind related entities by FKs (InstanceGroupMember, InstanceGroupPolicy and InstanceGroupMetadata), and the latter is accepted by public DB API methods (this must be a miss in the DB schema design, because uuid  could be easily used for both use cases).
Having two 'id' columns is both misleading and error-prone. E. g. instance_group_delete() deletes the instance group (and all related entities) given its UUID value:
def instance_group_delete(context, group_uuid):
    """"""Delete an group.""""""
    session = get_session()
    with session.begin():
        count = _instance_group_get_query(context,
                                          models.InstanceGroup,
                                          models.InstanceGroup.uuid,
                                          group_uuid,
                                          session=session).soft_delete()
        if count == 0:
            raise exception.InstanceGroupNotFound(group_uuid=group_uuid)
        # Delete policies, metadata and members
        instance_models = [models.InstanceGroupPolicy,
                           models.InstanceGroupMetadata,
                           models.InstanceGroupMember]
        for model in instance_models:
            model_query(context, model, session=session).\
                    filter_by(group_id=group_uuid).\
                    soft_delete()
Related entities are filtered by 'group_id' column, but 'group_uuid' value is passed. Despite that these two columns are of different types, this statement is executed successfully on MySQL and SQLite (though WHERE clause never evaluates to True, so related entities aren't actually deleted), but fails on PostgreSQL, which is more strict when checking data types of values."
1211915,1211915,neutron,5becbbec6a686667925f399904cebd97b583c35a,1,0, ,Bug #1211915 “Connection to neutron failed,"http://logs.openstack.org/64/41464/4/check/gate-tempest-devstack-vm-neutron/4288a6b/console.html
Seen testing https://review.openstack.org/#/c/41464/
2013-08-13 17:34:46.774 | Traceback (most recent call last):
2013-08-13 17:34:46.774 |   File ""tempest/scenario/test_network_basic_ops.py"", line 176, in test_003_create_networks
2013-08-13 17:34:46.774 |     router = self._get_router(self.tenant_id)
2013-08-13 17:34:46.775 |   File ""tempest/scenario/test_network_basic_ops.py"", line 141, in _get_router
2013-08-13 17:34:46.775 |     router.add_gateway(network_id)
2013-08-13 17:34:46.775 |   File ""tempest/api/network/common.py"", line 78, in add_gateway
2013-08-13 17:34:46.776 |     self.client.add_gateway_router(self.id, body=body)
2013-08-13 17:34:46.776 |   File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 108, in with_params
2013-08-13 17:34:46.776 |     ret = self.function(instance, *args, **kwargs)
2013-08-13 17:34:46.776 |   File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 396, in add_gateway_router
2013-08-13 17:34:46.777 |     body={'router': {'external_gateway_info': body}})
2013-08-13 17:34:46.777 |   File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 987, in put
2013-08-13 17:34:46.777 |     headers=headers, params=params)
2013-08-13 17:34:46.778 |   File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 970, in retry_request
2013-08-13 17:34:46.778 |     raise exceptions.ConnectionFailed(reason=_(""Maximum attempts reached""))
2013-08-13 17:34:46.778 | ConnectionFailed: Connection to neutron failed: Maximum attempts reached"
1212179,1212179,nova,4054cc4a22a1fea997dec76afb5646fd6c6ea6b9,1,1,“CVE References”,[OSSA 2013-024] nova should check the is_public of...,"If creating a flavor with is_public ""false"", the flavor should be accessible only by admin or user who is granted to access.
Now ""get flavor details"" API checks the is_public of flavor but ""create an instance"" API does not check.
In the following case, a user (not admin) cannot access non-public flavor through ""get flavor details"" API, this is right behavior.
However, he can access non-public flavor through ""create an instance"" API.
=== admin ==============================================================
$ nova flavor-create --is-public false private-flavor 6 512 0 1
+----+----------------+-----------+------+-----------+------+-------+-------------+-----------+
| ID | Name           | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----+----------------+-----------+------+-----------+------+-------+-------------+-----------+
| 6  | private-flavor | 512       | 0    | 0         |      | 1     | 1.0         | False     |
+----+----------------+-----------+------+-----------+------+-------+-------------+-----------+
$
$ curl -i http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/flavors/5 -X GET -H ""X-Auth-Project-Id: demo"" -H ""User-Agent: python-novaclient"" -H ""Accept: application/json"" -H ""X-Auth-Token: [..]""
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 428
X-Compute-Request-Id: req-53ab8206-f458-441c-a0ca-d17e333f4247
Date: Wed, 14 Aug 2013 09:38:10 GMT
{""flavor"": {""name"": ""m1.xlarge"", ""links"": [{""href"": ""http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/flavors/5"", ""rel"": ""self""}, {""href"": ""http://192.168.0.30:8774/7a5c62d3cadb40d28e3c25acf7a05b05/flavors/5"", ""rel"": ""bookmark""}], ""ram"": 16384, ""OS-FLV-DISABLED:disabled"": false, ""vcpus"": 8, ""swap"": """", ""os-flavor-access:is_public"": true, ""rxtx_factor"": 1.0, ""OS-FLV-EXT-DATA:ephemeral"": 0, ""disk"": 160, ""id"": ""5""}}
$
=== user(""demo"" user on devstack) ==============================================================
$ nova flavor-list
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| ID | Name      | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| 1  | m1.tiny   | 512       | 1    | 0         |      | 1     | 1.0         | True      |
| 2  | m1.small  | 2048      | 20   | 0         |      | 1     | 1.0         | True      |
| 3  | m1.medium | 4096      | 40   | 0         |      | 2     | 1.0         | True      |
| 4  | m1.large  | 8192      | 80   | 0         |      | 4     | 1.0         | True      |
| 42 | m1.nano   | 64        | 0    | 0         |      | 1     | 1.0         | True      |
| 5  | m1.xlarge | 16384     | 160  | 0         |      | 8     | 1.0         | True      |
| 84 | m1.micro  | 128       | 0    | 0         |      | 1     | 1.0         | True      |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
$
$ curl -i http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/flavors/6 -X GET -H ""X-Auth-Project-Id: demo"" -H ""User-Agent: python-novaclient"" -H ""Accept: application/json"" -H ""X-Auth-Token: [..]""
HTTP/1.1 404 Not Found
Content-Length: 78
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-a7ac7e99-6d29-4893-97a7-6705083739df
Date: Wed, 14 Aug 2013 09:36:52 GMT
{""itemNotFound"": {""message"": ""The resource could not be found."", ""code"": 404}}
$
$ curl -i http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/servers -X POST -H ""X-Auth-Project-Id: demo"" -H ""User-Agent: python-novaclient"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -H ""X-Auth-Token: [..]"" -d '{""server"": {""min_count"": 1, ""flavorRef"": ""6"", ""name"": ""test-not-public"", ""imageRef"": ""428f795d-01b0-44c8-a162-9ad86d1fea35"", ""max_count"": 1}}'
HTTP/1.1 202 Accepted
Location: http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/servers/91407b32-7ed1-4108-9e62-192b3312ff20
Content-Type: application/json
Content-Length: 440
X-Compute-Request-Id: req-7e561044-100a-4d70-8b83-cebd21dca8e2
Date: Wed, 14 Aug 2013 09:41:50 GMT
{""server"": {""security_groups"": [{""name"": ""default""}], ""OS-DCF:diskConfig"": ""MANUAL"", ""id"": ""91407b32-7ed1-4108-9e62-192b3312ff20"", ""links"": [{""href"": ""http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/servers/91407b32-7ed1-4108-9e62-192b3312ff20"", ""rel"": ""self""}, {""href"": ""http://192.168.0.30:8774/7a5c62d3cadb40d28e3c25acf7a05b05/servers/91407b32-7ed1-4108-9e62-192b3312ff20"", ""rel"": ""bookmark""}], ""adminPass"": ""xvFhTwd3yAzE""}}
$ nova list
+--------------------------------------+-----------------+--------+------------+-------------+------------------+
| ID                                   | Name            | Status | Task State | Power State | Networks         |
+--------------------------------------+-----------------+--------+------------+-------------+------------------+
| 91407b32-7ed1-4108-9e62-192b3312ff20 | test-not-public | ACTIVE | None       | Running     | private=10.0.0.3 |
+--------------------------------------+-----------------+--------+------------+-------------+------------------+
$"
1212710,1212710,cinder,c77868f44b1e72be2b65f697a1f0f3b32126e581,1,1, ,cinder-scheduler fails to transmit the image_id pa...,"This break the create volume from image feature with RBD backend and perhaps other backends as well?
On a grizzly setup with both glance and cinder configured with RBD
backend, I cannot find a way to create a new volume from a (raw) image.
The resulting volume is correctly created but remains full of NULL bytes.
$ glance show 92af7175-9478-42c4-8ed0-b336362ff3f7
URI: https://example.com:9292/v1/images/92af7175-9478-42c4-8ed0-b336362ff3f7
Id: 92af7175-9478-42c4-8ed0-b336362ff3f7
Public: Yes
Protected: No
Name: precise-server-cloudimg-amd64.raw
Status: active
Size: 2147483648
Disk format: raw
Container format: bare
Minimum Ram Required (MB): 0
Minimum Disk Required (GB): 0
Owner: 8226a848c4eb43e69b1a0f00c582e31f
Created at: 2013-08-11T07:22:13
Updated at: 2013-08-11T07:29:10
$ cinder create 10 --image-id 92af7175-9478-42c4-8ed0-b336362ff3f7
--display-name boot-from-volume
In the following logs, cinder-scheduler gets the correct image_id
parameter but cinder-volume gets None instead which looks weird.
==> cinder-scheduler.log <==
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp] received
{u'_context_roles': [u'Member', u'_member_', u'admin'],
u'_context_request_id': u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac',
u'_context_quota_class': None, u'_unique_id':
u'5e5eca077ae149a6a64b1760bc55cc55', u'_context_read_deleted': u'no',
u'args': {u'request_spec': {u'volume_id':
u'79e98020-555c-4179-97ff-867a79a37160', u'volume_properties':
{u'status': u'creating', u'volume_type_id': None, u'display_name':
u'boot-from-volume', u'availability_zone': u'nova', u'attach_status':
u'detached', u'source_volid': None, u'metadata': {}, u'volume_metadata':
[], u'display_description': None, u'snapshot_id': None, u'user_id':
u'01d6651d807649718e01be2999b11af0', u'project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', u'id':
u'79e98020-555c-4179-97ff-867a79a37160', u'size': 10}, u'
volume_type': {}, u'image_id': u'92af7175-9478-42c4-8ed0-b336362ff3f7',
u'source_volid': None, u'snapshot_id': None}, u'volume_id':
u'79e98020-555c-4179-97ff-867a79a37160', u'filter_properties': {},
u'topic': u'cinder-volume', u'image_id':
u'92af7175-9478-42c4-8ed0-b336362ff3f7', u'snapshot_id': None},
u'_context_tenant': u'8226a848c4eb43e69b1a0f00c582e31f',
u'_context_auth_token': '<SANITIZED>', u'_context_is_admin': False,
u'version': u'1.2', u'_context_project_id':
u'8226a848c4eb43e69b1a0f00c582e31f',
u'_context_timestamp': u'2013-08-11T22:34:22.569488', u'_context_user':
u'01d6651d807649718e01be2999b11af0', u'_context_user_id':
u'01d6651d807649718e01be2999b11af0', u'm
ethod': u'create_volume', u'_context_remote_address': u'1.2.3.4'}
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp] unpacked
context: {'user_id': u'01d6651d807649718e01be2999b11af0', 'roles':
[u'Member', u'_member_', u'adm
in'], 'timestamp': u'2013-08-11T22:34:22.569488', 'auth_token':
'<SANITIZED>', 'remote_address': u'1.2.3.4', 'quota_class': None,
'is_admin': False, 'user': u'01d66
51d807649718e01be2999b11af0', 'request_id':
u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac', 'project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', 'read_deleted': u'no', 'tenant
': u'8226a848c4eb43e69b1a0f00c582e31f'}
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp] Making
asynchronous cast on cinder-volume.sun2-test...
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp]
UNIQUE_ID is b3bc44f910054661a7a532bd6bfbe5bb.
==> cinder-volume.log <==
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp] received
{u'_context_roles': [u'Member', u'_member_', u'admin'],
u'_context_request_id': u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac',
u'_context_quota_class': None, u'_unique_id':
u'b3bc44f910054661a7a532bd6bfbe5bb', u'args': {u'request_spec': None,
u'volume_id': u'79e98020-555c-4179-97ff-867a79a37160',
u'allow_reschedule': True, u'filter_properties':
u'92af7175-9478-42c4-8ed0-b336362ff3f7', u'source_volid': None,
u'image_id': None, u'snapshot_id': None}, u'_context_tenant':
u'8226a848c4eb43e69b1a0f00c582e31f', u'_context_auth_token':
'<SANITIZED>', u'_context_timestamp': u'2013-08-11T22:34:22.569488',
u'_context_is_admin': False, u'version': u'1.4', u'_context_project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', u'_context_user':
u'01d6651d807649718e01be2999b11af0', u'_context_
read_deleted': u'no', u'_context_user_id':
u'01d6651d807649718e01be2999b11af0', u'method': u'create_volume',
u'_context_remote_address': u'1.2.3.4'}
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp] unpacked
context: {'user_id': u'01d6651d807649718e01be2999b11af0', 'roles':
[u'Member', u'_member_', u'admin'], 'timestamp':
u'2013-08-11T22:34:22.569488', 'auth_token': '<SANITIZED>',
'remote_address': u'1.2.3.4', 'quota_class': None, 'is_admin': False,
'user': u'01d6651d807649718e01be2999b11af0', 'request_id':
u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac', 'project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', 'read_deleted': u'no', 'tenant
': u'8226a848c4eb43e69b1a0f00c582e31f'}
2013-08-12 00:34:23    DEBUG [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: creating lv of size 10G
2013-08-12 00:34:23     INFO [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: creating
2013-08-12 00:34:23    DEBUG [cinder.utils] Running cmd (subprocess):
rbd --help
2013-08-12 00:34:23    DEBUG [cinder.utils] Running cmd (subprocess):
rbd create --pool volumes --size 10240
volume-79e98020-555c-4179-97ff-867a79a37160 --new-format
2013-08-12 00:34:23    DEBUG [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: creating export
2013-08-12 00:34:24     INFO [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: created successfully
2013-08-12 00:34:24     INFO [cinder.volume.manager] Clear capabilities
cinder.conf:
[DEFAULT]
rootwrap_config = /etc/cinder/rootwrap.conf
api_paste_confg = /etc/cinder/api-paste.ini
iscsi_helper = tgtadm
volume_name_template = volume-%s
volume_group = cinder-volumes
verbose = True
auth_strategy = keystone
state_path = /var/lib/cinder
lock_path = /var/lock/cinder
volumes_dir = /var/lib/cinder/volumes
rabbit_host=127.0.0.1
sql_connection=mysql://cinder:cinder@127.0.0.1/cinder?charset=utf8
api_paste_config=/etc/cinder/api-paste.ini
debug=True
rabbit_userid=nova
osapi_volume_listen=0.0.0.0
rabbit_virtual_host=/
scheduler_driver=cinder.scheduler.simple.SimpleScheduler
rabbit_hosts=127.0.0.1:5672
rabbit_ha_queues=False
rabbit_password=secret
rabbit_port=5672
rpc_backend=cinder.openstack.common.rpc.impl_kombu
sql_idle_timeout=3600
volume_driver=cinder.volume.drivers.rbd.RBDDriver
rbd_user=volumes
max_gigabytes=25000
rbd_pool=volumes
rbd_secret_uuid=XXXX
glance_api_version=2
glance-api.conf:
[DEFAULT]
verbose = True
debug = True
default_store = rbd
bind_host = 0.0.0.0
bind_port = 9292
log_file = /var/log/glance/api.log
backlog = 4096
sql_connection = mysql://glance:glance@127.0.0.1/glance
sql_idle_timeout = 3600
workers = 2
show_image_direct_url = True
registry_host = 0.0.0.0
registry_port = 9191
registry_client_protocol = http
notifier_strategy = noop
rabbit_host = localhost
rabbit_port = 5672
rabbit_use_ssl = false
rabbit_userid = guest
rabbit_password = guest
rabbit_virtual_host = /
rabbit_notification_exchange = glance
rabbit_notification_topic = notifications
rabbit_durable_queues = False
filesystem_store_datadir = /var/lib/glance/images/
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_user = images
rbd_store_pool = images
rbd_store_chunk_size = 8
delayed_delete = False
scrub_time = 43200
scrubber_datadir = /var/lib/glance/scrubber
image_cache_dir = /var/lib/glance/image-cache/
[keystone_authtoken]
auth_host = 127.0.0.1
auth_port = 35357
auth_protocol = http
admin_tenant_name = services
admin_user = glance
admin_password = secret
[paste_deploy]
flavor=keystone+cachemanagement"
1212768,1212768,nova,105d99d4f52fa849e80829dad3ed12b116820118,1,1,“When claims are rejected indicate specific reason” ,When claims are rejected indicate specific reason ...,"In resource_tracker.py upon failure, we raise a generic ""ComputeResourcesUnavailable"".  The logging presents good information as to why there was a failure.  We should either provide back data in the ComputeResourcesUnavailble object or raise different exceptions based on what is not available.  This may point to a bigger issue of a broken scheduler."
1212772,1212772,neutron,62040d03b3b7f02ff7f67ed08e0dc867eef183dc,1,0,“external gateway modes doesn't work with ML2”,external gateway modes doesn't work with ML2,"When ML2 plugin is used, L3 agent won't set SNAT rules between internal networks when used with ML2 plugin.
external gateway modes introduces a new ""enable_snat"" comlumn to Router DB, but as ML2 plugin was merged right after this extension, it isn't included in alembic migration script [1], so this column won't be present for this plugin. As a consequence, L3 agents won't set-up SNAT rules in neutron routers.
[1]https://github.com/openstack/neutron/blob/master/neutron/db/migration/alembic_migrations/versions/128e042a2b68_ext_gw_mode.py"
1212868,1212868,neutron,71ecc6ba46433999cf6b6c346670337b875832a0,1,1,“ This patch changes L3 agent to enable SNAT by default if plugin doesn't support ext-gw-mode extension.”,L3 agent won't set up SNAT rules to external netwo...,"If neutron plugin doesn't support ext-gw-mode, L3 agent won't set-up SNAT rules on router (By default, it'll assume enable_snat==false). In order to behave like prior to ext-gw-mode extension introduction, it should enbale SNAT rules in that case."
1212915,1212915,nova,9795c3e9fdc1f6253f3099414e379367b50587dc,0,0,Test files,nova api “show,"Nova api ""show"" do not fetch the error message of vm in havana.
In /nova/nova/api/openstack/compute/views/servers.py, fault = instance.get(""fault"", None), cannot get the fault message.
I tried instance[""fault""], If the vm in error state, it get the error message.
So i think it's some lazy load problem of the instance object and instance fault."
1213541,1213541,neutron,e52b5e8e98dd640c69d009a3d5546a479e394d81,1,1,Typos and 1 LINE of code: LOG = logging.getLogger(__name_₎,typos in neutron.api.extensions and hacking check,"In neutron/api/extensions.py, there are some typos, no needed blank lines and docstring needed to be improved (according to HACKING.rst)"
1213930,1213930,neutron,6b0c899d36d5a98963342e9cffb351fc3a1f4fd2,1,1, ,id() builtin used instead of subnet id in error me...,"This occurs twice in _validate_subnet:
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1049
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1063"
1213964,1213964,cinder,587eb5a6b9acc9f02eee1147dbaafb3782c295d5,1,1, ,"when using qpid, metadata arguments cause volume c...","volume cloning seems to fail when using qpid if some metadata is passed, see the following:
 # cinder create --metadata 'Type=Test' --source-volid d1ce1fc3-10fe-475f-9dc4-51261d04c323 1
 ERROR: The server has either erred or is incapable of performing the requested operation.
interestingly, this does not seem to happen when a new (non clone) volume is created
cinder api.log reports some rather long traceback:
 InternalError: Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 511, in dispatch
    self.engine.dispatch()
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 815, in dispatch
    self.process(ssn)
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 1050, in process
    self.send(snd, msg)
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 1261, in send
    body = enc(msg.content)
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/message.py"", line 28, in encode
    sc.write_primitive(type, x)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 300, in write_list
    type = self.encoding(o)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 59, in encoding
    raise CodecException(""no encoding for %r"" % obj)
 CodecException: no encoding for <cinder.db.sqlalchemy.models.VolumeMetadata object at 0x458e610>"
1214162,1214162,nova,3292103d80676fc789615465759de966dba4e9f7,0,0,Bug in the future,Bug #1214162 “dhcp_options_enabled option be removed when neutro... ,"The dhcp_options_enabled is currently set to False because the neutron extra_dhcp_opts patches have not landed yet. This should land in the Havana release of Neutron, as such dhcp_options_enabled option can then be defaulted to True or the conditional in the code can be removed (the desired approach). This is explained in https://review.openstack.org/#/c/31061/, and in Neutron: https://review.openstack.org/#/c/30441/ and https://review.openstack.org/#/c/30447/, for the python-neutronclient. So for Icehouse release of Nova this option should be removed and the code modified to support the dhcp_options."
1214406,1214406,nova,0efbcf4e168d030907adbadd6ab7fccf766f4068,1,1, ,nova.virt.block_device.get_swap should have better...,"As per markmc's comments on https://review.openstack.org/#/c/39086/24 - the function is used to get the swap out of the list context as the block_device_info data structure (used internally by the virt drivers) needs 'swap' field to be either a single dict or none. However if passed something that is not an obvious list of swap looking things - the function will happily reutrn the passed list.
More safe and correct behaviour would be to return None (or raise)."
1214720,1214720,nova,b36826ef3ddeafcf5f16034bccd971de800f677a,1,1, ,nova-manage db archive_deleted_rows fails if max_r...,"The nova-manage db archive_deleted_rows fails if max_rows is a large number (I tried 1 million but a smaller value may also cause issues) because it receives an exception from the sqlalchemy and db layer regarding the number of parameters on the sql statement.
Database has a limite of maximum total length of host and indicator variables in SQL statement.  When I ran the archive, the table had 165822 rows in it and 18489 of those were not deleted. Therefore, 147333 rows had deleted=1. So get error from Database."
1214850,1214850,nova,a25b2ac5f440f7ace4678b21ada6ebf5ce5dff3c,1,0,Software evolution. After some point they need to use two DC and the software was not prepared,vmware driver does not work with more than one dat...,"""CreateVM_Task"", vm_folder_ref,
config=config_spec, pool=res_pool_ref)
specifies a vm_folder_ref that has no relationship to the datastore.
This may lead to VM construction and placement errors.
NOTE: code selects the 0th datacenter"
1214985,1214985,nova,5c95824191855b0012268f641e5e738888b8d13f,1,1, ,powervm driver is not properly cleaning up compres...,"This is on the master branch (current havana trunk).  Ran tempest against the powervm driver with a backing VIOS hypervisor, there are no instances left over after the tests are done (everything was successful):
http://paste.openstack.org/show/44772/
But on the backing hypervisor, there are leftover image files:
http://paste.openstack.org/show/44773/
I found that the powervm method that is supposed to remove the files isn't using the --force option with the rm command:
https://github.com/openstack/nova/blob/master/nova/virt/powervm/operator.py#L800
Also, this method isn't even used in the code:
https://github.com/openstack/nova/blob/master/nova/virt/powervm/operator.py#L785"
1215019,1215019,nova,43e102cc215aabc72a0ef84cf48965c4db2d58d4,1,1, “The powervm driver should log ssh stderr as warning rather than debug”,The powervm driver should log ssh stderr as warnin...,"There are 4 methods in the powervm driver that run ssh calls to the backing hypervisor (2 in blockdev, 2 in operator) and check for stderr output but only log it at debug level, which could be masking more serious issues. The stderr results should be logged at warning instead.
https://github.com/openstack/nova/blob/master/nova/virt/powervm/blockdev.py#L563
https://github.com/openstack/nova/blob/master/nova/virt/powervm/blockdev.py#L580
https://github.com/openstack/nova/blob/master/nova/virt/powervm/operator.py#L694
https://github.com/openstack/nova/blob/master/nova/virt/powervm/operator.py#L711"
1215270,1215270,neutron,884b08e1b20a2f80fb6eba6f0c7405979b914817,1,1,Missing code,metadata namespace_proxy missing help information ...,"When i use `neutron-ns-metadata-proxy  --help`, some option doesn't have help text. It is because in neutron/agent/metadata/namespace_proxy.py:
opts = [
        cfg.StrOpt('network_id'),
        cfg.StrOpt('router_id'),
        cfg.StrOpt('pid_file'),
        cfg.BoolOpt('daemonize', default=True),
        cfg.IntOpt('metadata_port',
                   default=9697,
                   help=_(""TCP Port to listen for metadata server ""
                          ""requests."")),
    ]"
1215387,1215387,neutron,9da60d0a417dd70c16ae34f5877c564e425e4cf8,1,1, ,can not stop l3-agent router forwarding packets by...,"We can not stop router forwarding packets by admin_state_up false.
Master branch has this problem (stable/grizzly branch don't have the problem).
I know the cause. When run router-update --admin_state_up false, transitions as follows:
sync_routers(l3_rpc_base)→list_active_sync_routers_on_active_l3_agent(agent schedulers_db)→get_sync_data(l3_db)→_get_sync_routers(l3_db)
list_active_sync_routers_on_active_l3_agent pass key 'active=True', and router that set admin_state_up false is ignored by filters['admin_state_up'] = [active] in _get_sync_routers.
I think that active=True is not identical  admin_state_up True and that is more similar status Active.
I will modify here."
1215390,1215390,nova,52a9137d18369848feb1f496497523098d08ceff,1,1,"“we rename the 'stats' attribute of the class to 'compute_node' (due to the fact that the corresponding relation actually maps to ComputeNode class, not to ComputeNodeStat)”",Incorrect relationship declaration in models.Compu...,"db.sqlalchemy.models.ComputeNodeStat.stats field is not properly named: since it is a relation that maps to db.sqlalchemy.models.ComputeNode, it should be named 'compute_node', not 'stats'. Besides, the call to the relationship() method lacks the foreign_keys parameter."
1215772,1215772,nova,c5402ef4fc509047d513a715a1c14e9b4ba9674f,1,0,“the root cause is nova not support cinderclient v2 yet”,Nova Attach/Detach Volume can't work if volume end...,"Cannot attach or detach volume if use v2 volume endpoint.
How to reproduce this bug:
1, Remove the volume v1 endpoint from keystone, use ""keystone endpoint-delete""
2. Create the v2 volume endpoint via CLI command: keystone endpoint-create
3.export OS_VOLUME_API_VERSION=2
4. run 'cinder list' ok, and create one volume
5. run 'nova volume-attach' or 'nova detach' command will be failed. return 500 error.
After investigated, I found the root cause is nova not support cinderclient v2 yet, we need make some code change to let nova to support cinderclient v2."
1216247,1216247,glance,ba1f41d89d38286f769cfdf40b337fe5c7dad578,1,1, “This is due to an incorrect regular expression used int he code”,glance-replicator commands fail due to incorrect r...,"When I pass correct parameters to the glance-replicator tool I get the following output:
ERROR: Bad format of the given arguments.
rohit@precise-dev-102:~/devstack$ glance-replicator compare 10.2.3.1:9292  10.2.3.2:9292
replication_compare compare <fromserver:port> <toserver:port>
    Compare the contents of fromserver with those of toserver.
    fromserver:port: the location of the master glance instance.
    toserver:port:   the location of the slave glance instance.
ERROR: Bad format of the given arguments.
This is due to an incorrect regular expression used int he code to match the input parameters.
The same regex is used by all the replicator functions to verify the parameters:
​https://github.com/openstack/glance/blob/master/glance/cmd/replicator.py#L58
SERVER_PORT_REGEX = '\w+:\w+'
The character class \w = [a-zA-Z0-9_]. So if we pass an IP Address or an FQDN having characters outside the set, this check will fail.
The regex should be corrected for glance-replicator to work correctly."
1216929,1216929,nova,840599bc2d496874e5e15639afa643fed1bbe3bb,1,1, ,"when ephemeral_gb and swap is default value 0, the...","when I port flaovr tempest test in nova v3. I find when create flavor with default value of ephemeral_gb and swap (both of them are 0). the response is """".  I think it's bug. I look into the code find that the issue is
swap"": flavor.get(""swap"") or """",
""ephemeral"": flavor.get(""ephemeral_gb"") or """",
it's a logic bug in nova/api/openstack/compute/views/flavors.py, I think.
the tempest log is the following:
2013-08-26 21:45:10.222 31704 INFO tempest.common.rest_client [-] Request: POST http://192.168.1.101:8774/v3/flavors
2013-08-26 21:45:10.222 31704 DEBUG tempest.common.rest_client [-] Request Headers: {'Content-Type': 'application/json', 'Accept': 'application/json', 'X-Auth-Token': '<Token omitted>'} _log_request /opt/stack/tempest/tempest/common/rest_client.py:295
2013-08-26 21:45:10.222 31704 DEBUG tempest.common.rest_client [-] Request Body: {""flavor"": {""disk"": 10, ""vcpus"": 1, ""ram"": 512, ""name"": ""test_flavor_832042179"", ""id"": 1663317675}} _log_request /opt/stack/tempest/tempest/common/rest_client.py:299
2013-08-26 21:45:10.628 31704 INFO tempest.common.rest_client [-] Response Status: 200
2013-08-26 21:45:10.628 31704 DEBUG tempest.common.rest_client [-] Response Headers: {'date': 'Mon, 26 Aug 2013 13:45:10 GMT', 'content-length': '369', 'content-type': 'application/json', 'x-compute-request-id': 'req-7ba21477-fda1-41f0-a782-3d58b05f293a'} _log_response /opt/stack/tempest/tempest/common/rest_client.py:310
2013-08-26 21:45:10.628 31704 DEBUG tempest.common.rest_client [-] Response Body: {""flavor"": {""name"": ""test_flavor_832042179"", ""links"": [{""href"": ""http://192.168.1.101:8774/v3/flavors/1663317675"", ""rel"": ""self""}, {""href"": ""http://192.168.1.101:8774/flavors/1663317675"", ""rel"": ""bookmark""}], ""ram"": 512, ""ephemeral"": """", ""disabled"": false, ""vcpus"": 1, ""swap"": """", ""os-flavor-access:is_public"": true, ""rxtx_factor"": 1.0, ""disk"": 10, ""id"": ""1663317675""}} _log_response /opt/stack/tempest/tempest/common/rest_client.py:314"
1217552,1217552,cinder,37e775f12592824de17aee73216549bcb182c7cc,1,1,,/tmp space near full causes failure of create from...,"when a cinder create using an --image-id as source for the volume happens in conjunction with inadequate space free on /tmp the create fails due to the conversion to bare format running out of disk space.
converting in /tmp seems to me to be a reasonable choice at first but not when you consider that image files in glance can be quite large. Doesn't cinder need some way to allow defining the conversion space to be somewhere other than /tmp. Many conversions will happen on VM's with small memory and disk footprints. Perhaps the conversion would better take place somewhere defined by the admin?"
1217586,1217586,nova,cda106c399a81d4027d3bb99f452fd4064405009,1,1,Change Ieaf256a5c0f54804686318d6cbd5877a5003c9eb made the powervm driver raise NotImplementedError from the plug_vifs method which causes this failure on the compute node when it starts up,init_host fails due to powervm driver raising NotI...,"Change Ieaf256a5c0f54804686318d6cbd5877a5003c9eb made the powervm driver raise NotImplementedError from the plug_vifs method which causes this failure on the compute node when it starts up:
http://paste.openstack.org/show/45241/
Looking at other virt drivers that don't support the plug_vifs method (hyperv and vmware), they simply pass, which is what the powervm driver use to do before that change.
The powervm driver should either revert to pass on plug_vifs or the nova compute manager should catch NotImplementedError from driver.plug_vifs and swallow it here:
https://github.com/openstack/nova/blob/2013.2.b2/nova/compute/manager.py#L638"
1217679,1217679,nova,d6214f122feb99be74b6654bf86d1677926fa628,1,1,,there is an  Unexpected API Error when call remove...,"When port flavor_access tempest tests into v3, I found if the remove_tenant_access and add_tenant_access called as non admin user an  Unexpected API Error arose.
I look into code, find out the issue is that flavors.add_flavor_access and flavors.remove_flavor_access require admin privilege in DB level but the policy doesn't require it. and the exception is not catched. I think there is the same issue in nova v2 api.
I also think we should remove the privilege check in DB level, but it need more tests, and can be remove in another patch or blue-print.
the tempest log is:
2013-08-28 11:56:08.154 2220 INFO tempest.common.rest_client [-] Request: POST http://192.168.1.101:8774/v3/flavors/155214353/action
2013-08-28 11:56:08.154 2220 DEBUG tempest.common.rest_client [-] Request Headers: {'Content-Type': 'application/json', 'Accept': 'application/json', 'X-Auth-Token': '<Token omitted>'} _log_request /opt/stack/tempest/tempest/common/rest_client.py:295
2013-08-28 11:56:08.154 2220 DEBUG tempest.common.rest_client [-] Request Body: {""add_tenant_access"": {""tenant_id"": ""9bfa07133a42464a8701e3cf367bbb4d""}} _log_request /opt/stack/tempest/tempest/common/rest_client.py:299
2013-08-28 11:56:08.169 2220 INFO tempest.common.rest_client [-] Response Status: 500
2013-08-28 11:56:08.169 2220 DEBUG tempest.common.rest_client [-] Response Headers: {'date': 'Wed, 28 Aug 2013 03:56:08 GMT', 'content-length': '202', 'content-type': 'application/json; charset=UTF-8', 'x-compute-request-id': 'req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab'} _log_response /opt/stack/tempest/tempest/common/rest_client.py:310
2013-08-28 11:56:08.169 2220 DEBUG tempest.common.rest_client [-] Response Body: {""computeFault"": {""message"": ""Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.\n<class 'nova.exception.AdminRequired'>"", ""code"": 500}} _log_response /opt/stack/tempest/tempest/common/rest_client.py:314
the nova log is:
2013-08-28 11:56:08.165 DEBUG routes.middleware [-] Matched POST /flavors/155214353/action from (pid=12748) __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-08-28 11:56:08.166 DEBUG routes.middleware [-] Route path: '/flavors/:(id)/action', defaults: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x4e72050>} from (pid=12748) __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-08-28 11:56:08.166 DEBUG routes.middleware [-] Match dict: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x4e72050>, 'id': u'155214353'} from (pid=12748) __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-08-28 11:56:08.166 DEBUG nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Action: 'action', body: {""add_tenant_access"": {""tenant_id"": ""9bfa07133a42464a8701e3cf367bbb4d""}} from (pid=12748) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:927
2013-08-28 11:56:08.166 DEBUG nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Calling method <bound method FlavorActionController._add_tenant_access of <nova.api.openstack.compute.plugins.v3.flavor_access.FlavorActionController object at 0x50f5f50>> from (pid=12748) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:928
2013-08-28 11:56:08.167 ERROR nova.api.openstack.extensions [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Unexpected exception in API method
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 469, in wrapped
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/flavor_access.py"", line 176, in _add_tenant_access
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     flavors.add_flavor_access(id, tenant, context)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/flavors.py"", line 245, in add_flavor_access
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     return db.flavor_access_add(ctxt, flavorid, projectid)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/db/api.py"", line 1424, in flavor_access_add
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     return IMPL.flavor_access_add(context, flavor_id, project_id)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 106, in wrapper
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     nova.context.require_admin_context(args[0])
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/context.py"", line 195, in require_admin_context
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     raise exception.AdminRequired()
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions AdminRequired: User does not have admin privileges
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions
2013-08-28 11:56:08.167 INFO nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] HTTP exception thrown: Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.AdminRequired'>
2013-08-28 11:56:08.168 DEBUG nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Returning 500 to user: Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.AdminRequired'> from (pid=12748) __call__ /opt/stack/nova/nova/api/openstack/wsgi.py:1188
2013-08-28 11:56:08.168 INFO nova.osapi_compute.wsgi.server [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] 192.168.1.101 ""POST /v3/flavors/155214353/action HTTP/1.1"" status: 500 len: 409 time: 0.0126910"
1217998,1217998,neutron,3e1116eb0f1d94530707cd6ef4b37f17e9a13918,1,1,Duplicated of  1210877,CONSTRAINT routerl3agentbindings failure during ga...,"http://logs.openstack.org/60/43760/3/check/gate-tempest-devstack-vm-neutron/dd1e380/logs/screen-q-svc.txt.gz
Shows a constraint failure, which is almost always evidence of some kind of race/poor error handling:
2013-08-27 18:45:06.141 29478 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 424, in _process_data
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     **args)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 44, in dispatch
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 47, in sync_routers
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     plugin.auto_schedule_routers(context, host, router_ids)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/agentschedulers_db.py"", line 302, in auto_schedule_routers
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     self, context, host, router_ids)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     context.session.add(binding)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     self.commit()
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     self._prepare_impl()
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     self.session.flush()
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     raise exception.DBError(e)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_2` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('c0771426-7e66-430e-beb9-4c3334e43039', '4dae479a-33a1-4d60-9a82-f0ee09cb9491', '039d8fe8-8993-4c9e-89a4-196dccb2878a')
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp
2013-08-27 18:45:06.142 29478 ERROR neutron.openstack.common.rpc.common [-] Returning exception (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_2` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('c0771426-7e66-430e-beb9-4c3334e43039', '4dae479a-33a1-4d60-9a82-f0ee09cb9491', '039d8fe8-8993-4c9e-89a4-196dccb2878a') to caller
2013-08-27 18:45:06.142 29478 ERROR neutron.openstack.common.rpc.common [-] ['Traceback (most recent call last):\n', '  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 424, in _process_data\n    **args)\n', '  File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 44, in dispatch\n    neutron_ctxt, version, method, namespace, **kwargs)\n', '  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n    result = getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 47, in sync_routers\n    plugin.auto_schedule_routers(context, host, router_ids)\n', '  File ""/opt/stack/new/neutron/neutron/db/agentschedulers_db.py"", line 302, in auto_schedule_routers\n    self, context, host, router_ids)\n', '  File ""/opt/stack/new/neutron/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers\n    context.session.add(binding)\n', '  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__\n    self.commit()\n', '  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit\n    self._prepare_impl()\n', '  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl\n    self.session.flush()\n', '  File ""/opt/stack/new/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap\n    raise exception.DBError(e)\n', ""DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_2` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('c0771426-7e66-430e-beb9-4c3334e43039', '4dae479a-33a1-4d60-9a82-f0ee09cb9491', '039d8fe8-8993-4c9e-89a4-196dccb2878a')\n""]
Full gate logs are here:
http://logs.openstack.org/60/43760/3/check/gate-tempest-devstack-vm-neutron/dd1e380/"
1218057,1218057,nova,6653abe5b11b2fccdc0c02083297335d42d57fe1,0,0,Feature “to allow the newly created volume file to be”,Nova libvirt driver not passing REUSE flag when _s...,"For filesystem backed Cinder drivers like GPFS, a new volume  file is created before the swap operation is invoked as part of the online volume migration sequence.
For this reason, the call to blockRebase from . _swap_volume (in nova.virt.libvirt.driver) should include the flag  VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT to allow the newly created volume file to be written to."
1218185,1218185,neutron,7753ce5fc3e489857e785dac08c951d32050f8d5,0,0,Test files,Use assertEqual instead of assertEquals,"https://review.openstack.org/#/c/27818/  introduces thee spot of assertEquals() which is deprecated
see https://review.openstack.org/#/c/27818/8/neutron/tests/unit/openvswitch/test_ovs_db.py"
1218338,1218338,neutron,55bfb412f1732e97da76f1b34e581d2429df38e6,1,1, ,alembic autogenerate detects table deletion of ser...,"When I run neutron-db-manage revision --autogenerate, it detects lbaas table deletion
even when loadbalancer service plugins are declared in neutron.conf.
The reason is table definitions of service plugins are not loaded when ""revision --autogenerate"" is run."
1218556,1218556,neutron,8d88ee7411d43f148b45d0a145fe32a75765a3ac,1,0,“Some distributions (e.g. openSUSE) have udev rules installed by default” Change environment,veth pair connecting between physical and integrat...,"Sometimes after restarting the  openvswitch-agent the veth pair that connects the physical bridge with the integration bridge doesn't come up correctly. (Which of cause disconnects any running VM instance from the network)
# /etc/init.d/openstack-neutron-openvswitch-agent restart
# ip addr show
[..]
83: phy-br-eth1: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast state DOWN qlen 1000
    link/ether 3a:6c:d6:a4:1c:89 brd ff:ff:ff:ff:ff:ff
84: int-br-eth1: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast state DOWN qlen 1000
    link/ether a2:12:2a:e5:b8:e4 brd ff:ff:ff:ff:ff:ff
[..]
I was able to reproduce this problem on openSUSE 12.3 and SLES 11. Ubuntu seems to be unaffected by this.
Doing a manual ""ip link set up dev <device>"" on both ends of the veth pair fixes the problem. (until another restarted might bring it back)
I think I was able to track this down to a race condition between udev  (and its network rules) and the ip commands that the openvswitch-agent during startup. Among other things the agent does this during startup:
ip link delete  int-br-fixed
ip link add int-br-fixed type veth peer  name phy-br-fixed
ip link set int-br-fixed up
ip link set phy-br-fixed up
The ip link delete and ip link add command cause several udev events to be fired. However on my system the processing of the udev rules takes so long that the ""remove"" events are not completely processed before the ip link add command is started. Which causes the interface to be down after the above commands completed.
A possible fix for this is to call ""udevadm settle"" after the ip link delete call.
I will upload a draft patch for review shortly."
1218588,1218588,neutron,dc48ac1a9c02c236157347f715190a2b1107ec70,1,1, ,Bug #1218588 “Cisco N1K plugin,"Plugin: Cisco plugin with N1KV.
n1kv:profile_id is mandatory and hence the network and port creation dont go through if no n1kv:profile_id parameter is passed.
This raises a network binding/ port binding error.
2013-03-15 12:45:47    DEBUG [routes.middleware] Match dict: {'action': u'index', 'controller': wsgify(quantum.api.v2.resource.resource, RequestClass=<class 'quantum.api.v2.resource.Request'>), 'format': u'json'}
2013-03-15 12:45:47    ERROR [quantum.api.v2.resource] index failed
Traceback (most recent call last):
  File ""/opt/stack/quantum/quantum/api/v2/resource.py"", line 95, in resource
    result = method(request=request, **args)
  File ""/opt/stack/quantum/quantum/api/v2/base.py"", line 198, in index
    return self._items(request, True)
  File ""/opt/stack/quantum/quantum/api/v2/base.py"", line 168, in _items
    obj_list = obj_getter(request.context, **kwargs)
  File ""/opt/stack/quantum/quantum/plugins/cisco/n1kv/n1k_quantum_plugin.py"", line 427, in get_networks
    self._extend_network_dict_provider(context, net)
  File ""/opt/stack/quantum/quantum/plugins/cisco/n1kv/n1k_quantum_plugin.py"", line 159, in _extend_network_dict_provider
    network[provider.NETWORK_TYPE] = binding.network_type
AttributeError: 'NoneType' object has no attribute 'network_type'
2013-03-15 12:45:47    DEBUG [eventlet.wsgi.server] 10.0.2.15 - - [15/Mar/2013 12:45:47] ""GET //v2.0/networks.json?router%3Aexternal=True HTTP/1.1"" 500 215 0.052251"
1218621,1218621,neutron,3ba80960075bf7873e96914f1be54ae176069a8d,0,0,Test files,Fix unsuitable assertTrue/assertFalse in api unitt...,"Some usages of assertTrue()/assertFalse() are incorrect, improve them to more explicit assert  from the unit test suite (like assertIsNotNone or assertIn)."
1218779,1218779,nova,281e2a10628eb394b6b905286070c4f25ce9f134,1,1,“Fix wrong method call in baremetal”,'PXE' object has no attribute 'get_pxe_config_file...,"The baremetal driver's throwing the following error:
ERROR nova.virt.baremetal.driver [req-aa418c3f-a45b-40a2-b0b3-dff16886a8ab 94242353fea44662973dbe7ce3dc980c 93e2f6898a394052a6c1b7b22aedcf1f] Exception no pxe bootfile-name path: 'PXE' object has no attribute 'get_pxe_config_file_path'
The error happens because of this line in the baremetal/driver.py file.
nova/virt/baremetal/driver.py:511:            bootfile_path =            self.driver.get_pxe_config_file_path(instance)
Looking at the baremetal/pxe.py you'll see that the `get_pxe_config_file` method is not part of the PXE class, but that method exist in the file."
1218861,1218861,nova,a957beb94720ca3f171149155f9dcabd20e6141b,1,1, ,Nova compute throws an exception if ephemeral is p...,"Alex Xu	reports on a related code review https://review.openstack.org/#/c/41647/
""""""
Xavier, When create vm as below, i got some error from nova-compute side:
'{""server"": {""name"": ""vm3"", ""image_ref"": ""b8cd5faa-a65f-4e47-bfc8-68061574b428"", ""flavor_ref"": ""1"", ""max_count"": 1, ""min_count"": 1, ""os-block-device-mapping:block_device_mapping"": [{""device_name"": ""/dev/vdc"", ""source_type"": ""blank"", ""destination_type"": ""local"", ""boot_index"": 0}], ""networks"": [{""uuid"": ""b6ba34f1-5504-4aca-825b-04511c104802""}]}}'
2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] Traceback (most recent call last): 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/compute/manager.py"", line 1018, in _build_instance 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] set_access_ip=set_access_ip) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/compute/manager.py"", line 1392, in _spawn 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] LOG.exception(_('Instance failed to spawn'), instance=instance) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/compute/manager.py"", line 1388, in _spawn 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] block_device_info) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1689, in spawn 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] admin_pass=admin_password) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2002, in _create_image 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] size = eph['size'] * 1024 * 1024 * 1024 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] TypeError: unsupported operand type(s) for *: 'NoneType' and 'int'
That because I miss 'volume_size' in the request.
""""""
I was also able to reproduce on devstack with latest master."
1219097,1219097,cinder,d2479f1e88e1eff777afbc922ae1da63afcc7716,1,0,“fails due to db schema change”,online volume migration fails due to db schema cha...,"After installing latest cinder code and applying db sync operation, online volume migration now fails with the following traceback:
2013-08-30 14:26:43.411 1399033 ERROR cinder.openstack.common.rpc.amqp [req-78555fa4-c79c-4160-993d-05d60b001222 d8a70c91de4d41cc9bf994c8d23e4479 af64382b5b1449189608c0a3c9c39a0c] Exception during message handling
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 433, in _process_data
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 686, in migrate_volume_completion
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     self.db.finish_volume_migration(ctxt, volume_id, new_volume_id)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/db/api.py"", line 198, in finish_volume_migration
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     return IMPL.finish_volume_migration(context, src_vol_id, dest_vol_id)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/db/sqlalchemy/api.py"", line 120, in wrapper
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/db/sqlalchemy/api.py"", line 1121, in finish_volume_migration
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     filter_by(id=dest_vol_id).\
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/query.py"", line 2679, in delete
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     result = session.execute(delete_stmt, params=self._params)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/db/sqlalchemy/session.py"", line 485, in _wrap
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     raise exception.DBError(e)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp DBError: (IntegrityError) (1451, 'Cannot delete or update a parent row: a foreign key constraint fails (`cinder`.`volume_admin_metadata`, CONSTRAINT `volume_admin_metadata_ibfk_1` FOREIGN KEY (`volume_id`) REFERENCES `volumes` (`id`))') 'DELETE FROM volumes WHERE volumes.id = %s' ('86475de8-daba-45e3-9c15-76ca0941efb6',)
There is new table in cinder db volume_admin_metadata with FK relationship with volumes.  This admin metadata needs to be handled in the same way as normal volume metadata."
1219693,1219693,nova,11b9305e3f0dcff7f5f92863cfd3679fd7b6a6a3,1,0,"Database bugs “PostgreSQL gives an error if you try to shove 256+ characters into a 255-character field. MySQL truncates by default, but can be configured to give an error.”",create aggregate,"Create aggregate fails when input metadata ""availability_zone"" length >255(models.py value = Column(String(255), nullable=False)),Exception is raised but record in DB is not rolled back."
1220011,1220011,neutron,d632b66dc8b701ca777af4335b6505b4c4cd7828,1,1, ,Bug #1220011 “Cannot clear binding,"binding:profile attribute is a dict, but there is no way to clear it.
None is used to indicate to clear the corresponding attribute in general.
The validator for port binding should accept None.
It can be fixed by changing the type:dict to type:dict_or_none in the extension.
It is required by NEC plugin portbinding support ( https://blueprints.launchpad.net/neutron/+spec/nec-port-binding ) but it requires a change in the common code, so this bug will be fixed in a separate patch."
1220112,1220112,nova,035f05eb531cb8dd805685272d46ecd8cb0ad2b3,1,1,Backwards compatibility,scheduler rpcapi 2.9 is not backwards compatible,"patch 552693e4ad51291c8bfd28cd1939ed3609f6eeac (https://review.openstack.org/#/c/37933/) which added compute scheduler rpcapi version 2.9 does not provide any backwards compatibility, this should be done using 'self.can_send_version'
The result of this is, if compute is running grizzly and trunk is running with
[upgrade_levels]
scheduler=grizzly
in nova.conf
nova-conductor logs:
1e5a084a4838b5d258532645eacc'} from (pid=861) _safe_log /opt/stack/nova/nova/openstack/common/rpc/common.py:277
2013-09-03 04:32:36.347 DEBUG qpid.messaging.io.ops [-] SENT[4a35cb0]: SessionCompleted(commands=[0-16]) from (pid=861) write_op /usr/lib/python2.7/site-packages/qpid/messaging/driver.py:671
2013-09-03 04:32:36.351 ERROR nova.openstack.common.rpc.amqp [req-b596a82a-0be1-4b90-a93a-d05ae7011e0b demo demo] Exception during message handling
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     **args)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/conductor/manager.py"", line 745, in build_instances
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     legacy_bdm_in_spec=legacy_bdm)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/rpcapi.py"", line 112, in run_instance
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     legacy_bdm_in_spec=legacy_bdm_in_spec), version='2.9')
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 169, in cast
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     self._set_version(msg, version)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 72, in _set_version
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     raise rpc_common.RpcVersionCapError(version_cap=self.version_cap)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp RpcVersionCapError: Specified RPC version cap, 2.6, is too low
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp"
1220286,1220286,cinder,29e889bde9364f30c3f1cbeb7ee835b036451cba,1,1, ,Volume operations fail for thin provisioning the f...,"The create_thin_pool method is called when thin provisioning is enabled and the pool doesn't exist. At the end of this method, self.vg_thin_pool is set[0] with the full pool_path which makes volume's creation fail since it tries to build the pool_path again.[1]
This is the command it generates:
sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvcreate -T -V 1g -n volume-b0c7232b-4214-4d13-ac20-4894333b627d stack-volumes/stack-volumes/stack-volumes-pool
This stack-volumes/stack-volumes/stack-volumes-pool should be stack-volumes/stack-volumes-pool
[0] https://github.com/openstack/cinder/blob/master/cinder/brick/local_dev/lvm.py#L340
[1] https://github.com/openstack/cinder/blob/master/cinder/brick/local_dev/lvm.py#L353"
1220436,1220436,cinder,4fe60f6192abdf154cbf7f65021a47d7e339aa76,1,1, ,test_cinder_quota_class_show failes during gate jo...,"http://logs.openstack.org/42/44542/1/gate/gate-tempest-devstack-vm-postgres-full/2d73c40/console.html
2013-09-03 19:29:16.317 | ======================================================================
2013-09-03 19:29:16.318 | FAIL: tempest.cli.simple_read_only.test_cinder.SimpleReadOnlyCinderClientTest.test_cinder_quota_class_show
2013-09-03 19:29:16.318 | tempest.cli.simple_read_only.test_cinder.SimpleReadOnlyCinderClientTest.test_cinder_quota_class_show
2013-09-03 19:29:16.318 | ----------------------------------------------------------------------
2013-09-03 19:29:16.319 | _StringException: Empty attachments:
2013-09-03 19:29:16.319 |   stderr
2013-09-03 19:29:16.319 |   stdout
2013-09-03 19:29:16.320 |
2013-09-03 19:29:16.320 | pythonlogging:'': {{{2013-09-03 19:23:46,092 running: '/usr/local/bin/cinder --os-username admin --os-tenant-name admin --os-password secret --os-auth-url http://127.0.0.1:5000/v2.0/   quota-class-show abc'}}}
2013-09-03 19:29:16.320 |
2013-09-03 19:29:16.321 | Traceback (most recent call last):
2013-09-03 19:29:16.321 |   File ""tempest/cli/simple_read_only/test_cinder.py"", line 56, in test_cinder_quota_class_show
2013-09-03 19:29:16.321 |     params='abc'))
2013-09-03 19:29:16.322 |   File ""tempest/cli/__init__.py"", line 86, in cinder
2013-09-03 19:29:16.322 |     'cinder', action, flags, params, admin, fail_ok)
2013-09-03 19:29:16.323 |   File ""tempest/cli/__init__.py"", line 102, in cmd_with_auth
2013-09-03 19:29:16.323 |     return self.cmd(cmd, action, flags, params, fail_ok)
2013-09-03 19:29:16.323 |   File ""tempest/cli/__init__.py"", line 123, in cmd
2013-09-03 19:29:16.324 |     result)
2013-09-03 19:29:16.324 | CommandFailed: Command '['/usr/local/bin/cinder', '--os-username', 'admin', '--os-tenant-name', 'admin', '--os-password', 'secret', '--os-auth-url', 'http://127.0.0.1:5000/v2.0/', 'quota-class-show', 'abc']' returned non-zero exit status 1"
1220459,1220459,nova,9adba6727bbec051c1437ee95367ecb97b207f35,1,1, ,VMware Driver reports incorrect disk usage,"VMware VCDriver currently reports the disk usage 'local_gb_used' metric as the capacity of a randomly chosen datastore in the vCenter cluster. The right value would be the aggregate capacity of all datastores in the cluster.
Eg.
With a cluster having 3 Datastores (3x 100GB LUN), only 100GB appears in ""free_disk_gb"" when you execute ""nova hypervisor-show""."
1220521,1220521,neutron,d6f50e8476feb3c76db485fa033d0b1c22ee024d,1,1, ,incorrect format type in BigSwitch plugin log line...,"The BigSwitch plugin incorrectly tries to log the response code it gets from an HTTP request as an integer when it should be a string, resulting in these errors in the output:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 552, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 516, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
TypeError: %d format: a number is required, not str
Logged from file plugin.py, line 339"
1220557,1220557,cinder,6956065f0b336c0d4edeba265fae8c9f7cddb736,1,1, ,no conversion type in LOG.debug string,"method extend_volume in cinder/volume/drivers/rbd.py
> LOG.debug(_(""Extend volume from %(old_size) to %(new_size)""),
Convert type 's' is required.
method extend_volume in cinder/volume/drivers/sheepdog.py
> LOG.debug(_(""Extend volume from %(old_size) to %(new_size)""),
Convert type 's' is required."
1220692,1220692,neutron,6d1037335bbe969cfbc6d9657c24865ca226c7e9,1,1, ,LBaaS HAProxy agent outputs traceback in get_stats...,"I found the following error in q-lbaas log after creating a vip on a pool.
2013-09-04 21:41:59.830 10678 DEBUG neutron.openstack.common.periodic_task [-] Running periodic task LbaasAgentManager.collect_stats run_periodic_tasks /opt/stack/neutron/neutron/openstack/common/periodic_task.py:176
2013-09-04 21:41:59.831 10678 ERROR neutron.services.loadbalancer.drivers.haproxy.agent_manager [-] Error upating stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Traceback (most recent call last):
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/agent_manager.py"", line 137, in collect_stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     stats = driver.get_stats(pool_id)
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 168, in get_stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     pool_stats['members'] = self._get_servers_stats(parsed_stats)
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 188, in _get_servers_stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     if stats['type'] == TYPE_SERVER_RESPONSE:
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager KeyError: 'type'
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager"
1220813,1220813,cinder,193ad7994d536f644d73849512b574d4f5ee7546,1,1,"Changed requirements333, the commit before the change was done in may 2012 and the bug report in September 2013",inconsistent i18n message,"in cinder/exception.py: https://github.com/openstack/cinder/blob/master/cinder/exception.py#L242
message = _(""Invalid metadata"") + "": %(reason)s""
is not consistent i18n usage. In most of cases, we use _() for whole string instead of leaving something out.
More than inconsistent, some text in leaving strings may be not translated, for example, ':' in English is different '：' in Chinese."
1220856,1220856,nova,d2874463010b129e62a568357cfa1df3ab4805e6,1,1, ,libvirt error when create VM with 2 NICs under Dev...,"Under a stock DevStack setup on bare metal, I started stack.sh, which creates a public network (w/o DHCP) and a private network (with DHCP) and a router.  On the host I created a br-ex and added eth3 to the bridge. The public network is connected via a physical switch, to another host (also running Devstack).
I am able to create VMs, and ping between them, and I can also (with the new VPNaaS feature under development) ping VMs over the public (provider?) network.
If I create a VM (seen this with cirros and others) and specify the private network, or create a VM with the public network, they launch fine. However, if I try to boot an instance with two NICs, the launch fails:
 localnet=`neutron net-list | grep private | cut -f 2 -d'|' | cut -f 2 -d' '`
 nova boot --image cirros-0.3.1-x86_64-uec --flavor 1 mary --nic net-id=$localnet
pubnet=`neutron net-list | grep public | cut -f 2 -d'|' | cut -f 2 -d' '`
nova boot --image cirros-0.3.1-x86_64-uec --flavor 1 peter --nic net-id=$pubnet
nova boot --image cirros-0.3.1-x86_64-uec --flavor 1 paul --nic net-id=$localnet --nic net-id=$pubnet
nova list
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| ID                                   | Name  | Status | Task State | Power State | Networks            |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| 0fc9c41c-fcd7-45a8-ae10-568b506a331f | mary  | ACTIVE | None       | Running     | private=10.2.0.4    |
| 76925e14-a0b7-4285-9197-5ff0e92f5bb4 | paul  | ERROR  | None       | NOSTATE     |                     |
| c8cd45a4-10d9-4d8c-9a5a-f806e63683c0 | peter | ACTIVE | None       | Running     | public=172.24.4.235 |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
Looking at the logs, I see that the screen-n-cpu.log reports an error and has a traceback:
2013-09-04 18:01:47.963 ERROR nova.compute.manager [req-187bbd89-f40d-4637-b549-0edfc9b66419 admin admin] [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] Instance failed to spawn
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] Traceback (most recent call last):
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1293, in _spawn
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     block_device_info)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1699, in spawn
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     block_device_info)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2697, in _creat\
e_domain_and_network
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     domain = self._create_domain(xml, instance=instance, power_on=power_on\
)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2652, in _creat\
e_domain
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     domain.XMLDesc(0))
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2647, in _creat\
e_domain
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     domain.createWithFlags(launch_flags)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 17\
9, in doit
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     result = proxy_call(self._autowrap, f, *args, **kwargs)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 13\
9, in proxy_call
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     rv = execute(f,*args,**kwargs)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77\
, in tworker
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     rv = meth(*args,**kwargs)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 581, in createW\
ithFlags
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed',\
 dom=self)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] libvirtError: internal error Cannot instantiate filter due to unresolvable\
 variables: DHCPSERVER
There is no DHCP server on the public network, but there is one on the local network.
This is consistently reproducible and occurs with other image types.
The nova code is off the master branch (havana) with the SHA1 of 86c97ff from 6 days ago."
1220904,1220904,nova,d61eb32eeeaf213912383fb14bf86f2511518943,1,0,"“As some environments/architectures may have different PXE binary,this needs to be an option.” Change in the environment",PXE dhcp_option needs to provide the bootfile_name...,"in the nova/virt/baremetal/driver.py the call to pxe.py pxe.get_pxe_config_file_path(instance) need to return the bootfile name not the path.
Line 84 reads only my_ip from nova conf. But, bootfile_path in line 512 (bootfile_path = self.driver.get_pxe_config_file_path(instance)) is getting set from pxe.py which is getting the value “/tftpboot/<instance-id>/config”. This value should actually be  “pxelinux.0” (or absolute location of pxelinux.0) .  baremetal does not find the bootfile in “/tftpboot/<instance-id>/config”, it finds it in /tftpboot/pxelinux.0 (where tftproot=/tftpboot set in nova.conf).
In pxe.py,
def get_pxe_config_file_path(instance):
    """"""Generate the path for an instances PXE config file.""""""
    return os.path.join(CONF.baremetal.tftp_root, instance['uuid'], 'config')"
1220947,1220947,cinder,75e39fc7b232ac54422ba6a3b5e58308e54297b1,1,1,"“should have been synchronized with the synchronize annotation, but were not.”",3par extend_volume methods not synchronized,"The HP3PAR FC and iSCSI drivers are not synchronizing entry into methods ""extend_volume""."
1221026,1221026,nova,fcceb93bc842ee2ba61dd21205d09c66d1f08634,1,1,“fix conversion type missing”,conversion type missing in log message,"target codes:
def _log_progress_if_required(left, last_log_time, virtual_size):
    if timeutils.is_older_than(last_log_time, PROGRESS_INTERVAL_SECONDS):
        last_log_time = timeutils.utcnow()
        complete_pct = float(virtual_size - left) / virtual_size * 100
        LOG.debug(_(""Sparse copy in progress, ""
                    ""%(complete_pct).2f%% complete. ""
                    ""%(left) bytes left to copy""),                       <====== here miss a conversion type like 's'
            {""complete_pct"": complete_pct, ""left"": left})
    return last_log_time"
1221036,1221036,neutron,ee26de1ed22fc67591a7b5465132f78af1f1c0f4,1,1, ,conversion type missing in log message,"target codes: (https://github.com/openstack/neutron/blob/master/neutron/plugins/mlnx/agent/eswitch_neutron_agent.py)
  def provision_network(self, port_id, port_mac,
                          network_id, network_type,
                          physical_network, segmentation_id):
        LOG.info(_(""Provisioning network %s""), network_id)
        if network_type == constants.TYPE_VLAN:
            LOG.debug(_(""creating VLAN Network""))
        elif network_type == constants.TYPE_IB:
            LOG.debug(_(""creating IB Network""))
        else:
            LOG.error(_(""Unknown network type %(network_type) ""      <======== miss a conversion type here like 's'
                        ""for network %(network_id)""),                                          <======== miss a conversion type here like 's'
                      {'network_type': network_type,
                       'network_id': network_id})
            return
        data = {
            'physical_network': physical_network,
            'network_type': network_type,
            'ports': [],
            'vlan_id': segmentation_id}
        self.network_map[network_id] = data"
1221190,1221190,nova,dc8de426066969a3f0624fdc2a7b29371a2d55bf,1,1, ,[0SSA 2014-009] Image format not enforced when usi...,"Rescuing an instance seems to guess the image format at some point. This allows reading files from the compute host via the qcow2 backing file.
Requirements:
- instances spawned using libvirt
- use_cow_images = False in the config
To reproduce:
1. Create a qcow2 file backed by the path you want to read from the compute host. (qemu-img create -f qcow2 -b /path/to/the/file $((1024*1024)) evil.qcow2)
2. Spawn an instance, scp the file into it.
3. Overwrite the disk inside the instance (dd if=evil.qcow2 of=/dev/vda)
4. Shutdown the instance.
5. Rescue the instance
6. While in rescue mode, login and read /dev/vdb - beginning should be read from the qcow backing file
Libvirt description of the rescued instance will contain the entry for the second disk with attribute type=""qcow2"", even though it should be ""raw"" - same as the original instance.
Mitigating factors:
- files have to be readable by libvirt/kvm
- apparmor/selinux will limit the number of accessible files
- only full blocks of the file are visible in the rescued instance, so short files will not be available at all and long files are going to be truncated
Possible targets:
- private snapshots with known uuids, or instances of other tenants are a good target for this attack"
1221244,1221244,nova,af911f12fe726ae17601e2381455742882ca71e8,1,1,"“Fix misuse of ""instance"" parameter”",baremetal driver misuses “instance,"These methods that handle block device mapping pass instance['name'] to attach/detach_volume(), but it should be instance itself.
    def _attach_block_devices(self, instance, block_device_info):
        block_device_mapping = driver.\
                block_device_info_get_mapping(block_device_info)
        for vol in block_device_mapping:
            connection_info = vol['connection_info']
            mountpoint = vol['mount_device']
            self.attach_volume(
                    connection_info, instance['name'], mountpoint)
    def _detach_block_devices(self, instance, block_device_info):
        block_device_mapping = driver.\
                block_device_info_get_mapping(block_device_info)
        for vol in block_device_mapping:
            connection_info = vol['connection_info']
            mountpoint = vol['mount_device']
            self.detach_volume(
                    connection_info, instance['name'], mountpoint)"
1221315,1221315,neutron,44fd0f74e5b19593e9d37eaec1f87134003b10f4,0,0,Feature3 “_validate_network_tenant must be less strict”,_validate_network_tenant_ownership must be less st...,"Neutron, currently does a strict validation code in https://github.com/openstack/neutron/blob/master/neutron/api/v2/base.py#L618
so that for non-shared network the subnets and ports must belong to the same tenant as the network. In the case of a “service VM” created by an admin user, this function should return thus allowing admin users to create ports and networks in a tenant network.
Original code: https://github.com/openstack/neutron/blob/master/neutron/api/v2/base.py#L604
Proposed Fix:
    def _validate_network_tenant_ownership(self, request, resource_item):
        # TODO(salvatore-orlando): consider whether this check can be folded
        # in the policy engine
        if self._resource not in ('port', 'subnet') or request.context.is_admin:
            return
        network = self._plugin.get_network(
            request.context,
            resource_item['network_id'])
        # do not perform the check on shared networks
        if network.get('shared'):
            return
        network_owner = network['tenant_id']
        if network_owner != resource_item['tenant_id']:
            msg = _(""Tenant %(tenant_id)s not allowed to ""
                    ""create %(resource)s on this network"")
            raise webob.exc.HTTPForbidden(msg % {
                ""tenant_id"": resource_item['tenant_id'],
                ""resource"": self._resource,
            })"
1221336,1221336,neutron,abf2f328e87195e87a03ccdef11d3bd787aec891,1,1,“Last minute modification introduced typo”,"Midonet plugin, FIP and MD server connectivity pro...","While doing more thorough integration tests, we discovered two problems:
1. Floating IP is not working correctly. When the packet for a floating ip reaches the router, the router drops it due to a security rule wrongly set.
2. Last minute modification introduced typo in the code that made MD server unreachable"
1221493,1221493,nova,341b9866ce337034b6d8ca768d03581b4bf40a5c,1,1,"“the message ""An unknown exception occurred."" was obtained.”",Exception message that is different from the expec...,"When an exception PciConfigInvalidWhitelist occurs, the message ""An unknown exception occurred."" was obtained."
1221500,1221500,neutron,0739b2fe827b3beefb5ad81c063cbc4d00525a9d,1,1,,NotImplementedError is a Python built-in exception...,"$ git grep \\.NotImplementedError
neutron/extensions/l3.py:288:        raise qexception.NotImplementedError()
neutron/extensions/l3.py:291:        raise qexception.NotImplementedError()
neutron/plugins/nicira/NeutronPlugin.py:1091:                raise q_exc.NotImplementedError(_(""admin_state_up=False """
1221525,1221525,nova,ea91ad2b2317cb88fb14c0cdaf31187eca93ff57,1,0,"“This is not an issue with most volume backends, but it causes the attach to fail when using Storwize as a Cinder backend.” Compatibility or bug3",Cannot attach a volume to a Hyper-V Nova instance ...,"I found we can not attach one volume to one instance(this instance is in hyperV compute node).
Test Env:
Cinder using Storwizedriver, and nova compute node is hyperV env.
When I want to attach one available volume to an instance which in hyper compute node,  after ran ""nova volume-attach"" command, the volume status still is available(available-> attaching->availble), and from hyper compute node, in compute.log. I got the following error message:
2013-09-05 19:54:30.273 2204 AUDIT nova.compute.manager [req-a62e1672-c825-488f-998f-c5c58c64ccde 427700be22794f588b97ff07854e2031 dd8f0667af5c44bab29899fb88adae96] [instance: 1b892ca0-3b5f-4792-a394-e9de661b429a] Attaching volume a053f7f3-8ffd-4392-b278-3791c2cd29bd to /dev/sdb
2013-09-05 19:54:30.305 2204 INFO nova.virt.hyperv.basevolumeutils [req-a62e1672-c825-488f-998f-c5c58c64ccde 427700be22794f588b97ff07854e2031 dd8f0667af5c44bab29899fb88adae96] The ISCSI initiator name can't be found. Choosing the default one
2013-09-05 19:54:30.336 2204 INFO urllib3.connectionpool [-] Starting new HTTP connection (1): 9.123.137.71
2013-09-05 19:54:31.618 2204 ERROR nova.virt.hyperv.volumeops [req-a62e1672-c825-488f-998f-c5c58c64ccde 427700be22794f588b97ff07854e2031 dd8f0667af5c44bab29899fb88adae96] Attach volume failed: <x_wmi: Unexpected COM Error (-2147352567, 'Exception occurred.', (0, u'SWbemObjectEx', u'Generic failure ', None, 0, -2147217407), None)>
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops Traceback (most recent call last):
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\volumeops.py"", line 113, in attach_volume
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops     self._login_storage_target(connection_info)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\volumeops.py"", line 100, in _login_storage_target
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops     target_portal)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\volumeutilsv2.py"", line 53, in login_storage_target
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops     TargetPortalPortNumber=target_port)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\wmi.py"", line 431, in __call__
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops     handle_com_error ()
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\wmi.py"", line 241, in handle_com_error
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops     raise klass (com_error=err)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops x_wmi: <x_wmi: Unexpected COM Error (-2147352567, 'Exception occurred.', (0, u'SWbemObjectEx', u'Generic failure ', None, 0, -2147217407), None)>
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops"
1221527,1221527,nova,8b6bc869394bdcbe9cbead8bda8d45be75d2874b,1,1,“Exception message that is different from the expected” . Duplicate.,Exception message that is different from the expec...,"When an exception ProjectUserQuotaNotFound occurs, the message ""Quota
could not be found"" was obtained.  I think that expecting ""Quota for
user XXX in project YYY could not be found."" as follows.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/tests/test_quota.py"", line 480, in test_get_by_project_and_user_with_wrong_resource
    'fake_user', 'wrong_resource')
  File ""/opt/stack/nova/nova/quota.py"", line 1066, in get_by_project_and_user
    user_id, resource)
  File ""/opt/stack/nova/nova/tests/test_quota.py"", line 247, in get_by_project_and_user
    user_id=user_id)
ProjectUserQuotaNotFound: Quota for user fake_user in project test_project could not be found.
As a similar problem, when an exception ConsoleTypeUnavailable occurs,
the message ""Unacceptable parameters."" was obtained.  I think that
expecting ""Unavailable console type XXX."" as follows.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/tests/virt/libvirt/test_libvirt.py"", line 5020, in test_get_spice_console_unavailable
    conn.get_spice_console(instance_ref)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2155, in get_spice_console
    ports = get_spice_ports_for_instance(instance['name'])
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2153, in get_spice_ports_for_instance
    raise exception.ConsoleTypeUnavailable(console_type='spice')
ConsoleTypeUnavailable: Unavailable console type spice.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/tests/virt/libvirt/test_libvirt.py"", line 4979, in test_get_vnc_console_unavailable
    conn.get_vnc_console(instance_ref)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2135, in get_vnc_console
    port = get_vnc_port_for_instance(instance['name'])
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2133, in get_vnc_port_for_instance
    raise exception.ConsoleTypeUnavailable(console_type='vnc')
ConsoleTypeUnavailable: Unavailable console type vnc."
1221620,1221620,nova,59fb3c18759bb2529a9c1dea445c2d5caf6746da,1,1,"“This particular error is caused by this change https://review.openstack.org/#/c/43151/""",Bug #1221620 “KeyError,"Traceback while scheduling both overcloud nodes on tripleo ci
Last succesfull run was 05-Sep-2013 01:54:10 (UTC)
So something changed after this run https://review.openstack.org/#/c/43968/
although scheduling of baremetal node seems to work on seed ....
INFO nova.scheduler.filter_scheduler [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] Attempting to build 1 instance(s) uuids: [u'f71e3e47-f2a2-4a13-9
WARNING nova.scheduler.utils [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] Failed to scheduler_run_instance: 'service'
WARNING nova.scheduler.utils [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] [instance: f71e3e47-f2a2-4a13-92c0-c3397acaf409] Setting instance to ERR
ERROR nova.openstack.common.rpc.amqp [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] Exception during message handling
TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
TRACE nova.openstack.common.rpc.amqp     **args)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/manager.py"", line 160, in run_instance
TRACE nova.openstack.common.rpc.amqp     context, ex, request_spec)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/manager.py"", line 147, in run_instance
TRACE nova.openstack.common.rpc.amqp     legacy_bdm_in_spec)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 87, in schedule_run_instance
TRACE nova.openstack.common.rpc.amqp     filter_properties, instance_uuids)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 326, in _schedule
TRACE nova.openstack.common.rpc.amqp     hosts = self.host_manager.get_all_host_states(elevated)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/host_manager.py"", line 432, in get_all_host_states
TRACE nova.openstack.common.rpc.amqp     service = compute['service']
TRACE nova.openstack.common.rpc.amqp KeyError: 'service'
TRACE nova.openstack.common.rpc.amqp"
1221646,1221646,nova,9501fa954b4d724b665207c2a0188e92f691fdfb,1,1,“access_ipv4 should be access_ip_v4”,v3 server's rebuild with access_ip_* doesn't work ...,"if node.hasAttribute(""access_ipv4""):
            rebuild[""access_ip_v4""] = node.getAttribute(""access_ip_v4"")
        if node.hasAttribute(""access_ipv6""):
            rebuild[""access_ip_v6""] = node.getAttribute(""access_ip_v6"")
access_ipv4 should be access_ip_v4
We can't merge this patch https://review.openstack.org/#/c/41349/
So we need fix current code."
1221663,1221663,neutron,155cb48bca9b66fa5188e5c1c63adf696cd6d127,0,0,“Make neutron.common.log.log print module path” feature or bug3,neutron.common.log.log does not print module path,"neutron.common.log.log is useful for logging arguments of a method.
It outputs class name and method name, but module path is not output.
A module path is useful to search the log message and it is better to contain a module path
[Current]
2013-09-06 17:00:22.021 25612 DEBUG neutron.common.log [-] StubOFCDriver method create_network called with arguments (u'ofc-f1c7f9f5691044f5b5d395d1a7c0', u'ID=8faea3be-66d5-4cbf-8631-810f491c6d0a Name=ext_net at Neutron.', '8faea3be-66d5-4cbf-8631-810f491c6d0a') {}  wrapper /opt/stack/neutron/neutron/common/log.py:33
[Proposed]
2013-09-06 17:00:22.021 25612 DEBUG neutron.common.log [-] neutron.tests.unit.nec.stub_ofc_driver.StubOFCDriver method create_network called with arguments (u'ofc-f1c7f9f5691044f5b5d395d1a7c0', u'ID=8faea3be-66d5-4cbf-8631-810f491c6d0a Name=ext_net at Neutron.', '8faea3be-66d5-4cbf-8631-810f491c6d0a') {}  wrapper /opt/stack/neutron/neutron/common/log.py:33"
1221726,1221726,neutron,0ff9373de6e11d7040b6b289cb3239a9ee9a924d,0,0,Test files,Lbaas tests can't be run alone by tox,"When running 'tox -epy27 services.loadbalancer' the following failures are encountered:
Traceback (most recent call last):
  File ""/home/eugene/quantum/neutron/tests/unit/services/loadbalancer/drivers/haproxy/test_agent.py"", line 32, in setUp
    cfg.CONF.register_opts(agent.OPTS)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1540, in __inner
    result = f(self, *args, **kwargs)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1673, in register_opts
    self.register_opt(opt, group, clear_cache=False)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1544, in __inner
    return f(self, *args, **kwargs)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1662, in register_opt
    if _is_opt_registered(self._opts, opt):
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 486, in _is_opt_registered
    raise DuplicateOptError(opt.name)
DuplicateOptError: duplicate option: periodic_interval"
1221946,1221946,cinder,4ea6dfd0b1140e437703128bf52d65dbd2751751,1,0,"In the beginning the code only need two args but then it evolved and now it need three args so they change the method. “NMS call nms.folder.create doesn't support third argument, that is why this call changed to nms.folder.create_with_opts.”",Bug in Nexenta NFS volume driver,"Exception in during volume creation in Nexenta NFS volume driver
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 215, in create_volume
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     flow.run(context.elevated())
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/taskflow/decorators.py"", line 105, in wrapper
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     return f(self, *args, **kwargs)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/taskflow/patterns/linear_flow.py"", line 232, in run
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     run_it(r)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/taskflow/patterns/linear_flow.py"", line 212, in run_it
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     self.rollback(context, cause)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/taskflow/patterns/linear_flow.py"", line 172, in run_it
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     result = runner(context, *args, **kwargs)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/taskflow/utils.py"", line 260, in __call__
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     self.result = self.task(*args, **kwargs)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/flows/create_volume/__init__.py"", line 1441, in __call__
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     **volume_spec)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/flows/create_volume/__init__.py"", line 1418, in _create_raw_volume
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     return self.driver.create_volume(volume_ref)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/nfs.py"", line 96, in create_volume
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     self._do_create_volume(volume)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/nexenta/nfs.py"", line 109, in _do_create_volume
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     {'compression': self.configuration.nexenta_volume_compression}
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/nexenta/jsonrpc.py"", line 82, in __call__
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     raise NexentaJSONException(response['error'].get('message', ''))
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp NexentaJSONException: Fewer items found in D-Bus signature than in Python arguments
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp"
1222656,1222656,nova,724493d21fdfcbb4c095b54975c0c1d612f0a856,1,1, ,Forget to change volume's status when an error occ...,"If the blockrebase(swap volume) fails, the status of the attached volume remains of detaching.
Tried Commit ID:b037993984229bb698050f20e8719b8c06ff2be3
1.Before Swap Volume
$ cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 66230802-aed6-4a90-9dec-42fec910d13d |   in-use  |    vol01     |  1   |     None    |  False   | 86d8d79c-be27-43c4-b148-b5637c435899 |
| c3d51a52-764a-4007-976a-136b544c561b | available |    vol02     |  1   |     None    |  False   |                                      |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
2.Swap volume running
$ cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 66230802-aed6-4a90-9dec-42fec910d13d | detaching |    vol01     |  1   |     None    |  False   | 86d8d79c-be27-43c4-b148-b5637c435899 |
| c3d51a52-764a-4007-976a-136b544c561b | attaching |    vol02     |  1   |     None    |  False   |                                      |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
3.An error occurs　in swap volume
For example, cancel a blockrebase job.
libvirtError: virDomainGetBlockJobInfo() failed
4.After an error occurs
$ cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 66230802-aed6-4a90-9dec-42fec910d13d | detaching |    vol01     |  1   |     None    |  False   | 86d8d79c-be27-43c4-b148-b5637c435899 |
| c3d51a52-764a-4007-976a-136b544c561b | available |    vol02     |  1   |     None    |  False   |                                      |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+"
1222663,1222663,nova,ed8b4706f554e5ae5d57f6017b1fd762216dad97,0,0,"Feature, “Catch more accuracy exception for _lookup_by_name”",libvirt - should catch more accuracy exception for...,"When an instance is not found by libvirtd, _lookup_by_name in libvirt
driver raises InstanceNotFound.
It's better to catch this exception instead of NotFound who is the father
of InstanceNotFound."
1222907,1222907,cinder,867b131e09a43ba7ed36267dcdec751abd652f14,1,1, ,GlusterFS clone from snapshot may select wrong sou...,"_copy_volume_from_snapshot appears to select its source file using _local_path_volume but needs to resolve source file by using the snap_info metadata.
Found this while filling in gaps in the unit test code, will add tests to cover this."
1223198,1223198,nova,911fe582ec68a1a2df0abb2cb11352c9106842c2,1,1,"“in some cases the message is incorrect.""",Invalid exception message in create_volume_from_im...,"When the PowerVMFileTransferFailed occurs in
PowerVMLocalVolumeAdapter.create_volume_from_image, in some cases the
message is incorrect."
1223205,1223205,nova,18f417c14a9313d90918285b22263623ba0c7e22,1,1, ,get console failed because can't load attribute 'p...,"Try get console as below:
curl -i http://cloudcontroller:8774/v3/servers/570a0058-3094-4201-b313-a337a984f773/consoles -X GET
Then get error in nova-api as below:
2013-09-10 14:59:46.107 ERROR nova.api.openstack.extensions [req-8f981cd0-96ef-4c95-b2b5-2d42a666deed admin admin] Unexpected exception in API method
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 469, in wrapped
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/consoles.py"", line 98, in index
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     for console in consoles])
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/consoles.py"", line 30, in _translate_keys
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     pool = cons['pool']
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/openstack/common/db/sqlalchemy/models.py"", line 59, in __getitem__
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     return getattr(self, key)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/attributes.py"", line 168, in __get__
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     return self.impl.get(instance_state(instance),dict_)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/attributes.py"", line 453, in get
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     value = self.callable_(state, passive)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/strategies.py"", line 481, in _load_for_state
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     (mapperutil.state_str(state), self.key)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions DetachedInstanceError: Parent instance <Console at 0x635d5d0> is not bound to a Session; lazy load operation of attribute 'pool' cannot proceed
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions"
1223253,1223253,nova,605e5950751cd4d19ca7fd81c04c52cfa41ce7f6,1,1,"“Subprocess output is bytestrings, not unicode”","bad logging from baremetal_deploy_helper.py, line ...","2013-09-10 07:48:36,767.767 5769 ERROR nova.virt.baremetal.deploy_helper [req-f649058f-8b8f-4392-a221-ca55a96178b0 None None] StdOut  :
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 851, in emit
    msg = self.format(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 724, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)
Logged from file baremetal_deploy_helper.py, line 217
Program stdout can be arbitrary bytes; we need to convert it to unicode before trying to log it."
1223309,1223309,nova,c19ea390ecbe045728444d49f40920329a1d0743,1,1,“Added os-security-groups prefix”,v3 security groups's attribute without prefix in c...,"Both for xml and json:
{
    ""server"": {
        ""admin_pass"": ""%(password)s"",
        ""id"": ""%(id)s"",
        ""links"": [
            {
                ""href"": ""http://openstack.example.com/v3/servers/%(uuid)s"",
                ""rel"": ""self""
            },
            {
                ""href"": ""http://openstack.example.com/servers/%(uuid)s"",
                ""rel"": ""bookmark""
            }
        ],
        ""security_groups"": [{""name"": ""test""}]
    }
}
<?xml version='1.0' encoding='UTF-8'?>
<server xmlns:atom=""http://www.w3.org/2005/Atom"" xmlns=""http://docs.openstack.org/compute/api/v1.1"" id=""%(id)s"" admin_pass=""%(password)s"">
  <metadata/>
  <atom:link href=""%(host)s/v3/servers/%(uuid)s"" rel=""self""/>
  <atom:link href=""%(host)s/servers/%(uuid)s"" rel=""bookmark""/>
  <security_groups>
   <security_group name=""test"" />
  </security_groups>
</server>
'security_groups' should be 'os-security-groups:security_groups'"
1223345,1223345,nova,6ae5899a86c0d863f1c64fdd960b109fb2a5c80f,1,1, ,Soft-deleted instance files deleted by periodic cl...,The db query used to look for instances in need of cleanup lacks a filter for soft-deleted instances.
1223452,1223452,nova,3c88fce604959a68f48d71274e0d93b74da17e34,1,1, ,"TrustedFilter checks compute trust level, not hype...","The TrustedFilter uses host_state.host as the name that will be checked against the remote attestation service.
This works for the KVM case because the compute node and the hypervisor are the same; however we must be checking host_state.nodename which is the hostname for the hypervisor which will be registered with the attestation server."
1223559,1223559,nova,0663778c531a69bf55ed8f88cd828febd69be44c,1,1, ,PCI passthrough failing on extra_info,"Havana3 is installed using Packstack on CentOS 6.4.
Nova-compute dies right after start with error ""NameError: global name '_' is not defined"".
Here is the info:
* /etc/nova/nova.conf:
pci_alias={""name"":""test"", ""product_id"":""7190"", ""vendor_id"":""8086"", ""device_type"":""ACCEL""}
pci_passthrough_whitelist=[{""vendor_id"":""8086"",""product_id"":""7190""}]
 With that configuration, nova-compute fails with the following log:
* /var/log/nova/compute.log:
  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
    **args)
  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 567, in object_action
    result = getattr(objinst, objmethod)(context, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/objects/base.py"", line 141, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/objects/pci_device.py"", line 242, in save
    self._from_db_object(context, self, db_pci)
NameError: global name '_' is not defined
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/threadgroup.py"", line 117, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     x.wait()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/threadgroup.py"", line 49, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return self.thread.wait()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/eventlet/greenthread.py"", line 166, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return self._exit_event.wait()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/eventlet/event.py"", line 116, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return hubs.get_hub().switch()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/eventlet/hubs/hub.py"", line 177, in switch
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/eventlet/greenthread.py"", line 192, in main
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/service.py"", line 65, in run_service
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     service.start()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/service.py"", line 164, in start
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self.manager.pre_start_hook()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 805, in pre_start_hook
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self.update_available_resource(nova.context.get_admin_context())
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 4773, in update_available_resource
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     rt.update_available_resource(context)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return f(*args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/compute/resource_tracker.py"", line 318, in update_available_resource
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self._sync_compute_node(context, resources)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/compute/resource_tracker.py"", line 347, in _sync_compute_node
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self._update(context, resources, prune_stats=True)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/compute/resource_tracker.py"", line 420, in _update
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self.pci_tracker.save(context)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/pci/pci_manager.py"", line 126, in save
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     dev.save(context)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/objects/base.py"", line 134, in wrapper
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     ctxt, self, fn.__name__, args, kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/conductor/rpcapi.py"", line 497, in object_action
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     objmethod=objmethod, args=args, kwargs=kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/rpcclient.py"", line 85, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return self._invoke(self.proxy.call, ctxt, method, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/rpcclient.py"", line 63, in _invoke
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return cast_or_call(ctxt, msg, **self.kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/proxy.py"", line 126, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     result = rpc.call(context, real_topic, msg, timeout)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/__init__.py"", line 139, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return _get_impl().call(CONF, context, topic, msg, timeout)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 794, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     rpc_amqp.get_connection_pool(conf, Connection))
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 574, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     rv = list(rv)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 539, in __iter__
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     raise result
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup NameError: global name '_' is not defined
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     **args)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 567, in object_action
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     result = getattr(objinst, objmethod)(context, *args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/objects/base.py"", line 141, in wrapper
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return fn(self, ctxt, *args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/objects/pci_device.py"", line 242, in save
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self._from_db_object(context, self, db_pci)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup NameError: global name '_' is not defined"
1223803,1223803,nova,0ff26160f9a7ae2a4df33c4aaf0dbb59d76ac8d5,1,0,"“V3 API is broken as the JSON format was changed, but the corresponding XML serialisation/deserialisation wasn’t.”, software evolution",V3 API server metada xml serialize/deserialize is ...,XML serialization and deserialization of updating a server metadata through the V3 API is broken. As is deserialization for retrieving a single metadata item
1223843,1223843,nova,64aa897c8a2408093e277972f1e702e1a4287db2,1,1,“run_instance() doesn't properly handle instances being deleting”,run_instance() doesn't properly handle instances b...,The UnexpectedTaskStateError that this currently results in should be caught and handled correctly to avoid unnecessary tracebacks in the compute log.
1223859,1223859,nova,1957339df302e2da75e0dbe78b5d566194ab2c08,1,1, ,Network cache not correctly updated during “interf...,"The network cache is not correctly updated when running ""nova interface-attach"": only the latest allocated IP is used. See this log:
http://paste.openstack.org/show/46643/
Nevermind the error reported when running ""nova interface-attach"": I believe it is an unrelated issue, and I'll write another bug report for it.
I noticed this issue a few months ago, but haven't had time to work on it. I'll try and submit a patch ASAP. See my analysis of the issue here: https://bugs.launchpad.net/nova/+bug/1197192/comments/3"
1223890,1223890,nova,b65eecf2d8b4df9330c09dd31b818bbd5c0da3cb,1,1,"""Due to a typo”",device_type not respected,"When launching an instance with block_device_mapping_v2, if source_type=""image"", destination_type=""volume"" and device_type=""cdrom"", the block device is attached as ""disk"" running on ""ide"" bus.  Expected to be ""cdrom"" on ""ide"" bus.
example with python-novaclient: https://dpaste.de/uojBC/
libvirt.xml generated from above command: https://dpaste.de/bSw71/"
1223975,1223975,Nova,fc786dd469a15bda1b9d3c7bacf9f1771b9b9956,1,1,The BFC ESTA MAL,multipath tool sends IOs periodically after volume...,"When detaching a volume, Nova doesn't disconnect the iSCSI portal and just return for there are other volumes attaching to the host. But the mutipath mapping device descriptor is there and multipath tool sends IOs periodically to storage array. This leads to some cinder storage drivers, such as huawei, can not unmap that volume for it detect periodic IOs.
So we need to remove the multipath device descriptor in this case."
1224014,1224014,nova,e80121a2c5ccac42f857b0628ea6f340eda1ca3a,1,1, ,live_migrate task ignores extra_specs,"The new live_migrate task in the conductor does not pass extra_specs from the flavor through to the filters - thus giving an incorrect result.
This showed up when using the TrustedFilter which depends on extra_specs (set by nova.scheduler.utils.build_request_spec) - however nova.conductor.tasks.live_migrate.LiveMigrationTask._get_candidate_destination does not use this build_request_spec and builds it's own - which missed this extra_specs value.
Marked as a security vulnerability as it means that the use of live migration will bypass filters intended to provide a secure environment such as TrustedFilter."
1224030,1224030,cinder,f8ac42460dca22f83a59097530f73b282129cc2b,1,0,“uses arguments for qemu-img (--backing-chain and --output=json) that are newer than the versions”,GlusterFS snapshot code uses new qemu-img argument...,"The GlusterFS snapshot code introduced in Havana uses arguments for qemu-img (--backing-chain and --output=json) that are newer than the versions of qemu-img that Cinder is generally expected to support.
Additionally, some code was added to parse qemu-img output rather than using the well-tested image_utils methods which do this today.
This code should not call qemu-img --backing-chain or --output=json, and should leverage image_utils as much as possible for the work being done with qemu_img."
1224251,1224251,nova,ab55af8ed5afd0765a23a85d608d4b6d35bdd166,1,0,“Modify this behavior to match EC2” software evolution,Bug #1224251 “disassociate_address on unassociated address does ... ,"disassociate_address does not return the appropriate EC2 response when disassociating an unassociated address. Despite this seeming failure, EC2 will respond success.
EC2 will return as such:
<DisassociateAddressResponse xmlns=""http://EC2.amazonaws.com/doc/2012-08-15/"">
    <requestId>aabbccdd-0146-4952-bdbe-710e4fee8387</requestId>
    <return>true</return>
</DisassociateAddressResponse>
yet, when the EC2 api encounters this scenario, it responds with a 400:
ERROR:boto:400 Bad Request
ERROR:boto:<?xml version=""1.0""?>
<Response><Errors><Error><Code>InvalidInstanceID.NotFound</Code><Message>Instance None could not be found.</Message></Error></Errors><RequestID>req-7a9b4d03-b9f5-4243-95d8-f1ccab180631</RequestID></Response>
The EC2 api should evaluate whether it receives an instance_id from:
instance_id = self.network_api.get_instance_id_by_floating_address(context, public_ip)
and, if so, then continue attempting to disassociate. Else, bail out early and return successfully."
1224334,1224334,cinder,4d43983726f4b5d1d93337be005d9e1c165c4939,1,0,"“The HyperV driver doesn't support CHAP authentication,” DocImpact",HyperV doesn't work with CHAP in Storwize/SVC driv...,"The HyperV driver doesn't support CHAP authentication, which is always enabled by the Storwize/SVC driver."
1224429,1224429,cinder,13387c01390e70c9d7811760e603e6306d6d7ea7,1,1, ,Don't use ModelBase.save() inside of block session...,"Current code use ModelBase.save() always submit a commit even inside a block fo with session.begin().
this is not purpose of using session.begin() to organize some operations  in one transaction
1)session.begin() will return a SessionTransaction instance, then call SessionTransaction.__enter__()
and do something in with block, then call SessionTransaction.__exit__(), in method __exit__() will commit or rollback automatically. See https://github.com/zzzeek/sqlalchemy/blob/master/lib/sqlalchemy/orm/session.py#L454
2) There is also suggestion  metioned in https://github.com/openstack/oslo-incubator/blob/master/openstack/common/db/sqlalchemy/session.py#L71
3) ModelBase.save() begin another transaction see https://github.com/openstack/oslo-incubator/blob/master/openstack/common/db/sqlalchemy/models.py#L47
so we'd better don't use  ModelBase.save() inside of block session.begin()"
1224677,1224677,nova,1967cee5abb71a23bccd440da9a27309a8d67081,1,1, ,_default_block_device_names() throws unhandled exc...,"During instance build, there is a call to _default_block_device_names() that contains a db call to update the instance. If the instance is deleted before this call it results in a InstanceNotFound exception that goes unhandled and ends up in the compute log. Since this is an expected error, it should be handled correctly."
1224712,1224712,nova,8b9d6f6fedbbd47932cd672f51d4db7031724e84,1,1,“this does seem to be problematic”,Cannot get ComputeNodeStat by DB utility of comput...,"When there is hypervisor gets removed, the compute_node_get_all() will not return stat for new added hypervisors.
In the following codes of compute_node_get_all() of nova/db/sqlalchemy, it assume all the record in compute_node_stats should have a matched compute node. However in current implementation of nova conductor API of compute_node_delete(), the records in compute_node_stats is not deleted. Therefore when a hypervisor gets removed, there is no node matching the record of compute_node_stats which belongs to the removed hypervisor in following codes. As a result, all the nodes will be set with 'stats' of [].
    # Join ComputeNode & ComputeNodeStat manually.
    # NOTE(msdubov): ComputeNode and ComputeNodeStat map 1-to-Many.
    #                Running time is (asymptotically) optimal due to the use
    #                of iterators (itertools.groupby() for ComputeNodeStat and
    #                iter() for ComputeNode) - we handle each record only once.
    compute_nodes.sort(key=lambda node: node['id'])
    compute_nodes_iter = iter(compute_nodes)
    for nid, nsts in itertools.groupby(stats, lambda s: s['compute_node_id']):
        for node in compute_nodes_iter:
            if node['id'] == nid:
                node['stats'] = list(nsts)
                break
            else:
                node['stats'] = []
    return compute_nodes
We need enhance either nova conductor API to clean up all the record related with instance."
1224790,1224790,cinder,996f7f949002d3d23048550b69579889f4e1c5a2,1,1," ""Fixes the use of exception.InvalidInput with the wrong arguments”",Argument of exception.InvalidInput are incorrect i...,"cinder/volume/drivers/netapp/iscsi.py and
cinder/volume/drivers/netapp/utils.py are used as follows.
  raise exception.InvalidInput(data=msg)
I think the following is correct.
  raise exception.InvalidInput(reason=msg)"
1224960,1224960,nova,2d13161643faf2b663b50d2e191232fcdeefd815,1,0,BIC or evolution333 “Nova was sending the host that previously hosted the vm”,"Bug #1224960 “wrong port-binding info after live-migration "" ","When live migrating a VM, nova-compute send the new port-binding info to neutron. But the host sent in the port-binding info is the previous host, not the host on which the VM is migrated.
I've attached a patch to resolv this, but I will ask for review as soon as I will hae time to investigate deeper."
1224967,1224967,neutron,f0f87ca56bb76ebad94b6f01e04b475bd9362bdf,1,1,"“First of all, I think there is a bug in nova” We are in neutro, how this can work",port down after live migration,"I'm using live-migration with devstack and ML2 plugin (the same error occurs with the OVS plugin)
https://wiki.openstack.org/wiki/Devstack/LiveMigration
First of all, I think there is a bug in nova :
https://bugs.launchpad.net/nova/+bug/1224960
I've proposed a patch attached to this bug to resolv it quickly.
it seems that there is also something that goes wrong in neutron.
after live-migrating a VM, the port is correctly created on the new host and the dataplane seems correct. But the port is still down in the port table, and the vif-type is ""binding_failed"" in ml2_port_bindings table"
1224978,1224978,neutron,db5e370b0d68c3e71626c99941fe487059b3cf88,0,0,"“With introduction of multi-segment support in ml2 plugin, agent misconfiguration could happen under exceptionnal circumstances:” Feature or bug3",port binding on multi segment networks could lead ...,"With introduction of multi-segment support in ml2 plugin, agent misconfiguration could happen under exceptionnal circumstances:
Supposing a multi-segment provider network is created with different network types (suppose a flat and a vlan segment), and two ports are bound on an agent supporting the associated physical network.
Portbinding validation will occur on network's segments list returned by db.get_network_segments funtion . As this function may not always returns segments in the same order, one port may be bound to the flat segment while the other will be bound to vlan one.
In that case, ports wouldn't be properly plugged in agent, as they'd recieve two contradictory segment details for the same network.
OVS agent would probably bound the two ports within the same segment as they both would use the same LocalVLANMapping
LB agent would probably add two uplink interfaces under the same qbr[net-id] bridge"
1224981,1224981,neutron,8eb142748d9f63a0e4d5902f55bb66281d0df615,1,1,“due to a string concatenation bug”,ncs mechanism driver uses wrong URL on full sync,"The URL used in the ""full sync"" operation of the ML2 mechanism driver is incorrect due to a string concatenation bug."
1225094,1225094,cinder,2cae463f18a8a0a579e6dd22ec918af79934ac00,1,0,“nms.folder.create_with_opts not supported on NexentaSto”,Nexenta NFS volume driver error in _do_create_volu...,"NMS method nms.folder.create_with_opts not supported on NexentaStor Appliance version 3.1.4.2, instead of this method must be used nms.folder.create_with_props"
1225194,1225194,cinder,863ae9f484089aa5d9e576255b39288da82c1100,1,1,Wrong logic,LVM clear_volume should raise exc. if volume_clear...,"Currently if a user configures volume_clear='non_existent_volume_clearer' in cinder.conf, the LVM driver will silently delete a volume and not wipe it.
Instead, the delete operation should fail, leaving the volume in the 'error_deleting' state.  This prevents a user from believing their volumes are being wiped when they are not due to misconfiguration."
1225659,1225659,nova,84a797ec8c801057fef7a3c30ab74426e4c3fbdc,1,1, ,Wrong handling of Instance expected_task_state,"When Instance.save called the expected_task_state parameter passed to it in order to specify what is instance expected task state.
There many cases when we would expect that there no task state for the vm (task_state=None in db). The expected_task_state=None is overwritten by the default substitution in the save() arguments and due to this not passed correctly to the db.instance_update_and_get_original method call."
1226171,1226171,nova,dbdb938fc08452ea80379652ee101b6ebe006e3f,1,0,“Update user_id length to match Keystone schema “,"When using per-domain-identity backend, user_ids c...","When using the per-domain-identity backend usernames could end up colliding when multiple LDAP backends are used since we extract very limited information from the DN.
Example
cn=example user, dc=example1,dc=com
cn=example user, dc=example2,dc=com
Would net the same ""user_id"" of ""example user""
This can also affect groups in the same manner."
1226190,1226190,cinder,8dc60e928d35ac29eef62a2d7963059edac55577,1,1, “The correct description should be 'Enable monkey patching’”. Bug in the Comments,monkey_patch config option description is wrong,"The ""monkey_patch"" BoolOpt is described as ""Whether to log monkey patching"" but appears to act as ""Enable monkey patching"".
Option definition:  http://git.openstack.org/cgit/openstack/cinder/tree/cinder/common/config.py?id=e34ab12#n189
Use: http://git.openstack.org/cgit/openstack/cinder/tree/cinder/utils.py?id=e34ab12#n593"
1226226,1226226,Nova,9b88e54998f0e197a29413a28dc101c5bbb7e89e,1,1,"""Workloads should not require a Fixed IP Address for migration""",pre_live_migration should not require a static IP ...,"The nova/compute/manager.py - pre_live_migration method is checking to ensure that there is at least a single fixed IP Address on the instance.  The comment above it indicates that this shouldn't be required (and it is not clear that it is even required).  Live migrations are blocked for systems that do not have a fixed IP Address.
If a system does not have a fixed IP, then the following error is thrown:
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 88, in wrapped
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp     event_type, level, payload)
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 76, in wrapped
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3850, in pre_live_migration
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp     instance_uuid=instance['uuid'])
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp FixedIpNotFoundForInstance: Instance eb586597-3565-4283-88d3-d0664f4d5747 has zero fixed ips."
1226229,1226229,Neutron,f96cf93e70f8c434499efb903820aa72665a7fd4,1,1,"“The root cause is when deployment finished, we only update router status to
    active if the status is in pending create.”",Edge service router status in PENDING_CREATE even ...,"When creating a logic router using plugin NvpAdvancedPlugin with service_router=True, the Edge service router status sometimes show PENDING_CREATE even when the Edge is created successfully.
Also, if the plugin is restarted, deleting an logic router does not delete Edge router."
1226337,1226337,cinder,1aef7b6304af6c19b8ceb81cbd9a3c83bb4880de,1,1, ,tempest.scenario.test_volume_boot_pattern.TestVolu...,"When running tempest tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern fails with the server going into an ERROR state.
From the console log:
Traceback (most recent call last):
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 154, in test_volume_boot_pattern
keypair)
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 53, in _boot_instance_from_volume
create_kwargs=create_kwargs)
  File ""tempest/scenario/manager.py"", line 390, in create_server
self.status_timeout(client.servers, server.id, 'ACTIVE')
  File ""tempest/scenario/manager.py"", line 290, in status_timeout
self._status_timeout(things, thing_id, expected_status=expected_status)
  File ""tempest/scenario/manager.py"", line 338, in _status_timeout
self.config.compute.build_interval):
  File ""tempest/test.py"", line 237, in call_until_true
if func():
  File ""tempest/scenario/manager.py"", line 329, in check_status
raise exceptions.BuildErrorException(message)
  BuildErrorException: Server %(server_id)s failed to build and is in ERROR status
Details: <Server: scenario-server-89179012> failed to get to expected status. In ERROR state.
The exception:
http://logs.openstack.org/64/47264/2/gate/gate-tempest-devstack-vm-full/dced339/logs/screen-n-cpu.txt.gz#_2013-09-24_04_44_31_806
Logs are located here:
http://logs.openstack.org/64/47264/2/gate/gate-tempest-devstack-vm-full/dced339
-----------------
Originally the failure was (before some changes to timeouts in tempest):
t178.1: tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern[compute,image,volume]_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
2013-09-16 15:59:44,214 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:44,417 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:45,348 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:45,495 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:47,644 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:48,762 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:49,879 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:50,980 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:00:52,581 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:00:52,897 Authentication (publickey) successful!
2013-09-16 16:00:53,105 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:00:53,428 Authentication (publickey) successful!
2013-09-16 16:00:53,431 Secsh channel 1 opened.
2013-09-16 16:00:53,607 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:00:53,875 Authentication (publickey) successful!
2013-09-16 16:00:53,880 Secsh channel 1 opened.
2013-09-16 16:01:58,999 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:01:59,288 Authentication (publickey) successful!
2013-09-16 16:01:59,457 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:01:59,784 Authentication (publickey) successful!
2013-09-16 16:01:59,801 Secsh channel 1 opened.
2013-09-16 16:02:00,005 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:00,080 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:01,127 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:01,192 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:01,414 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:02,494 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:03,615 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:04,724 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:05,825 Starting new HTTP connection (1): 127.0.0.1
}}}
Traceback (most recent call last):
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 157, in test_volume_boot_pattern
    ssh_client = self._ssh_to_server(instance_from_snapshot, keypair)
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 101, in _ssh_to_server
    private_key=keypair.private_key)
  File ""tempest/scenario/manager.py"", line 453, in get_remote_client
    return RemoteClient(ip, username, pkey=private_key)
  File ""tempest/common/utils/linux/remote_client.py"", line 47, in __init__
    if not self.ssh_client.test_connection_auth():
  File ""tempest/common/ssh.py"", line 148, in test_connection_auth
    connection = self._get_ssh_connection()
  File ""tempest/common/ssh.py"", line 70, in _get_ssh_connection
    time.sleep(bsleep)
  File ""/usr/local/lib/python2.7/dist-packages/fixtures/_fixtures/timeout.py"", line 52, in signal_handler
    raise TimeoutException()
TimeoutException
Full logs here:
http://logs.openstack.org/80/43280/9/gate/gate-tempest-devstack-vm-full/cba22ae/testr_results.html.gz
Looks a bit like the instance failed to boot."
1226348,1226348,Nova,ff693722639387a7a3ea62e291a8306569392ab9,1,1, ,pci passthrough scheduler hasattr dones not work f...,"2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     legacy_bdm_in_spec)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 87, in schedule_run_instance
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     filter_properties, instance_uuids)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 336, in _schedule
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     filter_properties, index=num)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/host_manager.py"", line 397, in get_filtered_hosts
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     hosts, filter_properties, index)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/filters.py"", line 82, in get_filtered_objects
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     list_objs = list(objs)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/filters.py"", line 43, in filter_all
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     if self._filter_one(obj, filter_properties):
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filters/__init__.py"", line 27, in _filter_one
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     return self.host_passes(obj, filter_properties)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filters/pci_passthrough_filter.py"", line 41, in host_passes
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     return host_state.pci_stats.support_requests(
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp AttributeError: 'NoneType' object has no attribute 'support_requests'
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp"
1226366,1226366,neutron,96e0eb23a458f068f540d98132ee261053c45585,1,1,,Raise an exception if starting neutron-l3-agent wi...,"Version : Havana
OS : RHEL
Running Neutron services  : neutron-server, neutron-openvswitch-agent, neutron-l3-agent
No routers being created, then the following exception will be raised.
2013-09-16 08:51:32.820 23737 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     **args)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/common/rpc.py"", line 44, in dispatch
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/db/l3_rpc_base.py"", line 54, in sync_routers
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     l3plugin.auto_schedule_routers(context, host, router_ids)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/db/l3_agentschedulers_db.py"", line 241, in auto_schedule_routers
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     self, context, host, router_ids)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     context.session.add(binding)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 449, in __exit__
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     self.commit()
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 361, in commit
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     self._prepare_impl()
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 340, in _prepare_impl
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     self.session.flush()
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     raise exception.DBError(e)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_1` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('28570d41-e7ab-4ca5-bed5-922bfbc154fc', '', '0214005e-8876-4bb1-a3b5-68f629b6e987')
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp
The root cause is that the router_ids transferred is defined as [''], which is not None, but have an invalid empty value, the DB can't take this value in."
1226400,1226400,neutron,20b97388efcc3540066ca7743c761c1ec899dd2c,1,0,“The python neutron client for the V2 API expects the neutron API”,Not all exception information is propagated throug...,"The python neutron client for the V2 API expects the neutron API to send information back such as the type and detail of the exception the body of the message (serialized as a dict). See exception_handler_v20 in client.py. However, the neutron v2 api only returns the exception message. This means that the client can only propagate up a generic NeutronClientException exceptions rather than more specific ones related to the actual problem.
All of the necessary information is present in the v2 api resource class at the point the webob exception is raised to include at least the type of the exception (detail appears not to be used in neutron exceptions) so the format of the message returned should be changed to include a type field and a message field rather than just the message."
1226442,1226442,cinder,a8fcfdbe8b2f46a100f986eeef5c7d43aef69bb7,1,1,“information in 'src_id' is lost from the exception message.”,Argument of exception.VolumeNotFound is incorrect ...,"cinder/volume/drivers/storwize_svc.py is used as follows.
  raise exception.VolumeNotFound(exception_msg,
                                 volume_id=src_id)
If 'exception_msg' is used, 'volume_id=src_id' is ignored.
Likewise,
  raise exception.SnapshotNotFound(exception_msg,
                                   snapshot_id=src_id)
If 'exception_msg' is used, 'snapshot_id=src_id' is ignored.
Therefore, information in 'src_id' is lost from the exception message."
1226803,1226803,neutron,37b78c5d48a9a5da1e9b0187b2004d5352e7c877,1,1, ,bigswitch plugin doesn't pass context to all netwo...,There are several places in the bigswitch plugin where the db context is not correctly passed down to update network calls so the required data isn't available in the database during certain operations (e.g. floating IP update).
1226814,1226814,neutron,276a3b422d54b6976fed8c23ed54bd9bb707e4ff,0,0,Feature or bug3 “Allows 'external' keyword in addition to 'any' keyword”,Bug #1226814 “BigSwitch plugin,"In this BigSwitch plugin router rules support, the plugin only accepts the 'any' keyword. However, it should accept both the 'external' keyword as well, which looks the same to neutron (0.0.0.0/0) but has a different meaning on the backend controller."
1226826,1226826,nova,4845d08967cdff22d53d55467245402e10ae9814,1,1,"“The VMwareVCDriver method ""detach_volume"" is missing the “encryption""""",Bug #1226826 “VMwareVCDriver,"When VMwareVCDriver is being used, detaching a volume will fail. The following error shows up in the n-cpu log:
Traceback (most recent call last):
 File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
   **args)
 File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
   result = getattr(proxyobj, method)(ctxt, **kwargs)
 File ""/opt/stack/nova/nova/exception.py"", line 89, in wrapped
   payload)
 File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
   return f(self, context, *args, **kw)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 244, in decorated_function
   pass
 File ""/opt/stack/nova/nova/compute/manager.py"", line 230, in decorated_function
   return function(self, context, *args, **kwargs)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function
   e, sys.exc_info())
 File ""/opt/stack/nova/nova/compute/manager.py"", line 259, in decorated_function
   return function(self, context, *args, **kwargs)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 3720, in detach_volume
   self._detach_volume(context, instance, bdm)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 3692, in _detach_volume
   self.volume_api.roll_detaching(context, volume_id)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 3685, in _detach_volume
   encryption=encryption)
ypeError: detach_volume() got an unexpected keyword argument 'encryption'
Full log of detach operation here:
http://paste.openstack.org/show/47179/
The solution is to update the VMwareVCDriver method ""detach_volume"" to take an ""encryption"" argument."
1226830,1226830,cinder,587a7e35aa7f0c1cccfa366b795424ba55808e1a,1,1,“Wrong arguments orders in”,Bug in __init__ of cinder.brick.initiator.connecto...,"Wrong arguments orders in super call of cinder.brick.initiator.connector.RemoteFSConnector
class InitiatorConnector(executor.Executor):
    def __init__(self, root_helper, driver=None,
                 execute=putils.execute, *args, **kwargs):
        super(InitiatorConnector, self).__init__(root_helper, execute,
                                                 *args, **kwargs)
        if not driver:
            driver = host_driver.HostDriver()
        self.set_driver(driver)
class RemoteFsConnector(InitiatorConnector):
    """"""Connector class to attach/detach NFS and GlusterFS volumes.""""""
    def __init__(self, mount_type, root_helper, driver=None,
                 execute=putils.execute, *args, **kwargs):
        self._remotefsclient = remotefs.RemoteFsClient(mount_type,
                                                       execute, root_helper)
        super(RemoteFsConnector, self).__init__(driver, execute, root_helper,
                                                *args, **kwargs)
driver and execute arguments should be after root_helper"
1226959,1226959,cinder,3cc149a802346bef063bcb865182e4e21ec8b086,1,1, ,Argument of exception.GlanceConnectionFailed is in...,"cinder/cinder/image/glance.py is used as follows.
  raise exception.GlanceConnectionFailed(netloc=netloc,
                                         reason=str(e))
'netloc' is always ignored."
1226965,1226965,nova,6dc290a88a3240b9f454d5140ab14d499d9ab1b2,1,1, ,"nova sync_power_states broken when db is stopped, ...","the Nova has the logic to synchronize the state of virtual machine with the record in database, when the state is stopped but
the virtual machine is in running state, it will try to stop the virtual machine with compute api call. But the compute api
call has the check that only allow to execute the call when VM in ACTIVE,RESCUED, ERROR state.  So the sync logic is broken here.
option 1:
   allow to run stop API when VM in stopped state
option 2:
   add another method in API  such as force_stop , and sync_power_states will use this api to stop the VM.
option 3:
    sync_power_states to call the rpcapi to stop the VM"
1227027,1227027,nova,8a34fc3d48c467aa196f65eed444ccdc7c02f19f,1,1,“In the following commit:“,[OSSA 2014-001] Insecure directory permissions wit...,"In the following commit:
commit 46de2d1e2d0abd6fdcd4da13facaf3225c721f5e
Author: Rafi Khardalian <email address hidden>
Date:   Sat Jan 26 09:02:19 2013 +0000
    Libvirt: Add support for live snapshots
    blueprint libvirt-live-snapshots
There was the following chunk of code
         snapshot_directory = CONF.libvirt_snapshots_directory
         fileutils.ensure_tree(snapshot_directory)
         with utils.tempdir(dir=snapshot_directory) as tmpdir:
             try:
                 out_path = os.path.join(tmpdir, snapshot_name)
-                snapshot.extract(out_path, image_format)
+                if live_snapshot:
+                    # NOTE (rmk): libvirt needs to be able to write to the
+                    #             temp directory, which is owned nova.
+                    utils.execute('chmod', '777', tmpdir, run_as_root=True)
+                    self._live_snapshot(virt_dom, disk_path, out_path,
+                                        image_format)
+                else:
+                    snapshot.extract(out_path, image_format)
Making the temporary directory 777 does indeed give QEMU and libvirt permission to write there, because it gives every user on the whole system permission to write there. Yes, the directory name is unpredictable since it uses 'tempdir', this does not eliminate the security risk of making it world writable though.
This flaw is highlighted by the following public commit which makes the mode configurable, but still defaults to insecure 777.
https://review.openstack.org/#/c/46645/"
1227079,1227079,neutron,c3b4f24883c04cd139292ecf9cee31cff43e3973,0,0,Bug in test,"Midonet plugin, add multiple subnets tests",The plugin is skipping the unit tests for multiple subnets
1227366,1227366,cinder,cbb057169094e9d633162cc5645fa19a82325d49,1,1,,GPFS Driver does not  limit clone depth for snapsh...,GPFS Driver does not  limit clone depth for snapshots - there is a potential for snapshot clones to grow without bound regardless of the setting of gpfs_max_clone_depth config flag.
1227477,1227477,cinder,61135b6a5f1724888e92471b68f91e4a825bf4f0,0,0,Refactoring “remove VolumeNotFoundForInstance class”,VolumeNotFoundForInstance class is used only in te...,"If we modify test_xiv_ds8k.py properly,
VolumeNotFoundForInstance class can be removed."
1227655,1227655,nova,de648bb7bfa9985ab67c0754c25d35cd71f6eb81,0,0, Bug in test,nova.tests.objects.test_quotas fails H233 hacking ...,"I hit this locally, not sure how it got through the gate:
pep8 runtests: commands[0] | flake8
./nova/tests/objects/test_quotas.py:49:9: H233  Python 3.x incompatible use of print operator"
1227971,1227971,neutron,396a0e42755af96d8b96d42ae1bcf4ae4604443d,1,1, ,Incorrect usage of _fields method in provider_conf...,"When fields filtering is applied, neutron server prints a trace:
 ERROR neutron.api.v2.resource [-] index failed
 TRACE neutron.api.v2.resource Traceback (most recent call last):
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
 TRACE neutron.api.v2.resource     result = method(request=request, **args)
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 273, in index
 TRACE neutron.api.v2.resource     return self._items(request, True, parent_id)
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 227, in _items
 TRACE neutron.api.v2.resource     obj_list = obj_getter(request.context, **kwargs)
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/servicetype_db.py"", line 63, in get_service_providers
 TRACE neutron.api.v2.resource     return self.conf.get_service_providers(filters, fields)
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/services/provider_configuration.py"", line 162, in get_service_providers
 TRACE neutron.api.v2.resource     return self._fields(res, fields)
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/services/provider_configuration.py"", line 151, in _fields
 TRACE neutron.api.v2.resource     return dict(((key, item) for key, item in resource.items()
 TRACE neutron.api.v2.resource AttributeError: 'list' object has no attribute 'items'"
1228008,1228008,neutron,77aee9e71523c543286d05a2f553d4a30778d17c,0,0,Bug or feature3 “should support update with value None”,extra_dhcp_opt extension should support update wit...,"we should support update extra_dhcp_opt with value None, if opt_value is None, then delete extra_dhcp_opt on that port"
1228212,1228212,neutron,d4d06dfd18d8d085ccd390d6ebae7e3808fa8d09,0,0,Bug in test,TestNecPortBindingValidatePortInfo unit tests fail...,"These two tests in neutron trunk are failing on my 64-bit xubuntu 12.04 using python 2.7:
http://paste.openstack.org/show/47323/
When I run the NECPluginV2._validate_portinfo method in python, I get this:
mriedem@ubuntu:~$ python
Python 2.7.3 (default, Aug  1 2012, 05:16:07)
[GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> datapath_id = '0x1234567890abcdef'
>>> dpid = int(datapath_id, 16)
>>> dpid
1311768467294899695L
>>> dpid > 0xffffffffffffffffL
False
>>> hex(dpid)
'0x1234567890abcdefL'
>>>
So the int(datapath_id, 16) conversion is returning it as a longinteger, so the 'L' is appended at the end.
http://docs.python.org/2/library/functions.html#int
""If the argument is outside the integer range, the function returns a long object instead.""
http://docs.python.org/2/reference/lexical_analysis.html#integers
I'm able to fix this in the tests by simply using assertIn rather than assertEqual:
self.assertIn('0x1234567890abcdef', portinfo['datapath_id'])
I'm not sure if this is a bug in the tests though or something that should be handled in the _validate_portinfo method, i.e. if you get past all of the current validation OK, then simply return the input argument in the dict that goes back."
1228442,1228442,neutron,78aa12567d226fe73df483d75e0d1980e6d3625e,1,1, ,FWaaS plugin should allow creating only one firewa...,"The reference implementation of the FWaaS iptables agent/driver supports only one firewall per tenant in Havana release. However, the FWaaS plugin will let you create more than one firewall. This should not be allowed since it creates an inconsistent state with only the most recent policy being applied."
1228449,1228449,nova,204d3ebec8522cfd01f260720c56e3cd879948cf,1,1, ,i18n issues for nova/virt/baremetal/virtual_power_...,"In nova source code, some LOG messages in  nova/virt/baremetal/virtual_power_driver.py are not wrapped with _() which will causei18n problems, we need fix this small but important issue, to make our codes professional."
1228827,1228827,neutron,793c76344715790531d39436508b188b536e2ff5,1,1, ,add request timeout handling for Mellanox Neutron ...,"Mellanox Neutron Agent communicates with eswitchd to execute vports discovery and configuration.
In case eswitchd is not reachable, request timeout is received. The current handling of the timeout was to mark out of sync flag, flush the agent cache and try again. This behavior leads to miss the removed vports and their configuration is not removed. The request timeout was received in case group of VMs was removed at once. The solution is to add number of retries with increasing waiting time in case  request timeout is received. If request timeout persists, exit the agent process."
1228847,1228847,nova,be5d39eacc84c11f96357a624ffa1cdeda8a568d,1,1,,Bug #1228847 “VMware,"When an exception occurs in the VMWare driver, for example when there are no more IP addresses available, then the following exception is returned:
2013-09-22 05:26:22.522 ERROR nova.compute.manager [req-b29710eb-5cb9-4de1-adca-919119b10460 demo demo] [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] Error: Exception in __deepcopy__ Method not found: 'VimService.VimPort.__deepcopy__'
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] Traceback (most recent call last):
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1038, in _build_instance
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     set_access_ip=set_access_ip)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/compute/claims.py"", line 53, in __exit__
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     self.abort()
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/compute/claims.py"", line 107, in abort
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     LOG.debug(_(""Aborting claim: %s"") % self, instance=self.instance)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/openstack/common/gettextutils.py"", line 228, in __mod__
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     return copied._save_parameters(other)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/openstack/common/gettextutils.py"", line 186, in _save_parameters
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     self.params = copy.deepcopy(other)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 285, in _deepcopy_inst
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     return x.__deepcopy__(memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 195, in vim_request_handler
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     _(""Exception in %s "") % (attr_name), excep)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] VimException: Exception in __deepcopy__ Method not found: 'VimService.VimPort.__deepcopy__'
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]
2013-09-22 05:37:01.926 DEBUG nova.virt.vmwareapi.driver [req-8c58e23f-3970-4c4c-bfbf-39060c0da3ba demo demo] 'VMwareAPISession' object has no attribute 'vim' from (pid=2206) __del__ /opt/stack/nova/nova/virt/vmwareapi/driver.py:705"
1229082,1229082,neutron,f1d91b2456353b92bda7017422aaa97d6720d7a9,1,1,“Recently merged patch - https://review.openstack.org/#/c/42090 - adds an ability to update status of a member according to health statistics”,LBaaS inactive members are not passed to agent on ...,"Recently merged patch - https://review.openstack.org/#/c/42090 - adds an ability to update status of a member according to health statistics coming from lbaas agent so that if a member stops responding it is marked as INACTIVE. Such members however are not passed to the agent on next reload_pool operation (pool update, new member, new monitor, etc) so remain INACTIVE even if corresponding instance is repaired."
1229217,1229217,neutron,684c9b0b10535eb91b2f679de82c460a864f2cc0,1,0,Feature or bug3 “The Cisco Nexus plugin currently has no support for migration”,Cisco plugin migration support,"The Cisco Nexus plugin currently has no support for migration events via update_port when an instance is migrated from Nova. As a result the plugin currently does not do anything when an instance is migrated.
Portbinding support has been added to the Cisco plugin via this bug: https://bugs.launchpad.net/neutron/+bug/1218033
The plugin should be able to detect changes in the binding:host_id and unconfigure/reconfigure ports on the Nexus appropriately."
1229366,1229366,glance,4a83dbb9e6ef9cc69176d8d96de6993ee4b45f77,1,1, ,incorrect response code from v2 delete image membe...,"Currently on success, DELETE /v2/image/{imageId}/members/{memberId} returns 200 with no response body.
According to the Glance API contract, a 200 means you should expect a response body.  Success with no response body should be 204."
1229508,1229508,neutron,752b7ab29ddc75d8ae54c52d824ecc94a9551b56,1,0,"“From the initial commit of NEC plugin, network_id of packetfilters table is nullable=False [1], but in folsom_initial db migration script nullable is set to True.”",Bug #1229508 “nec plugin,"From the initial commit of NEC plugin, network_id of packetfilters table is nullable=False [1],
but in folsom_initial db migration script nullable is set to True.
nullable=False is just a more strict constraint than nullable=True,
so NEC plugin works but it is better to be fixed in the migration.
I will add the migration to network_id to nullable=False both upgrade and downgrade
to make sure nullable=False in any revision.
[1] https://github.com/openstack/neutron/blob/stable/folsom/quantum/plugins/nec/db/models.py#L53"
1229548,1229548,neutron,5dcbddf516750e21359c859f8da0ab829f67d3f9,1,1, ,Advanced service router becomes destination of a f...,"If we initiate a connection to a floating ip right after creating/associating it with a VM's port, the connection goes to advanced service router for a short period if time, instead to the VM the floating ip is supposed to be associated with.
The root cause is when creating/associating a floating ip using advanced service router, in addition to create a DNAT rule, we also need to configure the floating ip on advanced service router's vNic so the advanced service router can reply ARP request for that IP. We configure the IP on the vnic before the DNAT rule is configured, therefore, there is a small window, after IP is configured but before DNAT is configured, that the traffic to floating IP will reach advanced service router.
The fix is to configure the DNAT rule before applying the IP on advanced service router's vNic."
1229625,1229625,nova,9c044d2c94812e18cf84927fbf719cd073fe6c4f,1,1, ,"Bug #1229625 "" hairpin mode on vnet bridge ports causes false po... ","This is bug 1011134 again happening in a cloud that does not have the ipv6 flag set,
so the previous patch from https://review.openstack.org/14017
is not used.
Guest VMs will try to configure IPv6 link-local addrs even without the outer parts supporting it
and can throw errors when they see inbound packets with their own MAC address.
Note: I think, this bug can not be unit-tested as it requires a complex setup including running a VM in a cloud."
1229655,1229655,neutron,8a37d04bfda7a583d02a3fac262f4f9242d7ed76,1,1,"“dns nameserver information  was not being passed down to the midonet client API.""",host routes and dns name server not set in midonet...,"When creating a subnet in the Midonet plugin, the dns name servers and host routes are not set."
1229753,1229753,nova,016e39734e1b70c1b004f1ae366c22b091463932,1,1,"“Using blame, here is at least one patch that added code which is causing one of the failures but it wasn't caught in the gate:”",Several flake8 errors in new xenserver code,"I don't know why the gate isn't catching these, but I got the latest code this morning and ran a clean tox pep8 which resulted in these failures:
http://paste.openstack.org/show/47427/
Using blame, here is at least one patch that added code which is causing one of the failures but it wasn't caught in the gate:
https://review.openstack.org/#/c/47901/"
1229759,1229759,cinder,b30402eeeddefb855126a32fee15db164684ba8c,1,0,"“It's necessary to support XenServer, Windows, VMware ESX, etc.”",Huawei driver can not attach volume to Windows OS,"Huawei drivers create Linux hosts by default when attaching volumes. It's necessary to support XenServer, Windows, VMware ESX, etc."
1229867,1229867,cinder,f4dc301efda13a6a1769b071b1955a4869110b98,1,1,"“This reverts commit d5cd652.""",Bug #1229867 “TypeError,"Today... after d5cd6528f361979b073aabd036be0d28dc1c4b95 landed I'm now seeing the following exceptions in /var/log/cinder/scheduler.log:
2013-09-24 17:29:00.771 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 1 seconds
2013-09-24 17:29:01.777 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 2 seconds
2013-09-24 17:29:03.781 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 4 seconds
2013-09-24 17:29:07.787 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 8 seconds
2013-09-24 17:31:47.402 10117 ERROR cinder.volume.flows.create_volume [req-fe4faf9f-c837-4263-b1f6-ed0e85bc1366 fc85ee7925894313aa61221957a7b6ce 5c97c1ad1ea24177936ea41043b403a5] Failed to schedule_create_volume: create_volume() takes at least 6 arguments (7 given)
2013-09-24 17:31:47.428 10117 WARNING cinder.taskflow.utils [-] Activating 3 rollbacks due to <cinder.taskflow.utils.FlowFailure object at 0x3102290>.
2013-09-24 17:31:47.429 10117 ERROR cinder.openstack.common.rpc.amqp [req-fe4faf9f-c837-4263-b1f6-ed0e85bc1366 fc85ee7925894313aa61221957a7b6ce 5c97c1ad1ea24177936ea41043b403a5] Exception during message handling
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/scheduler/manager.py"", line 94, in create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     flow.run(context)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 105, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(self, *args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 232, in run
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     run_it(r)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 212, in run_it
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     self.rollback(context, cause)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 172, in run_it
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     result = runner(context, *args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/utils.py"", line 260, in __call__
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     self.result = self.task(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 148, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 264, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 199, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 234, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 177, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1678, in schedule_create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     _error_out_volume(context, db, volume_id, reason=e)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1663, in schedule_create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     filter_properties)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/scheduler/chance.py"", line 81, in schedule_create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     image_id=image_id)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp TypeError: create_volume() takes at least 6 arguments (7 given)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp"
1229954,1229954,neutron,893e10b0add63b364eb6d92715967c45028fa692,1,1,"“Below command can't work because list element ""allowed_address_pairs"" hasn't been updated into the varaiable “attr.PLURALS""""",updating address pairs with xml doesn't work,"Works fine: neutron port-update 493199d2-d792-4017-b015-894e85a8e32a --allowed-address-pairs list=true type=dict ip_address=10.0.0.1
Generates:
    [{u'ip_address': u'10.0.0.1'}]
neutron port-update 493199d2-d792-4017-b015-894e85a8e32a --allowed-address-pairs list=true type=dict ip_address=10.0.0.1  --request-format xml
Generates:
    {'allowed_address_pair': {'ip_address': '10.0.0.1'}}
2013-09-24 14:10:34.021 19471 ERROR neutron.api.v2.resource [-] update failed
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 460, in update
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource     allow_bulk=self._allow_bulk)
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 589, in prepare_request_body
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource     attr_vals['validate'][rule])
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/extensions/allowedaddresspairs.py"", line 53, in _validate_allowed_address_pairs
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource     raise AllowedAddressPairsMissingIP()
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource AllowedAddressPairsMissingIP: AllowedAddressPair must contain ip_address
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource"
1229994,1229994,nova,7910385825ccfa705785af360fcd5717656e3557,1,1, ,Bug #1229994 “VMwareVCDriver,"Image snapshot through the VC cluster driver may fail if, within the datacenter containing the cluster managed by the driver, there are one or more hosts in maintenance mode with access to the datastore containing the disk image snapshot.
A sign that this situation has occurred is the appearance in the nova compute log of an error similar to the following:
2013-08-02 07:10:30.036 WARNING nova.virt.vmwareapi.driver [-] Task [DeleteVirtualDisk_Task] (returnval){
value = ""task-228""
_type = ""Task""
} status: error The operation is not allowed in the current state.
What this means is that even if all hosts in cluster are running fine in normal mode, a host outside of the cluster going into maintenance mode may
lead to snapshot failure.
The root cause of the problem is due to an issue in VC's handler of the VirtualDiskManager.DeleteVirtualDisk_Task API, which may incorrectly pick a host in maintenance mode to service the disk deletion even though such an operation will be rejected by the host under maintenance."
1230069,1230069,cinder,d534811cbcfe591688b5e9c6becbed2224be3c31,1,1,“This patch fixes the calls to be according to the function definition.”,Passing mandatory args as optional to create_volum...,"Filter scheduler and create_volume flow both pass request_specs and filter_properties as optional arguments to create_volume in volume_rpcapi.  This does not fit how the call is defined.
Also, the create_volume call in the migration code in manager.py should not allow rescheduling."
1230083,1230083,neutron,31def3ccf425a8f1a617a674a30d29f89d733432,0,0,Bug in test files,OVS sg_rpc test does not load OVS plugin,"When I ran
    OS_DEBUG=1 python setup.py testr --testr-args='neutron.tests.unit.openvswitch.test_ovs_security_group'
I got the following result in testrepository log: http://paste.openstack.org/show/47468/
According to the log, neutron.tests.unit.test_extension_security_group.SecurityGroupTestPlugin is loaded instead of OVS plugin. This means OVS plugin is not well tested and DB plugin with security group is tested again.
This comes from the behavior of Python multiple inheritance.
If a class (C1) inherits multiple classes (P1, P2) which have a common parent class (GP),
for example, C1.setUp() calls super(), P1.setUp is called first, then P2.setUp is called and finally GP.setUp is called.
The behavior is tricky and such usage should be avoided.
In Neutron tests multiple inheritance is used in many places.
I am afraid OVS plugin security group rcp test case is just one of them."
1230102,1230102,nova,3dad53ce1d0263786c3f9ff585dc446a0a9dbecf,1,1," “Remove REGEXP_LIKE operator for oracle db backend since it's not suitable,”","For oracle database, the usage of REGEXP is not co...","The REGEXP_LIKE operator for oracle db backend is not correct.
For the MySql database, the following SQL syntax is right:
---
select xxx from xxx where column REGEXP pattern
---
But for Oracle database, the following SQL syntax is not right:
---
select xxx from xxx where column REGEXP_LIKE pattern
---
It should be:
---
select xxx from xxx where REGEXP_LIKE (column, pattern)
---"
1230127,1230127,glance,0b246bb5f561d694a4fa4f41c90811a64c05b9fb,0,0,Test files,Use self.assertTrue instead of self.assertEqual(.....,"$ find . -name '*.*' | xargs grep self.assertEqual | grep True
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(response['x-image-meta-is_public'], 'True')
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
...
./tests/unit/test_swift_store.py:        self.assertEqual(connection.insecure, True)
./tests/unit/test_swift_store.py:        self.assertEquals(connection.snet, True)
./tests/unit/test_swift_store.py:        self.assertEquals(connection.snet, True)
$ find . -name '*.*' | xargs grep self.assertTrue
./tests/unit/test_context_middleware.py:        self.assertTrue(req.context.is_admin)
./tests/unit/test_context_middleware.py:        self.assertTrue(req.context.is_admin)
...
./tests/unit/test_context_middleware.py:        self.assertTrue(req.context.is_admin)
./tests/unit/test_context_middleware.py:        self.assertTrue(req.context.read_only)
./tests/unit/test_context_middleware.py:        self.assertTrue(req.context.is_admin)
The tests assertions need to be consistent: use assertTrue/assertFalse instead of assertEqual to test boolean values."
1230184,1230184,neutron,92b4c34e05770e8b3c31459158a018bdd5449c3e,1,1,“Missing allowed addr pairs support in NEC plugin.”,allowed addr pairs support in nec plugin,"Missing allowed addr pairs support in NEC plugin.
It is needed to support VRRP. I hope it is a part of the release while I understand it is a bit late to the race.
It took some time to investigate the failure in XML security group RPC test (bug 1229954) and bug 1230083 that OVS plugin is not loaded properly."
1230282,1230282,nova,fe513ec3707afd529270f0c4eef7c468ab2c6b91,1,1, ,Incorrect exception raised during evacuate instanc...,"An incorrect exception, which looks to contradict the corresponding log message logged
at this scenario seems to be raised in the evacuate instance API.
Evacuate instance needs to check that the compute service is actually down to perform evacuation of an instance,
but the exception raised when this check fails is incorrect:
https://github.com/openstack/nova/blob/master/nova/compute/api.py#L2907
    if self.servicegroup_api.service_is_up(service):
       ....
logs the message ""Instance compute service state on <compute_host> expected to be down, but it was up.""
if the check passes, but raises the exception exception.ComputeServiceUnavailable(Instance compute service state on <compute_host> expected to be down, but it was up.)
The exception raised and the message logged are contradictory."
1230296,1230296,cinder,3be220539a529572f30f3345fe80f2aa572730a3,1,0,“errors occur when attaching volumes with Huawei HVS iSCSI driver.”,HTTPS error occurs when attaching a volume with Hu...,"HTTPS errors occur when attaching volumes with Huawei HVS iSCSI driver.
If the iSCSI initiator is not added to a host, the error will occurs.
But if the iSCSI initiator was added to a host already, no error occurs when attaching volumes to this host."
1230323,1230323,neutron,fbc6b991a79a147bf1411b643f5c304062e5956a,1,1, ,Race condition with multiple neutron-servers can a...,"In an environment with multiple neutron-servers, I have observed that a router can get scheduled to an l3-agent more than once.  A ""neutron l3-agent-list-hosting-router <router id>""  will show the router scheduled twice to the same l3-agent or perhaps to two different agents.  This can be reproduced using devstack.  A second neutron-server on another host has to be configured.  Executing a script against each of the neutron-servers which adds (neutron l3-agent-router-add)  and removes (neutron l3-agent-router-remove) a router from an l3 agent is the quickest way to reproduce the race condition.  There is no locking or other coordination across multiple neutron-server processes to prevent this."
1231215,1231215,nova,f05a0c486d8478cdea3808108b172b2487b38bf4,1,0,“we should keep consistency of these two places.”,to_xml should be used in post_live_migration_at_de...,"in the post_live_migration_at_destination method, the to_xml method is called but the return value never be used, and the comment say that the reason is 'the uuid is not included in to_xml() result'.
but I think the uuid is already  included in to_xml() result now, so we may refactor post_live_migration_at_destination method and change it to use the correct way to get the xml of migrated instance.
# In case of block migration, destination does not have
# libvirt.xml
disk_info = blockinfo.get_disk_info(CONF.libvirt_type,
                                    instance)
self.to_xml(instance, network_info, disk_info,
            block_device_info, write_to_disk=True)
# libvirt.xml should be made by to_xml(), but libvirt
# does not accept to_xml() result, since uuid is not
# included in to_xml() result.
dom = self._lookup_by_name(instance[""name""])
self._conn.defineXML(dom.XMLDesc(0))"
1231770,1231770,neutron,1eb8fa563d13e46f0a7011143f65e371ae091684,0,0," ""In order to support NVP advanced lbaas/firewall UI, it is needed to”",Edge LbaaS/Firewall should return router_id when g...,"In order to support NVP advanced lbaas/firewall UI, it is needed to return the router_id attribute when getting vip/firewall.
Also, it is needed to refactor duplicate code for delete_vip/delete_pool/delete_health_monitor"
1231913,1231913,neutron,f4b78c7f17e29448ed54b136eeb4ac700b324120,1,1, ,Floating IP is not removed when VM is terminated i...,"In Midonet plugin, when a VM terminates, floating IP does not get diassociated properly.  This left the floating IP associated with a terminated instance and required you to explicitly diassociate in a separate step.  This also left MidoNet resources dangling around when the VM terminates."
1231914,1231914,neutron,3568a9cac73a2da19e86d82f561be10ae9dbe9a0,1,1, ,"Midonet plug, add dhcp opt121 route when a new sub...",When a subnet is created (except the first one) another ip is added to the dhcp port. Midonet plugin should react and add the correct route opt121
1231915,1231915,neutron,338569c7c30509bce8565272023dfd791f4e9609,1,1, ,session_persistence attr of VIP is not returned wh...,"During the investigation of bug 1231704,
I noticed it is valid that there is no session persistence attribute
and it means no session persistence mechanism is used in load balancing.
It seems an intentional behavior according to the code:
https://github.com/openstack/neutron/blob/master/neutron/db/loadbalancer/loadbalancer_db.py#L241
IMHO, it is confusing that it is determined whether session_persisntece is returned or not by session persistence mode. I would suggest that returning ""session_persistence: null"" (or {}) when no session_persistence is used.
In addition, if we send {'session_persistence': null} in lb-vip-update, session_persistence is cleared.
It looks reasonable to me that ""session_persistence = null"" means no session_persistence is used.
From the point of view of API, I believe all defined attributes in an extension should be returned in a response.
I would like to ask opinions from the community."
1232172,1232172,cinder,5eddb2e2245e9c42f458de201ade166b727ee8f4,0,0,feature. “Add the API implementation of extend_volume for the VMware vmdk driver.”,Bug #1232172 “vmware,"the VMwareVcVmdkDriver currently does not support extending volumes. When executing:
     cinder extend 666a2dae-db61-43bd-9e37-de1c2695c998 5
The following error is seen in the screen-c-vol.log:
     Traceback (most recent call last):
       File ""/opt/stack/cinder/cinder/volume/manager.py"", line 845, in extend_volume
         self.driver.extend_volume(volume, new_size)
       File ""/opt/stack/cinder/cinder/volume/driver.py"", line 430, in extend_volume
         raise NotImplementedError(msg)
     NotImplementedError: Extend volume not implemented"
1232177,1232177,cinder,b715701ff4387d55e7eb4c301a0edea92fa4e0e8,1,1, ,volume manager fails in init_host on delete_volume...,"cinder.volume.manager.init_host attempts to delete volumes that have status of ""deleting"".   But set_initialized is not called until after this step.
And manager.delete_volume has new decorator @utils.require_driver_initialized.
So the call to delete_volume from init_host will always fail because the driver is not yet marked as initialized.
Proposed solution:  move set_initailized up after the call to check_for_setup_error."
1232179,1232179,nova,f2f58eef93aec45e46a9a2ab06fc8b00a9420350,1,0,“based on evidence it looks like caller passing information with key metadata (change back in August3)”,Aggregate metadata is not correctly handled by com...,"Hopefully this is not covered under a different bug.  What I'm seeing is that compute does not properly handle metadata in the ""aggregate"" class of API commands.  Specifically, after adding metadata and performing an add-host you get the following stacktrace exception:
2013-09-27 17:48:44.396 DEBUG nova.openstack.common.rpc.amqp [-] received {u'_context_roles': [u'admin'], u'_context_request_id': u'req-643da000-e5ae-482c-ba01-027ee61f5085', u'_context_quota_class': None, u'_context_user_name': u'admin', u'_context_project_name': u'admin', u'_context_service_catalog': [{u'endpoints_links': [], u'endpoints': [{u'adminURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537', u'region': u'RegionOne', u'publicURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537', u'internalURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537', u'id': u'1733df26b558409ca93bb1ffe8851929'}], u'type': u'volume', u'name': u'cinder'}], u'_context_tenant': u'0fa0013cceb4430ba09e88a18d344537', u'_context_auth_token': '<SANITIZED>', u'args': {u'aggregate': {u'name': u'foo', u'availability_zone': u'nova', u'deleted': False, u'created_at': u'2013-09-27T17:47:24.000000', u'updated_at': None, u'hosts': [u'devstack2'], u'deleted_at': None, u'id': 2, u'metadata': {u'hypervisor': u'true', u'availability_zone': u'nova'}}, u'host': u'devstack2', u'slave_info': None}, u'namespace': None, u'_context_instance_lock_checked': False, u'_context_timestamp': u'2013-09-27T17:48:44.352804', u'_context_is_admin': True, u'version': u'2.14', u'_context_project_id': u'0fa0013cceb4430ba09e88a18d344537', u'_context_user': u'bd2e35f5cdfa4272b7f03d601c95a61d', u'_unique_id': u'13c5f2d81c484bbebdd8214574060ab3', u'_context_read_deleted': u'no', u'_context_user_id': u'bd2e35f5cdfa4272b7f03d601c95a61d', u'method': u'add_aggregate_host', u'_context_remote_address': u'172.24.4.10'} from (pid=10210) _safe_log /opt/stack/nova/nova/openstack/common/rpc/common.py:277
013-09-27 17:48:44.397 DEBUG nova.openstack.common.rpc.amqp [-] unpacked context: {'read_deleted': u'no', 'project_name': u'admin', 'user_id': u'bd2e35f5cdfa4272b7f03d601c95a61d', 'roles': [u'admin'], 'timestamp': u'2013-09-27T17:48:44.352804', 'auth_token': '<SANITIZED>', 'remote_address': u'172.24.4.10', 'quota_class': None, 'is_admin': True, 'user': u'bd2e35f5cdfa4272b7f03d601c95a61d', 'service_catalog': [{u'endpoints': [{u'adminURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537', u'region': u'RegionOne', u'id': u'1733df26b558409ca93bb1ffe8851929', u'internalURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537', u'publicURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537'}], u'endpoints_links': [], u'type': u'volume', u'name': u'cinder'}], 'request_id': u'req-643da000-e5ae-482c-ba01-027ee61f5085', 'instance_lock_checked': False, 'project_id': u'0fa0013cceb4430ba09e88a18d344537', 'user_name': u'admin', 'tenant': u'0fa0013cceb4430ba09e88a18d344537'} from (pid=10210) _safe_log /opt/stack/nova/nova/openstack/common/rpc/common.py:277
2013-09-27 17:48:44.400 ERROR nova.openstack.common.rpc.amqp [req-643da000-e5ae-482c-ba01-027ee61f5085 admin admin] Exception during message handling
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     **args)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     payload)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 4993, in add_aggregate_host
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     slave_info=slave_info)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/virt/xenapi/driver.py"", line 628, in add_to_aggregate
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     return self._pool.add_to_aggregate(context, aggregate, host, **kwargs)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/virt/xenapi/pool.py"", line 77, in add_to_aggregate
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     if not pool_states.is_hv_pool(aggregate['metadetails']):
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp KeyError: 'metadetails'
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp
Based on evidence it looks like caller passing information with key metadata (change back in August?) while we are looking for it in the Xen code as metadetails.  The key name needs to change to correspond.
This was produced on a devstack with a commit of ab5a99bbca4d68002c887b5dd7b3741b57f650ee for nova."
1232287,1232287,cinder,db3088b888e9560df93ff608b1e45a28fac14b6f,1,1,“that bootable attribute is missing from:”,v2 api does not show bootable attr on “cinder  lis...,"I switched to use v2 api and noticed ""cinder list"" does not return value of bootable flag on volumes.
v2 api does show bootable.
I believe the error is that bootable attribute is missing from:
cinder.api.v2.views.volume.ViewBuilder.detail"
1232698,1232698,cinder,36c420157674e8ed34d407f19062b06a657e6490,1,1, ,migrate does not convert string to bool for force_...,Volume migration API does not convert string to bool for the force_host_copy parameter.
1232787,1232787,swift,94090e8760da324d6586b5d45b7c339457e33647,1,1, ,Bulk Delete should use POST not DELETE,"The DELETE verb applies to a single resource, and doesn't define any semantics for the body.
http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html#sec9.7
The swift Bulk Delete command affects multiple resources specified in a DELETE body.
http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.bulk
While Bulk Delete is a welcome operation, its usage of DELETE is unusual: affecting multiple resources and relying on reading content.
More typically, such an operation employs POST (or PUT), which folks including api-craft usually agree is the best ""catch-all"" verb for behaviors such as those affecting multiple resources.  That's the TL;DR; of the thread below.
https://groups.google.com/forum/#!searchin/api-craft/Regarding$20Bulk$20actions/api-craft/wY-W1NdZDRs/7YDwMhCR608J
Note that this topic isn't nasal or abstract.  The current behavior is unsupported using the built-in java http client.  Even if third-party libraries can work around this behavior, it is probably best to not be a snowflake wrt http verb semantics where possible!
http://stackoverflow.com/questions/9100776/http-delete-with-request-body-issues"
1233026,1233026,nova,aa8938d696e6e2ecf23709f748de825dc2d07fd0,1,1, ,exception.InstanceIsLocked is not caught in start ...,"when port nova-v3-test: test_server_actions.ServerActionsV3TestXML.test_lock_unlock_server. We found the exception.InstanceIsLocked is not caught in start and stop server API.
the following is the nova log:
2013-09-30 15:03:29.306 ^[[00;32mDEBUG nova.api.openstack.wsgi [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;32m] ^[[01;35m^[[00;32mAction: 'action', body: <?xml version=""1.0"" encoding=""UTF-8""?>
<stop xmlns=""http://docs.openstack.org/compute/api/v1.1""/>^[[00m ^[[00;33mfrom (pid=23798) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:935^[[00m
2013-09-30 15:03:29.307 ^[[00;32mDEBUG nova.api.openstack.wsgi [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;32m] ^[[01;35m^[[00;32mCalling method <bound method ServersController._stop_server of <nova.api.openstack.compute.plugins.v3.servers.ServersController object at 0x577c250>>^[[00m ^[[00;33mfrom (pid=23798) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:936^[[00m
2013-09-30 15:03:29.339 ^[[00;32mDEBUG nova.api.openstack.compute.plugins.v3.servers [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;32m] ^[[01;35m[instance: cd4fec81-d2e8-43cd-ab5d-47da72dd90fa] ^[[00;32mstop instance^[[00m ^[[00;33mfrom (pid=23798) _stop_server /opt/stack/nova/nova/api/openstack/compute/plugins/v3/servers.py:1372^[[00m
2013-09-30 15:03:29.340 ^[[01;31mERROR nova.api.openstack.extensions [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[01;31m] ^[[01;35m^[[01;31mUnexpected exception in API method^[[00m
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m  File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 469, in wrapped
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m    return f(*args, **kwargs)
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m  File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/servers.py"", line 1374, in _stop_server
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m    self.compute_api.stop(context, instance)
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m  File ""/opt/stack/nova/nova/compute/api.py"", line 198, in wrapped
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m    return func(self, context, target, *args, **kwargs)
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m  File ""/opt/stack/nova/nova/compute/api.py"", line 187, in inner
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m    raise exception.InstanceIsLocked(instance_uuid=instance['uuid'])
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00mInstanceIsLocked: Instance cd4fec81-d2e8-43cd-ab5d-47da72dd90fa is locked
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m
2013-09-30 15:03:29.341 ^[[00;36mINFO nova.api.openstack.wsgi [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;36m] ^[[01;35m^[[00;36mHTTP exception thrown: Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.InstanceIsLocked'>^[[00m"
1233161,1233161,nova,4ffdf53550bd823f18c7a45c094adee18ed320d8,1,1, ,Bug #1233161 “VMWare,"2013-09-30 05:40:58.352 ERROR nova.openstack.common.periodic_task [req-126d36dc-15d2-4525-a2cd-9f4015987140 None None] Error during ComputeManager.update_available_resource: 'NoneType' object is not iterable
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task     task(self, context)
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/compute/manager.py"", line 4861, in update_available_resource
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task     nodenames = set(self.driver.get_available_nodes())
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 599, in get_available_nodes
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task     CONF.vmware.cluster_name)
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/virt/vmwareapi/vm_util.py"", line 1008, in get_all_cluster_refs_by_name
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task     cls_mor = find_entity_mor(cls, entity_path)
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/virt/vmwareapi/vm_util.py"", line 992, in find_entity_mor
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task     return [mor for mor in entity_list if mor.propSet[0].val == entity_name]
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task TypeError: 'NoneType' object is not iterable
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task"
1233188,1233188,nova,7a34be0ec0cd0cb9555fe64ff6c486faae1ae91d,1,1, ,Cant create VM with rbd backend enabled,"nova-compute.log:
2013-09-30 15:52:18.897 12884 ERROR nova.compute.manager [req-d112a8fd-89c4-4b5b-b6c2-1896dcd0e4ab f70773b792354571a10d44260397fde1 b9e4ccd38a794fee82dfb06a52ec3cfd] [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] Error: libvirt_info() takes exactly 6 arguments (7 given)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] Traceback (most recent call last):
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1037, in _build_instance
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     set_access_ip=set_access_ip)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1410, in _spawn
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     LOG.exception(_('Instance failed to spawn'), instance=instance)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1407, in _spawn
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     block_device_info)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2069, in spawn
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     write_to_disk=True)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 3042, in to_xml
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     disk_info, rescue, block_device_info)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2922, in get_guest_config
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     inst_type):
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2699, in get_guest_storage_config
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     inst_type)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2662, in get_guest_disk_config
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     self.get_hypervisor_version())
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] TypeError: libvirt_info() takes exactly 6 arguments (7 given)"
1233271,1233271,neutron,5652e20b0bdc3a707d11d2b9ee521066b8f1150f,1,1, ,Floating IPs momentarily removed from router on ag...,"I noticed that floating IPs would be removed momentarily from the router on agent restart.  This would cause a momentarily outage in network connectivity.  This should be avoided.
I noticed this when testing with this change:  https://review.openstack.org/#/c/30988/ which prevents routers from being destroyed and re-added."
1233288,1233288,cinder,203e8bf9daf35319d56612de9c8b75d36fcfafd4,0,0,Bug in comments “should be editted to be more clear.”,Edit GPFS config flag help text for clarity,The help text for gpfs config flags should be editted to be more clear.
1233561,1233561,nova,01a44568cc60bb5a6dd7b55d69b20bba57d1b94b,1,1, ,server's action confirm_resize return wrong status...,"server's action confirm_resize return 204 now, but it should be 202
    @wsgi.response(202)
    @wsgi.serializers(xml=FullServerTemplate)
    @wsgi.deserializers(xml=ActionDeserializer)
    @wsgi.action('confirm_resize')
    def _action_confirm_resize(self, req, id, body):
        context = req.environ['nova.context']
        instance = self._get_server(context, req, id)
        try:
            self.compute_api.confirm_resize(context, instance)
        except exception.MigrationNotFound:
            msg = _(""Instance has not been resized."")
            raise exc.HTTPBadRequest(explanation=msg)
        except exception.InstanceInvalidState as state_error:
            common.raise_http_conflict_for_instance_invalid_state(state_error,
                    'confirm_resize')
        return exc.HTTPNoContent()
The 'return exc.HTTPNoContent()' overwrite the '@wsgi.response(202)'"
1233563,1233563,nova,ef3fbe99af498a40555b44f9ef0fcf6b88eb5d30,1,1, ,unshelve feature does not work,"When unshelving a shelved server, the server cannot be changed to 'Active' forever:
$ nova list
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| ID                                   | Name | Status            | Task State | Power State | Networks         |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| 919234d1-4a3d-4c26-bddd-77d004f7d41e | vm01 | SHELVED_OFFLOADED | unshelving | Shutdown    | private=10.0.0.3 |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
and nova-scheduler outputs the following error messages:
2013-10-01 17:22:55.456 ERROR nova.openstack.common.rpc.amqp [req-51700174-fc3a-48f4-8962-8f54d6f30164 admin demo] Exception during message handling
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/common.py"", line 439, in inner
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     return catch_client_exception(exceptions, func, *args, **kwargs)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/common.py"", line 420, in catch_client_exception
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     return func(*args, **kwargs)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/manager.py"", line 298, in select_destinations
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     filter_properties)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 144, in select_destinations
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     filter_properties, instance_uuids)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 288, in _schedule
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     scheduler_hints = filter_properties.get('scheduler_hints') or {}
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp AttributeError: 'list' object has no attribute 'get'"
1233789,1233789,nova,3a5e1faee04671f2e88b28d805b191b480054254,0,0,"“will result in verbose exception” bug in the future, refactoring",Object actions via conductor will result in verbos...,See  http://logs.openstack.org/87/44287/9/check/gate-tempest-devstack-vm-full/c3a07eb/logs/screen-n-cond.txt.gz
1233861,1233861,cinder,645a84f990c90e28548cf35b4b5f242eb0e0c286,1,1,"“It appears that this is caused by not setting the charset in the migration,”",Mysql foreign key failure during db migration from...,"This happens during the upgrade from grizzly to havana using packages from the Ubuntu Cloud Archive on an Ubuntu 12.04.3 system.
root@openstack01:~# cinder-manage db sync
2013-10-01 23:35:04.977 7923 INFO migrate.versioning.api [-] 9 -> 10...
2013-10-01 23:35:05.000 7923 ERROR 010_add_transfers_table [-] Table |Table('transfers', MetaData(bind=Engine(mysql://cinder:eNgoyam3@127.0.0.1/cinder?charset=utf8)), Col
umn('created_at', DateTime(), table=<transfers>), Column('updated_at', DateTime(), table=<transfers>), Column('deleted_at', DateTime(), table=<transfers>), Column('delete
d', Boolean(), table=<transfers>), Column('id', String(length=36), table=<transfers>, primary_key=True, nullable=False), Column('volume_id', String(length=36), ForeignKey
('volumes.id'), table=<transfers>), Column('display_name', String(length=255), table=<transfers>), Column('salt', String(length=255), table=<transfers>), Column('crypt_ha
sh', String(length=255), table=<transfers>), Column('expires_at', DateTime(), table=<transfers>), schema=None)| not created!
2013-10-01 23:35:05.003 7923 CRITICAL cinder [-] (OperationalError) (1005, ""Can't create table 'cinder.transfers' (errno: 150)"") '\nCREATE TABLE transfers (\n\tcreated_at
 DATETIME, \n\tupdated_at DATETIME, \n\tdeleted_at DATETIME, \n\tdeleted BOOL, \n\tid VARCHAR(36) NOT NULL, \n\tvolume_id VARCHAR(36), \n\tdisplay_name VARCHAR(255), \n\t
salt VARCHAR(255), \n\tcrypt_hash VARCHAR(255), \n\texpires_at DATETIME, \n\tPRIMARY KEY (id), \n\tCHECK (deleted IN (0, 1)), \n\tFOREIGN KEY(volume_id) REFERENCES volume
s (id)\n)ENGINE=InnoDB\n\n' ()
2013-10-01 23:35:05.003 7923 TRACE cinder Traceback (most recent call last):
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/bin/cinder-manage"", line 695, in <module>
2013-10-01 23:35:05.003 7923 TRACE cinder     main()
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/bin/cinder-manage"", line 692, in main
2013-10-01 23:35:05.003 7923 TRACE cinder     fn(*fn_args)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/bin/cinder-manage"", line 228, in sync
2013-10-01 23:35:05.003 7923 TRACE cinder     return migration.db_sync(version)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/cinder/db/migration.py"", line 33, in db_sync
2013-10-01 23:35:05.003 7923 TRACE cinder     return IMPL.db_sync(version=version)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/migration.py"", line 77, in db_sync
2013-10-01 23:35:05.003 7923 TRACE cinder     return versioning_api.upgrade(get_engine(), repository, version)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/migrate/versioning/api.py"", line 186, in upgrade
2013-10-01 23:35:05.003 7923 TRACE cinder     return _migrate(url, repository, version, upgrade=True, err=err, **opts)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""<string>"", line 2, in _migrate
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/migration.py"", line 43, in patched_with_engine
2013-10-01 23:35:05.003 7923 TRACE cinder     return f(*a, **kw)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/migrate/versioning/api.py"", line 366, in _migrate
2013-10-01 23:35:05.003 7923 TRACE cinder     schema.runchange(ver, change, changeset.step)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/migrate/versioning/schema.py"", line 91, in runchange
2013-10-01 23:35:05.003 7923 TRACE cinder     change.run(self.engine, step)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/migrate/versioning/script/py.py"", line 145, in run
2013-10-01 23:35:05.003 7923 TRACE cinder     script_func(engine)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/migrate_repo/versions/010_add_transfers_table.py"", line 47, in upg
rade
2013-10-01 23:35:05.003 7923 TRACE cinder     transfers.create()
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/schema.py"", line 614, in create
2013-10-01 23:35:05.003 7923 TRACE cinder     checkfirst=checkfirst)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1479, in _run_visitor
2013-10-01 23:35:05.003 7923 TRACE cinder     conn._run_visitor(visitorcallable, element, **kwargs)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1122, in _run_visitor
2013-10-01 23:35:05.003 7923 TRACE cinder     **kwargs).traverse_single(element)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/sql/visitors.py"", line 111, in traverse_single
2013-10-01 23:35:05.003 7923 TRACE cinder     return meth(obj, **kw)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/ddl.py"", line 89, in visit_table
2013-10-01 23:35:05.003 7923 TRACE cinder     self.connection.execute(schema.CreateTable(table))
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 662, in execute
2013-10-01 23:35:05.003 7923 TRACE cinder     params)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 720, in _execute_ddl
2013-10-01 23:35:05.003 7923 TRACE cinder     compiled
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 874, in _execute_context
2013-10-01 23:35:05.003 7923 TRACE cinder     context)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1024, in _handle_dbapi_exception
2013-10-01 23:35:05.003 7923 TRACE cinder     exc_info
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/util/compat.py"", line 195, in raise_from_cause
2013-10-01 23:35:05.003 7923 TRACE cinder     reraise(type(exception), exception, tb=exc_tb)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 867, in _execute_context
2013-10-01 23:35:05.003 7923 TRACE cinder     context)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 324, in do_execute
2013-10-01 23:35:05.003 7923 TRACE cinder     cursor.execute(statement, parameters)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 174, in execute
2013-10-01 23:35:05.003 7923 TRACE cinder     self.errorhandler(self, exc, value)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler
2013-10-01 23:35:05.003 7923 TRACE cinder     raise errorclass, errorvalue
2013-10-01 23:35:05.003 7923 TRACE cinder OperationalError: (OperationalError) (1005, ""Can't create table 'cinder.transfers' (errno: 150)"") '\nCREATE TABLE transfers (\n\
tcreated_at DATETIME, \n\tupdated_at DATETIME, \n\tdeleted_at DATETIME, \n\tdeleted BOOL, \n\tid VARCHAR(36) NOT NULL, \n\tvolume_id VARCHAR(36), \n\tdisplay_name VARCHAR
(255), \n\tsalt VARCHAR(255), \n\tcrypt_hash VARCHAR(255), \n\texpires_at DATETIME, \n\tPRIMARY KEY (id), \n\tCHECK (deleted IN (0, 1)), \n\tFOREIGN KEY(volume_id) REFERE
NCES volumes (id)\n)ENGINE=InnoDB\n\n' ()
2013-10-01 23:35:05.003 7923 TRACE cinder
mysql> SHOW ENGINE INNODB STATUS;
[...]
LATEST FOREIGN KEY ERROR
------------------------
131001 23:30:06 Error in foreign key constraint of table cinder/transfers:
FOREIGN KEY(volume_id) REFERENCES volumes (id) )ENGINE=InnoDB:
Cannot find an index in the referenced table where the
referenced columns appear as the first columns, or column types
in the table and the referenced table do not match for constraint.
Note that the internal storage type of ENUM and SET changed in
tables created with >= InnoDB-4.1.12, and such columns in old tables
cannot be referenced by such columns in new tables.
See http://dev.mysql.com/doc/refman/5.5/en/innodb-foreign-key-constraints.html
for correct foreign key definition.
[...]"
1233907,1233907,cinder,a9a36082528a6d46d950b5e83cb46c2af389317a,0,0,"Feature, “cinder should use cinder/utils.get_root_helper()”",cinder should use cinder/utils.get_root_helper(),"Cinder has many places where code manually constructs the same root_helper instead of using the
cinder.utils.get_root_helper to do the same thing."
1234002,1234002,cinder,2a630f71af21334a1f3dd213205efcba1ef38ac4,1,1,,allowed_rpc_exception_modules in cinder.conf.sampl...,"cinder.openstack.common.exception has already been deleted.
It seems to be a leak in 'Change-Id:
I6f46f90bd74cc26fc01667e467e3dab38037eec3'."
1234479,1234479,nova,6e30bbc126a1ad4eb87e735c812832f4fcc0054e,1,1, ,ipmi power probing is too fast,"baremetal nodes are failing to boot for me:
2013-10-03 01:55:15,392.392 7354 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): ipmitool -I lanplus -H 10.10.16.27 -U Administrator -f /tmp/tmpMxP1Xr power on execute /opt/stack/venvs/nova/local/lib/pyt
hon2.7/site-packages/nova/openstack/common/processutils.py:147
2013-10-03 01:55:15,573.573 7354 DEBUG nova.virt.baremetal.ipmi [-] ipmitool stdout: 'Chassis Power Control: Up/On
', stderr: '' _exec_ipmitool /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/ipmi.py:136
2013-10-03 01:55:15,707.707 7354 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): ipmitool -I lanplus -H 10.10.16.27 -U Administrator -f /tmp/tmpRYI8dn power status execute /opt/stack/venvs/nova/local/lib
/python2.7/site-packages/nova/openstack/common/processutils.py:147
2013-10-03 01:55:15,890.890 7354 DEBUG nova.virt.baremetal.ipmi [-] ipmitool stdout: 'Chassis Power is off
', stderr: '' _exec_ipmitool /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/ipmi.py:136
Note the gap between up and status checking is only 200ms (from IPMI return to status check) but in that interval this hardware doesn't toggle to on.
After the deploy fails, a manual check with
ipmitool -I lanplus -H 10.10.16.40 -U Administrator -P <password> chassis power status
returns
Chassis Power is on"
1234750,1234750,neutron,5f749768676e6739db1e01a03ddb7f3cb43d48f8,1,1, ,multiple external networks,"I have multiple external networks, and therefore each l3-agent is running with a particular ext network ID.
When I create a lrouter, which is scheduled to be on one l3-agent randomly. If I set the ext-gw for that lrouter later on to be an external network different from the ext-net the l3-agent is on, the entire lrouter somehow goes down, both internal and external interfaces.
I would prefer to see:
1) the operation of putting the lrouter onto the ""wrong"" ext-net is disallowed;
2) or better, we can move the lrouter to the right l3-agent with the correct ext-net."
1234759,1234759,nova,efdca8eeaffe6e63d57243147658f4691da3d89e,1,1, ,Hyper-V fails to spawn snapshots,"Creating a snapshot of an instance and then trying to boot from it will result the following Hyper-V exception: ""HyperVException: WMI job failed with status 10"". Here is the trace: http://paste.openstack.org/show/47904/ .
The ideea is that Hyper-V fails to expand the image, as it gets the request to resize it to it's actual size, which leads to an error."
1234902,1234902,Nova,ff693722639387a7a3ea62e291a8306569392ab9,0,0,"Fixes several misc typos in scheduler code
",Misc typos in scheduler code,Misc typos in scheduler code (comments)
1235022,1235022,nova,026843773b71ab0ac30abb36fe9eed92f5e11f7a,1,1, ,Bug #1235022 “VMware,"When using VMwareVC nova driver and VMwareVcVMDK cinder driver, booting from volume via the Horizon UI fails. The instance boots with ERROR status and the log shows ""Image  could not be found"". In addition, the user is unable to access the instances index page in Horizon due to an error 500 (other pages work, however). Steps to reproduce:
(Using horizon)
1. Create a volume from an image
2. Boot an instance from the volume
Expected result:
1. An instance is booted from the volume successfully
2. User is redirected to the instances index page in Horizon
Actual result:
1. Instance fails to boot with status ERROR
2. User is redirected to instances index page but page fails with 500 error. In debug mode, user sees TypeError at /project/instances: string indices must be integers (see link to trace below)
Nova log error:
 Traceback (most recent call last):
   File ""/opt/stack/nova/nova/compute/manager.py"", line 1037, in _build_instance
     set_access_ip=set_access_ip)
   File ""/opt/stack/nova/nova/compute/manager.py"", line 1410, in _spawn
     LOG.exception(_('Instance failed to spawn'), instance=instance)
   File ""/opt/stack/nova/nova/compute/manager.py"", line 1407, in _spawn
     block_device_info)
   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 623, in spawn
     admin_password, network_info, block_device_info)
   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 208, in spawn
     disk_type, vif_model, image_linked_clone) = _get_image_properties()
   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 187, in
     instance)
   File ""/opt/stack/nova/nova/virt/vmwareapi/vmware_images.py"", line 184, in
     meta_data = image_service.show(context, image_id)
   File ""/opt/stack/nova/nova/image/glance.py"", line 290, in show
     _reraise_translated_image_exception(image_id)
   File ""/opt/stack/nova/nova/image/glance.py"", line 288, in show
     image = self._client.call(context, 1, 'get', image_id)
   File ""/opt/stack/nova/nova/image/glance.py"", line 212, in call
     return getattr(client.images, method)(*args, **kwargs)
   File ""/opt/stack/python-glanceclient/glanceclient/v1/images.py"", line 114, in
     % urllib.quote(str(image_id)))
   File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 272,
     return self._http_request(url, method, **kwargs)
   File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 233,
     raise exc.from_response(resp, body_str)
 ImageNotFound: Image  could not be found.
Horizon error:
    Request Method:	GET
    Request URL:	http://10.20.72.218/project/instances/
    Django Version:	1.5.4
    Exception Type:	TypeError
    Exception Value:
    string indices must be integers
    Exception Location:	/opt/stack/horizon/openstack_dashboard/wsgi/../../openstack_dashboard/    dashboards/project/instances/views.py in get_data, line 92
    Python Executable:	/usr/bin/python
    Python Version:	2.7.3"
1235082,1235082,neutron,520a5e2e5fa749cc3e505c6ae526171714cd6369,0,0,Test in files,pythonic method name style not following pep8 in t...,"currently, most neutron codes name method name following pep8 recommendation while some code in test_ovs_tunnel.py not, this is found while review https://review.openstack.org/#/c/45725/
ref: pep8 recommendation: http://www.python.org/dev/peps/pep-0008/#method-names-and-instance-variables"
1235088,1235088,nova,ff56055a081a622e90b393b7f40dbe6b482abf65,1,0,"Changed requirements ""some logs have to be changed in order to support translation”",LOG.warn() should support translation,"$ grep 'LOG.warn([^_]' * -r
$ grep 'LOG.warn([^_]' * -r
nova/virt/xenapi/vm_utils.py:        LOG.warn(msg % e)
nova/virt/libvirt/driver.py:                LOG.warn(msg)
nova/virt/libvirt/volume.py:            LOG.warn(msg)
nova/virt/libvirt/volume.py:            LOG.warn(msg)
nova/virt/libvirt/volume.py:            LOG.warn(msg)
nova/virt/libvirt/volume.py:            LOG.warn(msg)
nova/tests/scheduler/test_host_manager.py:        host_manager.LOG.warn(""No service for compute ID 5"")
nova/tests/image/fake.py:        LOG.warn('Unable to find image id %s.  Have images: %s',
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:            LOG.warn(msg)
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:                    LOG.warn(msg)
nova/db/sqlalchemy/migrate_repo/versions/186_new_bdm_format.py:            LOG.warn(""Got an unexpected block device %s""
nova/openstack/common/db/sqlalchemy/session.py:            LOG.warn(msg % remaining)
nova/openstack/common/rpc/impl_zmq.py:                        LOG.warn(emsg)
nova/compute/api.py:                LOG.warn(msg)
nova/compute/api.py:                LOG.warn(msg)
nova/compute/api.py:                LOG.warn(msg)
nova/compute/manager.py:                LOG.warn(err_str % exc, instance=instance)
nova/compute/manager.py:                LOG.warn(e, instance=instance)
nova/network/linux_net.py:        LOG.warn(msg % {'num': num_rules, 'float': floating_ip})
nova/network/manager.py:                    LOG.warn(oversize_msg)
nova/api/openstack/compute/plugins/v3/quota_sets.py:                    LOG.warn(msg)
nova/api/openstack/compute/contrib/quotas.py:                    LOG.warn(msg)
nova/api/metadata/vendordata_json.py:                    LOG.warn(logprefix + _(""file does not exist""))
nova/api/metadata/vendordata_json.py:                    LOG.warn(logprefix + _(""Unexpected IOError when reading""))
nova/api/metadata/vendordata_json.py:                LOG.warn(logprefix + _(""failed to load json""))
They shoud make use of ""_"" defined in nova/openstack/common/gettextutils.py, like this:
  LOG.warn(_(""parent device '%s' not found""), dev)"
1235112,1235112,nova,07c24d6bbe6b55638f46d9f7dc9123c5d96eb453,1,1, ,VMware driver not discovering iscsi targets while ...,"VMware drivers cannot dynamically add iscsi targets presented to it while attaching a cinder volume. As a result, the instance cannot be attached to a cinder volume and fails with a message 'unable to find iscsi targets'.
This is because the driver fails to scan the Host Bus Adapter with the iscsi target portal (or target host). We need to fix the driver to scan the HBA by specifying the target portal."
1235118,1235118,nova,058ea40e7b7fb2181a2058e6118dce3f051e1ff3,0,0,"“remove deprecated”, refactoring",remove deprecated configuration variable for vnc_p...,in H it was decided that the support would be removed in I
1235148,1235148,cinder,2d8b905f474a879ad6a9a441e79bf7b05d2d39a5,1,1,Feature or bug3 “Provide the user with useful information”,Over quota message does not give user enough infor...,"The message cinder returns to the user when a create volume/snapshot request would exceed quota does not give the user the information they need to correct the problem. The error message returned is:
Requested volume or snapshot exceeds allowed Gigabytes quota.
The following goes to the api log:
Quota exceeded for 12345678, tried to create 2048G volume (2048G of 4048G already consumed)
However, in general, the user won't have access to the api.log, so those numbers should be passed to the user."
1235358,1235358,cinder,6e287c0f2bb7d4994d50f1763f412277e4dac6f7,1,0,“The problem appears to be in cinder when using the gluster or nfs drivers.”,invalid volume when source image virtual size is b...,"I created a volume from an image and booted an instance from it
when instance boots I get this: 'selected cylinder exceeds maximum supported by bios'
If I boot an instance from the same image I can boot with no issues so its just booting from the volume."
1235389,1235389,nova,a7885385fb34f7040684343deed040d302dbbfea,1,1, ,Quota violations should not cause a stacktrace in ...,"Right now, when we overrun a quota, we get an ugly stack trace in (at least) nova-api's log:
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack Traceback (most recent call last):
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/__init__.py"", line 119, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return req.get_response(self.application)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     application, catch_exc_info=False)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 545, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return self.app(env, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     response = self.app(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/wsgi.py"", line 917, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     content_type, body, accept)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/wsgi.py"", line 976, in _process_stack
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/wsgi.py"", line 1057, in dispatch
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return method(req=request, **action_args)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/compute/servers.py"", line 1248, in _action_resize
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return self._resize(req, id, flavor_ref, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/compute/servers.py"", line 1113, in _resize
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     self.compute_api.resize(context, instance, flavor_id, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/compute/api.py"", line 198, in wrapped
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return func(self, context, target, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/compute/api.py"", line 188, in inner
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return function(self, context, instance, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/compute/api.py"", line 215, in _wrapped
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return fn(self, context, instance, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/compute/api.py"", line 169, in inner
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return f(self, context, instance, *args, **kw)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/compute/api.py"", line 2304, in resize
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     resource=resource)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack TooManyInstances: Quota exceeded for ram: Requested 51137, but already used 128 of 51200 ram
See this example for more:
http://logs.openstack.org/23/49623/3/check/check-tempest-devstack-vm-full/b0d348a/logs/screen-n-api.txt.gz?level=TRACE
This happens a lot, clearly multiple times per every tempest run:
http://logstash.openstack.org/#eyJzZWFyY2giOiJAbWVzc2FnZTpcIlRvb01hbnlJbnN0YW5jZXM6IFF1b3RhIGV4Y2VlZGVkIGZvciByYW1cIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiODY0MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzgwOTA5MzQ3MjQ3fQ=="
1235399,1235399,nova,c17253bde86f62952351d42db4dba926b33269db,0,0,Test files,test_cell_update_without_type_specified test fails...,"The test_cell_update_without_type_specified test fails sporadically:
======================================================================
2013-10-04 18:17:30.357 | FAIL: nova.tests.api.openstack.compute.contrib.test_cells.CellsTest.test_cell_update_without_type_specified
2013-10-04 18:17:30.358 | tags: worker-3
2013-10-04 18:17:30.358 | ----------------------------------------------------------------------
2013-10-04 18:17:30.358 | Empty attachments:
2013-10-04 18:17:30.359 |   pythonlogging:''
2013-10-04 18:17:30.359 |   stderr
2013-10-04 18:17:30.359 |   stdout
2013-10-04 18:17:30.360 |
2013-10-04 18:17:30.360 | Traceback (most recent call last):
2013-10-04 18:17:30.360 |   File ""nova/tests/api/openstack/compute/contrib/test_cells.py"", line 290, in test_cell_update_without_type_specified
2013-10-04 18:17:30.360 |     self.assertEqual(cell['type'], 'parent')
2013-10-04 18:17:30.361 |   File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 322, in assertEqual
2013-10-04 18:17:30.361 |     self.assertThat(observed, matcher, message)
2013-10-04 18:17:30.361 |   File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 417, in assertThat
2013-10-04 18:17:30.362 |     raise MismatchError(matchee, matcher, mismatch, verbose)
2013-10-04 18:17:30.362 | MismatchError: 'child' != 'parent'
2013-10-04 18:17:30.362 | ======================================================================
More info:
https://jenkins02.openstack.org/job/gate-nova-python26/6256/console
http://logstash.openstack.org/#eyJzZWFyY2giOiJAbWVzc2FnZTpcIk1pc21hdGNoRXJyb3I6ICdjaGlsZCcgIT0gJ3BhcmVudCdcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiOTAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTM4MDkxMTY5ODYzMH0="
1235450,1235450,neutron,bd4a85d67f091382752d75b95f9cfd076431f30e,1,1,“vulnerability”,[OSSA 2013-033] Metadata queries from Neutron to N...,"The neutron metadata service works in the following way:
Instance makes a GET request to http://169.254.169.254/
This is directed to the metadata-agent which knows which router(namespace) he is running on and determines the ip_address from the http request he receives.
Now, the neturon-metadata-agent queries neutron-server  using the router_id and ip_address from the request to determine the port the request came from. Next, the agent takes the device_id (nova-instance-id) on the port and passes that to nova as X-Instance-ID.
The vulnerability is that if someone exposes their instance_id their metadata can be retrieved. In order to exploit this, one would need to update the device_id  on a port to match the instance_id they want to hijack the data from.
To demonstrate:
arosen@arosen-desktop:~/devstack$ nova list
+--------------------------------------+------+--------+------------+-------------+------------------+
| ID                                   | Name | Status | Task State | Power State | Networks         |
+--------------------------------------+------+--------+------------+-------------+------------------+
| 1eb33bf1-6400-483a-9747-e19168b68933 | vm1  | ACTIVE | None       | Running     | private=10.0.0.4 |
| eed973e2-58ea-42c4-858d-582ff6ac3a51 | vm2  | ACTIVE | None       | Running     | private=10.0.0.3 |
+--------------------------------------+------+--------+------------+-------------+------------------+
arosen@arosen-desktop:~/devstack$ neutron port-list
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                       |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
| 3128f195-c41b-4160-9a42-40e024771323 |      | fa:16:3e:7d:a5:df | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.1""} |
| 62465157-8494-4fb7-bdce-2b8697f03c12 |      | fa:16:3e:94:62:47 | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.4""} |
| 8473fb8d-b649-4281-b03a-06febf61b400 |      | fa:16:3e:4f:a3:b0 | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.2""} |
| 92c42c1a-efb0-46a6-89eb-a38ae170d76d |      | fa:16:3e:de:9a:39 | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.3""} |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
arosen@arosen-desktop:~/devstack$ neutron port-show  62465157-8494-4fb7-bdce-2b8697f03c12
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs |                                                                                 |
| device_id             | 1eb33bf1-6400-483a-9747-e19168b68933                                            |
| device_owner          | compute:None                                                                    |
| extra_dhcp_opts       |                                                                                 |
| fixed_ips             | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.4""} |
| id                    | 62465157-8494-4fb7-bdce-2b8697f03c12                                            |
| mac_address           | fa:16:3e:94:62:47                                                               |
| name                  |                                                                                 |
| network_id            | 5f68c45d-b729-4e21-9ded-089848eb4ef2                                            |
| security_groups       | 3e29d8e7-0195-4438-a49a-9706736b888d                                            |
| status                | ACTIVE                                                                          |
| tenant_id             | 0f9d696fc73d4110ab492ca105881b9b                                                |
+-----------------------+---------------------------------------------------------------------------------+
arosen@arosen-desktop:~/devstack$ neutron port-show  92c42c1a-efb0-46a6-89eb-a38ae170d76d
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs |                                                                                 |
| device_id             | eed973e2-58ea-42c4-858d-582ff6ac3a51                                            |
| device_owner          | compute:None                                                                    |
| extra_dhcp_opts       |                                                                                 |
| fixed_ips             | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.3""} |
| id                    | 92c42c1a-efb0-46a6-89eb-a38ae170d76d                                            |
| mac_address           | fa:16:3e:de:9a:39                                                               |
| name                  |                                                                                 |
| network_id            | 5f68c45d-b729-4e21-9ded-089848eb4ef2                                            |
| security_groups       | 3e29d8e7-0195-4438-a49a-9706736b888d                                            |
| status                | ACTIVE                                                                          |
| tenant_id             | 0f9d696fc73d4110ab492ca105881b9b                                                |
+-----------------------+---------------------------------------------------------------------------------+
From vm2 (eed973e2-58ea-42c4-858d-582ff6ac3a51):
$ curl http://169.254.169.254/latest/meta-data/hostname
vm2.novalocal
arosen@arosen-desktop:~/devstack$ neutron port-update 92c42c1a-efb0-46a6-89eb-a38ae170d76d   --device_id=1eb33bf1-6400-483a-9747-e19168b68933
 From vm2 (eed973e2-58ea-42c4-858d-582ff6ac3a51):
$ curl http://169.254.169.254/latest/meta-data/hostname
vm1.novalocal
In order to fix this issue I believe we need to also pass the tenant-id in the metadata request to nova. When nova receives the request it will now have to query it's database using the instance_id and check that the tenant_id's match. Using the tenant_id solves this issue as the user is not allowed to specify or update this field."
1236372,1236372,neutron,238689570145fdafe8e61967bb56133f167d39ab,1,1, ,"Router without active ports fails to be deleted, a...","Version
=======
Havana, RHEL, neutron+ovs, python-neutron-2013.2-0.3.3.b3.el6ost
Description
===========
It's impossible to delete a router while it still has inactive ports.
The error message states that the router still has active ports.
# neutron router-port-list router1
+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                            |
+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------------+
| 052a48e4-0868-4675-bef7-8f763dd697b4 |      | fa:16:3e:60:73:8d | {""subnet_id"": ""c5d63940-71e8-4338-865e-f5364fbe4e78"", ""ip_address"": ""10.35.214.1""}   |
| 065cef02-a949-45c9-b3df-e005dbf96c9a |      | fa:16:3e:a6:6d:d8 | {""subnet_id"": ""044bcc05-f37b-4d1d-a700-c91c4381fbc8"", ""ip_address"": ""10.35.211.1""}   |
| 7a020243-90e5-439d-90fb-ec96b07843e7 |      | fa:16:3e:04:0c:1f | {""subnet_id"": ""4081fbca-3e59-4be5-a98e-3c9e0d13d3a6"", ""ip_address"": ""10.35.212.1""}   |
| 7af56958-674e-472b-8dbe-09b60501a6e6 |      | fa:16:3e:1a:07:a4 | {""subnet_id"": ""ef8e7c03-f17f-4c3c-9afe-252aca1283fd"", ""ip_address"": ""10.35.170.102""} |
| f034cb8a-2a09-4d41-b46c-a08fe208461e |      | fa:16:3e:de:9d:32 | {""subnet_id"": ""cca4edc7-2872-4c1e-a270-3b0beb60f421"", ""ip_address"": ""10.35.213.1""}   |
+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------------+
# for i in `neutron router-port-list router1 | grep subnet_id | cut -d"" "" -f 2` ; do neutron port-show $i ; done | grep status
| status                | ACTIVE                                                                             |
| status                | ACTIVE                                                                             |
| status                | ACTIVE                                                                             |
| status                | ACTIVE                                                                               |
| status                | ACTIVE                                                                             |
# neutron router-delete router1
Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
# for i in `neutron router-port-list router1 | grep subnet_id | cut -d"" "" -f 2` ; do neutron port-update $i --admin_state_up False ; done | grep status
# for i in `neutron router-port-list router1 | grep subnet_id | cut -d"" "" -f 2` ; do neutron port-show $i ; done | grep status
| status                | DOWN                                                                               |
| status                | DOWN                                                                               |
| status                | DOWN                                                                               |
| status                | DOWN                                                                                 |
| status                | DOWN                                                                               |
# neutron router-delete router1
Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
From /var/log/neutron/server.log
================================
2013-10-07 16:39:40.429 2341 ERROR neutron.api.v2.resource [-] delete failed
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/db/l3_db.py"", line 266, in delete_router
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource     raise l3.RouterInUse(router_id=id)
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource RouterInUse: Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource
2013-10-07 16:40:34.870 2341 ERROR neutron.api.v2.resource [-] delete failed
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/db/l3_db.py"", line 266, in delete_router
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource     raise l3.RouterInUse(router_id=id)
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource RouterInUse: Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource"
1236459,1236459,cinder,5c321d758c9718d7dde555316ac4fbd2f7acf424,1,1,“In ae6b7642e8d32ef5fa75cdcfe55be23c052fd547 we added a key manager with a static key.”,"Bug #1236459 "" key manager is insecure warning messages” ","In ae6b7642e8d32ef5fa75cdcfe55be23c052fd547 we added a key manager with a static key.
This key manager (enabled by default) repeated logs the following WARNING message to the Cinder api.log file:
2013-10-07 15:10:17.714 553 WARNING cinder.keymgr.conf_key_mgr [-] This key manager is insecure and is not recommended for production deployments
-----
There are actually two issues here. Logging tons of warning messages by default is not ideal... and should be avoided, especially since at this time there is no ""production ready"" key manager implementation which an end user could configure."
1236585,1236585,nova,d87bd44d0211a633efd545dc7b2027a613897b85,1,1, ,tempest.api.compute.servers.test_list_servers_nega...,"See: http://logs.openstack.org/69/49169/2/check/check-tempest-devstack-vm-full/473539f/console.html
2013-10-07 18:57:09.859 | ======================================================================
2013-10-07 18:57:09.859 | FAIL: tempest.api.compute.servers.test_list_servers_negative.ListServersNegativeTestJSON.test_list_servers_by_changes_since[gate]
2013-10-07 18:57:09.860 | tempest.api.compute.servers.test_list_servers_negative.ListServersNegativeTestJSON.test_list_servers_by_changes_since[gate]
2013-10-07 18:57:09.860 | ----------------------------------------------------------------------
2013-10-07 18:57:09.860 | _StringException: Empty attachments:
2013-10-07 18:57:09.861 |   stderr
2013-10-07 18:57:09.861 |   stdout
2013-10-07 18:57:09.861 |
2013-10-07 18:57:09.862 | pythonlogging:'': {{{
2013-10-07 18:57:09.862 | 2013-10-07 18:38:33,059 Request: GET http://127.0.0.1:8774/v2/3b02395d4eec44958ffcc10ac2673fd2/servers?changes-since=2013-10-07T18%3A38%3A31.034080
2013-10-07 18:57:09.862 | 2013-10-07 18:38:33,490 Response Status: 200
2013-10-07 18:57:09.863 | 2013-10-07 18:38:33,490 Nova request id: req-906f2cf2-6508-4cd2-bf62-3595b18d223a
2013-10-07 18:57:09.863 | }}}
2013-10-07 18:57:09.863 |
2013-10-07 18:57:09.863 | Traceback (most recent call last):
2013-10-07 18:57:09.864 |   File ""tempest/api/compute/servers/test_list_servers_negative.py"", line 191, in test_list_servers_by_changes_since
2013-10-07 18:57:09.864 |     self.assertEqual(num_expected, len(body['servers']))
2013-10-07 18:57:09.864 |   File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 322, in assertEqual
2013-10-07 18:57:09.865 |     self.assertThat(observed, matcher, message)
2013-10-07 18:57:09.865 |   File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 417, in assertThat
2013-10-07 18:57:09.865 |     raise MismatchError(matchee, matcher, mismatch, verbose)
2013-10-07 18:57:09.865 | MismatchError: 3 != 2"
1236626,1236626,cinder,a5371712ce67b8e769f5fc12a3048dc5d0eef3eb,1,1, ,Nexenta iSCSI driver fix lu exists,"Catch ""does not exist"" exception of NMS scsidisk lu_exists method."
1236648,1236648,cinder,c47bb5b89e04403b46c224c1bda13d3d0e11eedd,1,0,“__metaclass__ is incompatible for python 3”,__metaclass__ is incompatible for python 3,"Some class uses __metaclass__ for abc.ABCMeta.
six be used in general for python 3 compatibility.
For example
import abc
import six
six.add_metaclass(abc.ABCMeta)
class FooDriver:
    @abc.abstractmethod
    def bar():
        pass"
1236709,1236709,glance,635ceac07969a700756e571ff5777993b59b9829,1,1, ,argument of glance-cache-manage command is confusi...,"step to reproduce:
 *execute ""glance-cache-manage"" command  as fallows
  -""glance-cache-manage queue-image 1fde5cfe-10be-4f38-86e7-9e54710d34f2 45f1c095-5b8c-4537-8dd3-a58f91d7b4e1""
result:
 *messeage is displayed as follows
  -Queue image 45f1c095-5b8c-4537-8dd3-a58f91d7b4e1 for caching? [y/N]
expected result:
   If the number of argument isn't  one,   commad  fails."
1236723,1236723,cinder,f330eac22ad47901ccd3252ffd73c46d7157644e,0,0,Refactoring “remove unused methods in driver.Scheduler”,HostManager doesn't have get_host_list() method,"cinder.scheduler.host_manager.HostManager doesn't have get_host_list()
and get_service_capabilities() method.
But I look like which cinder.scheduler.driver.Scheduler is trying to call them.
Are they not being used? I think they can be deleted."
1236741,1236741,neutron,3299eb3dfebde39f0bec73008c99976c0b852c89,0,0,"feature3, “Adding more tests for Radware LBaaS driver.”",Radware LBaaS driver unit-testing enhancement,More Radware LBaaS driver unit-testing should be added
1236744,1236744,nova,e33e1f3e816c64e55d23d32366f22a79bdb45b86,0,0,“Ensure that this is in the correct section”,netaddr is a third party import,Ensure that this is in the correct section (following the HACKING rules)
1236930,1236930,nova,2392313f562ba6a90ed1ec3fbc507862043fa44f,1,1,“nova compute api should throw exception for such state.” Feature,attempting to reboot a shutdown/suspened/crashed/p...,"I am running Havana from precise-proposed in the UCA (nova 1:2013.2~b3-0ubuntu1~cloud0).
To reproduce:
- start an instance
- reboot (sudo reboot) the compute node on which it is running
-  after the compute node is done booting, the instance will be off:
root@xen10:~# nova list
+--------------------------------------+------+---------+------------+-------------+-------------------------+
| ID                                   | Name | Status  | Task State | Power State | Networks                |
+--------------------------------------+------+---------+------------+-------------+-------------------------+
| 4824dce8-d876-4022-a446-3fc8d708ac62 | test | SHUTOFF | None       | Shutdown    | novanetwork=172.20.46.3 |
+--------------------------------------+------+---------+------------+-------------+-------------------------+
(note that although my hostname has ""xen"" in it, I'm using KVM. Haven't updated DNS yet...)
- attempt to reboot the instance (nova reboot 4824dce8-d876-4022-a446-3fc8d708ac62)
# nova show 4824dce8-d876-4022-a446-3fc8d708ac62
+--------------------------------------+----------------------------------------------------------+
| Property                             | Value                                                    |
+--------------------------------------+----------------------------------------------------------+
| status                               | SHUTOFF                                                  |
| updated                              | 2013-10-08T15:28:47Z                                     |
| OS-EXT-STS:task_state                | rebooting                                                |
The reboot fails. The compute node will log:
2013-10-08 11:28:55.579 1400 WARNING nova.compute.manager [req-11fe1624-22f6-4348-81c5-185d0ce0d3a0 a70453729dd84bfd8f31019b1bb91e40 46ab32189ab64a4c92f8f64e6c9ed028] [instance: 4824dce8-d876-4022-a446-3fc8d708ac62] trying to reboot a non-running instance: (state: 4 expected: 1)
- attempt to start the instance (nova start 4824dce8-d876-4022-a446-3fc8d708ac62):
produces console output:
ERROR: Instance 4824dce8-d876-4022-a446-3fc8d708ac62 in task_state rebooting. Cannot start while the instance is in this state. (HTTP 400) (Request-ID: req-732224e1-8c34-4754-84f7-7a8476673185)
- wait about 120 seconds, and the compute node will log:
2013-10-08 11:30:56.082 1400 WARNING nova.virt.libvirt.driver [req-11fe1624-22f6-4348-81c5-185d0ce0d3a0 a70453729dd84bfd8f31019b1bb91e40 46ab32189ab64a4c92f8f64e6c9ed028] [instance: 4824dce8-d876-4022-a446-3fc8d708ac62] Failed to soft reboot instance. Trying hard reboot.
Afterwards, the instance will be running.
It's confusing that the reboot logs a failure for a very obvious reason (an instance that is not running can't be *re*booted), yet the instance's state remains as ""rebooting"". I had expected that the reboot had failed, and openstack was in some consistant state. I was then again suprised when in fact it *was* still rebooting -- it just took two minutes to do so. Less confusing would be to catch the original error, and report the reboot as failed. The log messages are confusing, because the first sets the expectation that a non-running instance can't be rebooted, but it can (two minutes later)."
1236993,1236993,neutron,46495f4995b6604cb249b688aaa249ef4e8b18ef,1,0,"“This is the side effect of the recent commit: Quota DB driver has been recently enabled by default”""",Plugins without quota extension fail to start due ...,"As reported in bug 1236970, plugins without quota extensions fail to start due to missing table in the database.
This is the side effect of the recent commit: Quota DB driver has been recently enabled by default""
https://review.openstack.org/#/c/49993
If a plugin has no support of quota extensions, quota_driver should fallback to the config quota driver.
Otherwise we need to add quota extension support to all plugins."
1237082,1237082,neutron,345f8e1fdc65e0cf23369488d4318c0755d6b65d,1,1, ,ml2 vxlan interface with linuxbridge use proxy mod...,"when a vxlan interface is created in linuxbridge agent, it is set with the argument ""proxy"" (ie  ""ip link add vxlan1 type vxlan id 1 dev wlan0 proxy group 224.0.0.1""), which means that it uses an arp-proxy. This should only be done when L2-population is activated, otherwise it leads to unaccessibility between vm that are note hosted on the same host."
1237102,1237102,nova,73b3bf91df00059c69dc1dd81e4554ec24c647b1,1,1, ,Conductor does not properly copy objects during ch...,The conductor object_action() method does a shallow copy of the instance in order to do change tracking after the method is called. This is not sufficient as complex types like dicts and lists will not be copied and then the change detection logic will think those fields have not changed.
1237185,1237185,cinder,7aa0c1d9a4a02add6b00aeb764e9ba86a3c460ff,1,0, ,AttributeError occurs when Huawei HVS driver attac...,"If the iscsi target port IP is set incorrectly, the driver will get a None value. Here the dirver should raise an exception.
The log:
2013-09-26 00:39:41.832 ERROR cinder.openstack.common.rpc.amqp [req-c8478604-b0fd-47f4-a86e-4d6ac5e92bb8 46d5d9f00d9d4d86845ec9f57b5399d5 f5aa9910f241481e9518f331be373d32] Exception during message handling
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 561, in initialize_connection
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     conn_info = self.driver.initialize_connection(volume, connector)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/huawei/huawei_hvs.py"", line 78, in initialize_connection
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     return self.common.initialize_connection_iscsi(volume, connector)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/huawei/rest_common.py"", line 518, in initialize_connection_iscsi
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     (iscsi_iqn, target_ip) = self._get_iscsi_params(connector)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/huawei/rest_common.py"", line 1287, in _get_iscsi_params
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     target_iqn = self._get_tgt_iqn(target_ip)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/huawei/rest_common.py"", line 1087, in _get_tgt_iqn
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     split_list = ip_info.split(""."")
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp AttributeError: 'NoneType' object has no attribute 'split'
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp
2013-09-26 00:39:41.833 ERROR cinder.openstack.common.rpc.common [req-c8478604-b0fd-47f4-a86e-4d6ac5e92bb8 46d5d9f00d9d4d86845ec9f57b5399d5 f5aa9910f241481e9518f331be373d32] Returning exception 'NoneType' object has no attribute 'split' to caller"
1237209,1237209,neutron,39b8bddb1124d16eae15f667f8d921e8ddf5701d,1,1, ,Bug #1237209 “BigSwitch,"In the code to disassociate floating IPs in the BigSwitch plugin, it incorrectly updates the tenant's network on the backend controller rather than the external network."
1237303,1237303,nova,c2ae5fb5659faeadc6f4d8afe27eb68e3c0ade0c,1,1,“Update log message for add_host_to_aggregate“ bug in comments,Log message does not contain blank space,"liugya@liugya-ubuntu:~/devstack$ nova  aggregate-add-host  1 liugya-ubuntu
Aggregate 1 has been successfully updated.
+----+------+-------------------+--------------------+----------------------------------+
| Id | Name | Availability Zone | Hosts              | Metadata                         |
+----+------+-------------------+--------------------+----------------------------------+
| 1  | agg1 | zone1             | [u'liugya-ubuntu'] | {u'availability_zone': u'zone1'} |
+----+------+-------------------+--------------------+----------------------------------+
liugya@liugya-ubuntu:~/devstack$ nova  aggregate-add-host  2 liugya-ubuntu
ERROR: Cannot perform action 'add_host_to_aggregate' on aggregate 2. Reason: Host already in availability zonezone1.. (HTTP 409) (Request-ID: req-fe50ad9a-f20d-456c-8c1f-f236b8329b0c)
Two issues:
1) contain two period at the end of the log message
2) Should be availability zone zone1"
1237427,1237427,neutron,befa0b9184eb0c6248d06efa5b02be8217f1722e,0,0,Bug or feature3 “Implement local ARP responder onto OVS agent“,l2-pop ML2 mechanism driver need ARP responder wit...,"The l2-pop mechanism driver uses an ARP responder to avoid ARP broadcast but it only works with the LB agent.
We need to implement an ARP responder for the OVS agent also."
1237557,1237557,cinder,015555acb75ee4d9298915951d2bfaf0d19d2b02,1,1,Worng logic,Bug #1237557 “VMware,"With VC VMDK cinder driver, the size of volumes created from image is currently based on the size of the image and the size input by the user is ignored.  So if the image in the following command is 1GB,
    cinder create --image-id <image>  5
The 5GB is effectively ignored and the volume is created as 1GB.
Instead of this behavior, we should create a 5GB volume and then copy in the image."
1237611,1237611,nova,831e6013544326e32a1e252917301618c9ea7626,1,1, ,flavor name with only white spaces should return 4...,"When user passes only white spaces to flavor name, it creates flavor successfully.  Since name is a mandatory parameter, it should restrict user from passing white spaces. Also leading and trailing white spaces should be removed before saving it to the backend similar to the instance name.
{
    ""flavor"": {
        ""name"": ""   ""
        ""ram"": 1024,
        ""vcpus"": 2,
        ""disk"": 10,
        ""id"": ""10"",
        ""os-flavor-access:is_public"": false
    }
}
For example
name = ""   "" 		  #not allowed
name = ""extra large"" 	  #allowed
name = ""  extra large  "" #allowed, but leading and trailing white spaces will be trimmed before saving it to the backend.
Actual output: HTTP/1.1 200 OK
Expected  output: HTTP/1.1 400 Bad Request"
1237622,1237622,nova,db68dc8670fc8943dfc142ebb0f3f1f405c04e4b,1,1,Refactoring “is not actually implementing get_diagnostics”,vmware driver is not actually implementing get_dia...,"According to the hypervisor support matrix the vmware driver doesn't support the diagnostics API:
https://wiki.openstack.org/wiki/HypervisorSupportMatrix
But the code suggests otherwise:
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/driver.py#L268
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L1263
There is an unused get_diagnostics method in vmwareapi.vmops though:
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L1284
i'm guessing that was stubbed out before the get_info method was added and someone connected the dots to the vmwareapi driver code to use it.
We should remove the unused get_diagnostics method.
Note that there aren't actually really any unit tests associated with the vmwareapi diagnostics API support though so we should add that in when we remove the unused method."
1237679,1237679,nova,dced1df98e385ebb0615d3789f67d95add49c637,1,1,Test files,Fix useless vmware pause/unpause test methods,"The vmwareapi test class that tests the VMwareESXDriver has a test_pause and test_unpause method that simply passes:
https://github.com/openstack/nova/blob/master/nova/tests/virt/vmwareapi/test_vmwareapi.py#L632
https://github.com/openstack/nova/blob/master/nova/tests/virt/vmwareapi/test_vmwareapi.py#L635
Those APIs aren't supported by the VMwareESXDriver so they should actually test that the code raises NotImplementedError.
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L957
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L961
I think the test class was doing a pass for the esx driver because the test class for the vcdriver extends the esx driver test class, but the method isn't any different for the vcdriver test class so it should just assert it raises NotImplementedError either way."
1237795,1237795,nova,74289aacb5363bd72b449580325a651117e5342d,1,1, ,Bug #1237795 “VMware,When nova compute restarts the running instances on the hypervisor are queried. None of the instances would be matched - this would prevent the instance states being in sync with the state in the database. See _destroy_evacuated_instances (https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L531)
1237802,1237802,nova,e936cac67612015e1123b4539a60ec1aa1b1ff82,1,1, ,IPMI power manager hangs when the password is empt...,"While creating Baremetal Node by `nova baremetal-node-create' with --pm_password '' [1] and starting a deployment, IPMI power manager hangs with showing prompt ""Password:"" in nova-compute process. IPMI power manager creates an empty file and specifies it as the password file in the ipmitool command line, but ipmitool ignores that file [2].
This is an uncommon case that an administrator set password empty, but this is not a low importance bug due to it stops the thread.
I think we can avoid this bug by writing '\0' into the password file [3], since ipmitool checks return value of fgets() is not NULL which means the file is not start with EOF and no error had occurred.
[1] e.g.:
    $ nova baremetal-node-create --pm_address 192.0.2.200 --pm_user admin --pm_password '' service-host 1 1000 10000 00:11:22:33:44:55
[2] In ipmitool manpage:
    -f <password_file>
        Specifies a file containing the remote server password. If this option is absent, or if password_file is empty, the password will default to NULL.
[3] I checked that ipmitool works with a file containing '\0';
    # touch a
    # ipmitool -I lanplus -H 192.0.2.94 -f a -U administrator power status
    Unable to read password from file a
    Unable to read password from file a
    Password:                       <-- Enter
    Chassis Power is off
    # echo -ne '\0' > b
    # ipmitool -I lanplus -H 192.0.2.94 -f b -U administrator power status
    Chassis Power is off"
1237836,1237836,nova,27466f2a8f518cfda2935ddd882a99f6736296af,1,1,Bug in a msg333 “Fix error message of os-cells sync_instances api”,error message of os-cells sync_instances api is in...,"Though parameters of this api are 'updated_since' , 'project_id' and 'deleted',
the error message is ""Only 'updated_since' and 'project_id' are understood.""
It shoud be ""Only 'updated_since', 'project_id' and 'deleted' are understood."""
1237868,1237868,nova,9532e4ed969e47826801b14aff40dd9e356f2677,1,1,fix missing host when unshelving”,Fail to suspend a unshelved server,"After unshelved a shelved server(status: ACTIVE), it fails to suspend the server like the following:
$ nova list
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| ID                                   | Name | Status            | Task State | Power State | Networks         |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| 152a323d-6309-48ef-9c4a-2b02aaf1a58c | vm01 | SHELVED_OFFLOADED | None       | Shutdown    | private=10.0.0.2 |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
$ nova unshelve vm01
$ nova list
+--------------------------------------+------+--------+------------+-------------+------------------+
| ID                                   | Name | Status | Task State | Power State | Networks         |
+--------------------------------------+------+--------+------------+-------------+------------------+
| 152a323d-6309-48ef-9c4a-2b02aaf1a58c | vm01 | ACTIVE | None       | Running     | private=10.0.0.2 |
+--------------------------------------+------+--------+------------+-------------+------------------+
$ nova suspend vm01
ERROR: Unable to process the contained instructions (HTTP 422) (Request-ID: req-5c8edaf3-abb7-4d00-af96-e8a6d9777910)
$"
1237912,1237912,neutron,48e36b542904effb9b35c9d62bd0247e25f0dac6,1,1, ,Cannot update IPSec Policy lifetime,"When you try to update IPSec Policy lifetime, you get an error:
(neutron) vpn-ipsecpolicy-update ipsecpolicy --lifetime units=seconds,value=36001
Request Failed: internal server error while processing your request.
Meanwhile updating IKE Policy lifetime works well:
(neutron) vpn-ikepolicy-update ikepolicy --lifetime units=seconds,value=36001
Updated ikepolicy: ikepolicy"
1237944,1237944,nova,1ce493b9e4e0c381146287161ab2a3c75f1fc603,0,0,Bug in test,boto version breaks gating,"Boto version boto==2.14.0 breaks the gate with
2013-10-10 05:15:05.036 | Traceback (most recent call last):
2013-10-10 05:15:05.036 |   File ""/home/jenkins/workspace/gate-nova-python26/nova/tests/test_api.py"", line 389, in test_group_name_valid_length_security_group
2013-10-10 05:15:05.036 |     self.expect_http()
2013-10-10 05:15:05.036 |   File ""/home/jenkins/workspace/gate-nova-python26/nova/tests/test_api.py"", line 246, in expect_http
2013-10-10 05:15:05.037 |     is_secure).AndReturn(self.http)
2013-10-10 05:15:05.037 |   File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/mox.py"", line 765, in __call__
2013-10-10 05:15:05.037 |     return mock_method(*params, **named_params)
2013-10-10 05:15:05.038 |   File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/mox.py"", line 998, in __call__
2013-10-10 05:15:05.038 |     self._checker.Check(params, named_params)
2013-10-10 05:15:05.038 |   File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/mox.py"", line 929, in Check
2013-10-10 05:15:05.039 |     % (' '.join(sorted(still_needed))))
2013-10-10 05:15:05.039 | AttributeError: No values given for arguments: is_secure"
1237994,1237994,cinder,51fd5edb106e26d65696ce37a70eef6a4f75b1e2,1,0,“supports_thin_provisioning now uses a regexp to ensure parsing of lvm version succeeds when the build is customised;”,with lvm_type=thin parsing of lvm version fails,"supports_thin_provisioning fails on customised builds; RHEL for instance reports version as:
2.02.100(2)-RHEL6
2013-10-10 14:58:56.928 32460 TRACE cinder.service   File ""/usr/lib/python2.6/site-packages/cinder/brick/local_dev/lvm.py"", line 142, in supports_thin_provisioning
2013-10-10 14:58:56.928 32460 TRACE cinder.service     version_tuple = tuple(map(int, version.split('.')))
2013-10-10 14:58:56.928 32460 TRACE cinder.service ValueError: invalid literal for int() with base 10: '100-RHEL6'"
1238085,1238085,cinder,b6b9df2940ac07f715f759f4c315b2cf088c2320,1,1, ,Cinder Migrate from NFS fails due to nfs_mount_poi...,"""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp **args)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/utils.py"", line 808, in wrapper
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp return func(self, *args, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 779, in migrate_volume
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.db.volume_update(ctxt, volume_ref['id'], updates)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 772, in migrate_volume
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self._migrate_volume_generic(ctxt, volume_ref, host)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 710, in _migrate_volume_generic
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp new_volume['migration_status'] = None
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 690, in _migrate_volume_generic
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp remote='dest')
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/driver.py"", line 293, in copy_volume_data
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp {'status': dest_orig_status})
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/driver.py"", line 287, in copy_volume_data
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp remote=dest_remote)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/driver.py"", line 378, in _attach_volume
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp device_scan_attempts)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/utils.py"", line 798, in brick_get_connector
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp device_scan_attempts)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/brick/initiator/connector.py"", line 114, in factory
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp *args, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/brick/initiator/connector.py"", line 802, in __init__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp *args, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/brick/remotefs/remotefs.py"", line 41, in __init__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp err=_('nfs_mount_point_base required'))
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp InvalidParameterValue: An unknown exception occurred."
1238281,1238281,nova,c3330931113ac2edf5961a653e5c2cfe459c13a0,1,1,“ This addresses a minor (albeit important) logic error in the migration script”,Migration script deletes wrong compute node stats,"The nova DB migration script for 215 executes:
result = conn.execute(
        'update compute_node_stats set deleted = id, '
        'deleted_at = current_timestamp where compute_node_id not in '
        '(select id from compute_nodes where deleted <> 0)')
Need to remove the 'not' part so that we delete the right stats since the nested select finds all DELETED nodes.  The current SQL actually deletes all active compute nodes' stats."
1238366,1238366,glance,8f1c1b1a3be563db173d8096b59bcfa3964179ed,1,0,“Keep the date and content of man page update to date”,Keep the date and content of man page update to da...,See https://github.com/openstack/glance/blob/master/doc/source/man/glanceapi.rst
1238374,1238374,nova,c28c87db4c2b20e0f6c1375d2d4433e06c4e1743,1,1, ,TypeError in periodic task 'update_available_resou...,"this occurs while I creating an instance under my devstack env:
2013-10-11 02:56:29.374 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: 'NoneType' object is not iterable
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task     task(self, context)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/compute/manager.py"", line 4859, in update_available_resource
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task     rt.update_available_resource(context)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task     return f(*args, **kwargs)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 313, in update_available_resource
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task     self.pci_tracker.clean_usage(instances, migrations, orphans)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/pci/pci_manager.py"", line 285, in clean_usage
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task     for dev in self.claims.pop(uuid):
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task TypeError: 'NoneType' object is not iterable
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task"
1238430,1238430,nova,6ee68137c5ea9af99c1db4e2a2ce555f43a9e3e1,1,1,“The error message is not sufficient enough for debug of hyper-v”,"When resize failed in hyperv, the error message is...","2013-10-09 18:43:35.867 5236 ERROR nova.compute.manager [req-a8a83c04-03b4-4579-897f-a9825d17231e 657da0d8cf4440eca4a4ebad6fa1248a 0f7a0b0633c64e8888e6324bfe88dc16] [instance: a278d507-2112-40c2-912b-84b104300ed7] Error: Cannot resize a VHD to a smaller size
It would be helpful to debug the original and newer size in this case."
1238439,1238439,neutron,1763c80711993c55f4f13afe56f449b1dd6d3d3a,1,1, ,admin can not delete External Network because floa...,"HI
in admin role, I create External Network and router.  and create tenant A and userA.
Now the userA login, create network and router, create VM1 and assign Floating IP , access well ,perfectly.
Now I try to in admin roles  delete it.
1:  delete userA ,  no problem
2: delete tenantA ,  no problem
3: delete vm1, no problem
4: delete router, no problem
5: delete External Networknet,  report error,  I try to delete the port in sub panel, also fail.
check the Neutrion server log
TRACE neutron.api.v2.resource L3PortInUse: Port 2e5fa663-22e0-4c9e-87cc-e89c12eff955 has owner network:floatingip and therefore cannot be deleted directly via the port API."
1238561,1238561,neutron,a272a2838313b56edc1463f6d81a01414b37a78f,1,1, ,Mistake in usage alter_column's parameter type_,"In alembic alter_column have a parameter type_ not _type.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/338d7508968c_vpnaas_peer_address_.py"", line 47, in upgrade
    _type=sa.String(255), existing_type=sa.String(64))
  File ""<string>"", line 7, in alter_column
  File ""<string>"", line 1, in <lambda>
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/util.py"", line 271, in go
    raise TypeError(""Unknown arguments: %s"" % "", "".join(names))
TypeError: Unknown arguments: _type"
1238967,1238967,cinder,e4bde7adda4e885fe847e0bacc4d3fc84547e723,1,1, ,Broken NetAppDirect7modeISCSIDriver reports incorr...,"The driver is broken, reporting incorrectly the backends free capacities, as an infinite resource.
This causes two main problems:
- the scheduler always chooses the same netapp backend, and isn´t making any balance between the different backends.
- If the first storage backend has no real free capacity, the driver chooses it anyway, therefore after 3 schedule attemps, the volume creation stops with an error.
Is it possible to fix the driver, in order to report the real backends free capacity, so the scheduler balances the volume creation?
We can attach some log if is necessary.
Thanks in advance.
Agustin."
1239009,1239009,nova,e842fff31fd77289507d89387a62eee288aea6bc,0,0,feature. “Make scheduler disk_filter take swap into account”,scheduler filter disk_filter doesn't take swap int...,"from nova/scheduler/filters/disk_filter.py
    def host_passes(self, host_state, filter_properties):
        """"""Filter based on disk usage.""""""
        instance_type = filter_properties.get('instance_type')
        requested_disk = 1024 * (instance_type['root_gb'] +
                                 instance_type['ephemeral_gb'])
This should take into account swap, which is stored in MB, as well."
1239038,1239038,nova,103db29c224909cd260f1619a0a4578e172f6649,1,1, ,migrate server doesn't raise correct exception,"Migrate a non existent server throws HTTPBadRequest exception ,the correct is  InstanceNotFound exception.
live-migrate has the same problem.
The other server actions throw  InstanceNotFound exception.
I think it's a bug."
1239220,1239220,nova,eaf5636544a9b2ae1e87f74d0cdb989f8a41b008,0,0,Bug in test,boto version checking in test cases is not backwar...,"The changes in  commit:
  https://github.com/openstack/nova/commit/7161c62c22ebe609ecaf7e01d2feae473d01495a
Introduced version checking for boto 2.14; however string comparison is not effective for version checking as
'2.9.6' >= '2.14' == true
Resulting in older boto versions (as found in saucy) trying to test with the 2.14 code."
1239288,1239288,neutron,3ae0171834521a2152516e4bd7b0f8e46547bfda,0,0,Refactoring “Removing workflows from the Radware driver code”,Radware LBaaS driver - remove “workflows,"In order to gain more flexibility we decide to remove the ""workflows"" from the driver ""side"" and to load them on the ""vDirect side"".
This will give us more flexibility when we need to customize our solution.
With this change the driver will no longer updload the workflows into vDirect server.
It will only make sure that thw workflows are loaded on the vDirect server side."
1239637,1239637,neutron,54d79acaab50993f08b50f21019f82f17d38caa3,1,1, ,internal neutron server error on tempest VolumesAc...,"Logstash query:
@message:""DBError: (IntegrityError) null value in column \""network_id\"" violates not-null constraint"" AND @fields.filename:""logs/screen-q-svc.txt""
http://logs.openstack.org/22/51522/2/check/check-tempest-devstack-vm-neutron-pg-isolated/015b3d9/logs/screen-q-svc.txt.gz#_2013-10-14_10_13_01_431
http://logs.openstack.org/22/51522/2/check/check-tempest-devstack-vm-neutron-pg-isolated/015b3d9/console.html
2013-10-14 10:16:28.034 | ======================================================================
2013-10-14 10:16:28.034 | FAIL: tearDownClass (tempest.api.volume.test_volumes_actions.VolumesActionsTest)
2013-10-14 10:16:28.035 | tearDownClass (tempest.api.volume.test_volumes_actions.VolumesActionsTest)
2013-10-14 10:16:28.035 | ----------------------------------------------------------------------
2013-10-14 10:16:28.035 | _StringException: Traceback (most recent call last):
2013-10-14 10:16:28.035 |   File ""tempest/api/volume/test_volumes_actions.py"", line 55, in tearDownClass
2013-10-14 10:16:28.036 |     super(VolumesActionsTest, cls).tearDownClass()
2013-10-14 10:16:28.036 |   File ""tempest/api/volume/base.py"", line 72, in tearDownClass
2013-10-14 10:16:28.036 |     cls.isolated_creds.clear_isolated_creds()
2013-10-14 10:16:28.037 |   File ""tempest/common/isolated_creds.py"", line 453, in clear_isolated_creds
2013-10-14 10:16:28.037 |     self._clear_isolated_net_resources()
2013-10-14 10:16:28.037 |   File ""tempest/common/isolated_creds.py"", line 445, in _clear_isolated_net_resources
2013-10-14 10:16:28.038 |     self._clear_isolated_network(network['id'], network['name'])
2013-10-14 10:16:28.038 |   File ""tempest/common/isolated_creds.py"", line 399, in _clear_isolated_network
2013-10-14 10:16:28.038 |     net_client.delete_network(network_id)
2013-10-14 10:16:28.038 |   File ""tempest/services/network/json/network_client.py"", line 76, in delete_network
2013-10-14 10:16:28.039 |     resp, body = self.delete(uri, self.headers)
2013-10-14 10:16:28.039 |   File ""tempest/common/rest_client.py"", line 308, in delete
2013-10-14 10:16:28.039 |     return self.request('DELETE', url, headers)
2013-10-14 10:16:28.040 |   File ""tempest/common/rest_client.py"", line 436, in request
2013-10-14 10:16:28.040 |     resp, resp_body)
2013-10-14 10:16:28.040 |   File ""tempest/common/rest_client.py"", line 522, in _error_checker
2013-10-14 10:16:28.041 |     raise exceptions.ComputeFault(message)
2013-10-14 10:16:28.041 | ComputeFault: Got compute fault
2013-10-14 10:16:28.041 | Details: {""NeutronError"": ""Request Failed: internal server error while processing your request.""}"
1239709,1239709,nova,99cbfb3492c80ded53fa018fe5a0881f977dc407,1,1,“A typo made early in the object prototyping process has meant that”,NovaObject does not properly honor VERSION,"The base object infrastructure has been comparing Object.version instead of the Object.VERSION that *all* the objects have been setting and incrementing when changes have been made. Since the base object defined a .version, and that was used to determine the actual version of an object, all objects defining a different VERSION were ignored.
All systems in the wild currently running broken code are sending version '1.0' for all of their objects. The fix is to change the base object infrastructure to properly examine, compare and send Object.VERSION.
Impact should be minimal at this point, but getting systems patched as soon as possible will be important going forward."
1239747,1239747,nova,c31ae4987c5ae2015cc5379a4717490b75c62c9e,1,1, ,Logging “None,"The get_instance_disk_info method in the nova.virt.libvirt.driver logs a None value if a disk path is undefined. The log message is intended to show a path that does not represent a path, but in the case the path does not actually exist, it gives the following uninformative message:
2013-10-14 06:25:21.404 45722 DEBUG nova.virt.libvirt.driver [-] skipping None since it looks like volume get_instance_disk_info /usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py:4299
When the path is not defined it should log the instance it is looking at instead."
1239864,1239864,nova,f37905a3c1fd09597898c93a1cbc3050f335cf61,1,1,“Service groups using zookeeper don't work due to apparently improper handling of the zookeeper session creation.”,nova-api fails to query ServiceGroup status from Z...,"I am running with the ZooKeeper servicegroup driver on CentOS 6.4 (Python 2.6) with the RDO distro of Grizzly.
All nova services are successfully connecting to ZooKeeper, which I've verified using zkCli.
However, when I run `nova service-list` I get an HTTP 500 error from nova-api.  The nova-api log (/var/log/nova/api.log) shows:
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack   File ""/usr/lib/python2.6/site-packages/nova/servicegroup/api.py""\
, line 93, in service_is_up
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack     return self._driver.is_up(member)
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack   File ""/usr/lib/python2.6/site-packages/nova/servicegroup/drivers\
/zk.py"", line 116, in is_up
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack     all_members = self.get_all(group_id)
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack   File ""/usr/lib/python2.6/site-packages/nova/servicegroup/drivers\
/zk.py"", line 141, in get_all
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack     raise exception.ServiceGroupUnavailable(driver=""ZooKeeperDrive\
r"")
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack ServiceGroupUnavailable: The service from servicegroup driver ZooK\
eeperDriver is temporarily unavailable.
The problem seems to be around evzookeeper (using version 0.4.0).
To isolate the problem, I added some evzookeeper.ZKSession synchronous get() calls to test the roundtrip communication to ZooKeeper.  When I do a `self._session.get(CONF.zookeeper.sg_prefix)` in the zk.py ZooKeeperDriver __init__() method it works fine.  The logs show that this is immediately before the wsgi server starts up.
When I do the get() operation from within the ZooKeeperDriver get_all() method, the web request hangs indefinitely.  However, if I recreate the evzookeeper.ZKSession within the get_all() method (after the wsgi server has started) the nova-api request is successful.
diff --git a/nova/servicegroup/drivers/zk.py b/nova/servicegroup/drivers/zk.py
index 2a3edae..7de2488 100644
--- a/nova/servicegroup/drivers/zk.py
+++ b/nova/servicegroup/drivers/zk.py
@@ -122,7 +122,14 @@ class ZooKeeperDriver(api.ServiceGroupDriver):
         monitor = self._monitors.get(group_id, None)
         if monitor is None:
             path = ""%s/%s"" % (CONF.zookeeper.sg_prefix, group_id)
-            monitor = membership.MembershipMonitor(self._session, path)
+
+            null = open(os.devnull, ""w"")
+            local_session = evzookeeper.ZKSession(CONF.zookeeper.address,
+                                                  recv_timeout=
+                                                    CONF.zookeeper.recv_timeout,
+                                                  zklog_fd=null)
+
+            monitor = membership.MembershipMonitor(local_session, path)
             self._monitors[group_id] = monitor
             # Note(maoy): When initialized for the first time, it takes a
             # while to retrieve all members from zookeeper. To prevent"
1239952,1239952,nova,6594746108afa2db39ca2973b3dd0c37611f955e,1,1, ,boot_from_volume with down of cinder-api return 40...,"[Nova]2b17aa7f5af2adb418592b7169b50b231989bf37
Now, when I boot VM from volume with down of cinder-api, openstack returns 400.
It seems incompatibility in this case.
When HTTPClient exception ocuured, it should return 500Internal server error.
In fact, Nova turned into normal responses from HTTPClient exception.
$cinder list --all-tenants
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| 2facb85a-5ea1-4a9c-b615-e1dc75f30bb1 | available |     None     |  1   |     None    |  false   |             |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
1. die cinder-api service
2. boot_from_volume
$curl -v -H ""X-Auth-Token: $TOKEN"" -H ""Content-type: application/json"" -X POST http://192.168.122.180:8774/v2/7020c32ea9384e0291f64f4c1288b397/os-volumes_boot -d '{
    ""server"": {
        ""block_device_mapping"": [
            {
                ""delete_on_termination"": 0,
                ""device_name"": ""vda"",
                ""volume_id"": ""2facb85a-5ea1-4a9c-b615-e1dc75f30bb1"",
                ""volume_size"": 1
            }
        ],
        ""flavorRef"": ""1"",
        ""imageRef"": ""ce5505ca-4d22-4a57-bc63-261a1d6dd664"",
        ""name"": ""test-vm"",
        ""networks"": [
            {
                ""port"": ""cb10dd1d-87e5-421c-9382-32fe933317ea""
            }
        ]
    }
}'
* About to connect() to 192.168.122.180 port 8774 (#0)
*   Trying 192.168.122.180... connected
> POST /v2/7020c32ea9384e0291f64f4c1288b397/os-volumes_boot HTTP/1.1
> User-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3
> Host: 192.168.122.180:8774
> Accept: */*
> X-Auth-Token: --skip--
> Content-type: application/json
> Content-Length: 314
>
* upload completely sent off: 314out of 314 bytes
< HTTP/1.1 400 Bad Request
< Content-Length: 135
< Content-Type: application/json; charset=UTF-8
< X-Compute-Request-Id: req-0a8323af-bd7c-4701-ad5a-afa0774fb213
< Date: Tue, 15 Oct 2013 06:32:31 GMT
<
* Connection #0 to host 192.168.122.180 left intact
* Closing connection #0
{
    ""badRequest"": {
        ""code"": 400,
        ""message"": ""Block Device Mapping is Invalid: failed to get volume 2facb85a-5ea1-4a9c-b615-e1dc75f30bb1.""
    }
}
(For reference, this report is same as openstack-dev mail post.
== [openstack-dev] behaviour about boot-from-volume (possible bug))"
1240022,1240022,nova,a1dae85094fd1efc9c329885d212b684cd12ffa5,1,1,“missing in following files:”,Bug #1240022 “libvirt,"missing in following files:
    nova/virt/libvirt/config.py
    nova/virt/libvirt/driver.py
    nova/virt/libvirt/volume.py"
1240027,1240027,neutron,a269541c603f8923b35b7e722f1b8c0ebd42c95a,0,0,"feature, ""Allow multiple DNS forwarders for dnsmasq”",dhcp_agent only allows one dnsmasq_dns_server,"As quantum.agent.linux.dhcp.Dnsmasq makes use of the --server command line when starting dnsmasq there can only be one server configured for the dnsmasq_dns_server option in dhcp_agent.ini
This is not ideal.
Ideally if a network is not created with its own dns server options, the options provided in dnsmasq_dns_server should be used and put into the opts file just as the network-configured options would be.
This would enable dns to work without dns options configured on the network using defaults provided by the deployer."
1240125,1240125,neutron,202fe60b646429aabeb2ef8b1430a1ba9b356061,1,0,“Handle VLAN interfaces with Linux IP wrapper”,Linux IP wrapper cannot handle VLAN interfaces,"Interface VLAN name have a '@' character in their names when iproute2 utility list them.
But the usable interface name (for iproute2 commands) is the string before the '@' character, so this interface need a special parse.
$ ip link show
1: wlan0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000
    link/ether 6c:88:14:b7:fe:80 brd ff:ff:ff:ff:ff:ff
    inet 169.254.10.78/16 brd 169.254.255.255 scope link wlan0:avahi
       valid_lft forever preferred_lft forever
2: wlan0.10@wlan0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default
    link/ether 6c:88:14:b7:fe:80 brd ff:ff:ff:ff:ff:ff
3: vlan100@wlan0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default
    link/ether 6c:88:14:b7:fe:80 brd ff:ff:ff:ff:ff:ff"
1240247,1240247,nova,0493c803ae9612f87ed028e1a39e880aead5bdcb,1,1, ,API cell always doing local deletes,"It appears a regression was introduced in:
https://review.openstack.org/#/c/36363/
Where the API cell is now always doing a _local_delete()... before telling child cells to delete the instance.  There's at least a couple of bad side effects of this:
1) The instance disappears immediately from API view, even though the instance still exists in the child cell.  The user does not see a 'deleting' task state.  And if the delete fails in the child cell, you have a sync issue until the instance is 'healed'.
2) Double delete.start and delete.end notifications are sent.  1 from API cell, 1 from child cell.
The problem seems to be that _local_delete is being called because the service is determined to be down... because the compute service does not run in the API cell."
1240287,1240287,cinder,ec442e41d2d243003a42ed60ae862e96142a5cde,1,1, ,Bug #1240287 “cinder ,"On the environment where using Cinder LVM driver, extend_volume fails like the following.
# cinder extend 9dd8b44a-6835-40a6-ad9d-a103c25c532d 2
# cinder list
+--------------------------------------+-----------------+----------------+------+-------------+----------+-------------+
|                  ID                  |      Status     |  Display Name  | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------------+----------------+------+-------------+----------+-------------+
| 9dd8b44a-6835-40a6-ad9d-a103c25c532d | error_extending |     test03     |  1   |     None    |  false   |             |
+--------------------------------------+-----------------+----------------+------+-------------+----------+-------------+
2013-10-16 09:00:05.573 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] Error extending Volume
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Traceback (most recent call last):
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm   File ""/opt/openstack/cinder/cinder/brick/local_dev/lvm.py"", line 469, in extend_volume
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm     run_as_root=True)
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm   File ""/opt/openstack/cinder/cinder/utils.py"", line 142, in execute
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm     return processutils.execute(*cmd, **kwargs)
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm   File ""/opt/openstack/cinder/cinder/openstack/common/processutils.py"", line 173, in execute
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm     cmd=' '.join(cmd))
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm ProcessExecutionError: Unexpected error while running command.
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Command: sudo cinder-rootwrap /opt/openstack/cinder/etc/cinder/rootwrap.conf lvextend -L 2 cinder-volumes/volume-9dd8b44a-6835-40a6-ad9d-a103c25c532d
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Exit code: 3
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Stdout: '  Rounding up size to full physical extent 4.00 MiB\n'
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Stderr: ""  New size given (1 extents) not larger than existing size (256 extents)\n  Run `lvextend --help' for more information.\n""
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm
2013-10-16 09:00:05.577 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] Cmd     :sudo cinder-rootwrap /opt/openstack/cinder/etc/cinder/rootwrap.conf lvextend -L 2 cinder-volumes/volume-9dd8b44a-6835-40a6-ad9d-a103c25c532d
2013-10-16 09:00:05.580 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] StdOut  :  Rounding up size to full physical extent 4.00 MiB
2013-10-16 09:00:05.580 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] StdErr  :  New size given (1 extents) not larger than existing size (256 extents)
  Run `lvextend --help' for more information.
# cinder help extend
usage: cinder extend <volume> <new-size>
Attempt to extend the size of an existing volume.
Positional arguments:
  <volume>    Name or ID of the volume to extend.
  <new-size>  New size of volume in GB
In cinder cli, the integer of 'new-size' is in GB .
But In LVM command, (lvextend -L 2 cinder-volumes/volume-9dd8b44a-6835-40a6-ad9d-a103c25c532d ) '-L 2' is in MB.
so, If I really want to extend size of volume, 1 GB to 2 GB, I have to try like the following.
# cinder extend 54a7125f-3a8f-48dc-a8b8-8f623c742ecb 2048
# cinder list
+--------------------------------------+-----------------+--------------+------+-------------+----------+-------------+
|                  ID                  |      Status     | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------------+--------------+------+-------------+----------+-------------+
| 54a7125f-3a8f-48dc-a8b8-8f623c742ecb |    available    |  haha-vol01  | 2048 |     None    |  false   |             |
| 9dd8b44a-6835-40a6-ad9d-a103c25c532d | error_extending |    test03    |  1   |     None    |  false   |             |
+--------------------------------------+-----------------+--------------+------+-------------+----------+-------------+
# lvs
  LV                                          VG             Attr   LSize Origin Snap%  Move Log Copy%  Convert
  volume-54a7125f-3a8f-48dc-a8b8-8f623c742ecb cinder-volumes -wi-ao 2.00g
please check this bug. Thank you."
1240292,1240292,nova,100e962bb989ea3adc8324365dd84f87634691ea,1,1, ,Bug #1240292 “vmware,"I do not have concreate steps to reporduce this but following is the traceback from the logs:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
2013-10-15 16:25:19.677 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: free ram (MB): 0^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:388^[[00m
2013-10-15 16:25:19.677 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: free disk (GB): 0^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:389^[[00m
2013-10-15 16:25:19.678 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: VCPU information unavailable^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:396^[[00m
2013-10-15 16:25:19.678 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: no assignable PCI devices^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:403^[[00m"
1240299,1240299,cinder,de4392314f9b0def8fab65679ec5668aec98fbee,1,1, ,Bug #1240299 “don't secure delete thin provisioned volumes or sn... ,"Performing the secure delete operation on volumes and snapshots writes zeros (or other patterns) across the entire surface of an LVM volume on delete calls.  This is pointless with thin provisioning, and in fact results in defeating the purpose of thin provisioning as it requires actual allocation of all of the blocks.
Add a check before calling lvm.clear_volume and skip this step if thin provisioning is configured."
1240351,1240351,nova,edcc7dcbcbfa50a002767d437ea9cf344d1d7a37,1,1, ,v3 server controller load update extension point w...,"server's controller load the update extension point as below:
       # Look for implmentation of extension point of server update
        self.update_extension_manager = \
            stevedore.enabled.EnabledExtensionManager(
                namespace=self.EXTENSION_UPDATE_NAMESPACE,
                check_func=_check_load_extension('server_resize'),
                invoke_on_load=True,
                invoke_kwds={""extension_info"": self.extension_info},
                propagate_map_exceptions=True)
        if not list(self.update_extension_manager):
But it's checking function with wrong params, it should be _check_load_extension('server_update')."
1240374,1240374,nova,db0831ec96a6806035c51d94ade1e51eafb42162,1,1, ,some times availability-zone not take effect after...,"1. create a availability-zone: second_zone, add ns17 to the aggregate
[root@ns11 nova]# nova availability-zone-list
+----------------------------------+----------------------------------------+
| Name                             | Status                                 |
+----------------------------------+----------------------------------------+
| first_zone                       | available                              |
| |- ns11.sce.cn.ibm.com           |                                        |
| | |- nova-compute                | enabled :-) 2013-09-25T09:51:54.352341 |
| second_zone                      | available                              |
| |- ns17-osee.cn.ibm.com          |                                        |
| | |- nova-compute                | enabled :-) 2013-09-25T09:51:58.283034 |
+----------------------------------+----------------------------------------+
2. nova boot with the zone:
[root@ns11 ˜]# nova boot --image d7c64f67-3de7-4aa0-88fb-1292ff520404 --flavor 1 --availability-zone second_zone:ns17-osee.cn.ibm.com test_zone17_2
3.  [root@ns11 nova]# nova list
+--------------------------------------+-----------------+---------+-------------+-------------+------------------------+
| ID                                   | Name            | Status  | Task State  | Power State | Networks               |
+--------------------------------------+-----------------+---------+-------------+-------------+------------------------+
| 6f3ec942-6c8e-46fa-ad7e-d2c900921f65 | test_zone17     | ACTIVE  | None        | Running     | flat_net01=172.10.0.16 |
| 7273973d-a9e1-4917-ad59-c54ece9b0765 | test_zone17_1   | ACTIVE  | None        | Running     | flat_net01=172.10.0.18 |
| 27f71b26-b775-4516-ba2a-13d9e91e96dd | test_zone17_2   | ACTIVE  | None        | Running     | flat_net01=172.10.0.19 |
| 6350a439-2dd3-4b14-b99e-219fad27111b | zhikun_testvm   | ACTIVE  | None        | Running     | flat_net01=172.10.0.13 |
+--------------------------------------+-----------------+---------+-------------+-------------+------------------------+
4.  but the zone of the vm is: nova
[root@ns11 nova]# nova show 27f71b26-b775-4516-ba2a-13d9e91e96dd
+--------------------------------------+----------------------------------------------------------+
| Property                             | Value                                                    |
+--------------------------------------+----------------------------------------------------------+
| status                               | ACTIVE                                                   |
| flat_net01 network                   | 172.10.0.19                                              |
| updated                              | 2013-09-25T09:50:23Z                                     |
| OS-EXT-STS:task_state                | None                                                     |
| OS-EXT-SRV-ATTR:host                 | ns17-osee.cn.ibm.com                                     |
| key_name                             | None                                                     |
| image                                | cirros_kvm (d7c64f67-3de7-4aa0-88fb-1292ff520404)        |
| accessIPv6                           |                                                          |
| progress                             | 0                                                        |
| OS-EXT-STS:power_state               | 1                                                        |
| OS-EXT-AZ:availability_zone          | nova                                                     |
| config_drive                         |                                                          |
+--------------------------------------+----------------------------------------------------------+
[root@ns11 nova]# date
Wed Sep 25 18:05:26 CST 2013
5.  I did not configure default_zone in nova.conf
6.  It is very strange that this problem could not always been recreated. usually it happens when we create a new AZ, and then use the AZ boot instance. Seems wait for some time, may be one night or sooner, the AZ will become to correct one. I'm confused.
# Expected  results
The AZ should take effect any time if we did not set default AZ in nova.conf"
1240383,1240383,nova,65726540a19442c2882756b26959fdd1088d442e,1,1, ,Bug #1240383 “pci passthrough,"2013-10-17 06:00:23.066 ERROR nova.openstack.common.threadgroup [-] (u'A string is required here, not %s', 'list')
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
objects support now requied known the value you saved in the objects, but pci extra_info design for 'unkonw' usage. so can not get the type info.
refer the trace:
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 117, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     x.wait()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 49, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return self.thread.wait()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return self._exit_event.wait()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return hubs.get_hub().switch()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 65, in run_service
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     service.start()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/service.py"", line 164, in start
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     self.manager.pre_start_hook()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 801, in pre_start_hook
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     self.update_available_resource(nova.context.get_admin_context())
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 4868, in update_available_resource
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     rt.update_available_resource(context)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return f(*args, **kwargs)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 292, in update_available_resource
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     'pci_passthrough_devices')))
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/pci/pci_manager.py"", line 189, in set_hvdevs
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     dev_obj = pci_device.PciDevice.create(dev)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/pci_device.py"", line 174, in create
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     pci_device.update_device(dev_dict)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/pci_device.py"", line 136, in update_device
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     self.extra_info = extra_info
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/base.py"", line 68, in setter
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     field.coerce(self, name, value))
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/fields.py"", line 163, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return self._type.coerce(obj, attr, value)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/fields.py"", line 308, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     obj, '%s[""%s""]' % (attr, key), element)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/fields.py"", line 163, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return self._type.coerce(obj, attr, value)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/fields.py"", line 214, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     value.__class__.__name__)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup ValueError: (u'A string is required here, not %s', 'list')"
1240395,1240395,cinder,cf16029b69f50af1bc2e8627af5bc65ffac24992,1,1, ,Bug #1240395 “storwize,"The Storwize driver mistakenly checks on startup if vdisk_count > 0 for io groups, and fails if this is not the case.  This means that controllers with empty io groups cannot be used."
1240542,1240542,nova,db0daa3455f114e90cefbe02e205ca1d327fc45c,1,1, ,Bug #1240542 “libvirt,"This change: https://review.openstack.org/42877 introduced the ability to pass os_type property to glance when we are doing a snapshot, but the change doesn't manage correctly the None value for this property.
The result of that is that we have a porperty called 'os_type': None associated with the snapshotted image.
That is a problem as this property is used when we create the backing file for the ephemeral disks.
From nova/virt/libvirt/driver/py:
 # Lookup the filesystem type if required
 os_type_with_default = instance['os_type']
 if not os_type_with_default:
     os_type_with_default = 'default'
and then we create the ephemeral file name using this rule:
fname = ""ephemeral_%s_%s"" % (ephemeral_gb, os_type_with_default)
Now consider an instance with a flavor with an ephemeral disk of 50 G and without the os_type defined, the resulting backing file will be:
""ephemeral_50_default""
At this point if we create a snapshot from that instance and we boot from it we will have another backing file created and called:
""ephemeral_50_None""
That is bad for at least two reasons:
1 performance issue: we need to create a new backing file which is not actually necessary
2 resources leaking: those backing files consume disk capacity on the host"
1240572,1240572,neutron,3c00dd43f613c838f713a7cbf3cedb6767a8c52a,1,1, ,ovs_lib.OVSBridge flow managment methods poor desi...,"Unlike 'set_db_attribute()' and 'clear_db_attribute' methods which don't restrict our choice of desired attributes, flow managment methods such as 'add_flow', 'mod_flow' and 'delete_flows' internally make use of '_build_flow_expr_arr()' method, which validates and limits flow parameters. Beside from obvious inconsistency I see the following problems:
- If we misspell flow parameter it just will be ignored, and instead of exception (this is what we would expect) wrong flow will be defined.
- To add more flow options we have to keep hardcoding them in '_build_flow_expr_arr()'
My proposition is to use similar to 'set_db_attribute()' method approach - don't restrict which options we can use, just pack them and execute the command."
1240670,1240670,nova,cc70e3a28df3c8492769b305d248bb4d9bf32830,1,1,,"Nova compute service should be disabled, if libvir...","Nova compute service should be disabled on a node when a connection to libvirt is lost
and resumed when libvirtd become fuctional again.
This is in order to avoid new instances or migrations to be scheduled compute node, when it'd disconnected from libvirt."
1240790,1240790,neutron,d1b783fd673448381445dce82e9160ba2f715a7c,1,1,,Allow using ipv6 address with omiting zero,"Now neutron support ipv6 address like 2001:db8::10:10:10:0/120,
but don't support ipv6 address with omiting zero like 2001:db8:0:0:10:10:10:0/120
that will cause the exception ""'2001:db8:0:0:10:10:10:0/120' isn't a recognized IP subnet cidr, '2001:db8::10:10:10:0/120' is recommended"""
1240922,1240922,nova,d0948a1fb0a4c425310f0cf0aea5b28680dc4817,1,0,“As described in the bug - some hypervisors (libvirt) do not support this.”,VM don't resume after detaching volume,"Hi guys,
I have a suspend vm with an attached volume, if I detached volume while instance is in suspend state it can't be resumed properly.
It happens with both Windows and Linux vm's.
LibVirt error:
2494: error : qemuMonitorIORead:502 : Unable to read from monitor: Connection reset by peer
Packets versioning in Ubuntu 12.04:
ii  libvirt-bin                       1.0.2-0ubuntu11.13.04.2~cloud0              programs for the libvirt library
ii  libvirt-dev                       1.0.2-0ubuntu11.13.04.2~cloud0              development files for the libvirt library
ii  libvirt0                          1.0.2-0ubuntu11.13.04.2~cloud0              library for interfacing with different virtualization systems
ii  python-libvirt                    1.0.2-0ubuntu11.13.04.2~cloud0              libvirt Python bindings
ii  kvm                               1:84+dfsg-0ubuntu16+1.0+noroms+0ubuntu14.10 dummy transitional package from kvm to qemu-kvm
ii  qemu-common                       1.0+noroms-0ubuntu14.10                     qemu common functionality (bios, documentation, etc)
ii  qemu-kvm                          1.0+noroms-0ubuntu14.10                     Full virtualization on i386 and amd64 hardware
ii  nova-common                       1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - common files
ii  nova-compute                      1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - compute node
ii  nova-compute-kvm                  1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - compute node (KVM)
ii  python-nova                       1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute Python libraries
ii  python-novaclient                 1:2.13.0-0ubuntu1~cloud0                    client library for OpenStack Compute API"
1241017,1241017,nova,b9031fb0ad890ad45530ccfdb4ea9df8b715f111,1,1, ,Unshelve bypasses resource tracker,"When unshelving an offloaded instance, a new host is scheduled for the instance. However, the call to unshelve the instance on the new host does not go through the resource tracker, which may lead to over-allocation of the hosts resources."
1241098,1241098,neutron,8d0abf2db701803eaf11c0a8ff4d759b0da5dc39,1,1, ,Bug #1241098 “ML2 Cisco Nexus MD,Split ML2 cisco nexus event handers for update and delete into precommit (called during DB transactions) and postcommit (called after DB transactions) methods.
1241275,1241275,nova,51e5f52e4cb60e266ccde71f205c91eb8c97b48b,1,1, ,Nova / Neutron Client failing upon re-authenticati...,"By default, the token length for clients is 24 hours.  When that token expires (or is invalidated for any reason), nova should obtain a new token.
Currently, when the token expires, it leads to the following fault:
    File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 136, in _get_available_networks
      nets = neutron.list_networks(**search_opts).get('networks', [])
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 108, in with_params
      ret = self.function(instance, *args, **kwargs)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 325, in list_networks
      **_params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1197, in list
      for r in self._pagination(collection, path, **params):
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1210, in _pagination
      res = self.get(path, params=params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1183, in get
      headers=headers, params=params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1168, in retry_request
      headers=headers, params=params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1103, in do_request
      resp, replybody = self.httpclient.do_request(action, method, body=body)
    File ""/usr/lib/python2.6/site-packages/neutronclient/client.py"", line 188, in do_request
      self.authenticate()
    File ""/usr/lib/python2.6/site-packages/neutronclient/client.py"", line 224, in authenticate
      token_url = self.auth_url + ""/tokens""
    TRACE nova.openstack.common.rpc.amqp TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'
This error is occurring because nova/network/neutronv2/__init__.py obtains a token for communication with neutron.  Nova is then authenticating the token (nova/network/neutronv2/__init__.py - _get_auth_token).  Upon authentication, it passes in the token into the neutron client (via the _get_client method).  It should be noted that the token is the main element passed into the neutron client (auth_url, username, password, etc... are not passed in as part of the request)
Since nova is passing the token directly into the neutron client, nova does not validate whether or not the token is authenticated.
After the 24 hour period of time, the token naturally expires.  Therefore, when the neutron client goes to make a request, it catches an exceptions.Unauthorized block.  Upon catching this exception, the neutron client attempts to re-authenticate and then make the request again.
The issue arises in the re-authentication of the token.  The neutron client's authenticate method requires that the following parameters are sent in from its users:
 - username
 - password
 - tenant_id or tenant_name
 - auth_url
 - auth_strategy
Since the nova client is not passing these parameters in, the neutron client is failing with the exception above.
Not all methods from the nova client are exposed to this.  Invocations to nova/network/neutronv2/__init__.py - get_client with an 'admin' value set to True will always get a new token.  However, the clients that invoke the get_client method without specifying the admin flag, or by explicitly setting it to False will be affected by this.  Note that the admin flag IS NOT determined based off the context's admin attribute.
Methods from nova/network/neutronv2/api.py that are currently affected appear to be:
 - _get_available_networks
 - allocate_for_instance
 - deallocate_for_instance
 - deallocate_port_for_instance
 - list_ports
 - show_port
 - add_fixed_ip_to_instance
 - remove_fixed_ip_from_instance
 - validate_networks
 - _get_instance_uuids_by_ip
 - associate_floating_ip
 - get_all
 - get
 - get_floating_ip
 - get_floating_ip_pools
 - get_floating_ip_by_address
 - get_floating_ips_by_project
 - get_instance_id_by_floating_address
 - allocate_floating_ip
 - release_floating_ip
 - disassociate_floating_ip
 - _get_subnets_from_port"
1241337,1241337,nova,de4158861000dedc204314c545cc5682aa38f5f1,1,1,,VM do not resume if attach an volume when suspende...,"1) nova suspend vm2
2) nova attach vm2 6ac2e985-9586-438f-a027-bc9591fa5b43 /dev/sdb
3) nova volume-attach vm2 6ac2e985-9586-438f-a027-bc9591fa5b43 /dev/sdb
4) nova resume vm2
VM failed to resume and nova compute report the following errors.
2013-10-18 12:16:33.175 ERROR nova.openstack.common.rpc.amqp [req-a8b196e3-dbd5-45f4-814e-56a715b07fdf admin admin] Exception during message handling
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 354, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 244, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 230, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 295, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 259, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 3314, in resume_instance
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     block_device_info)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1969, in resume
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     block_device_info)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3206, in _create_domain_and_network
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     {'connection_info': jsonutils.dumps(connection_info)})
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 420, in block_device_mapping_update
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     context, bdm_id, values)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/conductor/api.py"", line 170, in block_device_mapping_update
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     context, values, create=False)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 244, in block_device_mapping_update_or_create
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     values=values, create=create)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/rpcclient.py"", line 85, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return self._invoke(self.proxy.call, ctxt, method, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/rpcclient.py"", line 63, in _invoke
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return cast_or_call(ctxt, msg, **self.kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 126, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     result = rpc.call(context, real_topic, msg, timeout)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/__init__.py"", line 139, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return _get_impl().call(CONF, context, topic, msg, timeout)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/impl_kombu.py"", line 816, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     rpc_amqp.get_connection_pool(conf, Connection))
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 572, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     rv = multicall(conf, context, topic, msg, timeout, connection_pool)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 558, in multicall
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     pack_context(msg, context)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 308, in pack_context
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     for (key, value) in context.to_dict().iteritems()])
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp AttributeError: 'NoneType' object has no attribute 'to_dict'
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp"
1241350,1241350,Nova,fefea36baff5f65c56984ae27074e4ad95a3b511,1,1,The BFC ESTA MAL,Bug #1241350 “VMware,"I found that when I run:
% nova volume-detach my_instance c54ad11f-4e51-41a0-97db-7e551776db59
where the volume with given id is currently attached to my running instance named my_instance, the operation completes successfully. Nevertheless a subsequent attach of the same volume again will fail. So:
% nova volume-attach my_instance c54ad11f-4e51-41a0-97db-7e551776db59 /dev/sdb
fails with the error that the volume's vmdk file is not found.
Cause:
During volume detach a delete_virtual_disk_spec is used to remove the device from the running instance. This spec also ""destroy""s the underlying vmdk file. The offending line is : https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vm_util.py#L471
Possible fix:
The fileOperation field of the device config during this reconfigure operation should be left unset. We should continue setting device_config.operation field to ""remove"". This will remove the device from the VM without deleting the underlying vmdk backing."
1241379,1241379,glance,0cbe012602c7d4240beaa2e4158e885a8ce7de5a,1,1, ,duplicate upload image,"Image status change ""active"" to ""saving"" when duplicate upload image to glance,details are as follows:
create image:
curl -i -X POST -H ""Content-Type:application/json"" -H ""X-Auth-Token:480194c5e9a046b9be8150c80c7f439b"" -d '{""name"":""test_img"",""container_format"":""bare"",""disk_format"":""qcow2"",""visibility"":""public""}' http://127.0.0.1:9292/v2/images
upload image:
curl -i -H ""Content-Type:application/octet-stream"" -H ""X-Auth-Token:480194c5e9a046b9be8150c80c7f439b"" -X PUT -T ./test.img http://127.0.0.1:9292/v2/images/04059ffa-56fa-4f67-a294-fdc10372e634/file
duplicate upload:
curl -i -H ""Content-Type:application/octet-stream"" -H ""X-Auth-Token:480194c5e9a046b9be8150c80c7f439b"" -X PUT -T ./xx.img http://127.0.0.1:9292/v2/images/04059ffa-56fa-4f67-a294-fdc10372e634/file"
1241602,1241602,neutron,01e45596fae2818b41c76b41bef46e86c5d33231,1,0,“Fix update_device_up method of linuxbridge plugin”,AttributeError in plugins/linuxbridge/lb_neutron_p...,"I'm running Ubuntu 12.04 LTS x64 + OpenStack Havana with the following neutron package versions:
neutron-common 2013.2~rc3-0ubuntu1~cloud0
neutron-dhcp-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-l3-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-metadata-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-plugin-linuxbridge 2013.2~rc3-0ubuntu1~cloud0
neutron-plugin-linuxbridge-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-server 2013.2~rc3-0ubuntu1~cloud0
python-neutron 2013.2~rc3-0ubuntu1~cloud0
python-neutronclient 2.3.0-0ubuntu1~cloud0
When adding a router interface the following error message in /var/log/neutron/server.log:
2013-10-18 15:35:14.862 15675 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp     **args)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/common/rpc.py"", line 44, in dispatch
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/linuxbridge/lb_neutron_plugin.py"", line 147, in update_device_up
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp     port = self.get_port_from_device.get_port(device)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp AttributeError: 'function' object has no attribute 'get_port'
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp
2013-10-18 15:35:14.862 15675 ERROR neutron.openstack.common.rpc.common [-] Returning exception 'function' object has no attribute 'get_port' to caller
2013-10-18 15:35:14.863 15675 ERROR neutron.openstack.common.rpc.common [-] ['Traceback (most recent call last):\n', '  File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data\n    **args)\n', '  File ""/usr/lib/python2.7/dist-packages/neutron/common/rpc.py"", line 44, in dispatch\n    neutron_ctxt, version, method, namespace, **kwargs)\n', '  File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n    result = getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/usr/lib/python2.7/dist-packages/neutron/plugins/linuxbridge/lb_neutron_plugin.py"", line 147, in update_device_up\n    port = self.get_port_from_device.get_port(device)\n', ""AttributeError: 'function' object has no attribute 'get_port'\n""]"
1241615,1241615,nova,effa12e8711ff6e23aa2d4b40f4e9f248e141915,1,0,“Which will return the old style (legacy) bdm format if called without legacy=True. “,rebuild with volume attached leaves instance witho...,"I created a backup to an instance which had no volume attached.
attached a volume -> rebuild the instance from the backup.
It appears as though the volume is not attached anymore after the rebuild, but if we try to attach it to the same device we get an error that a device is already attached:
2013-10-18 16:54:36.632 2478 DEBUG qpid.messaging [-] RETR[2fca830]: Message(properties={'x-amqp-0-10.routing-key': u'reply_70fb16e321724b38b3d3face4e83f363'}, content={u'oslo.message': u'{""_unique_id"": ""dd2a85b63c56498c8f2835f9b96e9bb9""
, ""failure"": null, ""_msg_id"": ""7ce524cebbb34aecab9d608a48103a1c"", ""result"": null, ""ending"": true}', u'oslo.version': u'2.0'}) _get /usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py:654
2013-10-18 16:54:36.633 2478 DEBUG qpid.messaging.io.ops [-] SENT[2fa6cb0]: MessageFlow(destination='0', unit=0, value=1L, id=serial(5206)) write_op /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:686
2013-10-18 16:54:36.634 2478 DEBUG qpid.messaging.io.ops [-] SENT[2fa6cb0]: SessionCompleted(commands=[0-5199]) write_op /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:686
2013-10-18 16:54:36.639 2478 ERROR nova.openstack.common.rpc.amqp [req-4a884bb4-5ba6-403d-8be7-df1eeebc1324 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] Exception during message handling
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 90, in wrapped
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 73, in wrapped
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3624, in reserve_block_device_name
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     return do_reserve()
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3613, in do_reserve
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     context, instance, bdms, device)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/utils.py"", line 135, in get_device_name_for_instance
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     mappings['root'], device)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/utils.py"", line 217, in get_next_device_name
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     raise exception.DevicePathInUse(path=device)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp DevicePathInUse: The supplied device path (/dev/vdc) is in use.
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp
2013-10-18 16:54:36.641 2478 ERROR nova.openstack.common.rpc.common [req-4a884bb4-5ba6-403d-8be7-df1eeebc1324 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] Returning exception The supplied device path (/dev/vdc) is i
n use. to caller
2013-10-18 16:54:36.642 2478 ERROR nova.openstack.common.rpc.common [req-4a884bb4-5ba6-403d-8be7-df1eeebc1324 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] ['Traceback (most recent call last):\n', '  File ""/usr/lib/p
ython2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data\n    **args)\n', '  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n    result = getattr(proxyo
bj, method)(ctxt, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 90, in wrapped\n    payload)\n', '  File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 73, in wrapped\n    return f(self, con
text, *args, **kw)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 243, in decorated_function\n    pass\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 229, in decorated_function\
n    return function(self, context, *args, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 271, in decorated_function\n    e, sys.exc_info())\n', '  File ""/usr/lib/python2.6/site-packages/nova/compu
te/manager.py"", line 258, in decorated_function\n    return function(self, context, *args, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3624, in reserve_block_device_name\n    return do_reserve()
\n', '  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner\n    return f(*args, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3613, in do_reserve\n    c
ontext, instance, bdms, device)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/utils.py"", line 135, in get_device_name_for_instance\n    mappings[\'root\'], device)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/u
tils.py"", line 217, in get_next_device_name\n    raise exception.DevicePathInUse(path=device)\n', 'DevicePathInUse: The supplied device path (/dev/vdc) is in use.\n']
2013-10-18 16:54:36.642 2478 DEBUG nova.openstack.common.rpc.amqp [req-4a884bb4-5ba6-403d-8be7-df1eeebc1324 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] UNIQUE_ID is 5d08dde3f40e4186b50e28a9dad5385b. _add_unique_id
/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py:341
If I try to attach the same volume to the same instance on a different device it seems to be working, however, when we try to detach it we get the following error:
2013-10-18 16:58:11.288 2478 DEBUG qpid.messaging [-] RCVD[2fca830]: Message(properties={'x-amqp-0-10.routing-key': u'reply_70fb16e321724b38b3d3face4e83f363'}, content={u'oslo.message': u'{""_unique_id"": ""20a035f66eac46f7b44e8420cd5a2d14""
, ""failure"": null, ""_msg_id"": ""27d7abfd907146c4a0e052cf067a4d69"", ""result"": null, ""ending"": true}', u'oslo.version': u'2.0'}) do_message_transfer /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:1295
2013-10-18 16:58:11.289 2478 DEBUG qpid.messaging [-] RETR[2fca830]: Message(properties={'x-amqp-0-10.routing-key': u'reply_70fb16e321724b38b3d3face4e83f363'}, content={u'oslo.message': u'{""_unique_id"": ""20a035f66eac46f7b44e8420cd5a2d14""
, ""failure"": null, ""_msg_id"": ""27d7abfd907146c4a0e052cf067a4d69"", ""result"": null, ""ending"": true}', u'oslo.version': u'2.0'}) _get /usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py:654
2013-10-18 16:58:11.289 2478 DEBUG qpid.messaging.io.ops [-] SENT[2fa6cb0]: MessageFlow(destination='0', unit=0, value=1L, id=serial(5308)) write_op /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:686
2013-10-18 16:58:11.290 2478 DEBUG qpid.messaging.io.ops [-] SENT[2fa6cb0]: SessionCompleted(commands=[0-5301]) write_op /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:686
2013-10-18 16:58:11.296 2478 ERROR nova.openstack.common.rpc.amqp [req-62e56d8a-9dc1-447c-acef-2b2dafdd5079 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] Exception during message handling
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 90, in wrapped
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 73, in wrapped
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3760, in detach_volume
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     self._detach_volume(context, instance, bdm)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3732, in _detach_volume
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     self.volume_api.roll_detaching(context, volume_id)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3725, in _detach_volume
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     encryption=encryption)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1205, in detach_volume
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     raise exception.DiskNotFound(location=disk_dev)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp DiskNotFound: No disk at vdc
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp
2013-10-18 16:58:11.298 2478 DEBUG qpid.messaging.io.raw [-] SENT[2fa6cb0]: '\x0f\x01\x00\x19\x00\x01\x00\x00\x00\x00\x00\x00\x04\n\x01\x00\x07\x00\x010\x00\x00\x00\x00\x01\x0f\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x02\n\x01\x00\x0
0\x08\x00\x00\x00\x00\x00\x00\x14\xb5' writeable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:480
2013-10-18 16:58:14.265 2478 DEBUG qpid.messaging.io.raw [-] READ[2fa6cb0]: '\x0f\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x01\n\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2013-10-18 16:58:14.266 2478 DEBUG qpid.messaging.io.ops [-] RCVD[2fa6cb0]: ConnectionHeartbeat() write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2013-10-18 16:58:14.291 2478 DEBUG qpid.messaging.io.raw [-] READ[2fca998]: '\x0f\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x01\n\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2013-10-18 16:58:14.291 2478 DEBUG qpid.messaging.io.ops [-] RCVD[2fca998]: ConnectionHeartbeat() write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2013-10-18 16:58:15.019 2478 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_volume_usage run_periodic_tasks /usr/lib/python2.6/site-packages/nova/openstack/common/periodic_task.py:176
2013-10-18 16:58:15.024 2478 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._instance_usage_audit run_periodic_tasks /usr/lib/python2.6/site-packages/nova/openstack/common/periodic_task.py:176
2013-10-18 16:58:15.024 2478 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager.update_available_resource run_periodic_tasks /usr/lib/python2.6/site-packages/nova/openstack/common/periodic_task.py:176
2013-10-18 16:58:15.025 2478 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""compute_resources"" lock /usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py:166
2013-10-18 16:58:15.025 2478 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock ""update_available_resource"" inner /usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py:245
2013-10-18 16:58:15.025 2478 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources"
1241625,1241625,nova,825499fffc7a320466e976d2842e175c2d158c0e,1,1,“Enable non-ascii characters in flavor names”,flavor name is restricted to ascii characters,Flavor name is restricted to [a-zA-Z0-9_.- ] during create. This will not allow characters from languages other than english.
1241681,1241681,nova,0c409d98bb8f04bc1ba9a47d86c13678ab9221e5,1,1, ,compute.instance.delete.end notification emitted w...,"The compute.instance.delete.end notification used to have a value for deleted_at. It seems that deleted_at is being set in the database, but the final notification for a deleted instance is emitted without it. It is important that the final notification for an instance be sent with up-to-date details as it is potentially the last notification that will be sent.
Sample Notification: http://paste.openstack.org/show/48702/"
1241685,1241685,nova,0a47253307d427c6e668d7cdf3bdf186dfc93858,0,0,Bug in comments,wrong code comments in contrib/consoles.py,"@wsgi.action('os-getVNCConsole')
    def get_vnc_console(self, req, id, body):
        """"""Get text console output.""""""
        context = req.environ['nova.context']
        authorize(context)
@wsgi.action('os-getSPICEConsole')
    def get_spice_console(self, req, id, body):
        """"""Get text console output.""""""
        context = req.environ['nova.context']
        authorize(context)"
1242338,1242338,neutron,c9b6c15d5de4d5ee326d7a870c2b2668f7909efa,1,1, ,Trying to remove a load balancer pool (which conta...,"I've tried to remove a pool that has 2 members and a health monitor, operation failed with the following popup:
""Error: Unable to delete pool. 409-{u'NeutronError': {u'message': u'Pool f5004d04-4461-4a9a-aa7c-04a9bdfde974 is still in use', u'type': u'PoolInUse', u'detail': u''}}""
I did expect this operation to fail, I just didn't expect it to be available in horizon while the pool still has other objects associated with it and I didn't expect it to leave the pool in ""PENDING_DELETE"" status.
The exception from the log file:
2013-10-20 16:12:13.564 22804 ERROR neutron.services.loadbalancer.drivers.haproxy.agent_manager [-] Unable to destroy device for pool: f5004d04-4461-4a9a-aa7c-04a9bdfde974
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Traceback (most recent call last):
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/services/loadbalancer/drivers/haproxy/agent_manager.py"", line 244, in destroy_device
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     self.driver.destroy(pool_id)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 92, in destroy
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     ns.garbage_collect_namespace()
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 141, in garbage_collect_namespace
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     self.netns.delete(self.namespace)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 440, in delete
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     self._as_root('delete', name, use_root_namespace=True)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 206, in _as_root
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     kwargs.get('use_root_namespace', False))
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 65, in _as_root
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     namespace)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 76, in _execute
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     root_helper=root_helper)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/utils.py"", line 61, in execute
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     raise RuntimeError(m)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager RuntimeError:
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Command: ['sudo', 'neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'delete', 'qlbaas-f5004d04-4461-4a9a-aa7c-04a9bdfde974']
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Exit code: 255
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Stdout: ''
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Stderr: 'Cannot remove /var/run/netns/qlbaas-f5004d04-4461-4a9a-aa7c-04a9bdfde974: Device or resource busy\n'
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager"
1242366,1242366,nova,ead9cfca93c9b5ca55e3ba269213c98d5e6e1d38,1,1, ,volume attach failed if attach again to an pause t...,"Steps are as following:
1) Create one VM
2) Attach volume to the VM
3) pause the VM
4) detach the volume
5) unpause the VM
6) re-attch the VM to same device, nova compute throw exception
2013-10-20 23:21:22.520 DEBUG amqp [-] Channel open from (pid=19728) _open_ok /usr/local/lib/python2.7/dist-packages/amqp-1.0.12-py2.7.egg/amqp/channel.py:420
2013-10-20 23:21:22.520 ERROR nova.openstack.common.rpc.amqp [req-5f0d786e-1273-4611-b0a5-a787754c6bc8 admin admin] Exception during message handling
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 244, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 230, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 259, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 3649, in attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     context, instance, mountpoint)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 3644, in attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     mountpoint, instance)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 3690, in _attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     connector)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 3680, in _attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     encryption=encryption)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1107, in attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     raise exception.DeviceIsBusy(device=disk_dev)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp DeviceIsBusy: The supplied device (vdb) is busy.
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp
^C2013-10-20 23:21:24.871 INFO nova.openstack.common.service [-] Caught SIGINT, exiting"
1242447,1242447,neutron,83a3e77e63851c290e2f849db6db555cffb1ef41,0,0,“deprecated”,X_TENANT_ID or X_ROLE in keystone auth are not dep...,"In neutron/auth.py, X_TENANT_ID and X_ROLE are used to extract context information.
They are now deprecated and X_PROJECT_ID and X_ROLES are recommended instead.
https://github.com/openstack/python-keystoneclient/blob/master/keystoneclient/middleware/auth_token.py"
1242510,1242510,neutron,31def3ccf425a8f1a617a674a30d29f89d733432,0,0,Bug in test files,ML2 plugin is not loaded properly in TestMl2SGServ...,"ML2 plugin is not loaded properly in TestMl2SGServerRpcCallBack.
By grepping a result in .testrepository, test_extension_security_group.SecurityGroupTestPlugin is loaded as a core plugin in ML2 test. It means Ml2 plugin is not tested in this case.
$ OS_DEBUG=1 python setup.py testr --slowest --testr-args='neutron.tests.unit.ml2'
$ grep ""Loading Plugin"" .testrepository/235 | cut -d ']' -f 2 | cut -d : -f 2 | sort | uniq -c
    835  neutron.plugins.ml2.plugin.Ml2Plugin
     35  neutron.services.l3_router.l3_router_plugin.L3RouterPlugin
     14  neutron.tests.unit.test_extension_security_group.SecurityGroupTestPlugin
    137  neutron.tests.unit.test_l3_plugin.TestL3NatServicePlugin"
1242511,1242511,cinder,cb1d1e4241c6ecb99797a46053c43b2e4b3b52e7,1,1, ,GPFS driver creates volume file on local disk when...,"I enable GPFS driver on one cinder volume node. After I unmount GPFS from this node, the GPFS driver can continue to handle creating volume request, so that the volume file is created on local disk, instead of GPFS NSD.
[root@zhaoqin-RHEL-GPFS1 gpfs]# mmumount fs1
Sun Oct 20 11:35:53 CDT 2013: mmumount: Unmounting file systems ...
[root@zhaoqin-RHEL-GPFS1 gpfs]# mount | grep gpfs
[root@zhaoqin-RHEL-GPFS1 gpfs]#
[root@zhaoqin-RHEL-GPFS1 gpfs]# cinder create 1
+---------------------+--------------------------------------+
|       Property      |                Value                 |
+---------------------+--------------------------------------+
|     attachments     |                  []                  |
|  availability_zone  |                 nova                 |
|       bootable      |                false                 |
|      created_at     |      2013-10-20T16:36:11.236108      |
| display_description |                 None                 |
|     display_name    |                 None                 |
|          id         | 78906f80-4809-49e9-aceb-b21d6fdf226b |
|       metadata      |                  {}                  |
|         size        |                  1                   |
|     snapshot_id     |                 None                 |
|     source_volid    |                 None                 |
|        status       |               creating               |
|     volume_type     |                 None                 |
+---------------------+--------------------------------------+
[root@zhaoqin-RHEL-GPFS1 gpfs]# ls /gpfs/fs1
volume-78906f80-4809-49e9-aceb-b21d6fdf226b
Then, I attempt to remove this volume file using cinder command, but fails.
[root@zhaoqin-RHEL-GPFS1 gpfs]# cinder delete 78906f80-4809-49e9-aceb-b21d6fdf226b
[root@zhaoqin-RHEL-GPFS1 gpfs]# cinder list
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
|                  ID                  |     Status     | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
| 78906f80-4809-49e9-aceb-b21d6fdf226b | error_deleting |     None     |  1   |     None    |  false   |             |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+"
1242532,1242532,neutron,1f4595872c2d06359b78795b406ac42fa99cc7a4,1,1,“missing”,mellanox plugin does not update port state up,There is a missing RPC call from Mellanox neutron agent to neutron server for update device_up once device configuration is applied.
1242549,1242549,cinder,1212f66b188bbe40ac7219908eb47af26ebf2edc,1,1, ,Fail to delete volume snapshot created by GPFS dri...,"I enable one cinder volume node with GPFS driver. Creating/deleting volume works correctly. However, deleting volume snapshot operation fails.
[root@zhaoqin-RHEL-GPFS1 install]# cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| 88a10b05-78d9-495d-beb4-52863c016638 | available | zhaoqin-lvm  |  1   |     lvm     |  false   |             |
| d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b | available | zhaoqin-gpfs |  1   |     gpfs    |  false   |             |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-create --display_name=gpfs_snapshot d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b
+---------------------+--------------------------------------+
|       Property      |                Value                 |
+---------------------+--------------------------------------+
|      created_at     |      2013-10-21T07:20:36.284189      |
| display_description |                 None                 |
|     display_name    |            gpfs_snapshot             |
|          id         | 6274e3a0-24f0-47dc-914d-235a13047b6a |
|       metadata      |                  {}                  |
|         size        |                  1                   |
|        status       |               creating               |
|      volume_id      | d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b |
+---------------------+--------------------------------------+
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-list
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
|                  ID                  |              Volume ID               |     Status     |    Display Name   | Size |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
| 6274e3a0-24f0-47dc-914d-235a13047b6a | d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b |   available    |   gpfs_snapshot   |  1   |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-delete 6274e3a0-24f0-47dc-914d-235a13047b6a
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-list
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
|                  ID                  |              Volume ID               |     Status     |    Display Name   | Size |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
| 6274e3a0-24f0-47dc-914d-235a13047b6a | d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b | error_deleting |   gpfs_snapshot   |  1   |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+"
1242644,1242644,Swift,8825c9c74a67d1cef28b6d2d18d5fbaf40e36f51,1,1, ,Bug #1242644 “proxy-server Error,"Hi,
I run tempest on a freshly installed packages in debian wheezy.
And tests around tempurl fail to compute the hmac in the tempurl middleware
(One of test that failed: tempest.api.object_storage.test_object_temp_url.ObjectTempUrlTest.test_put_object_using_temp_url)
The problem seems due that the HMAC key is 'unicode' instead of 'str'.
Here the error from the proxy-server daemon:
Oct 21 07:26:40 packages-va-wheezy-havana proxy-server Error: character mapping must return integer, None or unicode:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/catch_errors.py"", line 37, in handle_request
    resp = self._app_call(env)
  File ""/usr/lib/python2.7/dist-packages/swift/common/wsgi.py"", line 388, in _app_call
    resp = self.app(env, self._start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/healthcheck.py"", line 57, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/proxy_logging.py"", line 268, in __call__
    iterable = self.app(env, my_start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/memcache.py"", line 67, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/swob.py"", line 1188, in _wsgify_self
    return func(self, Request(env))(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/slo.py"", line 458, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/ratelimit.py"", line 266, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/formpost.py"", line 330, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/tempurl.py"", line 278, in __call__
    hmac_vals = self._get_hmacs(env, temp_url_expires, keys)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/tempurl.py"", line 381, in _get_hmacs
    for key in keys]
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/tempurl.py"", line 404, in _get_hmac
    env['PATH_INFO']), sha1).hexdigest()
  File ""/usr/lib/python2.7/hmac.py"", line 133, in new
    return HMAC(key, msg, digestmod)
  File ""/usr/lib/python2.7/hmac.py"", line 72,
Regards,
sileht"
1245335,1245335,Neutron,fe18aa12d48377e9dae7990396fdaf59c960f687,0,0,Test files,Mix use of test_config['plugin_name_v2'] and plugi...,"In NeutronDbPluginV2TestCase and its subclasses (most DB-based tests), there are two ways to specify a core plugin and an extension manager in the unit tests: test_config and ""plugin"" arguments of the constructor. Both are used and it sometimes makes it a bit difficult to debug.
It is better to unify the way to pass ""core plugin"" and ""extension manager"" into one.
I think it is better to remove test_config['plugin_name_v2'] and use ""plugin"" argument."
1245388,1245388,neutron,946ab2d6a518501f150a59d86428df1e986c6028,0,0,Test files,Bug #1245388 “LBaaS UT,Shouldn't use magic numbers for http error codes in unit tests: use codes from webop.exc like webob.exc.HTTPConflict.code
1245408,1245408,neutron,da139f615673172a1a54fb3065233d568db5e313,1,0,"“The DHCP driver used to use device_delegate, but that has changed to device_manager.” Software evolved",Midonet DhcpNoOpDriver references incorrect member...,"The Midonet DhcpNoOpDriver currently references device_delegate, which is now named device_manager. This bug is to correct the naming."
1245564,1245564,nova,b7bf623d79608df8ff5bf8040dc3590da7eeb236,1,1, ,Bug #1245564 “datastore selection is incorrect if token is being... ,"If the number of datastores is greater than the maximum, the PropertyCollector returns a token value that should be used to iterate over the entire result set. The get_datastore_ref_and_name function in vm_util.py is aware of tokens but it stops on the first found datastore (line 942) instead searching for the best match over the entire set.
We should improve get_datastore_ref_and_name to:
1) clearly specify how the datastore is selected (the current pydoc is wrong)
2) select the best match by iterating the whole result set"
1245694,1245694,Swift,56440eb95da79506cc27d92e07f0f5969cc683ce,0,0,"There is no BUG: “This will probably be added with a currently-in-progress ""feature discoverability"" patch, but it does not exist now.” THE LINK IS WRONG!! It should be:",Report swift server version,"Currently, there is no easy way to get the Swift server version. If I'm not wrong, the only way to do this is to run the following on the swift server/s.
python -c ""import swift; print swift.__version__""
Instead there should be some way for a client to request for the server version."
1245696,1245696,nova,fa1857184db370ff78b9b7344be5f3a1030b862f,1,0, “Fix response code and improve error message”,Fix response code and improve error message networ...,"When booting an instance if the network_uuid is not of a valid network the following error is returned:
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-f86b297f-bbec-4ca1-93f9-f495250f1a3f) (This doesn't really tell the user what resource is not found which should be improved).
In addition  http response code should be 400 not 404 to align with other resources i.e security_groups:
ERROR: Unable to find security_group with name 'asdfasdf' (HTTP 400) (Request-ID: req-ff8b528a-50cf-4ca5-9598-b9ed1a69482d)"
1245719,1245719,nova,85d0ace169be513c30b09e13a35fa7d912f5b380,1,1, ,RBD backed instance can't shutdown and restart,"Version: Havana w/ Ubuntu Repos. with Ceph for RBD.
When creating Launching a instance with ""Boot from image (Creates a new volume)"" this creates the instance fine and all is well however if you shutdown the instance I can't turn it back on again.
I get the following error in the nova-compute.log when trying to power on an shutdown instance.
#######################################################################################
2013-10-29 00:48:33.859 2746 WARNING nova.compute.utils [req-89bbd72f-2280-4fac-802a-1211ec774980 27106b78ceac4e389558566857a7875f 464099f86eb94d049ed1f7b0f0144275] [instance: cc370f6d-4be0-4cd3-9f20-bf86f5ad7c09] Can't access image $
2013-10-29 00:48:34.040 2746 WARNING nova.virt.libvirt.vif [req-89bbd72f-2280-4fac-802a-1211ec774980 27106b78ceac4e389558566857a7875f 464099f86eb94d049ed1f7b0f0144275] Deprecated: The LibvirtHybridOVSBridgeDriver VIF driver is now de$
2013-10-29 00:48:34.578 2746 ERROR nova.openstack.common.rpc.amqp [req-89bbd72f-2280-4fac-802a-1211ec774980 27106b78ceac4e389558566857a7875f 464099f86eb94d049ed1f7b0f0144275] Exception during message handling
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 353, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 294, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1832, in start_instance
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     self._power_on(context, instance)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1819, in _power_on
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     block_device_info)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1948, in power_on
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     self._hard_reboot(context, instance, network_info, block_device_info)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1903, in _hard_reboot
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     block_device_info)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4318, in get_instance_disk_info
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     dk_size = int(os.path.getsize(path))
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/genericpath.py"", line 49, in getsize
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     return os.stat(filename).st_size
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp OSError: [Errno 2] No such file or directory: '/var/lib/nova/instances/cc370f6d-4be0-4cd3-9f20-bf86f5ad7c09/disk'
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp
#######################################################################################
On Closer inspection It seems the libvirt.xml file for the instance gets all screwed up.
This is the libvirt.xml file for this instance before shutdown.
#######################################################################################
<domain type=""kvm"">
  <uuid>dc9749cd-0002-41c9-ac55-5f691637146a</uuid>
  <name>instance-00000004</name>
  <memory>524288</memory>
  <vcpu>1</vcpu>
  <sysinfo type=""smbios"">
    <system>
      <entry name=""manufacturer"">OpenStack Foundation</entry>
      <entry name=""product"">OpenStack Nova</entry>
      <entry name=""version"">2013.2</entry>
      <entry name=""serial"">4c4c4544-0053-3210-8032-b6c04f5a5931</entry>
      <entry name=""uuid"">dc9749cd-0002-41c9-ac55-5f691637146a</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev=""hd""/>
    <smbios mode=""sysinfo""/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset=""utc"">
    <timer name=""pit"" tickpolicy=""delay""/>
    <timer name=""rtc"" tickpolicy=""catchup""/>
  </clock>
  <cpu mode=""host-model"" match=""exact""/>
  <devices>
    <disk type=""network"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source protocol=""rbd"" name=""volumes/volume-5abadeb8-49e9-4628-a54d-d742d6f2e012"">
        <host name=""10.100.96.10"" port=""6789""/>
        <host name=""10.100.96.11"" port=""6789""/>
        <host name=""10.100.96.12"" port=""6789""/>
      </source>
      <auth username=""volumes"">
        <secret type=""ceph"" uuid=""13a673af-ff80-3036-8310-4c72f566673d""/>
      </auth>
      <target bus=""virtio"" dev=""vda""/>
      <serial>5abadeb8-49e9-4628-a54d-d742d6f2e012</serial>
    </disk>
    <interface type=""bridge"">
      <mac address=""fa:16:3e:45:17:d4""/>
      <model type=""virtio""/>
      <source bridge=""qbr125aa659-01""/>
      <target dev=""tap125aa659-01""/>
    </interface>
    <serial type=""file"">
      <source path=""/var/lib/nova/instances/dc9749cd-0002-41c9-ac55-5f691637146a/console.log""/>
    </serial>
    <serial type=""pty""/>
    <input type=""tablet"" bus=""usb""/>
    <graphics type=""vnc"" autoport=""yes"" keymap=""en-us"" listen=""10.100.32.10""/>
  </devices>
</domain>
#######################################################################################
All looks fine here i can see the ceph mons etc.
Next i'll shutdown the instance and as expected the file stays the same now this is the file if i try to power the instance back on again.
you'll notice all of the details regarding the disk have changed and now it thinks its a qcow2 disk located on the local hard drive... how does this happen?
#######################################################################################
<domain type=""kvm"">
  <uuid>dc9749cd-0002-41c9-ac55-5f691637146a</uuid>
  <name>instance-00000004</name>
  <memory>524288</memory>
  <vcpu>1</vcpu>
  <sysinfo type=""smbios"">
    <system>
      <entry name=""manufacturer"">OpenStack Foundation</entry>
      <entry name=""product"">OpenStack Nova</entry>
      <entry name=""version"">2013.2</entry>
      <entry name=""serial"">4c4c4544-0053-3210-8032-b6c04f5a5931</entry>
      <entry name=""uuid"">dc9749cd-0002-41c9-ac55-5f691637146a</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev=""hd""/>
    <smbios mode=""sysinfo""/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset=""utc"">
    <timer name=""pit"" tickpolicy=""delay""/>
    <timer name=""rtc"" tickpolicy=""catchup""/>
  </clock>
  <cpu mode=""host-model"" match=""exact""/>
  <devices>
    <disk type=""file"" device=""disk"">
      <driver name=""qemu"" type=""qcow2"" cache=""none""/>
      <source file=""/var/lib/nova/instances/dc9749cd-0002-41c9-ac55-5f691637146a/disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <interface type=""bridge"">
      <mac address=""fa:16:3e:45:17:d4""/>
      <model type=""virtio""/>
      <source bridge=""qbr125aa659-01""/>
      <target dev=""tap125aa659-01""/>
    </interface>
    <serial type=""file"">
      <source path=""/var/lib/nova/instances/dc9749cd-0002-41c9-ac55-5f691637146a/console.log""/>
    </serial>
    <serial type=""pty""/>
    <input type=""tablet"" bus=""usb""/>
    <graphics type=""vnc"" autoport=""yes"" keymap=""en-us"" listen=""10.100.32.10""/>
  </devices>
</domain>
#######################################################################################"
1245797,1245797,neutron,e19a2ae0b37a5ec74f77d3cdd8e639f2dc580ccf,0,0,Feature3 “MidonetInterfaceDriver should use the mm-ctl script to bind ports.”,MidonetInterfaceDriver does not use mm-ctl to bind...,MidonetInterfaceDriver should use the mm-ctl script to bind ports. Currently it uses Midonet API calls. This bug is to change MidonetInterfaceDriver to correctly use the mm-ctl script.
1245799,1245799,neutron,34f30a1d16d81be86a117690ccc5bdfc5bdd0659,1,1,“It's possible to set '@' or ':' character in Linux device name.”,IP lib fails when int name has '@' character and V...,"IP lib can not distinguish between interfaces with an '@' in their name to a VLAN interfaces.
And an interface name can have more than one '@' in their name."
1245885,1245885,neutron,cd542a7f1f2ea21d3694e70b8e6a6db0efbd3a01,1,0,“Previous default for eswitch/daemon_endpoint would set a TCP port (5001)”,Mellanox Neutron Agent is using keystone port,"Mellanox Neutron agent is configuredby default  to contact Eswitch Daemon using port 5001 which is used by keystone.
It should be changed to use port 60001."
1245909,1245909,cinder,d72914f739b1467ad849dd47fddd321965fed928,1,1, ,"when creating the thin lvm pool, its size is not c...","when the thin lvm pool needs to be created, it is incorrectly sized
a 64G VG results in a 64M POOL
the offending code seems to be here: https://github.com/openstack/cinder/blob/d951222c1440b31f5e994d43b4277344e0d8f63f/cinder/brick/local_dev/lvm.py#L333"
1246079,1246079,cinder,e7319bcebdc225a4b7f784acebbf2cd00f3a3e3b,1,1, ,Nexenta volume driver leaves snapshot uremoved,After deletion of a volume which was created by cloning Nexenta backend still contains snapshot used for cloning.
1246080,1246080,neutron,a0a462f0303f68d885ff344898437e310c64188c,1,1,Wrong logic,Cisco nexus plugin fails to untrunk vlan if other ...,"If two or more compute hosts have instances which are
sharing a given VLAN on a Nexus switch, and then
all instances on one of the hosts which are using that
VLAN are terminated, while instances which are using
that VLAN on other hosts remain active, then
the VLAN is not being untrunked from the
corresponding interface on the Nexus switch as
expected.
Note that the VLAN is correctly untrunked from
the Nexus interface when the instance being
terminated is the last instance which is using that
VLAN on the Nexus switch.
The correct logic should be:
If this the last instance using this VLAN on this switch interface:
____untrunk the vlan from the switch interface
____If this the last instance using this VLAN on this switch:
_________delete the VLAN from the switch
Note that this bug also exists in the Cisco ML2
mechanism driver, but the code which implements
this is being redesigned, so it will be addressed for
the ML2 separately."
1246103,1246103,nova,a3aeace9afd5c533f040452f33482fdb55d93927,1,0,“because it depends on the Python cinderclient library”,encryptors module forces cert and scheduler servic...,"When Nova Scheduler is installed via packstack as the only explicitly installed service on a particular node, it will fail to start.  This is because it depends on the Python cinderclient library, which is not marked as a dependency in 'nova::scheduler' class in Packstack."
1246206,1246206,nova,c27d2dcfa9ee1c91176bb98b0be5765948266e68,1,1,“fix missing datastore”,Bug #1246206 “VMware,Datatsore regex support is missing for the ESX driver
1246228,1246228,Cinder,0278a38153f9649aab1cc641bfabd8d5738d2d8c,1,1,Typo in code,Fix typo in types_manager.py,"Found in commit: 75bcf70ed4698f0ffba4c6a7236a1e30b1214b57
There is a typo in method name ""_notify_voloume_type_error"". It should be ""_notify_volume_type_error""."
1248818,1248818,Swift,d69e013519201af9af7683b1b6dfdf1efa226c7c,1,1,"""the Last-Modified header in Response didn't have a suitable
value""",if-(un)modified-since handler not work with the va...,"~/swift$ git log | head -n1
commit 429cb8b7453662e325d6120457638827527483cd
$ python -c 'import swift; print swift.__version__'
1.10.0.47.g429cb8b
Uploading a new object returns last-modified header
 $ curl -i -XPUT --data-binary '1' -H'x-auth-token:AUTH_tk035f3c147b5842a381a8f69afe6ec99d' http://127.0.0.
1:8080/v1/AUTH_test/cont1/obj1
HTTP/1.1 201 Created
Last-Modified: Thu, 07 Nov 2013 03:35:45 GMT
Content-Length: 0
Etag: c4ca4238a0b923820dcc509a6f75849b
Content-Type: text/html; charset=UTF-8
Date: Thu, 07 Nov 2013 03:35:45 GMT
Then try to GET the object with if-modified-since header value from last-modified returns 200, not 304
$ curl -i -H'if-modified-since: Thu, 07 Nov 2013 03:35:45 GMT' -H'x-auth-token:AUTH_tk035f3c147b5842a381a8
f69afe6ec99d' http://127.0.0.1:8080/v1/AUTH_test/cont1/obj1
HTTP/1.1 200 OK
Content-Length: 1
Accept-Ranges: bytes
Last-Modified: Thu, 07 Nov 2013 03:35:45 GMT
Etag: c4ca4238a0b923820dcc509a6f75849b
X-Timestamp: 1383795345.50550
Content-Type: application/x-www-form-urlencoded
Date: Thu, 07 Nov 2013 03:36:15 GMT
This leads the unnecessary download for most cases including the GET request from most of web browsers
Moreover, a GET request if-unmodified-since value returns 412, not 200
$ curl -i -H'if-unmodified-since: Thu, 07 Nov 2013 03:35:45 GMT' -H'x-auth-token:AUTH_tk035f3c147b5842a381
a8f69afe6ec99d' http://127.0.0.1:8080/v1/AUTH_test/cont1/obj1
HTTP/1.1 412 Precondition Failed
Content-Length: 92
Content-Type: text/html; charset=UTF-8
Date: Thu, 07 Nov 2013 03:36:30 GMT
<html><h1>Precondition Failed</h1><p>A precondition for this request was not met.</p></html>
For the ranged-GET request, the if-unmodified-since headers check the rest of contents are not modified and valid to continue the downloading, but fails always.
The reason the if-(un)modified-since header not working is last-modified header get the time value from x-timestamp value (floating type) and truncate it to integer value. So the last-modified header value always earlier than x-timestamp's one."
1249181,1249181,Swift,aa622eb799556fd8a7c96d9c431b7037eaf439d0,1,1, ,500 Error when getting /recon/diskusage,"I yanked a disk from my system by mistake .Then I execute the command ""curl -v http://proxy-ip:6000/recon/diskusage"" and 500 reported.
It is unreasonable that the recon daemon crash when the only one disk removed while the others disks works fine.
Thanks."
1249188,1249188,neutron,d1220a3c22fccc1d86eaafeebe7ef9c074d4fcb9,1,1, ,[neutron bandwidth metering] When I delete a label...,"[neutron bandwidth metering]
I delete a label using neutronclient CLI (neutron meter-label-delete).
Then router's tenant_id is omitted.
I don' t know why.
it's a bug maybe"
1249222,1249222,nova,cf6bf1c88687da5c18487ddfa2434a3943129847,1,1, ,Update quota-class-set and quota-set thrown 500 er...,"When update quota-class-set without the parameter ""quota_class_set"",the server throws a 500 error:
 ""The server has either erred or is incapable of performing the requested operation.""
Because the server doesn't check whether the parameter ""quota_class_set"" is in request body,the KeyError is not catched.
   for key in body['quota_class_set'].keys():
       ......
I think we should catch the KeyError and transfer the KeyError to 400(HTTPBadRequest) instead of 500.
Update quota-set has the same problem."
1249412,1249412,glance,5f37044d9a851fd02d4f1d918b1c276462a76034,0,0,“The V2 API doesn't not allow empty `container_format` and `disk_format`” feature,Images v2 api allows you to update container and d...,"Images v2 api allows you to update container and disk format after image upload
But does not allow you to upload an image without it being speficied"
1249957,1249957,neutron,cd33dbee09ca8335bcb5ba6975a8edd16258babe,0,0,“This bug is created to add this support.” Feature,Midonet plugin does not associate port at floating...,The Midonet plugin currently does not support associating floating IPs with ports at floating IP creation time. This bug is created to add this support.
1249971,1249971,cinder,2ed2f3aff8eee70e02eb995780af292057cfcad4,1,1, ,update quota sets API does not handle all fail cas...,"The update quota sets API should check whether 'quota_set' is in body.
The code to  update quota sets does the following:
for key in body['quota_set'].keys():"
1250206,1250206,nova,1167a0f808dd746875be50f31d7ebc56f79d45d6,1,1,“This seems to be happening because the Instance.destroy() function does not update the instance with the latest state “,Local delete emits delete.end notification without...,"In cases where Nova falls back to doing a local delete at the API (compute host down), it is possible for the delete.end notification to get emitted without deleted_at being set.
Steps to reproduce (with notifications enabled):
1) Create instance and wait for it to go active
2) Take compute host for the instance down
3) Delete instance
4) Observe compute.instance.delete.end notification with a blank deleted_at
This seems to be happening because the Instance.destroy() function does not update the instance with the latest state from the database after calling db.instance_destroy. It is important that the instance is updated with the data from the database as the driver automatically sets the deleted_at value."
1250228,1250228,glance,53d055d5b0924f1233ca370d94542e918eda095e,1,1,"“On an forbidden update, the message returned to the user is not tied to the type of object modified by the operation.""",Forbidden update to image member says 'image' inst...,"Incorrectly says 'image' instead of 'image member'
curl -i -X PUT -H ""Content-Type: application/json"" -H ""X-Auth-Token: $AUTH_TOKEN"" https://localhost/v2/images/ef4570bf-2e26-4921-810a-5f8499e9822f/members/5855250 -d '{""status"": ""accepted""}'
HTTP/1.1 403 Forbidden
Content-Type: text/html;charset=UTF-8
Via: 1.1 Repose (Repose/2.12)
Content-Length: 177
Date: Fri, 08 Nov 2013 19:13:37 GMT
x-openstack-request-id: req-11c02e0c-d5bd-4f17-ab03-b474b071b3f0
Server: Jetty(8.0.y.z-SNAPSHOT)
<html>
<head>
<title>403 Forbidden</title>
</head>
<body>
<h1>403 Forbidden</h1>
You are not permitted to modify 'status' on this image.<br /><br /> <------------------this should say 'image member'
The issue is in ImmutableMemberProxy in glance/api/authorization.py where it uses the  _immutable_attr function."
1250249,1250249,cinder,9834639454103ee2ba980628f0bc67ad0baa94ff,1,1, ,Bug #1250249 “3PAR,"3PAR Snapshot stuck in Error_Deleting status
Scenario
1. Create a volume vol1
2. Snapshot the volume vol1_snap
3. Create volume from snapshot vol1_snap_to_vol2
4. Try to delete snapshot
Talked to walt, we should be able to recover from this and return status to availalbe after sometime, instead of always being stuck in Error_Deleting status
2013-11-11 15:18:29.135 ERROR cinder.openstack.common.rpc.amqp [req-c8a80e4e-3679-42c6-919e-f82ab06a0970 0d4a18c85ff447d7b0dce7017793ce9c 0982d8dd597b40fb96b67e199137b06d] Exception during message handling
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/utils.py"", line 820, in wrapper
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 425, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     {'status': 'error_deleting'})
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 413, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     self.driver.delete_snapshot(snapshot_ref)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 154, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     self.common.delete_snapshot(snapshot)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 982, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     self.client.deleteVolume(snap_name)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/client.py"", line 216, in deleteVolume
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     response, body = self.http.delete('/volumes/%s' % name)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 327, in delete
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     return self._cs_request(url, 'DELETE', **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 231, in _cs_request
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 205, in _time_request
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     resp, body = self.request(url, method, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 199, in request
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     raise exceptions.from_response(resp, body)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp HTTPConflict: Conflict (HTTP 409) 32 - volume has a child
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp"
1250324,1250324,nova,710cf4e3a7ee3b89e462ec5ff2291e1177556819,1,1, ,Resize on hyperV does not preserve the config driv...,"When we configure this item config_drive_cdrom=True on hyperV, then boot an instance using config drive. A config drive image named 'configdrive.iso' will be created. But when we resize this instance, The config drive image will not preserved in the instance."
1250405,1250405,neutron,e2163abce55bba83f531b4a0c9286b7b329602e9,1,1,“there is mistake in DateTime”,Mistake in sqlalchemy DateTime type usage,"In migration f9263d6df56_remove_dhcp_lease there is mistake in DateTime type usage for mysql, used sa.DATETIME() instead of sa.DateTime()
   File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/f9263d6df56_remove_dhcp_lease.py"", line 46, in downgrade
    nullable=True))
  File ""<string>"", line 7, in add_column
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 363, in add_column
    schema=schema
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/ddl/impl.py"", line 127, in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/ddl/impl.py"", line 76, in _exec
    conn.execute(construct, *multiparams, **params)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
    compiled
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
    context)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
    context)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
    cursor.execute(statement, parameters)
ProgrammingError: (ProgrammingError) type ""datetime"" does not exist
LINE 1: ALTER TABLE ipallocations ADD COLUMN expiration DATETIME
                                                        ^
 'ALTER TABLE ipallocations ADD COLUMN expiration DATETIME' {}"
1250454,1250454,neutron,fe3fae10698ed465f45aec4a4022bc784307b4c3,1,0,“mismatch between 2a6d0b51f4bb_cisco_plugin_cleanup and the initial definition.”,create_table in downgrade does not match table's c...,"create_table in downgrade in migration 2a6d0b51f4bb_cisco_plugin_cleanup does not match table's content created by migration folsom_initial, because of this there is a problem with downgrade.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/2a6d0b51f4bb_cisco_plugin_cleanup.py"", line 87, in downgrade
    sa.PrimaryKeyConstraint(u'id')
  File ""<string>"", line 7, in create_table
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 631, in create_table
    self._table(name, *columns, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 117, in _table
    t = sa_schema.Table(name, m, *columns, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 318, in __new__
    table._init(name, metadata, *args, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 385, in _init
    self._init_items(*args)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 64, in _init_items
    item._set_parent_with_dispatch(self)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/events.py"", line 234, in _set_parent_with_dispatch
    self._set_parent(parent)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 2131, in _set_parent
    ""named '%s' is present."" % (table.description, col))
ArgumentError: Can't create ForeignKeyConstraint on table 'portprofile_bindings': no column named 'ports' is present."
1250574,1250574,neutron,0c85faa733aef714a8e47f647a931fb27857d4b9,1,1,“Add missing quota flags”,Quota l3 flags doesn't set in neutron.conf sample ...,"The quota flags in l3 extensions [1] are not defined in the configuration file sample [2].
[1] https://github.com/openstack/neutron/blob/master/neutron/extensions/l3.py#L139
[2] https://github.com/openstack/neutron/blob/master/etc/neutron.conf#L272"
1250580,1250580,nova,85332012dede96fa6729026c2a90594ea0502ac5,0,0,"Refactoring, “will enable reuse of the already set up client”",Excessive calls to keystone from neutron glue code...,"This has been noticed before, it just came up again with comments in https://bugs.launchpad.net/neutron/+bug/1250168
[12:23] <dims> salv-orlando, for 1250168, where are the keystone calls being made from? (you indicated a large percent of api calls are keystone in the last comment)
[12:35] <salv-mobile> It looks like all calls in admin context trigger a POST to keystone as the admon token is not cachex
[12:35] <salv-mobile> Cached
[12:36] <salv-mobile> I think this was intentional even if I know do not recall the precise reason
[12:40] <dims> salv-mobile, which file? in python-neutronclient?
[12:41] <salv-mobile> I am afk i think it's either nova.network.neutronv2.api or nova.network.neutronv2
[12:41] <dims> salv-mobile, thx, will take a peek
[12:42] <salv-mobile> The latter would be __init_.py of course"
1250633,1250633,glance,f035efd043aa75d662d2a26a963358db59b75628,1,0, ,core domain objects depend on config,"The core of the domain model should be as dependency free as possible.
However, since 830f27ba342ca0881e3677ac11c3d818962cba3e (change id Ic52ffb46df9438c247ba063748cadd69b9c90bcd) we have depended on configuration in glance.domain.__init__.
This configuration should instead be depended on in the infrastructure (probably in glance.gateway) and passed in as an initialization parameter to the domain model objects."
1250644,1250644,neutron,965542bfac90194bd032e5e6aeb6a507dcb11088,1,1, ,Bug #1250644 “dhcp_release not called causing new VMs to fail DH... ,"I've found some situations where dhcp_release is not called when a port is deleted.  When this happens, dnsmasq refused to give out the IP to a new port when the IP address gets recycled.  The result is that the VM with the new port cannot get its IP address on boot.
There are a few conceivable scenarios that lead to this.  I will attempt to describe some in the comments."
1250673,1250673,cinder,d869dee85aa9d32d1a397b954f6583d6bfa60c18,1,1, ,base driver _do_iscsi_discovery relies on volume['...,"The base driver file has a mechanism to attempt an iscsi target discovery in cases where the provider info is not present, this mechanism won't work in multi-backend mode because the sendtargets command is formed using volume['host'] to determine what ip to querie.  In the case of multi-backend this host entry will look something like:  ""cinder-host@backend-name"", the ""@backend-name"" is of course invalid for the iscsiadm call and will result in a failure."
1250767,1250767,neutron,daecbdfdebaf89163f6eaf6b72a4cde2322f1b08,0,0,"Refactoring, it is not a bug “Move Loadbalancer Noop driver to the unit tests”",Bug #1250767 “LBaaS,"As discussed in lbaas design session at the summit, it's better to move noop driver to unit tests folder to clearly indicate that this driver is not for production and should not be listed in providers.
This should avoid possible confusion for Neutron deployers."
1250816,1250816,cinder,b85cc49588c9f40e091801af33da8c7575058ec0,1,1, ,VMware VMDK driver should use host mount count as ...,Currently the VMDK driver picks a datastore for volume creation based on space utilization. The shell VM corresponding to the volume is migrated to a different host and datastore if the volume is attached to an instance created in a host which cannot access the volume's current datastore. These migrations could be minimized if a datastore connected to multiple hosts is picked at the time of volume creation.
1250841,1250841,neutron,d7743bbdc3e992d9f9cceaeaa2919cd70b422364,0,0, “test in files”,Move FWaaS Noop driver to unit tests directory,"Per discussion during I summit, so as to avoid confusion and not be listed in providers."
1251055,1251055,glance,55937308c2020cbc185f656779f5876eae6ca29c,1,1, ,A GET on imagedata in glance v2 returns 404 instea...,"A GET on imagedata in glance v2 returns 404 instead of 204 when data does not exist.
See http://docs.openstack.org/api/openstack-image-service/2.0/content/get-image-file.html
curl -i -X GET -H ""Content-Type: application/json"" -H ""X-Auth-Token: $AUTH_TOKEN"" http://localhost:9292/v2/images/e3a8964d-b320-450d-a729-1735dac05ba9/file
HTTP/1.1 404 Not Found
Content-Length: 150
Content-Type: text/html; charset=UTF-8
X-Openstack-Request-Id: req-fcc598de-6012-4576-a74b-833bfd6e1299
Date: Wed, 13 Nov 2013 20:51:31 GMT
<html>
 <head>
  <title>404 Not Found</title>
 </head>
 <body>
  <h1>404 Not Found</h1>
  No image data could be found<br /><br />
 </body>
</html>"
1251086,1251086,neutron,53886fc7374bc4f0bda1f8f6b80f815d718c81e6,0,0,deprecated code,nvp_cluster_uuid is no longer used in nvp.ini,remove it!
1251126,1251126,cinder,8f8be6815e6a7d26165ee6f185821597e56cb740,0,0,Bug in test files,test_get_dss_rp in test_vmware_vmdk.py not testing...,"The test ""test_get_dss_rp"" in test_vmware_vmdk.py is not testing the positive case as intended. Rather, it tests the negative case (no datastores) which is already covered by ""test_get_dss_rp_without_datastores""."
1251235,1251235,nova,6b9f9e6e9ae2fcf5c733b261717775970a9a4f62,1,1, ,IPv6 DAD failure due to hairpinning,"LibvirtGenericVIFDriver, when using the hybrid bridge method of plugging instances, needs to disable hairpinning to prevent IPv6 ICMPv6 packets from being sent back to the instance, which will cause IPv6 configuration to fail, because the instance will believe that the address it has configured has already been used."
1251261,1251261,nova,81fc3967bca0f154d72d5de7875b646e1b6c47c8,1,1,,finish_revert_migration does not include context a...,"When finish_revert_migration is called, the caller of
finish_revert_migration already includes context as a parameter,
but finish_revert_migration did not reuse this parameter and still
re-generate the context inside finish_revert_migration, we should
add context as a new parameter for finish_revert_migration so that
the functions inside of it can reuse context when needed.
For the function _create_domain_and_network in libvirt/driver.py,
it set context as an optional parameter, but context isn't really
an optional parameter for this method. We should always pass a
context down because it might be needed somewhere inside
_create_domain_and_network()."
1251422,1251422,neutron,ed83265bb794d38f6f66d9e5182a9abc6b82ed53,1,1, ,Bug #1251422 “deleting security-group fails when neutron and nvp... ,"%neutron security-group-list
+--------------------------------------+-----------------------------+--------------------------------+
| id                                   | name                        | description                    |
+--------------------------------------+-----------------------------+--------------------------------+
| 0163fc21-e0d2-4219-9efc-47c71e102ba8 | default                     | default                        |
| 1023c62b-029f-4faf-9257-97ffc870fffd | default                     | default                        |
| 7c096113-2e5d-4fc3-b3ba-8462ec5e6964 | default                     | default                        |
| b9fc7dc0-c22b-45ee-a811-d2a11b7e864f | security-tempest-1342522857 | description-tempest-1430856972 |
| dfc6bff9-b412-4f47-b38b-6d0079412a4d | default                     | default                        |
+--------------------------------------+-----------------------------
%neutron security-group-delete b9fc7dc0-c22b-45ee-a811-d2a11b7e864f
500-{u'NeutronError': {u'message': u'An unknown exception occurred.', u'type': u'NeutronException', u'detail': u''}}
neutron:/var/log/neutron.log
2013-11-14 05:08:05,037 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request [6997] Completed request 'POST https://x.x.x.x:443//ws.v1/security-profile': 201 (0.06 seconds)
2013-11-14 05:08:05,037 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request Reading X-Nvp-config-Generation response header: '37774691'
2013-11-14 05:08:05,038 (neutron.plugins.nicira.api_client.client): DEBUG client release_connection [6997] Released connection https://x.x.x.x:443. 9 connection(s) available.
2013-11-14 05:08:05,038 (neutron.plugins.nicira.api_client.request_eventlet): DEBUG request_eventlet _handle_request [6997] Completed request 'POST /ws.v1/security-profile': 201
2013-11-14 05:08:05,039 (neutron.plugins.nicira.nvplib): DEBUG nvplib create_security_profile Created Security Profile: {u'display_name': u'security-tempest-1342522857', u'_href': u'/ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', u'tags': [{u'scope': u'quan
tum', u'tag': u'2014.1.a230.g5c9c9b9'}, {u'scope': u'os_tid', u'tag': u'csi-tenant-tempest'}], u'logical_port_egress_rules': [{u'ethertype': u'IPv4', u'port_range_max': 68, u'port_range_min': 68, u'protocol': 17}], u'_schema': u'/ws.v1/schema/SecurityProfileConfig', u'log
ical_port_ingress_rules': [{u'ethertype': u'IPv4'}, {u'ethertype': u'IPv6'}], u'uuid': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f'}
2013-11-14 05:08:05,063 (neutron.openstack.common.rpc.amqp): DEBUG amqp notify Sending security_group.create.end on notifications.info
2013-11-14 05:08:05,063 (neutron.openstack.common.rpc.amqp): DEBUG amqp _add_unique_id UNIQUE_ID is 3221f6dd937e4657993532db6910d7fc.
2013-11-14 05:08:05,069 (amqp): DEBUG channel _do_close Closed channel #1
2013-11-14 05:08:05,070 (amqp): DEBUG channel __init__ using channel_id: 1
2013-11-14 05:08:05,071 (amqp): DEBUG channel _open_ok Channel open
2013-11-14 05:08:05,072 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 05:08:05] ""POST /v2.0/security-groups.json HTTP/1.1"" 201 954 0.180143
2013-11-14 05:08:05,280 (keystoneclient.middleware.auth_token): DEBUG auth_token __call__ Authenticating user token
2013-11-14 05:08:05,280 (keystoneclient.middleware.auth_token): DEBUG auth_token _remove_auth_headers Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role
2013-11-14 05:08:05,283 (iso8601.iso8601): DEBUG iso8601 parse_date Parsed 2013-11-15T05:08:04.000000Z into {'tz_sign': None, 'second_fraction': u'000000', 'hour': u'05', 'tz_hour': None, 'month': u'11', 'timezone': u'Z', 'second': u'04', 'tz_minute': None, 'year': u'2013', 'separator': u'T', 'day': u'15', 'minute': u'08'} with default timezone <iso8601.iso8601.Utc object at 0x1798c50>
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'2013' for 'year' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'11' for 'month' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'15' for 'day' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'hour' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'08' for 'minute' with default None
2013-11-14 05:08:05,285 (iso8601.iso8601): DEBUG iso8601 to_int Got u'04' for 'second' with default None
2013-11-14 05:08:05,285 (keystoneclient.middleware.auth_token): DEBUG auth_token _cache_get Returning cached token 2df68c733e72f70e2b09ac49a953f98d
2013-11-14 05:08:05,285 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: tempest with project_id : csi-tenant-tempest and roles: csi-tenant-tempest
2013-11-14 05:08:05,287 (routes.middleware): DEBUG middleware __call__ Matched GET /security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json
2013-11-14 05:08:05,287 (routes.middleware): DEBUG middleware __call__ Route path: '/security-groups/:(id).:(format)', defaults: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>}
2013-11-14 05:08:05,288 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>, 'id': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'format': u'json'}
2013-11-14 05:08:05,303 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 05:08:05] ""GET /v2.0/security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json HTTP/1.1"" 200 949 0.023315
2013-11-14 05:08:05,308 (keystoneclient.middleware.auth_token): DEBUG auth_token __call__ Authenticating user token
2013-11-14 05:08:05,308 (keystoneclient.middleware.auth_token): DEBUG auth_token _remove_auth_headers Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 parse_date Parsed 2013-11-15T05:08:04.000000Z into {'tz_sign': None, 'second_fraction': u'000000', 'hour': u'05', 'tz_hour': None, 'month': u'11', 'timezone': u'Z', 'second': u'04', 'tz_minute': None, 'year': u'2013', 'separator': u'T', 'day': u'15', 'minute': u'08'} with default timezone <iso8601.iso8601.Utc object at 0x1798c50>
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 to_int Got u'2013' for 'year' with default None
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 to_int Got u'11' for 'month' with default None
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 to_int Got u'15' for 'day' with default None
2013-11-14 05:08:05,312 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'hour' with default None
2013-11-14 05:08:05,312 (iso8601.iso8601): DEBUG iso8601 to_int Got u'08' for 'minute' with default None
2013-11-14 05:08:05,312 (iso8601.iso8601): DEBUG iso8601 to_int Got u'04' for 'second' with default None
2013-11-14 05:08:05,312 (keystoneclient.middleware.auth_token): DEBUG auth_token _cache_get Returning cached token 2df68c733e72f70e2b09ac49a953f98d
2013-11-14 05:08:05,312 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: tempest with project_id : csi-tenant-tempest and roles: csi-tenant-tempest
2013-11-14 05:08:05,314 (routes.middleware): DEBUG middleware __call__ Matched POST /security-group-rules.json
2013-11-14 05:08:05,314 (routes.middleware): DEBUG middleware __call__ Route path: '/security-group-rules.:(format)', defaults: {'action': u'create', 'controller': <wsgify at 41022736 wrapping <function resource at 0x270aed8>>}
2013-11-14 05:08:05,315 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'create', 'controller': <wsgify at 41022736 wrapping <function resource at 0x270aed8>>, 'format': u'json'}
2013-11-14 05:08:05,316 (neutron.openstack.common.rpc.amqp): DEBUG amqp notify Sending security_group_rule.create.start on notifications.info
2013-11-14 05:08:05,317 (neutron.openstack.common.rpc.amqp): DEBUG amqp _add_unique_id UNIQUE_ID is 66fa31f3dc8d4f2eb9cd7fb561a17f49.
2013-11-14 05:08:05,324 (amqp): DEBUG channel __init__ using channel_id: 1
2013-11-14 05:08:05,325 (amqp): DEBUG channel _open_ok Channel open
2013-11-14 05:08:05,383 (keystoneclient.middleware.auth_token): DEBUG auth_token __call__ Authenticating user token
2013-11-14 05:08:05,383 (keystoneclient.middleware.auth_token): DEBUG auth_token _remove_auth_headers Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role
2013-11-14 05:08:05,394 (neutron.plugins.nicira.api_client.client): DEBUG client acquire_connection [6998] Acquired connection https://x.x.x.x:443. 8 connection(s) available.
2013-11-14 05:08:05,394 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request [6998] Issuing - request PUT https://x.x.x.x:443//ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f
2013-11-14 05:08:05,394 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request Setting X-Nvp-Wait-For-Config-Generation request header: '37774691'
2013-11-14 05:08:05,400 (iso8601.iso8601): DEBUG iso8601 parse_date Parsed 2013-11-15T05:08:05Z into {'tz_sign': None, 'second_fraction': None, 'hour': u'05', 'tz_hour': None, 'month': u'11', 'timezone': u'Z', 'second': u'05', 'tz_minute': None, 'year': u'2013', 'separator': u'T', 'day': u'15', 'minute': u'08'} with default timezone <iso8601.iso8601.Utc object at 0x1798c50>
2013-11-14 05:08:05,400 (iso8601.iso8601): DEBUG iso8601 to_int Got u'2013' for 'year' with default None
2013-11-14 05:08:05,400 (iso8601.iso8601): DEBUG iso8601 to_int Got u'11' for 'month' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'15' for 'day' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'hour' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'08' for 'minute' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'second' with default None
2013-11-14 05:08:05,401 (keystoneclient.middleware.auth_token): DEBUG auth_token _cache_put Storing 81224fd97e0fca9805338cbbb08a3900 token in memcache
2013-11-14 05:08:05,402 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: service-user with project_id : csi-tenant-admin and roles: csi-tenant-admin,csi-role-admin
2013-11-14 05:08:05,404 (routes.middleware): DEBUG middleware __call__ No route matched for GET /ports.json
2013-11-14 05:08:05,405 (routes.middleware): DEBUG middleware __call__ Matched GET /ports.json
2013-11-14 05:08:05,405 (routes.middleware): DEBUG middleware __call__ Route path: '/ports{.format}', defaults: {'action': u'index', 'controller': <wsgify at 40952784 wrapping <function resource at 0x270ad70>>}
2013-11-14 05:08:05,405 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'index', 'controller': <wsgify at 40952784 wrapping <function resource at 0x270ad70>>, 'format': u'json'}
2013-11-14 05:08:05,423 (neutron.wsgi): INFO log write 10.140.129.30,x.x.x.x - - [14/Nov/2013 05:08:05] ""GET /v2.0/ports.json?network_id=07490058-67bd-4038-9ae2-bcc7aa973b02&device_owner=network%3Adhcp HTTP/1.1"" 200 136 0.039706
2013-11-14 05:08:05,456 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request [6998] Completed request 'PUT https://x.x.x.x:443//ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f': 200 (0.06 seconds)
2013-11-14 05:08:05,456 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request Reading X-Nvp-config-Generation response header: '37774694'
2013-11-14 05:08:05,457 (neutron.plugins.nicira.api_client.client): DEBUG client release_connection [6998] Released connection https://x.x.x.x:443. 9 connection(s) available.
2013-11-14 05:08:05,457 (neutron.plugins.nicira.api_client.request_eventlet): DEBUG request_eventlet _handle_request [6998] Completed request 'PUT /ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f': 200
2013-11-14 05:08:05,458 (neutron.plugins.nicira.nvplib): DEBUG nvplib update_security_group_rules Updated Security Profile: {u'display_name': u'security-tempest-1342522857', u'_href': u'/ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', u'tags': [{u'scope': u'quantum', u'tag': u'2014.1.a230.g5c9c9b9'}, {u'scope': u'os_tid', u'tag': u'csi-tenant-tempest'}], u'logical_port_egress_rules': [{u'ethertype': u'IPv4', u'port_range_max': 22, u'port_range_min': 22, u'protocol': 6}, {u'ethertype': u'IPv4', u'port_range_max': 68, u'port_range_min': 68, u'protocol': 17}], u'_schema': u'/ws.v1/schema/SecurityProfileConfig', u'logical_port_ingress_rules': [{u'ethertype': u'IPv4'}, {u'ethertype': u'IPv6'}], u'uuid': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f'}
2013-11-14 05:08:05,490 (neutron.openstack.common.rpc.amqp): DEBUG amqp notify Sending security_group_rule.create.end on notifications.info
2013-11-14 05:08:05,490 (neutron.openstack.common.rpc.amqp): DEBUG amqp _add_unique_id UNIQUE_ID is d7b3139142a04511867d57090f7e22cf.
2013-11-14 05:08:05,497 (amqp): DEBUG channel _do_close Closed channel #1
2013-11-14 05:08:11,993 (routes.middleware): DEBUG middleware __call__ Matched GET /security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json
2013-11-14 05:08:11,993 (routes.middleware): DEBUG middleware __call__ Route path: '/security-groups/:(id).:(format)', defaults: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>}
2013-11-14 05:08:11,993 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>, 'id': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'format': u'json'}
2013-11-14 05:08:12,001 (neutron.api.v2.resource): ERROR resource resource show failed
Traceback (most recent call last):
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
    result = method(request=request, **args)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 290, in show
    parent_id=parent_id),
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 258, in _item
    obj = obj_getter(request.context, id, **kwargs)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/db/securitygroups_db.py"", line 180, in get_security_group
    context, id), fields)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/db/securitygroups_db.py"", line 194, in _get_security_group
    raise ext_sg.SecurityGroupNotFound(id=id)
SecurityGroupNotFound: Security group b9fc7dc0-c22b-45ee-a811-d2a11b7e864f does not exist
2013-11-14 05:08:12,002 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 05:08:12] ""GET /v2.0/security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json HTTP/1.1"" 404 277 0.032987
* Delete
2013-11-14 18:50:04,498 (neutron.plugins.nicira.api_client.request_eventlet): DEBUG request_eventlet _handle_request [8422] Completed request 'DELETE /ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f': 404
2013-11-14 18:50:04,498 (NVPApiHelper): ERROR NvpApiClient request Received error code: 404
2013-11-14 18:50:04,498 (NVPApiHelper): ERROR NvpApiClient request Server Error Message: Security Profile 'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f' not registered.
2013-11-14 18:50:04,498 (neutron.plugins.nicira.nvplib): ERROR nvplib delete_security_profile Error. Unknown exception: An unknown exception occurred.. locals=[{'path': u'/ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'e': NotFound(u'An unknown exception occurred.',), 'spid': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'cluster': <neutron.plugins.nicira.nvp_cluster.NVPCluster object at 0x2d96f10>}]
2013-11-14 18:50:04,499 (neutron.api.v2.resource): ERROR resource resource delete failed
Traceback (most recent call last):
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
    result = method(request=request, **args)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
    obj_deleter(request.context, id, **kwargs)
    security_group['id'])
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/nicira/nvplib.py"", line 1148, in delete_security_profile
    raise exception.NeutronException()
NeutronException: An unknown exception occurred.
2013-11-14 18:50:04,500 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 18:50:04] ""DELETE /v2.0/security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json HTTP/1.1"" 500 248 0.071508"
1251589,1251589,nova,29f2d666e9ad64fd646d4711fd32cf325e039f5b,1,1, ,host update disable/enable report HTTP 400,"liugya@liugya-ubuntu:~$ nova host-update --status disable liugya-ubuntu
ERROR: Bad request (HTTP 400) (Request-ID: req-f8c083dc-a327-48dc-b705-293f03e834dd)
liugya@liugya-ubuntu:~$ nova service-list
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| Binary           | Host          | Zone     | Status   | State | Updated_at                 | Disabled Reason |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| nova-conductor   | liugya-ubuntu | internal | enabled  | up    | 2013-11-15T11:06:46.000000 | None            |
| nova-compute     | liugya-ubuntu | nova     | disabled | up    | 2013-11-15T11:06:42.000000 |                 | << disabled already
| nova-cert        | liugya-ubuntu | internal | enabled  | up    | 2013-11-15T11:06:38.000000 | None            |
| nova-network     | liugya-ubuntu | internal | enabled  | up    | 2013-11-15T11:06:39.000000 | None            |
| nova-scheduler   | liugya-ubuntu | internal | enabled  | up    | 2013-11-15T11:06:40.000000 | None            |
| nova-consoleauth | liugya-ubuntu | internal | enabled  | up    | 2013-11-15T11:06:45.000000 | None            |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+"
1251602,1251602,nova,f35c63df24d8f47927d055729e9ca08c7bc3fe8c,1,1,“missing exception info”,Some error notifications are missing exception inf...,"At the moment we are quite inconsistent with some of the error notifications.
Those triggered by the wrappers of compute manager calls are good. However:
* where an instance fault is registered inline, we don't always send an error notification
* where we send an error notification inline, we generally don't send the message and exception information in a consistent way
I am looking to resolve that, so there is more consistency when raising error notifications in the compute manager."
1251757,1251757,cinder,cbd7acfc294c4bf2ae0014bcf04646382bed469d,1,0,“Sync the following fixes from oslo-incubator:” The bug was in Oslo and Cinder only need to sync so it didn’t cause the bug.,"On restart of QPID broker, fanout no longer works","When the QPID broker is restarted, RPC servers attempt to re-connect.  This re-connection process is not done correctly for fanout subscriptions - two subscriptions are established to the same fanout address.
This problem is compounded by the fix to bug#1178375 https://bugs.launchpad.net/oslo/+bug/1178375
With this bug fix, when topology version 2 is used, the reconnect attempt uses a malformed subscriber address.
For example, I have a simple RPC server script that attempts to service ""my-topic"".   When it initially connects to the broker using topology-version 1, these are the subscriptions that are established:
(py27)[kgiusti@t530 work (master)]$ ./my-server.py --topology=1 --auto-delete server-02
Running server, name=server-02 exchange=my-exchange topic=my-topic namespace=my-namespace
Using QPID topology version 1
Enable auto-delete
Recevr openstack/my-topic ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic""}}
Recevr openstack/my-topic.server-02 ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic.server-02""}}
Recevr my-topic_fanout ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": false, ""type"": ""fanout""}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true, ""durable"": false}, ""durable"": true, ""name"": ""my-topic_fanout_489a3178fc704123b0e5e2fbee125247""}}
When I restart the qpid broker, the server reconnects using the following subscriptions
Recevr my-topic_fanout ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": false, ""type"": ""fanout""}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true, ""durable"": false}, ""durable"": true, ""name"": ""my-topic_fanout_b40001afd9d946a582ead3b7b858b588""}}
Recevr my-topic_fanout ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": false, ""type"": ""fanout""}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true, ""durable"": false}, ""durable"": true, ""name"": ""my-topic_fanout_b40001afd9d946a582ead3b7b858b588""}}
^^^^--- Note: subscribing twice to the same exclusive address!  (Bad!)
Recevr openstack/my-topic.server-02 ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic.server-02""}}
Recevr openstack/my-topic ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic""}}
When using topology=2, the failure case is a bit different.  On reconnect, the fanout addresses are lacking proper topic names:
Recevr amq.topic/topic/openstack/my-topic ; {""link"": {""x-declare"": {""auto-delete"": true, ""durable"": false}}}
Recevr amq.topic/fanout/ ; {""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true}}}
Recevr amq.topic/fanout/ ; {""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true}}}
Recevr amq.topic/topic/openstack/my-topic.server-02 ; {""link"": {""x-declare"": {""auto-delete"": true, ""durable"": false}}}
Note again - two subscriptions to fanout, and 'my-topic' is missing (it should be after that trailing /)
FYI - my test RPC server and client can be accessed here: https://github.com/kgiusti/oslo-messaging-clients"
1251803,1251803,nova,e1fdd8f9b1536c75e36dd74416dd3aad1495e203,1,1, ,Disabled Reason column becomes empty after update ...,"liugya@liugya-ubuntu:~$ nova host-update --status disable liugya-ubuntu
liugya@liugya-ubuntu:~$ nova service-list
/usr/lib/python2.7/dist-packages/gobject/constants.py:24: Warning: g_boxed_type_register_static: assertion `g_type_from_name (name) == 0' failed
  import gobject._gobject
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| Binary           | Host          | Zone     | Status   | State | Updated_at                 | Disabled Reason |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| nova-conductor   | liugya-ubuntu | internal | enabled  | up    | 2013-11-16T03:06:33.000000 | None            |
| nova-compute     | liugya-ubuntu | nova     | disabled | up    | 2013-11-16T03:06:29.000000 |                 | <<<<<<< Reason is empty now
| nova-cert        | liugya-ubuntu | internal | enabled  | up    | 2013-11-16T03:06:34.000000 | None            |
| nova-network     | liugya-ubuntu | internal | enabled  | up    | 2013-11-16T03:06:26.000000 | None            |
| nova-scheduler   | liugya-ubuntu | internal | enabled  | up    | 2013-11-16T03:06:31.000000 | None            |
| nova-consoleauth | liugya-ubuntu | internal | enabled  | up    | 2013-11-16T03:06:32.000000 | None            |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+"
1251822,1251822,nova,65ae11285fc2e3e11d5d07ad1e253f1a4f8ebccf,1,1, ,nova compute log incorrect message when enable/dis...,"nova host-update --status enable liugya-ubuntu
DEBUG nova.virt.libvirt.driver [req-e4a1d1cc-9799-4748-b065-6bb938126134 None None] Updating compute service status to: disabled from (pid=21833) set_host_enabled /opt/stack/nova/nova/virt/libvirt/driver.py:2628  << This should be ""updating to enabled""
 nova host-update --status disable liugya-ubuntu
DEBUG nova.virt.libvirt.driver [req-90b2cb09-e995-4676-8a1e-7774bb8a0f12 admin admin] Updating compute service status to: enabled from (pid=21833) set_host_enabled /opt/stack/nova/nova/virt/libvirt/driver.py:2628 << This should be ""updating to disabled"""
1251920,1251920,nova,9b44d9d75d3767c6b7d1af4f237da8c1cf16266a,1,1,“local has a broken TLS symbol”,Tempest failures due to failure to return console ...,"Logstash search: http://logstash.openstack.org/#eyJzZWFyY2giOiJmaWxlbmFtZTpjb25zb2xlLmh0bWwgQU5EIG1lc3NhZ2U6XCJhc3NlcnRpb25lcnJvcjogY29uc29sZSBvdXRwdXQgd2FzIGVtcHR5XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzODQ2NDEwNzIxODl9
An example failure is http://logs.openstack.org/92/55492/8/check/check-tempest-devstack-vm-full/ef3a4a4/console.html
console.html
===========
2013-11-16 21:54:27.998 | 2013-11-16 21:41:20,775 Request: POST http://127.0.0.1:8774/v2/3f6934d9aabf467aa8bc51397ccfa782/servers/10aace14-23c1-4cec-9bfd-2c873df1fbee/action
2013-11-16 21:54:27.998 | 2013-11-16 21:41:20,776 Request Headers: {'Content-Type': 'application/json', 'Accept': 'application/json', 'X-Auth-Token': '<Token omitted>'}
2013-11-16 21:54:27.998 | 2013-11-16 21:41:20,776 Request Body: {""os-getConsoleOutput"": {""length"": 10}}
2013-11-16 21:54:27.998 | 2013-11-16 21:41:21,000 Response Status: 200
2013-11-16 21:54:27.999 | 2013-11-16 21:41:21,001 Nova request id: req-7a2ee0ab-c977-4957-abb5-1d84191bf30c
2013-11-16 21:54:27.999 | 2013-11-16 21:41:21,001 Response Headers: {'content-length': '14', 'date': 'Sat, 16 Nov 2013 21:41:20 GMT', 'content-type': 'application/json', 'connection': 'close'}
2013-11-16 21:54:27.999 | 2013-11-16 21:41:21,001 Response Body: {""output"": """"}
2013-11-16 21:54:27.999 | }}}
2013-11-16 21:54:27.999 |
2013-11-16 21:54:27.999 | Traceback (most recent call last):
2013-11-16 21:54:27.999 |   File ""tempest/api/compute/servers/test_server_actions.py"", line 281, in test_get_console_output
2013-11-16 21:54:28.000 |     self.wait_for(get_output)
2013-11-16 21:54:28.000 |   File ""tempest/api/compute/base.py"", line 133, in wait_for
2013-11-16 21:54:28.000 |     condition()
2013-11-16 21:54:28.000 |   File ""tempest/api/compute/servers/test_server_actions.py"", line 278, in get_output
2013-11-16 21:54:28.000 |     self.assertTrue(output, ""Console output was empty."")
2013-11-16 21:54:28.000 |   File ""/usr/lib/python2.7/unittest/case.py"", line 420, in assertTrue
2013-11-16 21:54:28.000 |     raise self.failureException(msg)
2013-11-16 21:54:28.001 | AssertionError: Console output was empty.
n-api
====
2013-11-16 21:41:20.782 DEBUG nova.api.openstack.wsgi [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] Action: 'action', body: {""os-getConsoleOutput"": {""length"": 10}} _process_stack /opt/stack/new/nova/nova/api/openstack/wsgi.py:963
2013-11-16 21:41:20.782 DEBUG nova.api.openstack.wsgi [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] Calling method <bound method ConsoleOutputController.get_console_output of <nova.api.openstack.compute.contrib.console_output.ConsoleOutputController object at 0x3c1f990>> _process_stack /opt/stack/new/nova/nova/api/openstack/wsgi.py:964
2013-11-16 21:41:20.865 DEBUG nova.openstack.common.rpc.amqp [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] Making synchronous call on compute.devstack-precise-hpcloud-az2-663635 ... multicall /opt/stack/new/nova/nova/openstack/common/rpc/amqp.py:553
2013-11-16 21:41:20.866 DEBUG nova.openstack.common.rpc.amqp [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] MSG_ID is a93dceabf6a441eb850b5fbb012d661f multicall /opt/stack/new/nova/nova/openstack/common/rpc/amqp.py:556
2013-11-16 21:41:20.866 DEBUG nova.openstack.common.rpc.amqp [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] UNIQUE_ID is 706ab69dc066440fbe1bd7766b73d953. _add_unique_id /opt/stack/new/nova/nova/openstack/common/rpc/amqp.py:341
2013-11-16 21:41:20.869 22679 DEBUG amqp [-] Closed channel #1 _do_close /usr/local/lib/python2.7/dist-packages/amqp/channel.py:95
2013-11-16 21:41:20.869 22679 DEBUG amqp [-] using channel_id: 1 __init__ /usr/local/lib/python2.7/dist-packages/amqp/channel.py:71
2013-11-16 21:41:20.870 22679 DEBUG amqp [-] Channel open _open_ok /usr/local/lib/python2.7/dist-packages/amqp/channel.py:429
2013-11-16 21:41:20.999 INFO nova.osapi_compute.wsgi.server [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] 127.0.0.1 ""POST /v2/3f6934d9aabf467aa8bc51397ccfa782/servers/10aace14-23c1-4cec-9bfd-2c873df1fbee/action HTTP/1.1"" status: 200 len: 205 time: 0.2208662
n-cpu
=====
2013-11-16 21:41:20.878 23086 DEBUG nova.openstack.common.rpc.amqp [-] received {u'_msg_id': u'a93dceabf6a441eb850b5fbb012d661f', u'_context_quota_class': None, u'_context_request_id': u'req-7a2ee0ab-c977-4957-abb5-1d84191bf30c', u'_context_service_catalog': [{u'endpoints_links': [], u'endpoints': [{u'adminURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782', u'region': u'RegionOne', u'publicURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782', u'internalURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782', u'id': u'5725cc234a8346408dfe343aea62d3aa'}], u'type': u'volume', u'name': u'cinder'}], u'_context_auth_token': '<SANITIZED>', u'_context_user_id': u'eae9401ba3794296ae8248748ef8e26e', u'_reply_q': u'reply_546cb0ef381e4ff29c76ee96eba21c22', u'namespace': None, u'_context_is_admin': False, u'version': u'2.0', u'_context_timestamp': u'2013-11-16T21:41:20.781052', u'_context_user': u'eae9401ba3794296ae8248748ef8e26e', u'method': u'get_console_output', u'_context_remote_address': u'127.0.0.1', u'_context_roles': [u'_member_'], u'args': {u'instance': {u'vm_state': u'active', u'availability_zone': None, u'terminated_at': None, u'ephemeral_gb': 0, u'instance_type_id': 6, u'user_data': None, u'cleaned': False, u'vm_mode': None, u'deleted_at': None, u'reservation_id': u'r-dnps4t4l', u'id': 40, u'security_groups': [{u'deleted_at': None, u'user_id': u'eae9401ba3794296ae8248748ef8e26e', u'description': u'default', u'deleted': False, u'created_at': u'2013-11-16T21:37:03.000000', u'updated_at': None, u'project_id': u'3f6934d9aabf467aa8bc51397ccfa782', u'id': 47, u'name': u'default'}], u'disable_terminate': False, u'root_device_name': u'/dev/vda', u'display_name': u'ServerActionsTestJSON-instance-tempest-308881308', u'uuid': u'10aace14-23c1-4cec-9bfd-2c873df1fbee', u'default_swap_device': None, u'info_cache': {u'instance_uuid': u'10aace14-23c1-4cec-9bfd-2c873df1fbee', u'deleted': False, u'created_at': u'2013-11-16T21:37:03.000000', u'updated_at': u'2013-11-16T21:37:08.000000', u'network_info': [{u'ovs_interfaceid': None, u'network': {u'bridge': u'br100', u'label': u'private', u'meta': {u'tenant_id': None, u'should_create_bridge': True, u'bridge_interface': u'eth0'}, u'id': u'44a93331-6869-4e8a-8742-f92bb9cea7b5', u'subnets': [{u'ips': [{u'meta': {}, u'type': u'fixed', u'floating_ips': [], u'version': 4, u'address': u'10.1.0.6'}], u'version': 4, u'meta': {u'dhcp_server': u'10.1.0.1'}, u'dns': [{u'meta': {}, u'type': u'dns', u'version': 4, u'address': u'8.8.4.4'}], u'routes': [], u'cidr': u'10.1.0.0/24', u'gateway': {u'meta': {}, u'type': u'gateway', u'version': 4, u'address': u'10.1.0.1'}}, {u'ips': [], u'version': None, u'meta': {u'dhcp_server': None}, u'dns': [], u'routes': [], u'cidr': None, u'gateway': {u'meta': {}, u'type': u'gateway', u'version': None, u'address': None}}]}, u'devname': None, u'qbh_params': None, u'meta': {}, u'address': u'fa:16:3e:38:6d:dd', u'type': u'bridge', u'id': u'2bd53919-68f4-4fdb-bd51-dfc3238c6cbb', u'qbg_params': None}], u'deleted_at': None}, u'hostname': u'serveractionstestjson-instance-tempest-308881308', u'launched_on': u'devstack-precise-hpcloud-az2-663635', u'display_description': u'ServerActionsTestJSON-instance-tempest-308881308', u'key_data': None, u'deleted': False, u'config_drive': u'', u'power_state': 1, u'default_ephemeral_device': None, u'progress': 0, u'project_id': u'3f6934d9aabf467aa8bc51397ccfa782', u'launched_at': u'2013-11-16T21:37:18.000000', u'scheduled_at': u'2013-11-16T21:37:04.000000', u'node': u'devstack-precise-hpcloud-az2-663635', u'ramdisk_id': u'cd16eea0-986d-443f-a345-3b82c8f4fce6', u'access_ip_v6': None, u'access_ip_v4': None, u'kernel_id': u'b1803c3b-8f66-408b-9105-73568c7181ca', u'key_name': None, u'updated_at': u'2013-11-16T21:38:00.000000', u'host': u'devstack-precise-hpcloud-az2-663635', u'user_id': u'eae9401ba3794296ae8248748ef8e26e', u'system_metadata': {u'image_kernel_id': u'b1803c3b-8f66-408b-9105-73568c7181ca', u'instance_type_memory_mb': u'64', u'instance_type_swap': u'0', u'instance_type_vcpu_weight': None, u'instance_type_root_gb': u'0', u'instance_type_id': u'6', u'image_ramdisk_id': u'cd16eea0-986d-443f-a345-3b82c8f4fce6', u'instance_type_name': u'm1.nano', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'image_disk_format': u'ami', u'instance_type_flavorid': u'42', u'image_container_format': u'ami', u'instance_type_vcpus': u'1', u'image_min_ram': u'0', u'image_min_disk': u'0', u'image_base_image_ref': u'11f4317b-d294-4d21-b2c4-2d7198aa32b8'}, u'task_state': None, u'shutdown_terminate': False, u'cell_name': None, u'root_gb': 0, u'locked': False, u'name': u'instance-00000028', u'created_at': u'2013-11-16T21:37:03.000000', u'locked_by': None, u'launch_index': 0, u'memory_mb': 64, u'vcpus': 1, u'image_ref': u'11f4317b-d294-4d21-b2c4-2d7198aa32b8', u'architecture': None, u'auto_disk_config': False, u'os_type': None, u'metadata': {}}, u'tail_length': 10}, u'_unique_id': u'706ab69dc066440fbe1bd7766b73d953', u'_context_project_name': u'ServerActionsTestJSON-tempest-2102529866-tenant', u'_context_read_deleted': u'no', u'_context_tenant': u'3f6934d9aabf467aa8bc51397ccfa782', u'_context_instance_lock_checked': False, u'_context_project_id': u'3f6934d9aabf467aa8bc51397ccfa782', u'_context_user_name': u'ServerActionsTestJSON-tempest-2102529866-user'} _safe_log /opt/stack/new/nova/nova/openstack/common/rpc/common.py:277
2013-11-16 21:41:20.879 23086 DEBUG nova.openstack.common.rpc.amqp [-] unpacked context: {'tenant': u'3f6934d9aabf467aa8bc51397ccfa782', 'project_name': u'ServerActionsTestJSON-tempest-2102529866-tenant', 'user_id': u'eae9401ba3794296ae8248748ef8e26e', 'roles': [u'_member_'], 'timestamp': u'2013-11-16T21:41:20.781052', 'auth_token': '<SANITIZED>', 'remote_address': u'127.0.0.1', 'quota_class': None, 'is_admin': False, 'user': u'eae9401ba3794296ae8248748ef8e26e', 'service_catalog': [{u'endpoints': [{u'adminURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782', u'region': u'RegionOne', u'id': u'5725cc234a8346408dfe343aea62d3aa', u'internalURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782', u'publicURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782'}], u'endpoints_links': [], u'type': u'volume', u'name': u'cinder'}], 'request_id': u'req-7a2ee0ab-c977-4957-abb5-1d84191bf30c', 'instance_lock_checked': False, 'project_id': u'3f6934d9aabf467aa8bc51397ccfa782', 'user_name': u'ServerActionsTestJSON-tempest-2102529866-user', 'read_deleted': u'no'} _safe_log /opt/stack/new/nova/nova/openstack/common/rpc/common.py:277
2013-11-16 21:41:20.880 AUDIT nova.compute.manager [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] [instance: 10aace14-23c1-4cec-9bfd-2c873df1fbee] Get console output
2013-11-16 21:41:20.894 DEBUG nova.openstack.common.processutils [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf chown 1006 /opt/stack/data/nova/instances/10aace14-23c1-4cec-9bfd-2c873df1fbee/console.log execute /opt/stack/new/nova/nova/openstack/common/processutils.py:147
2013-11-16 21:41:20.993 DEBUG nova.openstack.common.rpc.amqp [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] UNIQUE_ID is 7abfb6072ea44239ae71367bde0a4485. _add_unique_id /opt/stack/new/nova/nova/openstack/common/rpc/amqp.py:341
2013-11-16 21:41:20.995 DEBUG nova.openstack.common.rpc.amqp [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] UNIQUE_ID is b463e1d350f441efb159170784f9ba41. _add_unique_id /opt/stack/new/nova/nova/openstack/common/rpc/amqp.py:341
In other words, nova doesn't log much here, but I think the console log might be genuinely empty?"
1251943,1251943,nova,f4646b74532d3e75a105290e8333bff92eb838ad,1,1, ,xenserver disable/enable host report error,"When XenServer disable/enable host, the api set_host_enabled() need to get service info for the target host, but the rpc call of set_host_enabled() did not transfer host as parameter, this will cause the api call failed.
def set_host_enabled(self, host, enabled):
        """"""Sets the specified host's ability to accept new instances.""""""
        # Since capabilities are gone, use service table to disable a node
        # in scheduler
        status = {'disabled': not enabled,
                'disabled_reason': 'set by xenapi host_state'
                }
        cntxt = context.get_admin_context()
        service = self._conductor_api.service_get_by_args(
                cntxt,
                host, <<<<<<<<<
                'nova-compute')
        self._conductor_api.service_update(
                cntxt,
                service,
                status)
        args = {""enabled"": jsonutils.dumps(enabled)}
        response = call_xenhost(self._session, ""set_host_enabled"", args)
        return response.get(""status"", response)
======================================
    def set_host_enabled(self, ctxt, enabled, host):
        cctxt = self.client.prepare(server=host)
        return cctxt.call(ctxt, 'set_host_enabled', enabled=enabled) <<<<<<<<<< No host"
1252175,1252175,cinder,9f138a1efdab9dd0f7b4bbfc9835bd53dfed762b,1,1,Bug in comments,Fixes error message when create snapshot,"Error message in create_snapshot not appropriate while volume is migrating,as follows:
    msg = _(""Volume cannot be deleted while migrating"")"
1252201,1252201,neutron,17b8cedeaa291f973e8311865b6560dc4806888d,1,1, ,fwaas service can't run in operation system withou...,"Fwaas service can't run in operation system without namespace feature although use_namespaces equals False
$ grep -r ""^use_namespaces ="" /etc/neutron/l3_agent.ini
use_namespaces = False
Bellow is error log:
2013-11-18 14:52:09.782 INFO neutron.agent.l3_agent [-] L3 agent started
2013-11-18 14:52:14.553 ERROR neutron.services.firewall.agents.l3reference.firewall_l3_agent [-] FWaaS RPC info call failed for '770d54af-2bb6-4233-8a39-1d5ad36fea59'.
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Traceback (most recent call last):
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 213, in process_router_add
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     self._process_router_add(ri)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 193, in _process_router_add
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     ri.router['tenant_id'])
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 92, in _get_router_info_list_for_tenant
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     local_ns_list = root_ip.get_namespaces(self.root_helper)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/agent/linux/ip_lib.py"", line 174, in get_namespaces
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     output = cls._execute('', 'netns', ('list',), root_helper=root_helper)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/agent/linux/ip_lib.py"", line 76, in _execute
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     root_helper=root_helper)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/agent/linux/utils.py"", line 75, in execute
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     raise RuntimeError(m)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent RuntimeError:
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'list']
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Exit code: 255
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Stdout: ''
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Stderr: 'Object ""netns"" is unknown, try ""ip help"".\n'
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent"
1252284,1252284,neutron,a5d996e381bd342f0204fcd99e8548cf28b20e92,1,1, ,OVS agent doesn't reclaim local VLAN,"Locally to an OVS agent, when the last port of a network disappears the local VLAN isn't reclaim."
1252409,1252409,nova,e4b0d8e9446ad3ad3a514b9cf65722f6ed95888d,1,1, ,use of threading in libvirt driver causes deadlock...,"the libvirt driver was running into this issue https://bitbucket.org/eventlet/eventlet/issue/137/use-of-threading-locks-causes-deadlock when trying to connect to the libvirtd daemon.
the driver has logging messages during the _connect method which was called within a native thread. The logging code takes out a eventlet lock and mixing OS native threads and eventlet locks causes deadlocks due to the above eventlet bug. The resulting deadlock was enough to hang the nova-compute process."
1252423,1252423,cinder,b9ff8cd7ad8c402787324d2baca9b32f61eafb4a,1,1,“This causes the volume clone from snapshot dd operation to fail”,Ensure ThinLVM LV is active when cloning or creati...,"Thin-provisioned LVs may not be activated automatically in all cases.  (RHEL 6.5 does not activate them automatically by default.)
This causes the volume clone from snapshot dd operation to fail, as the source device does not exist in /dev/mapper/.  We should ensure the LV is active before attempting to clone to a new volume."
1252506,1252506,neutron,89f25623a4345b5044a58cff36e7fd103c6b88e8,1,1, ,"_recycle_ip slow, multiple port deletes begin timi...","I've found that port delete fails if there are a number of port delete/create operations happening at once.
It seems to depend on database load but I've seen as few as about 8 parallel port deletes begin to timeout waiting for the appropriate locks inside of the _recycle_ip method in neutron/db/db_base_plugin_v2.py."
1252692,1252692,cinder,6bd4dae9680aae924ef24750e15f00711c14b1a3,1,1, ,Unrecognized Content-Type application/json,"Unrecognized Content-Type provided in request get_body /opt/stack/new/cinder/cinder/api/openstack/wsgi.py:796
Cinder api service logs the above entry, on every request made by the python-cinderclient, for example ""cinder list"".
 'application/json' should be an accepted content type, without any additional log.
You can see the above issue as the result of all tempest-devstack gate job as well."
1252693,1252693,nova,c1be8381e19390b0edca28c8ab3f77e6226f25e0,1,0,“SQLite doesn't provide a native implementation of BOOLEAN data type.”,Running of unit tests fails if SQLAlchemy >= 0.8.3...,"If SQLAlchemy >= 0.8.3 is used, running of unit tests fails. The following error is printed to stderr multiple times:
DBError: (IntegrityError) constraint failed u'UPDATE instance_system_metadata SET   updated_at=?, deleted_at=?, deleted=? WHERE instance_system_metadata.id = ?' ('2013-  11-19 10:37:44.378444', '2013-11-19 10:37:44.377819', 11, 11)
A few of our migrations change the type of deleted column from boolean to int. MySQL and SQLite don't have native boolean data type. SQLAlchemy uses int columns (e.g. in case of MySQL - tinyint) + CHECK constraint (something like CHECK (deleted in (0, 1))) to emulate boolean data type.
In our migrations when the type of column `deleted` is changed from boolean to int, the corresponding CHECK constraint is dropped too. But starting from SQLAlchemy version 0.8.3, those CHECK constraints aren't dropped anymore. So despite the fact that column deleted is of type int now, we still restrict its values to be either 0 or 1.
Migrations changing the data type of deleted columns rely on SQL rendered for CHECK constraints (e.g. https://github.com/openstack/nova/blob/master/nova/db/sqlalchemy/migrate_repo/versions/152_change_type_of_deleted_column.py#L172). There was a patch in SQLAlchemy 0.8.3 release that slightly changed the way DDL statements are rendered ((http://docs.sqlalchemy.org/en/latest/changelog/changelog_08.html#change-487183f04e6da9aa27d8817bca9906d1)). Unfortunately, due to the fact that nova migrations depend on such implementation details, this change to SQLAlchemy broke our code.
We must fix our migrations to work properly with new SQLAlchemy versions too (0.8.3+)."
1252806,1252806,neutron,9335ffd7c3eaad0d66d7d19e7760ae12476a5ea2,1,1, ,unable to add allow all ingress traffic security g...,"The following rule is unable to be installed:
$ neutron security-group-rule-create --direction ingress default
409-{u'NeutronError': {u'message': u'Security group rule already exists. Group id is 29dc1837-75d3-457a-8a90-14f4b6ea6db9.', u'type': u'SecurityGroupRuleExists', u'detail': u''}}
The reason for this is when the db query is done it passes this in as a filter:
{'tenant_id': [u'577a2f0c78fb4e36b76902977a5c1708'], 'direction': [u'ingress'], 'ethertype': ['IPv4'], 'security_group_id': [u'0fb10163-81b2-4538-bd11-dbbd3878db51']}
and the remote_group_id is wild carded thus it matches this rule:
[ {'direction': u'ingress',
  'ethertype': u'IPv4',
  'id': u'8d5c3429-f4ef-4258-8140-5ff3247f9dd6',
  'port_range_max': None,
  'port_range_min': None,
  'protocol': None,
  'remote_group_id': None,
  'remote_ip_prefix': None,
  'security_group_id': u'0fb10163-81b2-4538-bd11-dbbd3878db51',
  'tenant_id': u'577a2f0c78fb4e36b76902977a5c1708'}]"
1252827,1252827,nova,6471776b6b25bb4062238f7c1b732b2d6999ec65,1,1, ,Bug #1252827 “VMWARE,"I see that sometimes vmware driver reports 0 stats. Please take a look at the following log file for more information: http://162.209.83.206/logs/51404/6/screen-n-cpu.txt.gz
excerpts from log file:
2013-11-18 15:41:03.994 20162 WARNING nova.virt.vmwareapi.vim_util [-] Unable to retrieve value for datastore Reason: None
2013-11-18 15:41:04.029 20162 WARNING nova.virt.vmwareapi.vim_util [-] Unable to retrieve value for host Reason: None
2013-11-18 15:41:04.029 20162 WARNING nova.virt.vmwareapi.vim_util [-] Unable to retrieve value for resourcePool Reason: None
2013-11-18 15:41:04.029 20162 DEBUG nova.compute.resource_tracker [-] Hypervisor: free ram (MB): 0 _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:389
2013-11-18 15:41:04.029 20162 DEBUG nova.compute.resource_tracker [-] Hypervisor: free disk (GB): 0 _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:390
2013-11-18 15:41:04.030 20162 DEBUG nova.compute.resource_tracker [-] Hypervisor: VCPU information unavailable _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:397
During this time we cannot spawn any server. Look at the http://162.209.83.206/logs/51404/6/screen-n-sch.txt.gz
excerpts from log file:
2013-11-18 15:41:52.475 DEBUG nova.filters [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] Filter AvailabilityZoneFilter returned 1 host(s) get_filtered_objects /opt/stack/nova/nova/filters.py:88
2013-11-18 15:41:52.476 DEBUG nova.scheduler.filters.ram_filter [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] (Ubuntu1204Server, domain-c26(c1)) ram:-576 disk:0 io_ops:0 instances:1 does not have 64 MB usable ram, it only has -576.0 MB usable ram. host_passes /opt/stack/nova/nova/scheduler/filters/ram_filter.py:60
2013-11-18 15:41:52.476 INFO nova.filters [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] Filter RamFilter returned 0 hosts
2013-11-18 15:41:52.477 WARNING nova.scheduler.driver [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] [instance: 1a648022-1783-4874-8b41-c3f4c89d8500] Setting instance to ERROR state."
1252856,1252856,neutron,07d597079781967f5a149f1812ddca3897fa49d9,1,1,"""Refactor _spawn/destroy_metadata_proxy so”",Bug #1252856 “destroy_metadata_proxy not called from _destroy_ro... ,"In preparing the patch for bug 1250596 I found that _destroy_router_namespaces does not properly destroy the metadata proxy.
It may be that it was not called because _destroy_router_namespaces requires a router_info object that is not available in this context.  Maru suggested refactoring _destroy_metadata_proxy and _spawn_metadata_proxy to enable this."
1252864,1252864,cinder,d8a11168c908fe6c6a07fbb30a5bc88a6df6e939,1,1, ,Bug #1252864 “GlusterFS,"The snapshot_delete operation will fail if the backing file for the snapshot does not exist.  This happens in legitimate cases such as when snapshot_create fails due to permissions problems.
The driver should allow the manager to delete the snapshot in this case where this is no action required for the driver to delete anything."
1252915,1252915,cinder,d97304d18cf594e5982899d1c9f82a297842cd1d,1,1,Typo in method name,Fix typo in cinder.volume.API,"cinder.volume.API has typo in its method name.
'_valid_availabilty_zone' should be '_valid_availability_zone'."
1252921,1252921,neutron,68369fe6789d4ab053eb382458562394ce632de8,1,1, ,showing nonexistent NetworkGateway throws 500 inst...,"I'm implementing nvp network gateway to heat.
  https://blueprints.launchpad.net/heat/+spec/resource-type-nvp-network-gateway
Heat would check resource existence by resource-show after resource deleted.
However, show_network_gateway returns 500 instead of 404 when nvp network gateway isn't exist.
The result of this situation is that Heat is unable to delete nvp network gateway.
I think, neutron should return 404 when resource isn't exist.
Curl:
$ curl -i http://172.23.56.142:9696/v2.0/network-gateways/b5afd4a9-eb71-4af7-a082-8fc625a35b61 -X GET -H ""X-Auth-Token: 56c136ee847f476f9f0ba4c2ca78ae4b"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -H ""User-Agent: python-neutronclient""
HTTP/1.1 500 Internal Server Error
Content-Type: application/json; charset=UTF-8
Content-Length: 88
Date: Mon, 18 Nov 2013 10:30:44 GMT
{""NeutronError"": ""Request Failed: internal server error while processing your request.""}
Log:
2013-11-18 18:18:03.315 25570 DEBUG keystoneclient.middleware.auth_token [-] Received request from user: 54012987ac014457b9a0a8bcc10928ae with project_id : ed684e101d3243a69db07e744acad6f2 and roles: admin  _build_user_headers /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:922
2013-11-18 18:18:03.316 25570 DEBUG routes.middleware [-] Matched GET /network-gateways/3fe90063-9e96-45be-8989-335f582962be.json __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-11-18 18:18:03.317 25570 DEBUG routes.middleware [-] Route path: '/network-gateways/:(id).:(format)', defaults: {'action': u'show', 'controller': <wsgify at 65809104 wrapping <function resource at 0x3ebbf50>>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-11-18 18:18:03.317 25570 DEBUG routes.middleware [-] Match dict: {'action': u'show', 'controller': <wsgify at 65809104 wrapping <function resource at 0x3ebbf50>>, 'id': u'3fe90063-9e96-45be-8989-335f582962be', 'format': u'json'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-11-18 18:18:03.324 25570 ERROR neutron.api.v2.resource [-] show failed
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 290, in show
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     parent_id=parent_id),
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 258, in _item
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     obj = obj_getter(request.context, id, **kwargs)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 1951, in get_network_gateway
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     id, fields)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/dbexts/nicira_networkgw_db.py"", line 248, in get_network_gateway
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     gw_db = self._get_network_gateway(context, id)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/dbexts/nicira_networkgw_db.py"", line 133, in _get_network_gateway
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     return self._get_by_id(context, NetworkGateway, gw_id)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 145, in _get_by_id
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     return query.filter(model.id == id).one()
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2190, in one
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     raise orm_exc.NoResultFound(""No row was found for one()"")
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource NoResultFound: No row was found for one()
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource
2013-11-18 18:18:05.061 25570 DEBUG neutron.openstack.common.rpc.amqp [-] received {u'_context_roles': [u'admin'], u'_msg_id': u'3ce98f89ac844d2a8d38e0ae231ff447', u'_context_read_deleted': u'no', u'_reply_q': u'reply_70a7a3f33d8f4417b5225404d435d264', u'_context_tenant_id': None, u'args': {u'devices': [u'tap3fc478a3-07', u'tapfa59a846-8b', u'tapba4de5a8-85']}, u'namespace': None, u'_unique_id': u'44f59887e1084f9898cdcc57d3bcff78', u'_context_is_admin': True, u'version': u'1.1', u'_context_project_id': None, u'_context_timestamp': u'2013-11-14 10:51:59.225787', u'_context_user_id': None, u'method': u'security_group_rules_for_devices'} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276
2013-11-18 18:18:05.061 25570 DEBUG neutron.openstack.common.rpc.amqp [-] unpacked context: {'user_id': None, 'roles': [u'admin'], 'tenant_id': None, 'is_admin': True, 'timestamp': u'2013-11-14 10:51:59.225787', 'project_id': None, 'read_deleted': u'no'} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276"
1252967,1252967,nova,c91842af547f9d70ac2a538477d89da1e3efafa9,1,1, ,Bug #1252967 “VMware,"When performing a nova snapshot, the vmware driver takes a VM snapshot and copies out the snapshotted disk. After the operation, the VM snapshot should be deleted."
1253207,1253207,Swift,72ade27ea322056b993da8cfd389476461a845af,0,0,Bug in test files … Should we have to take into account3,functional tests use wrong date format,"In test/functional/tests.py, the TestFileComparison suite uses ""time.asctime("" to format dates in if-modified-since type headers.  They should be in HTTP-date or ISO-86601 or whatever format is correct there."
1253478,1253478,Swift,af2607c457171975fe4f512a92a87b827d5b6528,1,1,"With the new early majority code, Este escenario no lo tenian en cuenta Asia es BIC o no3 “that's the client problem- also it already existed,”",bulk delete 409s,"With the new early majority code, doing a bulk delete where you delete all the objects in a container and then try to delete the container at the end will very frequently fail. This is because it is very likely that all 3 objects have not been deleted by the time the middleware got a successful response.  A possible solution is to just sleep for a second and retry if you get a 409. idk
btw- this behavior would also happen for people emptying / deleting containers but I guess that's the client problem- also it already existed, its just now more frequent.
another possible solution is just to not do anything :)"
1253497,1253497,glance,11b5487eff312d4d914fbc2f861e18b031421dbe,0,0,"“should be to remove generate_uuid() from”, “deprecated”",Replace uuidutils.generate_uuid() with str(uuid.uu...,"http://lists.openstack.org/pipermail/openstack-dev/2013-November/018980.html
> Hi all,
>
> We had a discussion of the modules that are incubated in Oslo.
>
> https://etherpad.openstack.org/p/icehouse-oslo-status
>
> One of the conclusions we came to was to deprecate/remove uuidutils in
> this cycle.
>
> The first step into this change should be to remove generate_uuid() from
> uuidutils.
>
> The reason is that 1) generating the UUID string seems trivial enough to
> not need a function and 2) string representation of uuid4 is not what we
> want in all projects.
>
> To address this, a patch is now on gerrit.
> https://review.openstack.org/#/c/56152/
>
> Each project should directly use the standard uuid module or implement its
> own helper function to generate uuids if this patch gets in.
>
> Any thoughts on this change? Thanks.
>
Unfortunately it looks like that change went through before I caught up on
email. Shouldn't we have removed its use in the downstream projects (at
least integrated projects) before removing it from Oslo?
Doug"
1253657,1253657,cinder,20667f7d49eed88ddd528be73293a624b3af1ba0,1,1, ,NetApp Cinder driver doesn't work with vsadmin rol...,"If you configure the NetApp cinder driver with clustered mode and use a login/password with only the vsadmin role, many things will not work and APIs will fail with stack traces."
1253660,1253660,cinder,585f34ff7798ad271121d432e0cae8820bc54389,1,1, ,NetApp cinder driver reports 0 free space for the ...,"The NetApp driver (in clustered mode) collects free space in an asynchronous background job, which results in the free space being reported as 0 until the job has run at least once (takes 60 seconds). The driver should be changed to report the correct free space immediately upon starting up."
1253875,1253875,nova,1d769b0c37f24028bceae568ca66bb0426b28b69,1,1,"“Commit a52259e introduced some changes to debug log messages in the ImagePropertiesFilter but with that came a bug""",Bug #1253875 “Scheduler Filters,"I've been seeing this in the nova scheduler log a lot this week when looking through gate and check queue failures:
http://paste.openstack.org/show/53791/
Looks like it's probably due to a bad translation.
(9:31:29 PM) clarkb: mriedem: yes that is the sort of traceback that should be fixed. It is possible that that particular error is due to a bad translation
(9:31:58 PM) clarkb: since I think we are using local=en_US on the slaves and en is the source locale
http://logs.openstack.org/42/56642/2/gate/gate-tempest-devstack-vm-neutron/a712b82/logs/screen-n-sch.txt.gz"
1253891,1253891,swift,4ed1c8473f50f7f11082ad1ee389f5339707e212,1,1,“UNCAUGHT EXCEPTION”,uncaught exception in container-sync from use of v...,"See: http://logs.openstack.org/10/56310/4/check/check-grenade-devstack-vm/29a7de6/logs/syslog.txt
UNCAUGHT EXCEPTION
Traceback (most recent call last):
  File ""/usr/local/bin/swift-container-sync"", line 10, in <module>
    execfile(__file__)
  File ""/opt/stack/new/swift/bin/swift-container-sync"", line 20, in <module>
    run_daemon('container-sync')
  File ""/opt/stack/new/swift/swift/common/daemon.py"", line 186, in run_daemon
    daemon.run(**options)
  File ""/opt/stack/new/swift/swift/common/daemon.py"", line 75, in run
    self.run_forever(**kwargs)
TypeError: run_forever() got an unexpected keyword argument 'verbose'
Kibana search showing others: http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVU5DQVVHSFQgRVhDRVBUSU9OXCIgYW5kIE5PVCBtZXNzYWdlOlwiL29wdC9zdGFjay9vbGQvc3dpZnRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiY3VzdG9tIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7ImZyb20iOiIyMDEzLTA5LTAxVDE3OjQ3OjEwKzAwOjAwIiwidG8iOiIyMDEzLTExLTIwVDE3OjQ3OjEwKzAwOjAwIiwidXNlcl9pbnRlcnZhbCI6IjAifSwic3RhbXAiOjEzODUwOTUyNTkzMDUsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0="
1253910,1253910,cinder,6dfb3ede190cb013f747617e75218953330c54ca,0,0,“fix docstring”,Fixes inappropriate description about migrate_volu...,"""Migrate a volume to the specified host."" as the description of migrate_volume_completion func is inappropriate."
1253931,1253931,cinder,45489f9814fce0f7b3f6c947076529962902f729,0,0,"“Remove unused code in test_admin_actions.py”, refactoring",Remove unused code in test_admin_actions.py,"Remove unused code in test_admin_actions.py - in _migrate_volume_comp_exec(), admin_ctx is not used."
1253960,1253960,nova,1588de2714a0f28a0da219f319d6a7ac6c1f9bab,1,1,“return the hypervisor_version as an integer rather than a string.”,Bug #1253960 “docker driver reports hypervisor version in wrong ... ,"devstack, `nova --version` reports 2.15.0.81
VIRT_DRIVER=docker
database: PostgreSQL
when starting nova-compute I'm getting an exception:
2013-11-20 11:41:27.413 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released ""update_available_resource"" from (pid=15320) inner /opt/stack/nova/nova/openstack/common/lockutils.py:251
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/queue.py"", line 107, in switch
    self.greenlet.switch(value)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/service.py"", line 448, in run_service
    service.start()
  File ""/opt/stack/nova/nova/service.py"", line 164, in start
    self.manager.pre_start_hook()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 788, in pre_start_hook
    self.update_available_resource(nova.context.get_admin_context())
  File ""/opt/stack/nova/nova/compute/manager.py"", line 5065, in update_available_resource
    rt.update_available_resource(context)
  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 248, in inner
    return f(*args, **kwargs)
  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 326, in update_available_resource
    self._sync_compute_node(context, resources)
  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 349, in _sync_compute_node
    self._create(context, resources)
  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 365, in _create
    values)
  File ""/opt/stack/nova/nova/conductor/api.py"", line 236, in compute_node_create
    return self._manager.compute_node_create(context, values)
  File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 356, in compute_node_create
    return cctxt.call(context, 'compute_node_create', values=values)
  File ""/opt/stack/nova/nova/rpcclient.py"", line 85, in call
    return self._invoke(self.proxy.call, ctxt, method, **kwargs)
  File ""/opt/stack/nova/nova/rpcclient.py"", line 63, in _invoke
    return cast_or_call(ctxt, msg, **self.kwargs)
  File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 126, in call
    result = rpc.call(context, real_topic, msg, timeout)
  File ""/opt/stack/nova/nova/openstack/common/rpc/__init__.py"", line 139, in call
    return _get_impl().call(CONF, context, topic, msg, timeout)
  File ""/opt/stack/nova/nova/openstack/common/rpc/impl_kombu.py"", line 816, in call
    rpc_amqp.get_connection_pool(conf, Connection))
  File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 574, in call
    rv = list(rv)
  File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 539, in __iter__
    raise result
RemoteError: Remote error: DBError (DataError) invalid input syntax for integer: ""1.0""
LINE 1: ...L, NULL, 0, 5, 1, 11953, 54, 0, 512, 0, 'docker', '1.0', 'hg...
                                                             ^
(should be in fixed-width font, the circumflex accent points to '1.0')
This appears to be the same issue as #1195139  but with docker driver.  Also related to https://blueprints.launchpad.net/nova/+spec/save-hypervisor-version-as-string
nova-compute starts up OK after applying a similar change as for #1195139 :
--- nova/virt/docker/driver.py.orig
+++ nova/virt/docker/driver.py
@@ -159,7 +159,7 @@
             'local_gb_used': disk['used'] / unit.Gi,
             'disk_available_least': disk['available'] / unit.Gi,
             'hypervisor_type': 'docker',
-            'hypervisor_version': '1.0',
+            'hypervisor_version': utils.convert_version_to_int('1.0'),
             'hypervisor_hostname': self._nodename,
             'cpu_info': '?',
             'supported_instances': jsonutils.dumps(["
1254046,1254046,glance,e9468d77a41ada6d77021c32c5df4351d368ea72,1,0,“Original change to Nova: If4dd973acc23921dbc2bc69bb76225deb2802dad”,openstack.common.local module is out of date,"local has a broken TLS symbol - strong_store, fixed in oslo some time ago in Ib544be1485823f6c619312fdee5a04031f48bbb4. All direct and indirect (lockutils and rpc) usages of strong_store might be potentially affected.
Original change to Nova: https://review.openstack.org/#/c/57509/"
1254089,1254089,cinder,273d8765115f0936d24549db8698dbe7e8d2ec1a,1,1, ,wait_child() without greenthread.sleep() uses exce...,"https://bugs.launchpad.net/oslo/+bug/1095346 tracked a fix in service.py: wait() to prevent the cinder-volume process from hogging the CPU while looping in _wait_child()
However, the wait() function will also loop in _wait_child() when the parent catched SIGTERM and the parent is waiting to reap the children. This also causes excessive CPU usage if the child does not die quickly for some reason.
I've an instance of cinder-volume  that has been running for several days in this wait state. An strace of the parent shows:
wait4(0, 0x7fff081b7780, WNOHANG, NULL) = 0
wait4(0, 0x7fff081b7780, WNOHANG, NULL) = 0
wait4(0, 0x7fff081b7780, WNOHANG, NULL) = 0
......forever....
this is because the parent has caught SIGTERM is and is looping on wait_child without the eventlet.greenthread.sleep(.01)
during the normal running state of cinder-volume the parent does an epoll for the eventlet.greenthread.sleep(.01):
wait4(0, 0x7fff25185430, WNOHANG, NULL) = 0
select(0, NULL, NULL, NULL, {0, 9984})  = 0 (Timeout)
wait4(0, 0x7fff25185430, WNOHANG, NULL) = 0
select(0, NULL, NULL, NULL, {0, 9984})  = 0 (Timeout)
My CPU is pegged at 100% with this process (shown in top):
  PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+  COMMAND
27313 cinder    20   0  103m  25m 4080 R   100  0.0  56343:11 cinder-volume
To fix this, the second call to _wait_child() needs the same sleep as the first:
    def wait(self):
        """"""Loop waiting on children to die and respawning as necessary.""""""
        while self.running:
            wrap = self._wait_child()
            if not wrap:
                # Yield to other threads if no children have exited
                # Sleep for a short time to avoid excessive CPU usage
                # (see bug #1095346)
                eventlet.greenthread.sleep(.01)
                continue
            LOG.info(_('wait wrap.failed %s'), wrap.failed)
            while (self.running and len(wrap.children) < wrap.workers
                   and not wrap.failed):
                self._start_child(wrap)
        if self.sigcaught:
            signame = {signal.SIGTERM: 'SIGTERM',
                       signal.SIGINT: 'SIGINT'}[self.sigcaught]
            LOG.info(_('Caught %s, stopping children'), signame)
        for pid in self.children:
            try:
                os.kill(pid, signal.SIGTERM)
            except OSError as exc:
                if exc.errno != errno.ESRCH:
                    raise
        # Wait for children to die
        if self.children:
            LOG.info(_('Waiting on %d children to exit'), len(self.children))
            while self.children:
                self._wait_child()
Patch is :
--- a/cinder/service.py
+++ b/cinder/service.py
@@ -323,7 +323,10 @@ class ProcessLauncher(object):
         if self.children:
             LOG.info(_('Waiting on %d children to exit'), len(self.children))
             while self.children:
-                self._wait_child()
+               wrap = self._wait_child()
+               if not wrap:
+                   eventlet.greenthread.sleep(.01)
+                   continue"
1254174,1254174,nova,384cce84fde784f9e2f39db49502c66e20ff0b4c,1,1, ,bulk-delete-floating-ip does not free used quota,"The bulk-create-floating-ip and bulk-delete-floating-ip commands do not interact with floating_ip quotas.  This is by design, since they're for admins rather than tenants.
However, in one case this causes a bug.  If a tenant initially allocates the floating IP with create-floating-ip and consumed quota, and the admin later deletes the floating Ip with bulk-delete-floating-ip, the floating IP is freed but the quota is still consumed.
So we should change bulk-delete-floating-ip to release any quota that was associated with those floating IP addresses.  (In many cases there will not be any so we need to check.)
This is https://bugzilla.redhat.com/show_bug.cgi?id=1029756 (but that bug is mostly private so won't people outside Red Hat much good)."
1254210,1254210,glance,1772a9116d127294354addd830e16877658bbee3,1,1, ,"remove, add and save in glance.db.__init__ should ...",add and save method in ImageMemberRepo are returning a new object. It is expected to modify the values in-place rather than adding them in the new object.
1254246,1254246,neutron,5529071bf1393d0d448bc495cc906a68bc30a820,1,1, ,somehow getting duplicate openvswitch agents for t...,"While investigating spurious failures in our TripleO continous deployment, I had this problem:
+--------------------------------------+--------------------+-------------------------------------+-------+----------------+
| id                                   | agent_type         | host                                | alive | admin_state_up |
+--------------------------------------+--------------------+-------------------------------------+-------+----------------+
| 3a9c6aca-e91f-49c9-850a-67db219fdf58 | L3 agent           | overcloud-notcompute-wjo2jbvvd2sm   | :-)   | True           |
| 3fb9f6cf-b545-4a34-a490-dda834973d1e | Open vSwitch agent | overcloud-novacompute0-ubrjpv4jz64a | xxx   | True           |
| 855349b2-b0fc-4270-bb96-385b61aa5a6c | DHCP agent         | overcloud-notcompute-wjo2jbvvd2sm   | :-)   | True           |
| 8b8a4128-9716-42ee-b886-f053db166ce3 | Metadata agent     | overcloud-notcompute-wjo2jbvvd2sm   | :-)   | True           |
| c8297e0d-8575-47f0-ae65-499c1e0319b3 | Open vSwitch agent | overcloud-notcompute-wjo2jbvvd2sm   | :-)   | True           |
| f746fc1d-9083-46f4-a922-739c5d332d7c | Open vSwitch agent | overcloud-novacompute0-ubrjpv4jz64a | xxx   | True           |
+--------------------------------------+--------------------+-------------------------------------+-------+----------------+
Note that overcloud-novacompute0-ubrjpv4jz64a has _two_ Open vSwitch agents.
This caused many 'vif_type=binding_failed' errors when booting nova instances.
Deleting f746fc1d-9083-46f4-a922-739c5d332d7c resulted in the problem going away.
Seems like there might be a race if the agent restarts quickly, thus not seeing its own agent record and sending a second RPC to create one. I think, I am not entirely sure how this works, that is just a hypothesis."
1254318,1254318,cinder,3de7da12d1098ef777305099e5f4a039e536bf99,1,1, ,Properly handle volume cleanup with RBD driver,"If a volume gets deleted in the Ceph backend, and Cinder still knows about it in a 'deleting' state, cinder-volume on startup will try to clean it up. rbd.Image() will however raise a rbd.ImageNotFound exception in these cases. These should be disregarded so cinder can continue the delete if it's no longer in the backend.
Stack trace:
http://paste.openstack.org/show/53858/"
1254520,1254520,Neutron,4af2163bd41648d64cf1c3c838990737955d2133,1,0,"Enviromental change, this is a bug related with ovs-vsctl",Open vSwitch commands timeout on gate tests,"Error 242 occurs fairly often in gate tests: http://logstash.openstack.org/#eyJzZWFyY2giOiJcIkV4aXQgY29kZTogMjQyXCIgIiwiZmllbGRzIjpbImZpbGVuYW1lIl0sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNDMyMDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzg1MzE3OTg4NTQxLCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
This is actual an ALARM_CLOCK error [142] (rootwrap adds 100 to the error code), and means open vswitch times out; as the default timeout is 2 seconds there is a chance they could occur quite often in a rather stressful scenario as the one represented by parallel tests in tenant isolation.
It might be therefore advisable to allow for a configurable timeout on ovs commands, and increase this timeout for gate tests.
Kernel logs do not provide enough additional information.
It might be also worth adding open vswitch logs to the logs collected by devstack-gate"
1254521,1254521,glance,ab7ea6baf7e67ee6353257f112e315369dd8d36e,0,0,Feature3 “Set upload_image policy to control data upload”,Add upload_image policy for glance v1 api,"Currently there exists no policy to control data uploads.
https://bugs.launchpad.net/glance/+bug/1250918 is for adding upload_image policy in glance v2 api, this bug is for adding it to the glance v1 api."
1254530,1254530,neutron,4fd4d490ac1e12b2bf835949447d21b64c641616,1,1, ,Logging prints traces for q-svc and dhcp-agt,"q-svc and q-dhcp services are printing the following stack traces in their screen tabs:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 850, in emit
    msg = self.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 566, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 530, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 467, in format
    s = self._fmt % record.__dict__
KeyError: u'project_name'
As a workaround, colorized logging config could be fixed to remove project_name from the list of fields in neutron.conf and that solves the issue. However new devstack setup brings the problem back."
1254635,1254635,cinder,f1df39fe7be45c9d789f7b889de35328158079c4,0,0," Bug in the comments , “Fix docstring”",Fix docstring for Snapshot model.,"""Represents a block storage device that can be attached to a VM."" as the description of model Snapshot is incorrect."
1255142,1255142,Neutron,fbf50ad8cba857b62ecb24fccf778175db5b5534,1,1, ,unable to get router's external IP when non admin ...,"In order to set up VPNaaS, a user needs to know his router's external IP (to configure it as endpoint).
PROBLEM : When a user is not admin, the external IP of a router is not visible:
source openrc demo demo
neutron router-list
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| id                                   | name    | external_gateway_info                                                       |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| 2bd1f015-6c98-4861-a078-5a69256ca7b0 | router1 | {""network_id"": ""8ae6890d-5bb5-4f07-9059-77499628048c"", ""enable_snat"": true} |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
neutron router-port-list 2bd1f015-6c98-4861-a078-5a69256ca7b0
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                         |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| 8ae7206d-19af-4a2a-a15b-0f8cdb98861e |      | fa:16:3e:0a:ee:14 | {""subnet_id"": ""c69b14f9-c2e4-4877-8516-57ff2bdeaa9e"", ""ip_address"": ""172.17.0.1""} |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
It's visible only as admin:
source openrc admin demo
neutron router-port-list 2bd1f015-6c98-4861-a078-5a69256ca7b0
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                             |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
| 8ae7206d-19af-4a2a-a15b-0f8cdb98861e |      | fa:16:3e:0a:ee:14 | {""subnet_id"": ""c69b14f9-c2e4-4877-8516-57ff2bdeaa9e"", ""ip_address"": ""172.17.0.1""}     |
| fd56a686-480d-4ede-b021-010253c3de42 |      | fa:16:3e:a5:d2:92 | {""subnet_id"": ""29f5737c-417f-4aa9-a95e-2bef3a04729e"", ""ip_address"": ""192.168.57.226""} |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
Since users need to know the external IP of their router in order to set up VPNaaS this is quite blocking because it requires users to be admin in order to use this feature. It's not an issue for a private cloud, but a big issue for public clouds."
1255556,1255556,Glance,1f6381a73f5c99f1f731d6c4f9defb91bd2d042d,0,0,Error in test,Don't enable all stores by default,"Currently glance ""tries"" to load all stores even though the requirements are not meant. If a store cannot be loaded, it'll be disabled.
Although this is harmless, it is not user-friendly and creates some confusion when reading logs. Instead of trying to load and failing, glance should enable 2 stores by default - filesystem and http - and let the others be enabled manually by the user, which is what GridFS does.
TL;DR: The proposal is to remove all stores but file and http from here[0]
[0] https://git.openstack.org/cgit/openstack/glance/tree/glance/store/__init__.py#n39"
1257273,1257273,Glance,616d2f9a65f7995298153cdf8a1e94bb62912ebe,1,1,“Prevent creation of http images with invalid URIs”,Glance download fails when size is 0,"Glance images are not being fetched by glance's API v1 when the size is 0. There are 2 things wrong with this behaviour:
1) Active images should always be ready to be downloaded, regardless they're locally or remotely stored.
2) The size shouldn't be the way to verify whether an image has some data or not.
https://git.openstack.org/cgit/openstack/glance/tree/glance/api/v1/images.py#n455
This is happening in the API v1, but it doesn't seem to be true for v2."
1257295,1257295,Swift,eb4b29d243150d2f348d163ecc2fb552675891bf,0,0,List of known misspellings,openstack is full of misspelled words,"List of known misspellings
http://paste.openstack.org/show/54354
Generated with:
  pip install misspellings
  git ls-files | grep -v locale | misspellings -f -"
1257922,1257922,Glance,16c3a33c0b8c37ea76d45a90d6b332cfda416b3d,1,0,"""files still uses json.loads() instead of using the wrapper provided by glance.openstack.common.jsonutils""",Use glance.openstack.common.jsonutils to deal with...,"A great number of test files still uses json.loads() instead of using the wrapper provided by glance.openstack.common.jsonutils
All tests that use json directly  should make the switch to the corresponding wrapper in glance.common.openstack.jsonutils"
1259981,1259981,Nova,fefea36baff5f65c56984ae27074e4ad95a3b511,0,0,It is a bug or a feature3,Bug #1259981 “VMware,"Virtual devices need a unit number when they are attached to a controller. We cannot have two devices on the same controller with the same unit number.
Currently, the selection of unit numbers is spread all over the driver code, leaking to high-level functions like spawn() and rescue(). We need to factor this out into helper functions which take care of choosing a proper unit number and creating additional controllers if needed.
High-level functions need to communicate only the intent like 'attach CDROM' or 'attach disk' and shouldn't bother with details like unit numbers."
1264561,1264561,Swift,da794932ba5e627c2aaa58200116eeaf5234863b,1,0,"“Most stock Linux systems come with a soft limit of nproc set to 1024, which can be low now”",Quickly hit nproc limit,"Most stock Linux systems come with a soft limit of nproc set to 1024, which can be low now due to threads_per_disk giving us an effective multiplier on total number of object server processes.  If workers * threads_per_disk * disks > 1024, we get this error:
Dec 26 03:49:28 object-server ERROR __call__ error with HEAD /d95/236790/AUTH_sd-fuse/c0003125/lib/python2.6/site-packages/yum/pkgtag_db.pyc : #012Traceback (most recent call last):#012  File ""/opt/ss/lib/python2.7/site-packages/swift/obj/server.py"", line 631, in __call__#012    res = method(req)#012  File ""/opt/ss/lib/python2.7/site-packages/swift/common/utils.py"", line 1870, in wrapped#012    return func(*a, **kw)#012  File ""/opt/ss/lib/python2.7/site-packages/swift/common/utils.py"", line 686, in _timing_stats#012    resp = func(ctrl, *args, **kwargs)#012  File ""/opt/ss/lib/python2.7/site-packages/swift/obj/server.py"", line 514, in HEAD#012    obj)#012  File ""/opt/ss/lib/python2.7/site-packages/swift/obj/server.py"", line 114, in _diskfile#012    kwargs.setdefault('threadpool', self.threadpools[device])#012  File ""/opt/ss/lib/python2.7/site-packages/swift/obj/server.py"", line 86, in <lambda>#012    lambda: ThreadPool(nthreads=self.threads_per_disk))#012  File ""/opt/ss/lib/python2.7/site-packages/swift/common/utils.py"", line 2054, in __init__#012    thr.start()#012  File ""/opt/ss/lib/python2.7/threading.py"", line 494, in start#012    _start_new_thread(self.__bootstrap, ())#012error: can't start new thread
Since Swift already raises/sets nofile and data limits for the processes at startup, add some logic to do the same to nproc.  The 8192 choice is arbitrary on my part, but seems a reasonable number."
1267103,1267103,Glance,23fa48fa7b86f0767ff4bde9095393a8dfa2e1f7,0,0,“Here the list of misspelled word found in glance source” las misspellings no son un bag verdad3,"Bug #1267103 “missellings on sources "" ","Here the list of misspelled word found in glance source
(using misspellings)
http://paste.openstack.org/show/60786/"
1268480,1268480,Glance,3d03291df88a2a95e0cfb9ae8bb0b7a56ce1b846,0,0,Bug in test files,assertTrue(isinstance()) in tests should be replac...,"some of tests use different method of assertTrue(isinstance(A, B)) or assertEqual(type(A), B). The correct way is to use assertIsInstance(A, B) provided by testtools"
1269189,1269189,Neutron,2b9805105eac9594a038c8e09f4b6cfc6255c677,1,1, ,iptables_manager doesn't honor wrap target when re...,"iptables_manager permits to use a variable begining by a $ in order to reference a wrapped chain, the issue is that the remove rule method doesn't expand this variable, thus if a rule has been added with a variable, it is impossible to remove it since the iptables manager compares the expanded rule and the not expanded one, of course not equal."
1269515,1269515,Cinder,027b78a214e1f7ab7bbda342a024cb13dae8944d,1,1, ,3PAR iSCSI Driver using 'excessive' iscsi & fc por...,"According to the note at the end of http://docs.openstack.org/trunk/config-reference/content/enable-hp-3par-fibre-channel.html
""You can configure one or more iSCSI addresses by using the hp3par_iscsi_ips option. When you configure multiple addresses, the driver selects the iSCSI port with the fewest active volumes at attach time. The IP address might include an IP port by using a colon (:) to separate the address from port. If you do not define an IP port, the default port 3260 is used. Separate IP addresses with a comma (,). The iscsi_ip_address/iscsi_port options might be used as an alternative to hp3par_iscsi_ips for single port iSCSI configuration.""
The 3PAR Driver should not consume ALL N:S:P iSCSI Paths (and FC!) from the 3PAR to the host, especially when it is configured with a single port iSCSI Configuration when attaching a volume to a host.
Instead, it should simply choose the least used (at the time of connection).
Steps to reproduce:
1. Configure a 3PAR to a blade with more than one path: e.g. 2 iscsi & 2 fc
2. Create an iscsi volume
3. Attach the iscsi volume to an instance
4. Observe that 4 VLUNs are created, one for each path between the 3par & the instance (2 iscsi &  2 fc, yes, fc too even if this is an iscsi volume)"
1270020,1270020,Cinder,2331d3336a6adf4fc13a3b187e91a5d1b1f7c723,1,1,"""but the exception msg only contain info about ""available"". So fix it.”",Fix exception log msg in attach volume method,"Attach volume happened when volume's status is attaching or available, but the exception msg only contain info about ""available"". So fix it."
1273087,1273087,Glance,f6e7992a68f6ab36f87604fffbe5349f2a12b25e,0,0,"""There is no merge""",Image status stuck at 'saving' when an upload fail...,"Step 1: Create an empty image (using v1 or v2 - doesn't matter).
   glance image-show <IMAGE-ID> will show the image status as ""queued"".
Step 2: Upload an image file for the image, but cancel the upload midway.
   glance image-show <IMAGE-ID> will show the image status as ""saving"" even though the upload has been cancelled.
Expected result: The image status should be ""queued"" instead of ""saving""."
1274317,1274317,Nova,70f7761e6f053297266fd5cc63212e74e73ec1cc,1,1,"""the case where we do want the method to execute very often was missed""",heal_instance_info_cache_interval config is not ef...,"There is configuration item in /etc/nova/nova.conf that controls how often the instance info should be updated. By default the value is 60 seconds. However, the current implementation only uses that value to prevent over clocked.  Configure it to a different value in nova.conf does not has impact how often the task is executed.
If I change the code in  /usr/lib/python2.6/site-packages/nova/compute/manager.py with the spacing parameter, the configured value will be in action. Please fix this bug.
@periodic_task.periodic_task(spacing=CONF.heal_instance_info_cache_interval)
    def _heal_instance_info_cache(self, context):"
1276367,1276367,Neutron,c1eb61b98a37a10775affa845185388f05e3ceb4,0,0,"""Removes an incorrect and unnecessary return""-- There is a comment later that said ""We're going to revert this""","Bug #1276367 “Unnecessary return statement in ovs_lib  "" ","[openstack-dev] [neutron] unnecessary return statement in ovs_lib
Thu Jan 16 02:48:46 UTC 2014
Came across the following issue while looking at ovs_lib [1]:
The BaseOVS class has the add_bridge() method which after creating an OVS
bridge, returns an OVSBridge object. BaseOVS class is only used by
OVSBridge defined in the same file. OVSBridge has a create() method that
calls the add_bridge() nethod mentioned earlier but do not use the return
value. (See the methods add_bridge and create below.)
What seems odd is the return statement at the end of add_bridge() which is
not used anywhere and doesn't make much sense as far as I can see but I may
be missing something. The OVSBase is never directly used anywhere in
Neutron directory. Of course the return does not do any harm beyond
creating an unused object but it looks to me that it should be removed
unless there is a good reason (or a potential future use case) for it.
class BaseOVS(object):
        ...
    def add_bridge(self, bridge_name):
        self.run_vsctl([""--"", ""--may-exist"", ""add-br"", bridge_name])
        return OVSBridge(bridge_name, self.root_helper)
class OVSBridge(BaseOVS):
        ...
    def create(self):
        self.add_bridge(self.br_name)
[1]
https://github.com/openstack/neutron/blob/master/neutron/agent/linux/ovs_lib.py"
1277029,1277029,Neutron,aa85a97ca2dcb06996ed133d864705f1dca722b1,1,0,"Changed requirements ""Previously list operations in ovs_lib returns an empty list if RuntimeError occurs and a caller cannot distinguish an error from normal results.”",cannot distinguish error from no port in ovs_lib l...,"list operations in ovs_lib returns an empty list even if Runtime Error occurs.
As a result, when Runtime Error occurs, a caller thinks all ovs ports have disappeared.
ovs-vsctl sometimes fails (mostly raises alarm error?)
ovs_lib should provide a way to distinguish these two situations.
list operations in ovs_lib
- get_vif_port_set (used by OVS agent and ryu agent)
- get_vif_ports (used by NEC agent, OVS cleanup)
- get_bridge (used by OVS agent, OVS cleanup)
It affects all agent using the above list operation.
It affects OVS agent and NEC agent. It triggers unexpected port deletions.
In OVS cleanup, there is no negative effect. It just nothing for this case."
1279478,1279478,Cinder,027b78a214e1f7ab7bbda342a024cb13dae8944d,1,1,,3par create volume from snapshot doesn't allow cha...,"In horizon:
1. Create a volume
2. Create a snapshot of the volume
3. Create a volume based on the snapshot using a size greater than the original.
4. Horizon shows the volume with Status=Error
c-vol logs:
2014-02-12 11:11:20.785 ERROR cinder.volume.flows.manager.create_volume [req-80c893bb-ba58-4c15-a7a7-ded1ed74247e e1a0f96cf09141ec8e393429e82062df dcac4edbf1aa48afb109bed9529cdd6e] Volume e07e57c6-7cc1-4b60-8050-8187d99e0006: create failed
2014-02-12 11:11:20.786 DEBUG cinder.openstack.common.lockutils [req-80c893bb-ba58-4c15-a7a7-ded1ed74247e e1a0f96cf09141ec8e393429e82062df dcac4edbf1aa48afb109bed9529cdd6e] Released file lock ""00844d93-9233-4b14-a238-e94916936807-delete_snapshot"" at /opt/stack/data/cinder/cinder-00844d93-9233-4b14-a238-e94916936807-delete_snapshot for method ""_run_flow_locked""... from (pid=28714) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:239
2014-02-12 11:11:20.786 ERROR cinder.openstack.common.rpc.amqp [req-80c893bb-ba58-4c15-a7a7-ded1ed74247e e1a0f96cf09141ec8e393429e82062df dcac4edbf1aa48afb109bed9529cdd6e] Exception during message handling
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 462, in _process_data
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 355, in create_volume
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     _run_flow_locked()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 350, in _run_flow_locked
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     _run_flow()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 346, in _run_flow
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     flow_engine.run()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/lock_utils.py"", line 51, in wrapper
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 110, in run
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     self._run()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 118, in _run
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     self._revert(misc.Failure())
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 75, in _revert
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     misc.Failure.reraise_if_any(failures.values())
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 390, in reraise_if_any
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     failures[0].reraise()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 397, in reraise
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     six.reraise(*self._exc_info)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/task_action.py"", line 96, in execute
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     result = self._task.execute(**kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 598, in execute
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     **volume_spec)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 398, in _create_from_snapshot
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     snapshot_ref)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 145, in create_volume_from_snapshot
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     snapshot)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 797, in create_volume_from_snapshot
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     raise exception.InvalidInput(reason=err)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp InvalidInput: Invalid input received: You cannot change size of the volume.  It must
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp"
1280100,1280100,Glance,2cf64655da38d228b8892a86f005353c84ab212b,1,0,“StringIO.StringIO is incompatible for python 3”,StringIO.StringIO is incompatible for python 3,"Import StringIO
StringIO.StringIO()
should be :
Import six
six.StringIO() or six.BytesIO()
StringIO works for unicode
BytesIO works for bytes
For Python3 compatible."
1284549,1284549,Neutron,f96cf93e70f8c434499efb903820aa72665a7fd4,0,0,Bug in tests,Bug #1284549 “tests,"Some unit tests override notification driver, but doesn't clean it up properly.
So tests can be run against unintended notification driver resulting in error."
1284677,1284677,Glance,627d5fbc1303b3f9d9b2f89acf21359437f9a928,1,0,"""Python 3: do not use 'unicode()'"", ""The unicode() function is Python2-specific""",Bug #1284677 “Python 3,"The unicode() function is Python2-specific, we should use six.text_type() instead."
1285040,1285040,Swift,87710cc8a2654d872adb74d4fa4856c9beb41fbd,0,0,"""assertEquals is deprecated"" ","assertEquals is deprecated, need use assertEqual","assertEquals is deprecated in Python 2.7 , need drop it
http://docs.python.org/2/library/unittest.html#deprecated-aliases"
1285673,1285673,Cinder,051aa362ff1f52926b1725ab24b3b21bfb5e5e21,1,1,"“Updated code where service_get_all_by_topic is used to make use of disabled kwarg instead of filtering afterwards”
“This means it doesn't get all and in fact gets only services that are not disabled.”",service_get_all_by_topic filters out disabled serv...,"cinder/db/sqlalchemy/api.py
service_get_all_by_topic includes the filter disabled=False. This means it doesn't get all and in fact gets only services that are not disabled.
This method is used four times in the codebase
cinder/backup/api.py:        API._is_backup_service_enabled
this method filters out disabled services already so is not affected.
cinder/scheduler/host_manager.py:       HostManager.get_all_host_states
this method tries to log messages for disabled services, but can't because they've already been filtered out
cinder/volume/api.py:        API.list_availability_zones
list_availability_zones is not able to show disabled services
cinder/volume/api.py:        API.migrate_volume
not currently filtering out disabled hosts - would require patching"
1286809,1286809,Cinder ,2f5360753308eb8b10581fc3c026c1b66f42ebdc,0,0,Removing code is not a bug,Error retrieving title,Error retrieving bug report 1286809 for project Cinder : 404 Not Found
1287462,1287462,Neutron,2a2d566be3d69295f56d2db5b6004f12d0ec5b9f,1,0,"""The following patch renames the rest of q_exc to n_exc which were left when quantum was renamed to neutron"" ------ Aqui me vuelve la duda, habria algun commit que olvido cambiar esto33 Quien seria el responsible33",replace rest of q_exc to n_exc in code base,The following patch renames the rest of q_exc to n_exc which were left when quantum was renamed to neutron.
1290969,1290969,Glance,053b468326e2de6ffecbe77b78cb6a33220f9dec,1,1,“This change should be reverted and depending on the list should be logged as a deprecation warning for 1 cycle.”,Change to require listing all known stores in glan...,"This change:
https://review.openstack.org/#q,I82073352641d3eb2ab3d6e9a6b64afc99a30dcc7,n,z
Causes an upgrade failure if users are backed by swift, as it is not in the list of defaults anymore.
This change should be reverted and depending on the list should be logged as a deprecation warning for 1 cycle."
1291130,1291130,Neutron,fdbaba877f98bf20c1ff71c9bd0d04956120f845,0,0,Bug in tests,Calling stopall not necessary in individual unit t...,"Once https://bugs.launchpad.net/neutron/+bug/1290550 is closed, calling stopall on mock.patch from individual unit tests is no longer required. Calls to stopall in existing code should be removed to prevent duplication."
1291181,1291181,Cinder,07ad47ef80dd71e1216331b3b95f5be280455cbd,1,0,"“The blueprint https://blueprints.launchpad.net/cinder/+spec/vmdk-storage-policy-volume-type introduced ""pbm_default_policy""”",Bug #1291181 “vmware,"The blueprint https://blueprints.launchpad.net/cinder/+spec/vmdk-storage-policy-volume-type introduced ""pbm_default_policy"" to allow an admin to specify what storage policy to use when creating a volume with either:
 - no volume_type or
 - with a volume_type with no vmware:storage_policy extra spec.
This could instead use the ""default_volume_type"" config that Cinder already supports. This way the choice of using a volume_type to capture the required policy is not broken."
1291848,1291848,Glance,f6e7992a68f6ab36f87604fffbe5349f2a12b25e,0,0,"""Tracking the need of removing deprecation warning "", ""This commits removes the old `store` package from glance""",Remove deprecation warning when loading stores,Tracking the need of removing deprecation warning and the _EXTRA_STORES global from stores/__init__
1292784,1292784,Swift,2e8bc44c9d0586813587a5711f8e857a31393326,1,1,"""There was a path on container recreate that would sometimes allow db to get reinitialized without updating put_timestamp""",race with container recreate,"So I always hated the way that the status field worked on our db's - but when looking at it I thought I spotted a race, and now I'm pretty sure it's there.  Can someone check out this test case (redbo, put on your concurrency hat).
Basically if you PUT on a container if the file on disk already exists it will update the put_timestamp, if not it goes into initialize which *may* raise AlreadyExists - but in the second case we don't update put_timestamp.
Replication was fixing it eventually..."
1301854,1301854,Cinder,1fdb38fd4b43c2685658c1a7aa6a56c92ac31585,1,1,"“Therefore, relying on image size to invoke extend API might
result in VIM API fault, if the virtual disk size is same as the target
size”",Bug #1301854 “VMware,"Steps to reproduce:
1) Create a cinder volume with 1GB
2) Upload the volume created in step 1 to image
3) Create a cinder volume from the image created in step 2
The volume gets created correctly but the disk extend is throwing an exception with message  "" A specified parameter was not correct""."
1307878,1307878,Glance,006a4e5e31a12d049c3d6308c3a2d73f57cd980f,0,0,“to avoid problems in the future”,Fix instances of mutable default arguments to func...,"In a few points throughout the codebase, mutable lists and mutable dicts are being used as default function/method arguments.
In Python, this is an issue since functions are treated as objects that can maintain state between calls. As a result, this only gets set once, and it's possible for it to stack list values over time in cases when you might expect them to be empty. Depending on use, this can cause incredibly complex and yet very subtle bugs in code that reads just fine. In Glance's case, since a few instances of this are in several ACL-related methods in glance.store.*, there is *potential* for security concern (not confirmed).
Here's some additional information illustrating and explaining this behavior in Python:
http://effbot.org/zone/default-values.htm
http://stackoverflow.com/questions/1132941/least-astonishment-in-python-the-mutable-default-argument
There are no comments in the code I've seen that indicate this usage is meant specifically to take advantage of this subtlety in the language. We'd definitely want to document that if it is the case.
Wanted to create this as a discussion point if needed, and as a courtesy to attach it to the patch I'm going to push in a few minutes. The full test suites seem to pass locally, so will be curious what Jenkins has to say."
1311182,1311182,Cinder,03a41f863b160384593ef8df130f369a0c22d393,1,1, ,Bug #1311182 “GlusterfsException,"working with gluster backend for cinder, after failure to delete a snapshot on timeout, I reset the snapshot status to try and delete it again, in which time I immediately fail to delete the snapshot with the following error:
2014-04-22 18:25:48.499 2233 DEBUG cinder.openstack.common.processutils [req-ebe13b37-9fb4-496b-8869-cff771b9cd60 97e5450b24624fd78ad6fa6d8a14ef3d c3178ebef2c24d1b9a045bd67483a83c] Running cmd (subprocess): sudo cinder-rootwrap /etc/cind
er/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa execute /usr/lib/python2.6/site-packages/cinder/open
stack/common/processutils.py:142
2014-04-22 18:25:48.507 2233 DEBUG qpid.messaging.io.raw [-] SENT[3e0f710]: '\x0f\x00\x00:\x00\x00\x00\x00\x00\x00\x00\x00\x02\x01\x01\x00\x00(e20240e0-4cca-44eb-8847-cc486fa240fa:357\x0f\x00\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x02\x
07\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00' writeable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:480
2014-04-22 18:25:48.508 2233 DEBUG qpid.messaging.io.raw [-] READ[3e0f710]: '\x0f\x00\x00:\x00\x00\x00\x00\x00\x00\x00\x00\x02\x02\x01\x00\x00(e20240e0-4cca-44eb-8847-cc486fa240fa:357\x0f\x00\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x02\x
07\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2014-04-22 18:25:48.509 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3e0f710]: SessionAttached(name='e20240e0-4cca-44eb-8847-cc486fa240fa:357') write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:25:48.509 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3e0f710]: SessionCommandPoint(command_id=serial(0), command_offset=0) write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:25:48.741 2233 ERROR cinder.openstack.common.rpc.amqp [req-ebe13b37-9fb4-496b-8869-cff771b9cd60 97e5450b24624fd78ad6fa6d8a14ef3d c3178ebef2c24d1b9a045bd67483a83c] Exception during message handling
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 435, in delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     {'status': 'error_deleting'})
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 423, in delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     self.driver.delete_snapshot(snapshot_ref)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/lockutils.py"", line 247, in inner
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 557, in delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     self._delete_snapshot(snapshot)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 621, in _delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     raise exception.GlusterfsException(msg)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp GlusterfsException: No base file found for /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa.
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp
2014-04-22 18:26:03.171 2233 DEBUG qpid.messaging.io.raw [-] READ[3b385a8]: '\x0f\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x01\n\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2014-04-22 18:26:03.172 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3b385a8]: ConnectionHeartbeat() write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:26:04.888 2233 DEBUG qpid.messaging.io.raw [-] READ[3e0f710]: '\x0f\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x01\n\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2014-04-22 18:26:04.888 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3e0f710]: ConnectionHeartbeat() write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:26:07.577 2233 DEBUG cinder.openstack.common.periodic_task [-] Running periodic task VolumeManager._publish_service_capabilities run_periodic_tasks /usr/lib/python2.6/site-packages/cinder/openstack/common/periodic_task.py:176
this is the ls output showing that we see the mount:
[root@hostXX ~(keystone_admin)]# ls -l /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa
-rw-r--r--. 1 qemu qemu 10739843072 Apr 22 18:21 /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa
here is the snapshot in the storage:
[root@storage Dafna]# ls -l |grep c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d
-rw-r--r-- 2 qemu qemu 10739843072 Apr 22 18:21 volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa"
1314008,1314008,Nova,fdce41ac4f8de00769fbe6ba32265e35266df621,1,1,"The BFC ESTA MAL “In _report_final_resource_view() method, Nova will never record
""free pci"" in log message because 'pci_devices' is not in dict ‘resources'.""","Bug #1314008 """"free pci” in log message  never record” ","In _report_final_resource_view() method
 if 'pci_devices' in resources:
            LOG.audit(_(""Free PCI devices: %s"") % resources['pci_devices'])
but  in update_available_resource() method
if self.pci_tracker:
            self.pci_tracker.clean_usage(instances, migrations, orphans)
            resources['pci_stats'] = jsonutils.dumps(self.pci_tracker.stats)
        else:
            resources['pci_stats'] = jsonutils.dumps([])
resources has key ""pci_stats"" not  ""pci_devices""
https://review.openstack.org/#/c/90671/"
1315195,1315195,Cinder,541cc9d53ac999535edcb7a02d1a76a883cfb02b,0,0,"""In order to eliminate the need to have the hp3parclient in the global-requirements""",remove hp3parclient requirement from unit tests,"In order to eliminate the need to have the hp3parclient in the global-requirements project, we need to remove the hp3parclient from being imported in all 3par driver unit tests in cinder.
It should _at least_ be optional for the tests."
1315237,1315237,Glance,c9a034c838e2c5a1f636656ead9aa3b6f7d06603,1,1, ,Glance should not call configure and configure_add...,"Currently at startup Glance is first creating the stores and then verifying the default store (cmd/api.py)
 30         glance.store.create_stores()
 31         glance.store.verify_default_store()
In both of these calls, the store objects are created (store/base.py) which means that if the methods configure() and configure_add() methods are defined in the store, these methods will be called two times: 1 time in the ""create_stores()"" phase and 1 time in the verify_default_store() phase.
It turns out that the configure()/configure_add() method can contain remote calls (which can potentially be expensive): in which case we do not want them to happen two times. Having configure/configure_add called only one time will speed up the Glance startup time for some of the stores."
1315430,1315430,Neutron,fbc02fd569983122a9f5284a71f3c3aaaa03d260,1,1, ,Bug #1315430 “dhcp_lease_duration of 'infinite' is not configura... ,"In the dhcp-agent, it is not possible to set a dhcp_lease_duration of 'infinite' since the variable is set as an IntOpt in neutron/common/config.py.  There are instances, however, where an infinite lease duration may be desirable (particularly in a tripleo installation where the seed may go away but the undercloud and overcloud are expected to persist).  Additionally, dnsmasq itself supports the 'infinite' value for the lease-time in the dhcp-range configuration."
1316916,1316916,Nova,fdce41ac4f8de00769fbe6ba32265e35266df621,1,1,"The BFC ESTA MAL. “
we need to avoid this kind of audit logs and give more accurate info”",resource tracker report negative value for memory ...,"2014-05-05 10:51:33.732 4992 AUDIT nova.compute.resource_tracker [-] Free
ram (MB): -1559
2014-05-05 10:51:33.732 4992 AUDIT nova.compute.resource_tracker [-] Free
disk (GB): 29
2014-05-05 10:51:33.732 4992 AUDIT nova.compute.resource_tracker [-] Free
VCPUS: -3
was showed in my compute node logs which make me confusing
we need to avoid this kind of audit logs and give more accurate info
discussions here:
http://lists.openstack.org/pipermail/openstack-dev/2014-May/034312.html"
1317847,1317847,Glance,2e94076ca43ee3f31b1fc7f46b4c137d36bcd7db,1,1,"It depends when the translation policy enters, after the code or before3 Since I do not have the info I would say it is a BIC",debug level logs should not be translated,"According to the OpenStack translation policy, available at https://wiki.openstack.org/wiki/LoggingStandards, debug messages should not be translated. Like mentioned in several changes in Nova by garyk this is to help prioritize log translation."
1318333,1318333,Swift,872420efdb8e6e945cd2fe06994136b8c2ee153a,1,0,Depend when they decided the logging standards. But I think this is caused by the evolution,debug level logs should not be translated,"According to the OpenStack translation policy, available at https://wiki.openstack.org/wiki/LoggingStandards, debug messages should not be translated. Like mentioned in several changes in Nova by garyk this is to help prioritize log translation."
1327000,1327000,Neutron,fb9886b903434321e62373cb4c11ba014921e4df,1,1, ,Should not schedule dhcp-agent when dhcp port crea...,"Intended operation:
---
1. create network and subnet
  neutron net-create net
  neutron subnet-create --name sub net 20.0.0.0/24
2. create port for dhcp server. It is intended to assign specific ip address.
 neutron port-create --name dhcp --device-id reserved_dhcp_port --fixed-ip ip_address=20.0.0.10,subnet_id=sub net -- --device_owner network:dhcp
3. then schedule dhcp-agent manually
 neutron dhcp-agent-network-add 275f7a3f-0251-485c-aea1-9913e173dd1e net
---
But now force scheduling occurs at port creation (2.). So it is not available to schedule manually (3.)."
1327473,1327473,Nova,0bea84ac20fe498bd08f7212a0017196c8cb0812,0,0,I don't think this is describing a bug …. It is avoiding a wrong practise in Python,Don't use mutables as default args,"Passing mutable objects as default args is a known Python pitfall.
We'd better avoid this.
This is an  example show the pitfall:
http://docs.python-guide.org/en/latest/writing/gotchas/"
1329093,1329093,Swift,7c1c8c0b4d36ec1795c11950652fa42564352264,1,1, ,ssync doesn't sync X-Static-Large-Object,"if you upload a static large object:
https://gist.github.com/clayg/ae43968b46022ef79235
Then delete some of the .data files and run ssync (object-replicator)
Some of the newly replicated .data files don't have the X-Static-Large-Object metadata set (maybe some other metadata as well?)"
1332103,1332103,Glance,fe172d61145ce2cca388f880806b7cf3a6cae6c5,1,1,Im not sure if this bug was caused because of something changed in V2,Glance v2 api can not set or delete a property of ...,"Glance v2 api can not set or delete a property of a image when the value of the property is an empty string.
__delitem__ and __setitem__ methods of ExtraPropertiesProxy class in glance/api/property_protections.py may have some problems. When the value of a property is an empty string, the methods will raise KeyError."
1332482,1332482,Cinder,b07c1a759c8a0cdaaaaaf3e6f1ad7402bf631a44,1,1,"""This change fix those missing parameters""",Bug #1332482 “VMware,"The storage profile in the volume type is ignored while creating a volume from stream-optimized image. Even though the backing is placed in a compliant datastore, the profile is not associated with the backing."
1333219,1333219,Nova,d0225509bdb7882324726312f4a4a9f63375203e,1,0,There is a partial bug and a bug-close differents which one3 The one that is in this table is not marked as a fix commit in Launchpad but it is the bug-close according to the Label.,Virt driver impls don't match ComputeDriver base c...,"There are a number of problems where the virt driver impls do not match the API defined by the base ComputeDriver class.
For example
 - Libvirt:  Adds 'SOFT' as default value for 'reboot' method but no other class does
 - XenAPI: set_admin_passwd takes 2 parameters but base class defines it with 3 parameters in a different order
 - VMWare: update_host_status method which doesn't exist in base class & is never called in entire codebase
 - All: names of parameters are not the same as names of parameters in the base class
 - ...more...
These inconsistencies are functional bugs in the worst, or misleading to maintainers in the best case. It should be possible to write a test using the python 'inspect' module which guarantees that the sub-class APis actually match what they claim to implement from the base class."
1334711,1334711,Glance,1f965cac884165ce11c7c81bfdb982e7735e11e9,0,0,“This change adds the equivalent documentation for the registry.”,Document workers parameter for the registry,"The workers parameter applies to the registry as well as the api, ie adding workers=N to glance-registry.conf will increase the number of registry processes."
1336168,1336168,Glance,5148c9648fc959c1d807313176afe3fcf84b89cf,1,1, ,Chunksize should be configurable when using copy_f...,"When using the copy_from option, readers and writers can have different
speeds to respectively read and write.
A reader timeout will happen if the writer is slow and the writer is
being asked to write a lot. This is currently happening when using
the VMware store and copying from an HTTP server. The reader is reading
16MB which takes too long to upload to vCenter which is causing a
timeout from the HTTP server. The writer should be able to control the
size of the chunks being read when using copy_from: this way the writer
will write fast enough to not make the reader timeout.
This can be reproduced with the filesystem store and copying from http by adding sleep of 30/40 seconds in
https://github.com/openstack/glance/blob/master/glance/store/filesystem.py#L433 between each chunk read.
The chunk will become smaller and then zero.
This is simulating the effect of a slow writer."
1339401,1339401,Neutron,fbf0e621399027d5004bbc8c888d67a3c2851686,1,1,logic is now obsolete and can be removed’,Bug #1339401 “NSX,"since commit b50e66f the router interfaces will not be automatically delete anymore when a network is deleted.
Instead a 409 response code will be returned.
The NSX plugin still has logic to ensure correct backend state in case router interfaces are present on network_delete. This logic is useless and can be removed.
Also it performs some unnecessary queries, so it affects also plugin performance."
1340002,1340002,Glance,d6286b3fd2f9535c023fd50e1825111c9a85b17f,1,0,"""Some v2 exceptions raise unicodeError""",Some exceptions raise unicodeError,"Thie bug is a generic case for bug 1339775 [0], there are a lot same cases in codebase which are outside of v2 stuff, but the result is smimilar, UnicodeError exception will be raised.
[0] https://bugs.launchpad.net/glance/+bug/1339775"
1341144,1341144,Neutron,59da928e945ec58836d34fd561d30a8a446e2728,1,1,"""Change https://review.openstack.org/105411 introduced the following incorrect server_default value:""",Wrong server_default value for multicast_ip_index ...,"Change https://review.openstack.org/105411 introduced the following incorrect server_default value:
    multicast_ip_index = sa.Column(sa.Integer, default=0,
                                   server_default=sql.false())
in neutron/plugins/cisco/db/n1kv_models_v2.py for table cisco_network_profiles."
1341518,1341518,Neutron,95b3dd1ea114b1460c9d18b8a74bbc44ccf4b066,1,1,"""Migration 5446f2a45467_set_server_default try to set incorrect default boolean value on Integer column cisco_network_profiles.""",Incorrect default value on integer column,"Migration 5446f2a45467_set_server_default try to set incorrect default boolean value on Integer column cisco_network_profiles.
http://paste.openstack.org/show/86383/"
1341752,1341752,Cinder,e8273cdb3f4a838aad78f298658f7685bc90d02f,0,0,"I think the error is on the test, the source code of the software is not buggy",No handlers could be found for logger,"When I do run_tests.sh in cinder I get the following error at the end of the run.
vagrant@devstack:/opt/stack/cinder$  ./run_tests.sh
 [SNIP]
Slowest 10 tests took 11.54 secs:
cinder.tests.api.contrib.test_volume_actions.VolumeRetypeActionsTest
    test_update_readonly_flag                                             1.12
cinder.tests.test_storwize_svc.StorwizeSVCDriverTestCase
    test_storwize_svc_host_maps                                           1.39
    test_storwize_svc_multi_host_maps                                     1.11
    test_storwize_svc_retype_need_copy                                    1.09
    test_storwize_svc_retype_only_change_iogrp                            1.09
    test_storwize_svc_snapshots                                           1.13
    test_storwize_svc_volume_migrate                                      1.13
    test_storwize_svc_volume_params                                       1.15
    test_storwize_terminate_connection                                    1.10
    test_storwize_vdisk_copy_ops                                          1.23
Ran 3083 tests in 148.093s
OK
No handlers could be found for logger ""cinder.volume.drivers.san.hp.hp_lefthand_rest_proxy""
vagrant@devstack:/opt/stack/cinder$"
1343544,1343544,Cinder,4499b62738e5ad25838eae0398f0d15bf5132387,1,0,"""It looks like a limitation with logging in python 2.6.""",Bug #1343544 “AttributeError,"Running tempest against juno code on RHEL 6.5 (python 2.6), I'm seeing this in the cinder-volume logs:
I'm seeing several warnings from the taskflow code in the cinder volume log:
2014-07-17 02:36:27.703 57746 WARNING taskflow.utils.misc [req-b459108b-be18-4922-94b9-1f0281764bfb 4deacb6f0bb7409cb1bfab8d7080e61f 574d6c38c2fd4ce697071225ecdf2125 - - -] Failure calling callback <bound method DynamicLogListener._task_receiver of <cinder.flow_utils.DynamicLogListener object at 0x9997d4e0>> to notify about event SUCCESS, details: {'task_uuid': '6ef1507b-7749-40ab-8d5e-55bbf9347229', 'result': None, 'task_name': 'cinder.volume.flows.manager.create_volume.CreateVolumeOnFinishTask;volume:create, create.end'}
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc Traceback (most recent call last):
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc   File ""/usr/lib/python2.6/site-packages/taskflow/utils/misc.py"", line 596, in notify
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc     callback(event_type, *args, **kwargs)
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc   File ""/usr/lib/python2.6/site-packages/cinder/flow_utils.py"", line 104, in _task_receiver
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc     if (self._logger.isEnabledFor(base_logging.DEBUG) or
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc AttributeError: ContextAdapter instance has no attribute 'isEnabledFor'
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc
This is basically the same issue as keystone bug 1213284.
It looks like a limitation with logging in python 2.6.
We could just add isEnabledFor to the oslo log ContextAdapter in py26 for now to delegate to logger.isEnabledFor or punt (return False) until this is all moving to the server projects with oslo.log:
https://blueprints.launchpad.net/oslo/+spec/remove-context-adapter"
1346637,1346637,Nova,1deb31f85a8f5d1e261b2cf1eddc537a5da7f60b,1,0,"""The ESX driver was deprecated in Icehouse and should be removed in Juno""",Bug #1346637 “VMware,The ESX driver was deprecated in Icehouse and should be removed in Juno. This bug is for the removal of the ESX virt driver in nova.
1348624,1348624,Nova,fc69f038bb3dca554475bbdd6844996d2d07a23e,1,0,"“Unfortunately the Xen hypervisor uses a architecture name of 'x86_32' for i686 platforms which means it won't match the standard OS ‘uname'""",XenAPI driver uses a bogus architecture type for i...,The XenAPI driver simply parses the Xen hypervisor capabilities to report the architecture type in the supported instances list. Unfortunately the Xen hypervisor uses a architecture name of 'x86_32' for i686 platforms which means it won't match the standard OS 'uname' reported architecture used by other drivers.
1348869,1348869,Swift,f5caac43ac40e103f3f2156eedb313385dbb6ba8,1,0,“The tempfile.mktemp() function has been deprecated since Python 2.3 due to security issues.”,Avoid usage of insecure mktemp() function,"The tempfile.mktemp() function has been deprecated since Python 2.3 due to security issues.  There are more secure alternatives available, such as tempfile.TemporaryFile().  There are more details on this in the Python tempfile documentation.
Swift is using tempfile.mktemp() in a few locations in the profiling middleware:
  https://github.com/openstack/swift/blob/master/swift/common/middleware/x_profile/html_viewer.py
  https://github.com/openstack/swift/blob/master/swift/common/middleware/x_profile/profile_model.py
These should be modified to use a secure method of temporary file creation for security hardening reasons."
1349452,1349452,Nova,58d48ded2bbb90b8639a31b47e37e97c276eac87,1,1,"""Looks like it got changed a while back."", ""It looks like one or both of these changes is causing the issue""",apparent deadlock on lock_bridge in n-cpu,"It's not clear if n-cpu is dying trying to acquire the lock ""lock_bridge"" or if it's just hanging.
http://logs.openstack.org/08/109108/1/check/check-tempest-dsvm-full/4417111/logs/screen-n-cpu.txt.gz
The logs for n-cpu stop about 15 minutes before the rest of the test run, and all tests doing things that require the hypervisor executed after that point fail with different errors."
1349805,1349805,Cinder,5ee167c14c9978409db059054a02635de35a2e93,1,0,use the newly updated snapshot-ref object,snapshot has wrong status in notification-info whe...,", Snapshot's status have update to  'available' in database when snapshot created successfully, but in 'notification-info'  it still being 'creating'.
2014-07-28 10:42:25.711 3580 INFO cinder.volume.manager [req-3b3aadbc-eb00-4695-a24a-8ad8a8fa5194 ce5e930bf5a44167b149a2d5fd2302e6 7dfd3b6a98664f7cb78808f57b7984da - - -] snapshot f2264c3b-bb46-41fc-b59f-978a222d3ded: created successfully
2014-07-28 10:42:25.712 3580 INFO oslo.messaging._drivers.impl_zmq [req-3b3aadbc-eb00-4695-a24a-8ad8a8fa5194 ce5e930bf5a44167b149a2d5fd2302e6 7dfd3b6a98664f7cb78808f57b7984da - - -] 'notifications-info' {'event_type': 'snapshot.create.end',
 'message_id': '69eece40-3d3f-4d65-a239-4ba88d0a8ff5',
 'payload': {'availability_zone': u'nova',
             'created_at': '2014-07-28 02:42:25',
             'deleted': '',
             'display_name': None,
             'snapshot_id': u'f2264c3b-bb46-41fc-b59f-978a222d3ded',
             'status': u'creating',
             'tenant_id': u'7dfd3b6a98664f7cb78808f57b7984da',
             'user_id': u'ce5e930bf5a44167b149a2d5fd2302e6',
             'volume_id': u'd555f2ec-bc78-4f9b-a732-77c67b1fda3a',
             'volume_size': 1},
 'priority': 'INFO',
 'publisher_id': 'snapshot.controller1',
 'request_id': u'req-3b3aadbc-eb00-4695-a24a-8ad8a8fa5194',
 'timestamp': '2014-07-28 02:42:25.712147'}"
1352857,1352857,Neutron,fdee7801248b5605112e5e2ce0f06f8a5e9ecc6b,1,1, ,few VMs fail to get ip address in devstack,"I am running Juno int code on multi node devstack environment. I am trying to boot 10 VMs in each different network. If so, few VMs, will fail to boot. If i delete & recreate  a VM in the same network, it will fail to get the ip address.
Here are the steps followed to hit this issue
1. Create a network
2. Create a subnet
3. Create a distributed router. Add the subnet to this DVR
4. Boot a VM.
5. Follow the steps 1-4 for 9 more VMs.
6. In this case, few VMs will not get the ip address.
7. I see that dhcp request is not reaching the NN. Please note that few more VMs hosted on the same CN are getting the ip address.
On both Openstack Controller & Compute node logs does not show any Errors."
1352893,1352893,Neutron,b6e9922364fca4d8d141fbb2f27024f7db79ca9e,1,0,"""On systems where ipv6 module is not loaded in kernel we need """,ipv6 cannot be disabled for ovs agent,"If ipv6 module is not loaded in kernel ip6tables command doesn't work and fails  in openvswitch-agent when processing ports:
2014-08-05 15:20:57.089 3944 ERROR neutron.plugins.openvswitch.agent.ovs_neutron_agent [-] Error while processing VIF ports
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1262, in rpc_loop
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     ovs_restarted)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1090, in process_network_ports
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     port_info.get('updated', set()))
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/securitygroups_rpc.py"", line 247, in setup_port_filters
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self.prepare_devices_filter(new_devices)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/securitygroups_rpc.py"", line 164, in prepare_devices_filter
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self.firewall.prepare_port_filter(device)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self.gen.next()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/firewall.py"", line 108, in defer_apply
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self.filter_defer_apply_off()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_firewall.py"", line 370, in filter_defer_apply_off
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self.iptables.defer_apply_off()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_manager.py"", line 353, in defer_apply_off
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self._apply()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_manager.py"", line 369, in _apply
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     return self._apply_synchronized()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_manager.py"", line 400, in _apply_synchronized
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     root_helper=self.root_helper)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/utils.py"", line 76, in execute
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     raise RuntimeError(m)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent RuntimeError:
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Command: ['sudo', 'neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip6tables-restore', '-c']
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Exit code: 2
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Stdout: ''
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Stderr: ""ip6tables-restore v1.4.21: ip6tables-restore: unable to initialize table 'filter'\n\nError occurred at line: 2\nTry `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.\n""
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent
2014-08-05 15:20:58.261 3944 INFO neutron.plugins.openvswitch.agent.ovs_neutron_agent [-] Agent out of sync with plugin!
2014-08-05 15:20:58.749 3944 INFO neutron.agent.securitygroups_rpc [-] Preparing filters for devices set([u'5e646c57-0ce4-4705-9281-2cf991cd4135', u'1e0ea538-74a4-429d-97fb-08fbae37ad47'])"
1356368,1356368,Cinder,2981cdbb03c6f1239a58fedb260796667b8154ab,1,1,“https://github.com/openstack/cinder/commit/0505bb268942534ad5d6ecd5e34a4d9b0e7f5c04 appears to have introduced a bug”,Able to show volumes associated with different pro...,"https://github.com/openstack/cinder/commit/0505bb268942534ad5d6ecd5e34a4d9b0e7f5c04 appears to have introduced a bug making it possible to show the details of a volume under another project_id.
$ . ~/devstack/openrc demo demo
$ cinder list
+--------------------------------------+-----------+-------+------+-------------+----------+-------------+
|                  ID                  |   Status  |  Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-------+------+-------------+----------+-------------+
| c99fe35a-47ef-448a-af31-edf7d04cd44a | available | demo1 |  1   | lvmdriver-1 |  false   |             |
+--------------------------------------+-----------+-------+------+-------------+----------+-------------+
$ cinder list --all-tenants 1
+--------------------------------------+----------------------------------+-----------+-------+------+-------------+----------+-------------+
|                  ID                  |            Tenant ID             |   Status  |  Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+----------------------------------+-----------+-------+------+-------------+----------+-------------+
| c99fe35a-47ef-448a-af31-edf7d04cd44a | 807f2a0ef357420e9a70ac1a5fef7a4c | available | demo1 |  1   | lvmdriver-1 |  false   |             |
+--------------------------------------+----------------------------------+-----------+-------+------+-------------+----------+-------------+
$ cinder show ad3194a3-8304-4d17-8f66-f1a1a261d339
+------------------------------+--------------------------------------+
|           Property           |                Value                 |
+------------------------------+--------------------------------------+
|         attachments          |                  []                  |
|      availability_zone       |                 nova                 |
|           bootable           |                false                 |
|          created_at          |      2014-08-12T11:06:33.000000      |
|         description          |                 None                 |
|          encrypted           |                False                 |
|              id              | ad3194a3-8304-4d17-8f66-f1a1a261d339 |
|           metadata           |                  {}                  |
|             name             |                admin1                |
| os-vol-tenant-attr:tenant_id |   e66effcd5db642dfabceabc76ea78196   |
|             size             |                  1                   |
|         snapshot_id          |                 None                 |
|         source_volid         |                 None                 |
|            status            |              available               |
|           user_id            |   7e540d1944a74bd4a90aa9a8b15d09b0   |
|         volume_type          |             lvmdriver-1              |
+------------------------------+--------------------------------------+"
1358524,1358524,Cinder,826c4520d093f5e0809a08e10b5d18bf69adc698,0,0,“Improve Cinder API internal cache interface”,Improve Cinder API cache implementation,"The current cache implementation found in cinder.api.openstack.wsgi is too simple.
Only a bunch of generic methods are provided:
- cache_resource(self, resource_to_cache, id_attribute='id', name=None)
- cached_resource(self, name=None):
- cached_resource_by_id(self, resource_id, name=None):
As you can see, the name is optional. Current API code never explicitly provides the resource type name. This could be problematic is we try to cache 2 types of resources during the same request.
Furthermore, the documentation provided in cinder.api.openstack.wsgi is wrong:
    Different resources types might need to be cached during the same
    request, they can be cached using the name parameter. For example:
        Controller 1:
            request.cache_resource(db_volumes, 'volumes')
            request.cache_resource(db_volume_types, 'types')
        Controller 2:
            db_volumes = request.cached_resource('volumes')
            db_type_1 = request.cached_resource_by_id('1', 'types')
The second parameter of cache_resource is id_attribute, not name.
To improve the situation, it has been suggested to take the Nova's implementation instead.
The Nova's implementation provides a better interface. Each resource type has a dedicated method which makes the resource type cached explicitly mentioned.
An example of such implementation for Cinder would be:
- cache_db_volumes(volumes) vs cache_resource(volumes, name='volumes')
- cache_db_volume(volume) vs cache_resource(volumes, name='volumes')
- get_db_volumes() vs cached_resource('volumes')
- get_db_volume(id) vs cached_resource_by_id(id, name='volumes')
This interface makes it clear that a volume is added or retrieved from the cache. A side-effect is that code will be shorter.
The proposed implementation will be backward compatible for people with out-of-tree extensions by preserving existing methods and still using the same variable to store the cached resources."
1363899,1363899,Nova,1e54cae3d3f562046a50ab20ff60e2879ce80438,1,1, ,HyperV Vm Console Log issues,"The size of the console log can get bigger than expected because of a small nit when checking the existing log file size as well
as a wrong size constant.
The method which gets the serial port pipe at the moment returns a list which contains at most one element being the actual pipe path. In order to avoid confusion this should return the pipe path or None instead of a list."
1365606,1365606,Nova,fc82c6dbbd0fa1cdc130cefea534967e273d5570,1,1,The BFC ESTA MAL,Network deallocation can fail if a network has bee...,"The call to get_instance_nw_info fails with an error if a network is deleted during the deallocation. Networks are not supposed to be able to be deleted if they have fixed ips in use, but there is a race where a network can be deleted while an allocation is still in process. This is make many times worse by the fact that get_instance_nw_info makes 3*networks + 1 (db + rpc) calls. This should be converted to a) get everything in a single db request, b) handle networks not existing gracefully.
The traceback from get_nw_info failing can look like:
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 242, in deallocate_fixed_ip
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     address, instance=instance)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 931, in deallocate_fixed_ip
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     instance_uuid)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 395, in _do_trigger_security_group_members_refresh_for_instance
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     None, None)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     return func(*args, **kwargs)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 602, in get_instance_nw_info
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     rxtx_factor, host)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 625, in build_network_info_model
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     instance_host)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 701, in _get_subnets_from_network
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     network['project_id'], network['uuid'], vif.uuid)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/nova_ipam_lib.py"", line 46, in get_subnets_by_net_id
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     n = network_obj.Network.get_by_uuid(context.elevated(), net_id)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/objects/base.py"", line 110, in wrapper
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     args, kwargs)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py"", line 425, in object_class_action
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     objver=objver, args=args, kwargs=kwargs)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 150, in call
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     wait_for_reply=True, timeout=timeout)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     timeout=timeout)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 412, in send
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 405, in _send
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     raise result
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher NetworkNotFoundForUUID_Remote: Network could not be found for uuid 3af0a6e6-59f8-4d7e-90ec-b5b866f578f8
or:
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 242, in deallocate_fixed_ip
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     address, instance=instance)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 931, in deallocate_fixed_ip
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     instance_uuid)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 395, in _do_trigger_security_group_members_refresh_for_instance
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     None, None)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     return func(*args, **kwargs)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 598, in get_instance_nw_info
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     network = self._get_network_by_id(context, vif.network_id)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 1445, in _get_network_by_id
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     project_only='allow_none')
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/objects/base.py"", line 110, in wrapper
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     args, kwargs)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py"", line 425, in object_class_action
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     objver=objver, args=args, kwargs=kwargs)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 150, in call
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     wait_for_reply=True, timeout=timeout)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     timeout=timeout)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 412, in send
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 405, in _send
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     raise result
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher NetworkNotFound_Remote: Network 23 could not be found.
depending on where in the call stack the race occurs."
1365751,1365751,Nova,45553a6dda84ef6be38150071f8bb6fa9850a53e,0,0,"The only file that is not a test and has been changed was the hacking file ""Add a new HACKING rule for nova to prevent assert_called_once()
usage from creeping in""",Use of assert_called_once() instead of assert_call...,"mock.assert_called_once() is a noop, it doesn't test anything.
Instead it should be mock.assert_called_once_with()
This occurs in the following places:
  Nova
    nova/tests/virt/hyperv/test_ioutils.py
    nova/tests/virt/libvirt/test_driver.py
  Cliff
    cliff/tests/test_app.py
  Neutron
    neutron/tests/unit/services/l3_router/test_l3_apic_plugin.py
    neutron/tests/unit/services/loadbalancer/drivers/radware/test_plugin_driver.py
    neutron/tests/unit/test_l3_agent.py
    neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_sync.py
    neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_mechanism_driver.py"
1366205,1366205,Cinder,764c5ec749821d36bb0215dc6002d3caea95d3b1,1,1, ,Delete consistency group failed,"In recent testing, delete CG failed because it compares host@backend with host@backend#pool (in delete_consistencygroup in volume/manager.py).  So extract_host needs to be called to fix this problem.
Another issue is about deleting a CG with no host.  This will throw exception when extract_host(group['host']) is called in delete_consistencygroup in volume/rpcapi.py.  The solution is to check the host field in consistencygroup/api.py and delete it from db there."
1367514,1367514,Swift,51fa32f53d81864e9267fc40ce3fcfe494b62437,1,1,When we broke out stats per-policy for some reason we only did two of the three:,Accounts don't report per-policy container count,"Accounts have historically reported three numbers:
                container_count INTEGER,
                object_count INTEGER DEFAULT 0,
                bytes_used INTEGER DEFAULT 0,
When we broke out stats per-policy for some reason we only did two of the three:
            CREATE TABLE policy_stat (
                storage_policy_index INTEGER PRIMARY KEY,
                object_count INTEGER DEFAULT 0,
                bytes_used INTEGER DEFAULT 0
            );
We should track container_count in POLICY_STAT_TRIGGER_SCRIPT"
1367564,1367564,Glance,2e7de07c5a7c8f9d11c00499f7e85ac30f71d025,1,1,“should handle type specific prefix”. Is it a bug3,metadata definition property show should handle ty...,"metadata definition property show should handle type specific prefix
The metadata definitions API supports listing namespaces by resource type.  For example, you can list only namespaces applicable to images by specifying OS::Glance::Image
The API also support showing namespace properties for a specific resource type.  The API will automatically prepend any prefix specific to that resource type.  For example, in the OS::Compute::VirtCPUTopology namespace, the properties will come back with ""hw_"" prepended.
However, if you then ask the API to show the property with the prefix, it will return a ""not found"" error.   To actually see the details of the property, you have to know the base property (without the prefix).  It would be nice if the API would attempt to auto-resolve any automatically prefixed properties when showing a property.
This is evident from the command line.  If you look at the below interactions, you will see the namespaces listed, then limited to a particular resource type, then the properties shown for the namespace, and then a failure to show the property using the automatically prepended prefix.
* Apologize for formatting.
$ glance --os-image-api-version 2 md-namespace-list
+------------------------------------+
| namespace                          |
+------------------------------------+
| OS::Compute::VMware                |
| OS::Compute::XenAPI                |
| OS::Compute::Quota                 |
| OS::Compute::Libvirt               |
| OS::Compute::Hypervisor            |
| OS::Compute::Watchdog              |
| OS::Compute::HostCapabilities      |
| OS::Compute::Trust                 |
| OS::Compute::VirtCPUTopology       |
| OS::Glance:CommonImageProperties   |
| OS::Compute::RandomNumberGenerator |
+------------------------------------+
$ glance --os-image-api-version 2 md-namespace-list --resource-type OS::Glance::Image
+------------------------------+
| namespace                    |
+------------------------------+
| OS::Compute::VMware          |
| OS::Compute::XenAPI          |
| OS::Compute::Libvirt         |
| OS::Compute::Hypervisor      |
| OS::Compute::Watchdog        |
| OS::Compute::VirtCPUTopology |
+------------------------------+
$ glance --os-image-api-version 2 md-namespace-show OS::Compute::VirtCPUTopology --resource-type OS::Glance::Image
+----------------------------+----------------------------------------------------------------------------------+
| Property                   | Value                                                                            |
+----------------------------+----------------------------------------------------------------------------------+
| created_at                 | 2014-09-10T02:55:40Z                                                             |
| description                | This provides the preferred socket/core/thread counts for the virtual CPU        |
|                            | instance exposed to guests. This enables the ability to avoid hitting            |
|                            | limitations on vCPU topologies that OS vendors place on their products. See      |
|                            | also: http://git.openstack.org/cgit/openstack/nova-specs/tree/specs/juno/virt-   |
|                            | driver-vcpu-topology.rst                                                         |
| display_name               | Virtual CPU Topology                                                             |
| namespace                  | OS::Compute::VirtCPUTopology                                                     |
| owner                      | admin                                                                            |
| properties                 | [""hw_cpu_cores"", ""hw_cpu_sockets"", ""hw_cpu_maxsockets"", ""hw_cpu_threads"",        |
|                            | ""hw_cpu_maxcores"", ""hw_cpu_maxthreads""]                                          |
| protected                  | True                                                                             |
| resource_type_associations | [""OS::Glance::Image"", ""OS::Cinder::Volume"", ""OS::Nova::Flavor""]                  |
| schema                     | /v2/schemas/metadefs/namespace                                                   |
| visibility                 | public                                                                           |
+----------------------------+----------------------------------------------------------------------------------+
ttripp@ubuntu:/opt/stack/glance$ glance --os-image-api-version 2 md-property-show OS::Compute::VirtCPUTopology hw_cpu_cores
Request returned failure status 404.
<html>
 <head>
  <title>404 Not Found</title>
 </head>
 <body>
  <h1>404 Not Found</h1>
  Could not find property hw_cpu_cores<br /><br />
 </body>
</html> (HTTP 404)
ttripp@ubuntu:/opt/stack/glance$ glance --os-image-api-version 2 md-property-show OS::Compute::VirtCPUTopology cpu_cores
+-------------+---------------------------------------------------+
| Property    | Value                                             |
+-------------+---------------------------------------------------+
| description | Preferred number of cores to expose to the guest. |
| name        | cpu_cores                                         |
| title       | vCPU Cores                                        |
| type        | integer                                           |
+-------------+---------------------------------------------------+"
1367633,1367633,Nova,42cec80e6ce1c95326ab5871750401f3a8131f77,1,0,"""'createImage' server actions was missed for V2.1 API."". Problema!!!!!! Segun el desallorador alguien se olvido de cambiar el nombre a lo que falla (createImage->create_image) Hay algun commit en mi repo responsible de esto33 El que introduce la v33 Yo lo clasificaria como NoBIC",Server actions 'createImage' does not work for v2....,"'createImage' server action  does not work for V2.1 API.
This needs to be converted to V2.1 from V3 base code.
This needs to be fixed to make V2.1 backward compatible with V2 APIs"
1369516,1369516,Nova,2fc92d9bd7b4e212a3f28e57565a4cf260982f30,0,0,"Bug in test files, but due to evolution.",Convert libvirt driver test suites to use NoDBTest...,"A large number of libvirt test classes inherit from the TestCase class, which means they incur the overhead of database setup
nova/tests/virt/libvirt/test_blockinfo.py:class LibvirtBlockInfoTest(test.TestCase):
nova/tests/virt/libvirt/test_blockinfo.py:class DefaultDeviceNamesTestCase(test.TestCase):
nova/tests/virt/libvirt/test_dmcrypt.py:class LibvirtDmcryptTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class CacheConcurrencyTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtConnTestCase(test.TestCase,
nova/tests/virt/libvirt/test_driver.py:class HostStateTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class IptablesFirewallTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class NWFilterTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtUtilsTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtDriverTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtVolumeUsageTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtNonblockingTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtVolumeSnapshotTestCase(test.TestCase):
nova/tests/virt/libvirt/test_imagebackend.py:class EncryptedLvmTestCase(_ImageTestCase, test.TestCase):
nova/tests/virt/libvirt/test_vif.py:class LibvirtVifTestCase(test.TestCase):
Some of these do not even use the database so can be trivially changed. Others will need significant refactoring work to remove database access before they can be changed to NoDBTestCase"
1369558,1369558,Swift,6354e2da57a8d487caf3605d4005134f584cf935,1,1,Please ensure that the arg=None that are failing were incorrect when they were inserted.,Bug #1369558 “direct_client not passing args between some functi... ,"The call to _get_direct_account_container in direct_get_account
has several of its args =None instead of set to the value passed
to direct_get_account. Similarly it is not passing the timeout args.
The same applies to _get_direct_account_container in
direct_get_container.
The direct_get_container is only called by the account-reaper
and this bug will have limited impact on it. The marker,
maintained in reap_container, is ignored by direct_get_container.
This is not as bad as it sounds, if the account-reaper successfully
deletes the first 10K objects, assuming the container has > 10K
objects, the next call to direct_get_container will in fact return
the next 10K objects even though it sets marker=None (assuming the
first 10K objects were successfully deleted)."
1372862,1372862,Neutron,739b3cac3210e6a1ce942238ca3efa3ddf49cb4d,1,0,"""in neutron code, there still are some codes to enable translation tag for debug level log""",There is no need to enable globalizaion for debug ...,"Currently, in neutron code, there still are some codes to enable translation tag for debug level log."
1374783,1374783,Swift,64aa8062bc96799993deafbdf184adb72c509f20,0,0,Bug in test files,Some statements are evaluated twice in the setUp o...,It should be referenced by the last saved value. Remove the duplicated statements.
